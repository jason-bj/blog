<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>apt-dpkg-command</title>
    <url>/2019/11/25/apt-dpkg-command/</url>
    <content><![CDATA[dpkg vs aptdpkg only installs a package from local which apt-get installs it from local cache which is got from remote repo configure by /etc/apt/source.list, so when you run apt related commands better to run $ apt-get update first to sync local cache with remote repo.
dpkg only installs a package provided it does not solve dependency for the package, let’s say if a package depends on A, while A is not installed, install this package will fail, that’s the big difference for these two tools, so always use apt-get for published package while use dpkg for your own package.


dpkg commands# install the package$ sudo dpkg -i package.deb# if same file(path) in two debs, the later package can&#x27;t be installed successfully if no overwrite$ sudo dpkg -i --force-overwrite package.deb# show all installed packages$ dpkg --get-selections | grep green$ dpkg -l | grep greenOutput:root@dev:~/build# dpkg -l | grep dhcprc  dhcp                      2.5.1                            amd64        dhcp backendii: any package is installed without any errorrc: residual package(removed but config kept)iU: broken# show then content of it$ dpkg -c package.deb# show control file of it$ dpkg -I package.deb# show where the package is installed$ dpkg -L  ibus-gtk# remove deb package$ sudo dpkg -r ibus-gtk# Remove/Delete an installed package except configuration files$ sudo dpkg -P ibus-gtk# Remove/Delete everything including configuration files# repack---create debian package from installed files$ sudo apt-get install dpkg-repack$ dpkg-repack ibus-gtk# extract files from deb to outputdir$ dpkg-deb -x xxx.deb outputdir# extract control files like control/preinst/postinst$ dpkg -e xx.deb# show tar files in deb$ ar tv hello.debrw-r--r-- 0/0      4 Mar 28 23:17 2017 debian-binaryrw-r--r-- 0/0    270 Mar 28 23:17 2017 control.tar.gzrw-r--r-- 0/0   2176 Mar 28 23:17 2017 data.tar.xz# extract a debian$ ar xv hello.debdebian-binary  control.tar.gz data.tar.xz# only extract data file and extract data.tar.xz as well$ dpkg-deb -x hello.deb# check if deb package missed something$ lintian xx.deb# if a file is installed by apt/dpkg, check which package it belongs$ dpkg -S /bin/cat

what happens when you install a deb package from apt&#x2F;dpkg
-i, --install package-file...       Install the package. If --recursive or -R option is specified, package-file must refer to a directory instead.       Installation consists of the following steps:       1. Extract the control files of the new package.       2. If another version of the same package was installed before the new installation, execute prerm script of the old package.       3. Run preinst script, if provided by the package.       4. Unpack the new files, and at the same time back up the old files, so that if something goes wrong, they can be restored.       5.  If another version of the same package was installed before the new installation, execute the postrm script of the old package. Note that this script is executed after the preinst script of the new package, because new files are written at the same time old files are removed.       6. Configure the package. See --configure for detailed information about how this is done.--unpack package-file...       Unpack the package, but don not configure it. If --recursive or -R option is specified, package-file must refer to a directory instead.--configure package...|-a|--pending       Configure a package which has been unpacked but not yet configured.  If -a or --pending is given instead of package, all unpacked but unconfigured packages are configured.       To reconfigure a package which has already been configured, try the dpkg-reconfigure(8) command instead.       Configuring consists of the following steps:       1. Unpack the conf files, and at the same time back up the old conf files, so that they can be restored if something goes wrong.       2. Run postinst script, if provided by the package.

The control files(post install, preinstall etc)  are at /var/lib/dpkg/info
debug dpkg when install fails
Setting up gconf2 (2.28.1-6) ...dpkg: error processing gconf2 (--configure):subprocess installed post-installation script returned error exit status 247check why$ dpkg -i --debug=7337 xx.deb$ dpkg --configure -D 777 gconf2# OR run the post script with shell debug$ sh -x /var/lib/dpkg/info/gconf2.postinstpostinst configure 2.28.1-6# if there is no script at /var/lib/dpkg/info, extract it$ dpkg -e  xx.deb$ sh -x DEBIAN/postinst configure 2.28.1-6

apt commands# update local cache for apt$ sudo apt-get update$ sudo apt-get remove/install packageName# remove not delete cfg file, but purge did$ sudo apt-get purge packageName# remove files with pattern matches$ sudo apt-get remove &quot;green-*&quot;# only download package to /var/cache/apt/archives$ sudo apt-get install -d packageName# search package from local cache$ sudo apt-cache search *# show runtime depends directly$ sudo apt-cache depends common-dev# show runtime depends recursively$ sudo apt install apt-rdepends$ sudo apt-rdepends  common-dev# show who depends on me$ sudo apt-rdepends -r  xxx# fix pending issue that apt-get knows with -f option$ sudo apt-get install -f]]></content>
      <categories>
        <category>linux</category>
        <category>dpkg</category>
      </categories>
      <tags>
        <tag>dpkg</tag>
        <tag>apt</tag>
      </tags>
  </entry>
  <entry>
    <title>bash-basic</title>
    <url>/2019/12/23/bash-basic/</url>
    <content><![CDATA[BashIntroductionAs bash is old or native way to run commands, there are lots of scripts written with bash, so it’s a must to know bash and do some basic work.
Better use python instead of bash as you can call bash in python and use python powerful features
printingThere are several ways for printing and writing multiple lines, echo is the easy way to do
# print multiple lines$ echo -e &quot;a\nb&quot;$ echo &quot;ab&quot;# write multiple lines to a file$ echo -e &quot;a\nb&quot; &gt;file$ echo &quot;ab&quot; &gt;file$ cat &lt;&lt;END &gt;fileabEND


%%bashecho &quot;a b&quot;# printing in multiple linesecho &quot;ab&quot;# write in one line, printing in multiple linesecho -e &quot;a\nb&quot;

a b
a
b
a
b

special charactersSpecial characters here mean shell sees them special but not treated as regular pattern, if shell treats them as pattern, its meaning may be different. see an example.
$lsa.c abd.c abc.c$ grep &quot;ab*&quot; ab*abd.c:`a`abd.c:`ab`cabd.c:`ab`abc.c:`ab`babc.c:`ab`d
grep &quot;ab*&quot; ab* search pattern ab*(a, ab matched, abc not match) from file which starts with ab(abd.c, abc.c all match)
whitespace The most special char in shell is whitespace, whitespace is used as separator for parameters, like function parameter, test parameter etc.
; semi-colon is used to separate instructions if at same line, if each instruction at each line, ; can be ignored. $ls;date
* any more(0-)characters    #ls ab*—&gt;file ab matches
? any character, just one    #ls ab?  —&gt;file ab not match!!
[] any character inside it becomes normal except \ [] ! #ls test.[ch]
- indicates different meanings depends on its location[a-z] – linker[-az] – normal[az-] - normal! must be a start to negate the condition, can&#x27;t be other place[!a-z][\!a-z][a!z] error[a\!z] ok
| command pipe, #ls ab* | xargs cat
() group command, run command in subshell, #msg=$(echo hello)
&amp;  run in background
‘’  keep all(characters) as literal inside, Any char in &#39;&#39; are escaped, echo &#39;I\&#39;m a boy&#39; #error
“”  quotes string and evaluates variable  #echo &quot;$var&quot;
most chars in “” (except [$, &#96;, \]) are escaped
\ escape character
&#96;&#96; run command and get its output, #msg&#x3D;`echo hello`
Standard wildcards(globing)Standard wildcards (also known as globing patterns) are used by various command-line utilities to work with multiple files. Standard wildcards are used by nearly any command (including mv, cp, rm and many others). 

? (question mark)

this can represent any single character. If you specified something at the command line like “hd?” GNU&#x2F;Linux would look for hda, hdb, hdc and every other letter&#x2F;number between a-z, 0-9.


* (asterisk)

this can represent any number of characters (including zero, in other words, zero or more characters). If you specified a “cd*” it would use “cda”, “cdrom”, “cdrecord” and anything that starts with “cd” also including “cd” itself. “m*l” could by mill, mull, ml, and anything that starts with an m and ends with an l.


[ ] (square brackets)



specifies a range. If you did m[a,o,u]m it can become: mam, mum, mom if you did: m[a-d]m it can become anything that starts and ends with m and has any character a to d in between. For example, these would work: mam, mbm, mcm, mdm. This kind of wildcard specifies an “or” relationship (you only need one to match).


&#123; &#125; (curly brackets, also called brace)

terms are separated by commas and each term must be the name of something or a wildcard. This wildcard will copy anything that matches either wildcard(s), or exact name(s) (an “or” relationship, one or the other).


[!]

This construct is similar to the [ ] construct, except rather than matching any characters inside the brackets, it&#39;ll match any character, as long as it is not listed between the [ and ]. This is a logical NOT. For example rm myfile[!9] will remove all myfiles* (ie. myfiles1, myfiles2 etc) but won’t remove a file with the number 9 anywhere within it’s name.



variableLike Python, variable has week type(no need to declare it first), shell checks its type only when it runs, like others, shell has local, global, env variable, if no keyword is specified, default is global!
%%bashna=&quot;hi&quot;name=&quot;jason    kk&quot; # must wihout space at both side of =echo $name         # max match, space supressed# &#123;&#125; is needed to when link with others, to ensure where the edge of variable!!!!echo $&#123;na&#125;me # string link, nothing needed.# &quot;&quot; is needed to prevent space supressed.echo &quot;$name&quot; # keep orignal, no space supressed.# &quot;&quot; &#123;&#125; are necessary in some case, but can be ignore in others.echo $name, $&#123;name&#125;, &quot;$&#123;name&#125;&quot;echo &quot;$&#123;#name&#125;&quot; # the length of string variable, count of array $&#123;#name[@]&#125; if name is an array

jason kk
hime
jason    kk
jason kk, jason kk, jason    kk
11

local &#x2F;global varby default, variable is global, but you can only add local keyword in function to strict its scope, local keyword can’t be used outside of a function. but all variables(local global) can be seen in sushell(forked process) as well, but it&#39;s another copy of these variables, changed in subshell not see by parent!!!
special var
$?	Exit status of last task$$	PID of shell process$0	Filename of the shell script


%%bashvar_func() &#123;  local a=12  b=20 # global, can be seen outside of the function  c=$(    let a=a+1    echo $a  ) # run in subshell, still see a, get sushell output by $()  echo &quot;in function c=$c&quot;  echo &quot;in function a=$a&quot; # a in unchanged in parent&#125;# local d=23 error!  local can&#x27;t be used outside of functionvar_funcecho &quot;in main: a=$a&quot; #a is undefined with null value(as a in var_func is local)echo &quot;in main: b=$b&quot;

in function c=13
in function a=12
in main: a=
in main: b=20

env varEnv variable is defined outside of the a script, or you can define it in the script, so that all subshells can have it.
%%bashdeclare env_va=&quot;hello&quot;echo $env_vaecho $PATH # access default env variable

hello
/opt/llvm/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/jaluo/go:/home/jaluo/go/bin:/home/jaluo/.yarn_pkg/bin:/usr/lib64:/usr/local/go/bin:/home/jaluo/.local/bin:/root/.yarn_pkg/bin:/home/jaluo/go/bin:/home/jaluo/go:/usr/local/go/bin

array[pay attention] and mapyou can define an array with one line Fruits=(&#39;Apple&#39; &#39;Banana&#39; &#39;Orange&#39;) or add item at any slot Fruits[1]=&quot;Ab&quot; or remove one element unset Fruits[1], then you can access one item or all items or range items like below
index can be implicit or explicit, implicit starts from 0…n while explicit index can be number or literal!!!
arr[1]=&quot;ab&quot; or arr[KEY1]=&quot;ab&quot; # image it as map
- all items---echo $&#123;Fruits[@]&#125;- one item----echo $&#123;Fruits[0]&#125;- range-------echo $&#123;Fruits[@]:0:2&#125;# Range (from position 0, length 2) this is for number index only# if index is not number actually, image it as map!

Bash associative array implementation uses a hash library and stores hashes of indexes. These hashes are stored in buckets with 128 default number of buckets. The hash is calculated with the function hash_string() using a simple multiplication and a bitwise XOR. The keys of the associative array are listed in the order buckets appear. Bucket number is calculated by a bitwise AND operation between the hash value of the key and the number of buckets decreased by 1.
The order of keys output is sorted using the order of buckets in the hash table they are into
%%bash# Fruits=(&#x27;Apple&#x27; &#x27;Banana&#x27; &#x27;Orange&#x27;)# OR# no need declare array, just use it directly!!!Fruits[0]=&quot;Apple&quot;Fruits[1]=&quot;Banana&quot;Fruits[3]=&quot;Orange&quot;echo $&#123;Fruits[0]&#125;       # Element #0echo &quot;$&#123;Fruits[@]&#125;&quot;     # All elements, space-separatedecho $&#123;#Fruits[@]&#125;      # 3: Number of elements, no count null slotecho $&#123;#Fruits&#125;         # String length of the 1st elementecho $&#123;#Fruits[2]&#125;      # String length of the Nth element, no skip, index 2 is null slotecho &quot;$&#123;Fruits[@]:0:3&#125;&quot; # Range (from position 0, length 3), no count null slotecho &quot;$&#123;Fruits[@]:1&#125;&quot;   # Range (from position 1 to end), no count null slotecho ----Fruits=(&quot;$&#123;Fruits[@]&#125;&quot; &quot;Watermelon&quot;)    # PushFruits+=(&#x27;Watermelon&#x27;)                  # Also Pushecho &quot;$&#123;Fruits[@]&#125;&quot;# Fruits is reset with new indexFruits=($&#123;Fruits[@]/Ap*/&#125;)              # Remove by regex matchecho &quot;$&#123;Fruits[@]&#125;&quot; # index is not change, slot 1 is null!!unset &quot;Fruits[1]&quot;                         # Remove one item# Fruits=(&quot;$&#123;Fruits[@]&#125;&quot;)                 # Duplicate# Fruits=(&quot;$&#123;Fruits[@]&#125;&quot; &quot;$&#123;Veggies[@]&#125;&quot;) # Concatenate#lines=(`cat &quot;logfile&quot;`)                  # Read from fileecho &quot;iterate array entry&quot;for i in &quot;$&#123;Fruits[@]&#125;&quot;; do  # i is array entry itself  echo -n &quot;$i &quot;doneechoecho &quot;iterate array with index&quot;for i in &quot;$&#123;!Fruits[@]&#125;&quot;; do  # i is index  echo -n &quot;$&#123;Fruits[i]&#125; &quot;doneechoecho -------map--------------declare -A mm[a]=&quot;a&quot;m[b]=&quot;b&quot;echo $&#123;!m[@]&#125;declare -A test1=(    [d]=1w45    [e]=2dfg    [m]=3df    [o]=4df)declare -A test2=(    [d1]=1w45    [e2]=2dfg    [m3]=3df    [o4]=4df)declare -A test3=(    [1d]=1w45    [2e]=2dfg    [3m]=3df    [4o]=4df)echo &quot;check the order of key printed&quot;echo $&#123;!test1[@]&#125;echo $&#123;!test2[@]&#125;echo $&#123;!test3[@]&#125;

Apple
Apple Banana Orange
3
5
0
Apple Banana Orange
Banana Orange
----
Apple Banana Orange Watermelon Watermelon
Banana Orange Watermelon Watermelon
iterate array entry
Banana Watermelon Watermelon 
iterate array with index
Banana Watermelon Watermelon 
-------map--------------
b a
check the order of key printed
o m e d
d1 e2 m3 o4
1d 3m 2e 4o

functionUnsupported declare function parameter like function(a,b), use $1, $2 etc
parameter$#----number of parameters from command line(not count script name)$1----the first parameter# above rule apply function as well$0----script name(no parameter) with path together (basename $0 ,just the script name)
$ ./myspt.sh h1 h2
$#-----2$1-----&quot;hi&quot;$0-----./myspt.sh


#!/bin/bash# the parameters are regarded as one long quoted stringecho &quot;Using \&quot;\$*\&quot;:&quot;for a in &quot;$*&quot;; do    echo $a;done# the string is broken into words by the for loopecho -e &quot;\nUsing \$*:&quot;for a in $*; do    echo $a;done# it treats each element of $@ as a quoted string:# this is mostly what you want!!!echo -e &quot;\nUsing \&quot;\$@\&quot;:&quot;for a in &quot;$@&quot;; do    echo $a;done# it treats each element as an unquoted string, so the last one is again split by what amounts to for three fourecho -e &quot;\nUsing \$@:&quot;for a in $@; do    echo $a;done$ ./myspt.sh h1 h2Using &quot;$*&quot;:one two three fourUsing $*:onetwothreefourUsing &quot;$@&quot;:onetwothree fourUsing $@:onetwothreefour


The implementation of “$“ has always been a problem and realistically should have been replaced with the behavior of “$@”. In almost every case where coders use “$“, they mean “$@”. “$*” can cause bugs and even security holes in your software.

always use “$@” if you want to check all parameters as it’s mostly what you want
definition
no return keyword, output as return
no explicit parameter list
get the return value by command group () or command substitution &#96;&#96;
$() is the suggested way

%%bashmax () &#123;    if (($1 &gt; $2 )); then        echo &quot;$1&quot;    else        echo &quot;$2&quot;    fi&#125;# get function outputecho &quot;max is: $(max 1 2)&quot;let m=`max 1 2`let m=$(max 1 2) # run in subshell

max is: 2

Subshell()(group command) runs in subshell, all others run in main shell like &#123;&#125;, (()).
%%bash(a=12) # subshell&#123;  b=12; # main shell&#125;((c=13)) # main shellecho $a, $b, $c

, 12, 13

substitution[pay attention]command substitution
files=`ls *`
variable substitution
$&#123;var:-word&#125; #if var null or unset, return word, unchanged var$&#123;var:=word&#125; #if var null or unset, return word and set var=word$&#123;var:?word&#125; $if var null or unset, print word, unchanged var$&#123;var:+word&#125; $if var is set, return word, unchanged var
wildcard substitution
$ ls *$ ls [a]*$ ls *.[ch]

# The pattern matching is always greedy!!# FOO is variableFOO=&quot;hello&quot;$&#123;FOO%suffix&#125;	Remove suffix$&#123;FOO#prefix&#125;	Remove prefix # stop until first match$&#123;FOO%%suffix&#125;	Remove long suffix$&#123;FOO##prefix&#125;	Remove long prefix # stop until last match$&#123;FOO/from/to&#125;	Replace first match$&#123;FOO//from/to&#125;	Replace all$&#123;FOO/%from/to&#125;	Replace suffix$&#123;FOO/#from/to&#125;	Replace prefix


%%bashSTR=&quot;/path/to/foo.cpp&quot;echo $&#123;STR%.cpp&#125;    # /path/to/fooecho $&#123;STR%.cpp&#125;.o  # /path/to/foo.oecho ------------echo $&#123;STR##*.&#125;     # cpp (extension)echo $&#123;STR##*/&#125;     # foo.cpp (basepath)echo ------------echo $&#123;STR#*/&#125;      # path/to/foo.cppecho $&#123;STR##*/&#125;     # foo.cppecho ------------echo $&#123;STR/foo/bar&#125; # /path/to/bar.cppecho ------------SRC=&quot;/path/to/foo.cpp&quot;BASE=$&#123;SRC##*/&#125;   #=&gt; &quot;foo.cpp&quot; (basepath)echo $BASEDIR=$&#123;SRC%$BASE&#125;  #=&gt; &quot;/path/to/&quot; (dirpath)echo $DIR

/path/to/foo
/path/to/foo.o
------------
cpp
foo.cpp
------------
path/to/foo.cpp
foo.cpp
------------
/path/to/bar.cpp
------------
foo.cpp
/path/to/

compare or condition[] is an old way, used for string and number, file, here are options for it,
- file  -d 	-c 	-b  -f 	-e(exist)  -r 	-w 	-x(executable) -nt(timestamp newer than) -ot(timestamp older than)- string   != 	= 	-n(non-zero string) -z(zero string)- integer(two characters for each operator!!!)  -eq -ne -lt -le -gt -ge(great than)- logical  -a(and) -o(or) !(negative)

[[]] supports all options as [], As a rule of thumb, 

[[ is used for strings and files.
numbers, use an (())

  = Set Equal to *= Multiply /= divide %= Modulo += Add -= Subract&lt;&lt;= bitwise shift left&gt;&gt;= bitwise shift right &amp;= bitwise AND ^= bitwise XOR |= bitwise NOT == Test Equality!= Test Inequality &lt; Less than &gt; Greater than&lt;= Less than or equal&gt;= Greater than or equal

&amp;&amp; and || used for logical
[[ support pattern matching [[ STRING =~ PATTERN ]], it’s regular pattern not globing

Difference:
file=&quot;file name&quot;[[ -f $file ]] &amp;&amp; echo &quot;$file is a regular file&quot;# will work even though $file is not quoted and contains whitespace.# &quot;&quot; is a must as file has whitespace[ -f &quot;$file&quot; ] &amp;&amp; echo &quot;$file is a regular file&quot;#Parentheses() in [[ do not need to be escaped:[[ -f $file1 &amp;&amp; (-d $dir1 || -d $dir2) ]][ -f &quot;$file1&quot; -a \( -d &quot;$dir1&quot; -o -d &quot;$dir2&quot; \) ]

Implicit Conversion
The -eq(integer operators) causes the strings to be interpreted as integers if possible including base conversion
%%bash# implicit conversionif [ &#x27;1&#x27; -eq 1 ]; then  echo &quot;equal&quot;fi# comment it out to see implicit conversion# if [ &#x27;1&#x27; -eq &quot;a&quot; ]; then # bash: line 7: [: a: integer expression expected#    echo &quot;equal&quot;# fi# pattern matching for [[ ]]dig=123if [[ $dig =~ ^[0-9]+$ ]]; then  echo Numericelse  echo Non-numericfi

equal
Numeric

flow controlThere are several ways to do flow control, like if&#x2F;while&#x2F;for, let’s see each of them with example.
%%bashlet a=1if [ $a -gt 2 ]; then  echo &quot;a is greater than 2&quot;elif [ $a -gt 1 ]; then  echo &quot;a is greater than 1&quot;else  echo &quot;a equal 0 or 1&quot;fiwhile [[ $a -gt 0 ]]; do  echo $a  let a=a-1doneVAR=&quot;a b&quot;for i in $VAR # VAR separated by space by defaultdo  echo $idonefor i in &quot;$VAR&quot; # VAR as a wholedo  echo $idone# -----for loop in Linux treats pattern as filename when no files exist---# for file in /tmp/*# do#   echo $file // if there is no file under /tmp, echo will print /tmp/*# done# for file in /tmp/*# do#   if [ -e $file ]; then#     echo $file#   fi# done# for file in `ls`# for .. in expanded_variable for file in $(ls); do  echo $filedone# (())for ((i = 0; i &lt; 5; i++)); do  echo $idonea=1# case for shell pattern matchingcase $a in  &quot;1&quot; | &quot;2&quot;) echo &quot;\$a matches pattern&quot; ;;     # ;; is a must like break  &quot;12*&quot;) echo &quot;\$a matches another pattern&quot; ;; # with ;; outside of case, as ;; is a break!esac

a equal 0 or 1
1
a
b
a b
bash-basic.ipynb
0
1
2
3
4
$a matches pattern

signalSet signal handler with trap command
trap quit 2 3 9 #signal handlerquit()&#123;  echo &quot;You can&#x27;t kill me!!!!!!!!!!!!!!!!&quot;&#125;

string(slice of string)[pay attention]%%bash# $&#123;parameter:offset&#125;# $&#123;parameter:offset:length&#125;# string from index 0 !!!name=&quot;John&quot;# [1-2]echo $&#123;name:1:2&#125; #=&gt; &quot;oh&quot; (slicing)# [0-2)echo $&#123;name::2&#125; #=&gt; &quot;Jo&quot; (slicing)# [0-end)echo $&#123;name::-1&#125; #=&gt; &quot;Joh&quot; (slicing)# [-3-end] (end is -1, must add a space before negative number)echo $&#123;name: -3&#125; #=&gt; &quot;n&quot; (slicing from right)# count two from index -3echo $&#123;name: -3:2&#125; #=&gt; &quot;n&quot; (slicing from right)# convert a string to array with seperator,s=&quot;a,b&quot;items=($&#123;s//,/ &#125;)echo &quot;$&#123;items[@]&#125;&quot;echo &quot;$&#123;items[0]&#125;&quot;echo &quot;$&#123;#items[@]&#125;&quot;

oh
Jo
Joh
ohn
oh
a b
a
2

tipsdebug shellcheck syntax
$bash -n test.sh just check syntax of test.sh
debug shell
set -x #enable tracing will display all commands and their arguments as they execute.if [ -z &quot;$1&quot; ] ; then    echo &quot;ERROR: Insufficient Args.&quot;    exit 1fiset +x #disable tracing

redirect
stdin——————-0
stdout——————1
stderr——————2

&gt; just redirects the stdout to file without stderr.
$ls no_file.txt &gt;log.t 2&gt;&amp;1
cd vs pushd&#x2F;popdWith cd you need to write the path explicitly, while pushd/popd, you don&#39;t need remember the original path.
#pushd /home/lzq (save the current dir on stack and go to /home/lzq)#popd(return to the saved dir)

() vs (()) vs let vs expr
() is used for command groups with subshell, you can run any command in subshell
(()) and let are just for integer operation, not in subshell
expr is only for integer operation as well as expr is a command, so $ is a must for variable!!!
$[] is for integer operation as well

NOTE: bash only supports integer operation when do integer operation $ can be omitted for variable
%%bash(  echo hello  var1=&quot;subshell&quot;)echo $var1 #never see var1echo $(  echo hello  var1=&quot;subshell&quot;) # get output from subshell!!!# (()) only for integer, $ can be omitted.((var2 = &quot;ab&quot;))echo $var2 #integer value 0!!!((var2 = 10))echo $var2# letlet var2=10echo $var2let var2=&quot;ab&quot;echo $var2a=&quot;1&quot;b=&quot;2&quot;if [[ &quot;$a&quot; = &quot;$b&quot; ]]; then  echo &quot;equal&quot;fi# must add space for each operator!!!expr $RANDOM % 10b=`expr $RANDOM % 10 + 3`echo $becho `expr 2 + 3`echo &#x27;============$(())=========&#x27;# the last value is used as `return` valueecho $((2+3))echo $((2+3,4+5,a=15))echo &#x27;============$[]=========&#x27;echo $[2+3]echo $[2+3,4+5]echo $[a=15,5+5]echo $a

hello

hello
0
10
10
0
5
4
5
============$(())=========
5
15
============$[]=========
5
9
10
15

[] vs [[]][ (&quot;test&quot; command) and [[ (&quot;new test&quot; command) are used to evaluate expressions. [[ works only in Bash, Zsh and the Korn shell, and is more powerful; [ and test are available in POSIX shells.
difference
brace expansionUsed to generate list of string or number with prefix or suffix for each item, prefix or suffix is optional.
&#123;&#125; linked each item with prefix and suffix if has, print it as a single stringuse case

_&#123;a..f&#125;_
&#123;a,b,c&#125;

%%bashecho &#123;a,b&#125; # no space between each item echo &#123;a, b&#125; will get unexpected!!!echo _&#123;a, b&#125;_ # see the result# if you want to use space between item, escape itecho _&#123;a,\ b&#125;_ # see the resultecho not_prefix prefix_&#123;a,b&#125;_suffix not_suffix # see which part is prefix and suffixecho &#123;a..f&#125;echo 1.&#123;1..10&#125;for i in &#123;1..5..2&#125;; do # with step 2 default 1  echo $idone# print each item(item seperated by whitespace) on a new lineprintf &quot;%s\n&quot; &#123;item1,item2&#125;

a b
_&#123;a, b&#125;_
_a_ _ b_
not_prefix prefix_a_suffix prefix_b_suffix not_suffix
a b c d e f
1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 1.10
1
3
5
item1
item2

read a fileyou can read a whole file at one time or read it line by line
# read whole into memory at oncefor line in $(cat file.txt); do  echo $linedone# read line by line# read is keyword !!!while read line; do  echo $linedone &lt; file.txt

dict in shell [pay attention]In order to use dict, you must declare it first like this 

declare -A sounds

%%bash# dict requires declare first!!!declare -A soundssounds[dog]=&quot;bark&quot; # same: sounds[&#x27;dog&#x27;]=&quot;bark&quot; &#x27;&#x27; is only needed when has space in keysounds[cow]=&quot;moo&quot;sounds[bird]=&quot;tweet&quot;sounds[wolf]=&quot;howl&quot;echo $&#123;sounds[dog]&#125;  # same: echo $&#123;sounds[&#x27;dog&#x27;]&#125;echo &quot;$&#123;sounds[@]&#125;&quot;  # All valuesecho &quot;$&#123;!sounds[@]&#125;&quot; # All keysecho $&#123;#sounds[@]&#125;   # Number of elementsfor key in &quot;$&#123;!sounds[@]&#125;&quot;; do  echo $keydonefor val in &quot;$&#123;sounds[@]&#125;&quot;; do  echo $valdoneunset sounds[dog] # Delete dog

bark
tweet bark moo howl
bird dog cow wolf
4
bird
dog
cow
wolf
tweet
bark
moo
howl

difference with or without “” for variable expansionIn most shells, leaving a variable expansion unquoted is like invoking some sort of implicit split+glob operator.
$varIn another language would be written something like:glob(split($var))
$var is first split into a list of words according to complex rules involving the $IFS special parameter (the split part) and then each word resulting of that splitting is considered as a pattern which is expanded.
%%bashwds=&quot;hello boy&quot;for wd in $wdsdoecho $wddonefor wd in &quot;$wds&quot;doecho $wddoneo=$(ls -l)for lt in $odoecho $ltdonefor lt in &quot;$o&quot;doecho &quot;count&quot;echo $ltdone

hello
boy
hello boy
total
44
-rw-r--r--
1
jaluo
jaluo
44680
May
12
09:31
bash-basic.ipynb
count
total 44 -rw-r--r-- 1 jaluo jaluo 44680 May 12 09:31 bash-basic.ipynb

IFSIFS: The Internal Field Separator that is used for word splitting after expansion and to split lines into words with the read builtin command.  The default value is &lt;space&gt;&lt;tab&gt;&lt;newline&gt;.
%%bash# defautl IFSstr=&quot;hello boy&quot;for elm in $strdoecho $elmdoneIFS=&quot;,&quot;str=&quot;hello,boy&quot;for elm in $strdoecho $elmdone

hello
boy
hello
boy

include fileUse source or .
source /path/s.sh. /path/s.sh#. (dot) command is an alias to source

random number$RANDOM is the random number
%%bashecho $RANDOMecho $RANDOM

31673
15112

do another operation if first command runs ok%%bash# one wayif ls /tmp &gt;/dev/null;then  echo &quot;ls runs ok&quot;else  echo &quot;ls runs badly&quot;fiif ls /not_exist &gt;/dev/null 2&gt;&amp;1;then  echo &quot;ls runs ok&quot;else  echo &quot;ls runs badly&quot;fi# this way only support ok, no more controlls /tmp&gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo &quot;ls runs ok&quot;# same as the first wayls /tmp&gt;/dev/null 2&gt;&amp;1if [ $? -eq 0 ];then  echo &quot;ls runs ok&quot;else  echo &quot;ls runs badly&quot;fils /not_exist&gt;/dev/null 2&gt;&amp;1if [ $? -eq 0 ];then  echo &quot;ls runs ok&quot;else  echo &quot;ls runs badly&quot;fi

ls runs ok
ls runs badly
ls runs ok
ls runs ok
ls runs badly

quotesfor assignment in bash, the righ must be a single word, so S=&quot;a b&quot; has the same with S=a\ b, “” to make a string as a whole, like below
for i in &quot;a b&quot; # &quot;a b&quot; as a wholedo  echo $i # execute only oncedoneS=&quot;a b&quot;for i in $S  # same as for i in a b(&quot;&quot; is removed for var expansion)do  echo $i # execute twice!!!done

check string containsTo check if a string contains a substring
%%bash# wildcard way1STR=&#x27;GNU/Linux is an operating system&#x27;SUB=&#x27;Linux&#x27;if [[ &quot;$STR&quot; == *&quot;$SUB&quot;* ]]; then  echo &quot;It&#x27;s there.&quot;fi# case operator way2STR=&#x27;GNU/Linux is an operating system&#x27;SUB=&#x27;Linux&#x27;case $STR in  *&quot;$SUB&quot;*)    echo  &quot;It&#x27;s there.&quot;    ;;esac# Regex Operator way3STR=&#x27;GNU/Linux is an operating system&#x27;SUB=&#x27;Linux&#x27;if [[ &quot;$STR&quot; =~ &quot;$SUB&quot; ]]; then  echo &quot;It&#x27;s there.&quot;fi# grep way4STR=&#x27;GNU/Linux is an operating system&#x27;SUB=&#x27;Linux&#x27;if grep -q &quot;$SUB&quot; &lt;&lt;&lt; &quot;$STR&quot;; then  echo &quot;It&#x27;s there&quot;fi

It&#39;s there.
It&#39;s there.
It&#39;s there.
It&#39;s there

repeat char(str) n times%%bashprintf &#x27;=%.0s&#x27; &#123;1..10&#125;echofor i in `seq 1 10`doecho -n &quot;=&quot;doneechofor i in &#123;1..10&#125;doecho -n &quot;=&quot;done

==========
==========
==========

echo with colorRED=&#x27;\033[0;31m&#x27;NC=&#x27;\033[0m&#x27; # No Colorecho -e &quot;I $&#123;RED&#125;love$&#123;NC&#125; Stack Overflow&quot;# for simple, refer to https://svelte.dev/repl/1b3f49696f0c44c881c34587f2537aa2?version=4.0.5

Invert boolean variable%%bash[ $&#123;FOO:-false&#125; == false ] &amp;&amp; FOO=true || FOO=falseecho $FOO[ $&#123;FOO:-false&#125; == false ] &amp;&amp; FOO=true || FOO=falseecho $FOO

true
false

compile bash to binary$ sudo yum install -y shc$ cat hello.sh#!/usr/bin/bashecho hello$ shc -f hello.sh$ ls hello*hello.sh  hello.sh.x  hello.sh.x.c # .sh is the original script.# sh.x is the compiled binary.# .sh.x.c is the C source code generated from the .sh file prior to compiling to .sh.x.

trim character from stringuse tr or sed
%%bash# tr only supported delete &#x27;one character&#x27; from the whole string# &#x27;one character&#x27; can be a,b,\t,[:digit:]echo &quot;hello boy&quot; | tr -d &quot;o&quot;echo &quot;hello  boy  &quot; | tr -d &quot; &quot;echo &quot;1hello 2 boy&quot; | tr -d &quot;[:digit:]&quot;echo &quot;1hello 2 boy&quot; | sed &#x27;s/[0-9]//g&#x27;# delete space at head onlyechoecho &quot;   hello boy &quot; echo &quot;   hello boy &quot; | sed &#x27;s/^ *//&#x27;echo &quot;   hello boy &quot; | sed &#x27;s/^[[:space:]]*//&#x27;

hell by
helloboy
hello  boy
hello  boy

   hello boy 
hello boy 
hello boy 

Ref
bash cheatsheet
shell expansion
Shell-Parameter-Expansion

]]></content>
      <categories>
        <category>bash</category>
        <category>demo</category>
      </categories>
      <tags>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>bootstrap-component-cases</title>
    <url>/2020/08/05/bootstrap-component-cases/</url>
    <content><![CDATA[OverviewBootstrap which is the most popular HTML, CSS, and JavaScript framework for developing responsive, mobile-first websites, most of its code are CSS, CSS classes defines how each components looks like.


Component CSS classComponent CSS classes aim to style that kind of html element by setting several CSS properties, hence we should always use that bootstrap class for that kind of html element, so that you got the expected style, one exception is you can use button class to &lt;a&gt; element, even each CSS class is separately, but bootstrap gives some code snippets to use them together to make it looks better, always uses that code snippet like what bootstrap does.
&lt;!--form-group only adding padding or margin    you can use form-control without group, but group to make it looks better--&gt;&lt;div class=&quot;form-group&quot;&gt;  &lt;label for=&quot;email&quot;&gt;Email address:&lt;/label&gt;  &lt;input type=&quot;email&quot; class=&quot;form-control&quot; placeholder=&quot;Enter email&quot; id=&quot;email&quot;&gt;&lt;/div&gt;&lt;label for=&quot;email&quot;&gt;Email address:&lt;/label&gt;&lt;input type=&quot;email&quot; class=&quot;form-control&quot; placeholder=&quot;Enter email&quot; id=&quot;email&quot;&gt;

&lt;a href=&quot;#&quot; class=&quot;btn btn-info&quot; role=&quot;button&quot;&gt;Link Button&lt;/a&gt;&lt;input type=&quot;button&quot; class=&quot;btn btn-info&quot; value=&quot;Input Button&quot;&gt;&lt;input type=&quot;submit&quot; class=&quot;btn btn-info&quot; value=&quot;Submit Button&quot;&gt;

Utilities CSS classBootstrap 4 has a lot of utility&#x2F;helper classes to quickly style elements without using any CSS code, because bootstrap does this for you.
Utilities do NOT target a particular html element, but for all element with common CSS propertieslike

border
background-color
text
flex
block
padding
margin
height&#x2F;width
align
position

More detail refer to Bootstrap4 Utilities
AdminLTEAdminLTE is built on top of Bootstrap, AdminLTE provides a range of responsive, reusable, and commonly used components, it keeps all bootstrap class and added new one, so that it’s a super class of bootstrap, all bootstrap based websites can work as well with AdminLTE, the big benefits of using AdminLTE is that it provides lots of components that you can pick it directly and use it in your website.
AdminLTE depends on two main frameworks. Bootstrap 4 and jQuery 3.3.1, you should also include jQuery.min.js and bootstrap.min.js in your project as well, along with AdminLTE, no need to include bootstrap.css!!!
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/admin-lte/3.0.5/css/adminlte.min.css&quot; integrity=&quot;sha512-rVZC4rf0Piwtw/LsgwXxKXzWq3L0P6atiQKBNuXYRbg2FoRbSTIY0k2DxuJcs7dk4e/ShtMzglHKBOJxW8EQyQ==&quot; crossorigin=&quot;anonymous&quot; /&gt;&lt;!--...--&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js&quot; integrity=&quot;sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.bundle.min.js&quot; integrity=&quot;sha384-LtrjvnR4Twt/qOuYxE721u19sVFLVSA4hf/rRt6PrZTmiPltdZcI7q7PXQBYTKyf&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/admin-lte/3.0.5/js/adminlte.min.js&quot; integrity=&quot;sha512-++c7zGcm18AhH83pOIETVReg0dr1Yn8XTRw+0bWSIWAVCAwz1s2PwnSj4z/OOyKlwSXc4RLg3nnjR22q0dhEyA==&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;


Bootstrap 4
jQuery 3.3.1+
Popper.js 1.14But if you use a component which depends on plugins, you should also add that plugin CDN as well in your page.


How to use AdminLTE1. download the source and run it locally, so that you can see what component looks like and pick the one you needs
$ git clone https://github.com/ColorlibHQ/AdminLTE.git$ cd AdminLTE$ yarn install$ yarn dev
2. include the AdminLTE in your project
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/admin-lte/3.0.5/css/adminlte.min.css&quot; integrity=&quot;sha512-rVZC4rf0Piwtw/LsgwXxKXzWq3L0P6atiQKBNuXYRbg2FoRbSTIY0k2DxuJcs7dk4e/ShtMzglHKBOJxW8EQyQ==&quot; crossorigin=&quot;anonymous&quot; /&gt;&lt;!--...--&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/admin-lte/3.0.5/js/adminlte.min.js&quot; integrity=&quot;sha512-++c7zGcm18AhH83pOIETVReg0dr1Yn8XTRw+0bWSIWAVCAwz1s2PwnSj4z/OOyKlwSXc4RLg3nnjR22q0dhEyA==&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;
3. copy the code snippet that you needs to your project
Note: you can write code like what you did with bootstrap, but use predefined component from AdminLTE in any case
]]></content>
      <categories>
        <category>html</category>
        <category>bootstrap</category>
      </categories>
      <tags>
        <tag>html</tag>
        <tag>bootstrap</tag>
        <tag>AdminLTE</tag>
      </tags>
  </entry>
  <entry>
    <title>benchmark-tools</title>
    <url>/2019/11/29/benchmark-tools/</url>
    <content><![CDATA[IntroductionThere are lots of tools for traffic capturing, sending, replaying about protocols, here are some examples of these popular tools.
capture

tcpdump(linux)
wireshark(windows&#x2F;linux)

sending&#x2F;replay

sendip
scapy
tcprelay

http benchmark

ab
wrk

io benchmark

dd


Traffictcpdump vs wiresharktcpdump and wireshark are used to capture traffic on the internet, while wireshark provides filter for us after capture, that’s much helpful, while for tcpdump, there is no filter after capture, filter is only available when capturing, but you can save tcpdump output to xx.pcap than open it with wireshark, hence use filter provided by wireshark.
when capturing packets tcpdump and wireshark use the same syntax like host 10.10.2.2 and tcp port 5000.
tcpdump
# type:      host，net，port,# direction: src, dst, dst or src, dst and src# proto:     arp, rarp, tcp, udp# logical:   not，and, or# combine them together$ tcpdump -i eth0 host 10.10.10.8 and tcp src port 5000 and not dst port 6000# save to wireshark message$ tcpdump -i eth0 -s 0 -w /home/lzq/pt.pcap# vxlan use 4789, 11:4 is VNI# capture VNI 6452, vxlan packet$ tcpdump -i mirror-out port 4789 and udp[11:4]=6452

wireshark capture
syntax
support tcpdump syntax but more than that with extra options like this.dst port 135 and tcp port 135 and ip[2:2]==48icmp[icmptype]==icmp-echo and ip[2:2]==92 and icmp[8:4]==0xAAAAAAAAdst port 135 or dst port 445 or dst port 1433  and tcp[tcpflags] &amp; (tcp-syn) != 0 and tcp[tcpflags] &amp; (tcp-ack) = 0 and src net 192.168.0.0/24

wireshark filter
tips: exported filtered packets from wireshark.
At the wireshark main panel, apply a filter, then you see filtered packets displayed, then exported it by
File-&gt;Export specified Packets-&gt; All packets(displayed)!filter syntax
tcp.port eq 25 or icmptcp.port == 25 or icmpip.src==192.168.0.0/16 and ip.dst==192.168.0.0/16tcp.window_size == 0 &amp;&amp; tcp.flags.reset != 1Match packets containing the (arbitrary) 3-byte sequence 0x81, 0x60, 0x03 at the beginning of the UDP payload, skipping the 8-byte UDP header.udp[8:3]==81:60:03eth.addr[0:3]==00:06:5B

wireshark display
show absolute timeView-&gt;Time Display Format  
show absolute tcp seq numberPreference-&gt;Protocols-&gt;TCP [uncheck]Relative sequence numbers
for tcp retransmission how can I distinguish the original packet and retransmitted onCheck TsVal it&#39;s at packets&#39; Tcp options Tsval Or Tcp field checksum
sendip vs tcprelay vs scapysendip vs tcpreplay

sendip is used to send arbitrary IP packets while tcprelay is mostly used to replay(edit) captured packet(send it again).

sendip vs scapy  

Scapy: It operates at multiple layers of the OSI model. You can send packets at Layer 3 using send() or at Layer 2 using sendp(). This flexibility allows users to craft packets with specific Layer 2 headers when necessary.
Sendip: Primarily focuses on sending IP packets and operates mainly at Layer 3. It does not provide the same level of control over Layer 2 headers as Scapy does.

sendipsendip sends arbitrary IP packets(ip + upper protocol), like tcp, udp, icmp, rip etc
$ sudo sendip -v -d &quot;hello&quot; -p ipv4 -is 192.168.200.1 -id 192.168.200.2 -p udp -us 4000 -ud 1200 192.168.200.2# send ipv4 udp with src 192.168.200.1, src port 4000, payload hello# -is:  source ip# -id:  dst ip# -us:  udp src port# -ud:  udp dst port# -d   data(payload)$ sudo sendip -v -d &quot;hello&quot; -p ipv4 -is 192.168.200.1 -id 192.168.200.2 -p tcp -ts 4000 -td 1200 192.168.200.2

tcpreplaytcpreplay is a series of commands that can be used to replay packets captured(not only tcp packet) with more control, like sending duration, loop, send number, pps, modify packets etc.

tcpreplay:      only send packets captured
tcprewrite:     only modify(mac, vlan, ip, tcp&#x2F;udp etc)captured packets and save it to a file
tcpreplay-edit: modify on fly and send the modified packets &#x3D;&#x3D; tcprewrite + tcpreplay, so options for tcprewrite still work for tcpreplay-edit

$ tcpreplay -i eth0 -l 2 c.pcap      # replay twice on output device eth0$ tcpreplay-edit -i eth0 c.pcap      # same as replay$ tcprewrite --portmap=80:8000 --srcipmap=10.117.6.8:10.117.6.80 --fixcsum --ttl=125 --infile=c.pcap --outfile=b.pcap# --enet-dmac=00:12:13:14:15:16,00:22:33:44:55:66$ tcpreplay-edit --portmap=80:8000 --srcipmap=10.117.6.8:10.117.6.80 --fixcsum --ttl=125 c.pcap

scapyScapy is a Python-based interactive packet manipulation tool and library that is commonly used for network tasks such as scanning, tracerouting, probing, unit testing, attacking, and network discovery. It allows users to forge, send, dissect, and sniff network packets.
Here we only show packet crafting.
$ scapy# sr: send and receive at layer3# send: send packet at layer3 only# This sends SYN packets to ports 22 (SSH), 80 (HTTP), and 443 (HTTPS) on the specified IP address.&gt;&gt;&gt; sr(IP(dst=&quot;192.168.1.1&quot;)/TCP(dport=[22, 80, 443], flags=&quot;S&quot;))# This constructs a TCP SYN packet destined for port 80 on the specified IP address and sends it.&gt;&gt;&gt; packet = IP(dst=&quot;192.168.1.1&quot;)/TCP(dport=80, flags=&quot;S&quot;)&gt;&gt;&gt; send(packet)

curl vs ab vs wrkcurl used for function test supports get&#x2F;post etc. while ab&#x2F;wrk are used for performance test
ab and wrk both for http benchmark(software level, should not used for production), ab is from apache, old but more powerful with lots of options, while wrk is a newly tool which is very popular today.
curlcurl provides lots of options for sending one http request, by default, curl use http1.1 can change to http1.0 or http2.0
$ curl --http1.0 www.example.com$ curl --http1.1 www.example.com$ curl --http2 www.example.com$ curl www.example.com# By default, Post uses x-www-form-urlencoded format to send data# send post payload from a file$ curl -X POST -d &quot;@data.txt&quot; http:/www.test.com# send post request with chunked data$ curl -H &quot;Transfer-Encoding: chunked&quot; -d &quot;payload to send&quot; http://example.com# send post request with data$ curl -X POST -d &quot;name=Mona&amp;age=20&quot;  https://www.example.com/update_info# send post with json format, must set Content-type to json explicitly$ curl -X POST -d &quot;@json_data&quot; -H &quot;Content-Type: application/json&quot; https://www.example.com/update_info$ curl -X POST -d &#x27;&#123;&quot;key1&quot;:&quot;value1&quot;, &quot;key2&quot;:&quot;value2&quot;&#125;&#x27; -H &quot;Content-Type: application/json&quot; https://www.example.com/update_info# send password,  cookie when download a file$ curl -u &quot;user:passwd&quot; -b &quot;name=john123&quot; -O http://ubuntu.biz.net.id/18.04.2/ubuntu-18.04.2-desktop-amd64.iso# write cookie to a file$ curl -u &quot;user:passwd&quot; -c saved_cookie.txt -O http://ubuntu.biz.net.id/18.04.2/ubuntu-18.04.2-desktop-amd64.iso# read cookie from a file$ curl -u &quot;user:passwd&quot; -b saved_cookie.txt -O http://ubuntu.biz.net.id/18.04.2/ubuntu-18.04.2-desktop-amd64.iso# check http response headers, only headers returned$ curl -I http://www.yahoo.com# change agent$ curl -A &quot;Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0&quot; https://example.com/# send request with parameter$ curl -i http://localhost/login?key1=jason&amp;key2=jason2# send request with custom header$ curl -i -H &quot;Host: 10.10.10.3&quot; -H &quot;Content-Type: application/json&quot; http://123.123.123.123/login# bind src when sending$ curl --interface 10.10.10.10 http://123.123.123.123/# send SNI# first edit /etc/hosts with below content# 100.0.1.1 app1.xyz.com$ curl -k --resolve app1.xyz.com https://app1.xyz.com# curl use proxy$ curl -x $proxy_server:$port -U $user:$passwd www.google.com

abab provides lots of options for sending http request, like set cookie, header, authentication, ssl etc, by default ab sends GET request with default values. only supports http1.0
get
$ ab -n 1000 -c 10  http://127.0.0.1:8080/index.html# -n 1000 total requests# -c 10   send 10 at a time (concurrency, total request unchanged)$ ab -n 1000 -c 10 -C &quot;ck1=123&quot; -C &quot;ck2=124&quot; -H &quot;Accept-Encoding: gzip&quot; -k http://127.0.0.1:8080/index.html# with cookie(-C), http keep alive(-k), and headers(-H)$ ab -B 10.10.10.10 http//127.0.0.1:8080/index.html# with binding SRC

post with data
$ ab -t 30 -c 10 -p ./post.data -T &quot;application/x-www-form-urlencoded&quot; http://local/user# -t means run 30 second with 10 connection with post method$ ab -t 30 -c 10 -p ./post.data -T &quot;application/json&quot; http://localhost/user# with token auth$ ab -t 30 -c 10 -p ./post.data -T &quot;application/json&quot; -H &#x27;Authorization: Token abcd1234&#x27; http://localhost/user# with basic auth$ ab -t 30 -c 10 -p ./post.data -T &quot;application/json&quot; -A &#x27;user:passwd&#x27; http://localhost/user

wrkrun get with default header
$ wrk -t12 -c400 -d30s --latency http://127.0.0.1:8080/index.html# run get with(does NOT support setting out binding address)# -t12    12 threads to run# -c400   keep 400 connection open during the test# -d30s   test 30 seconds with get request

run get custom header
$ wrk -t2 -c10 -d30s --latency -s ./get.lua http://127.0.0.1/index.html# get.luawrk.method = &quot;GET&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;application/json&quot;

run post with data
$ wrk -t2 -c10 -d30s --latency -s ./post.lua http://127.0.0.1/user# post.luawrk.method = &quot;POST&quot;wrk.body   = &quot;foo=bar&amp;baz=quux&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;application/x-www-form-urlencoded&quot;# post.lua with json formatwrk.method = &quot;POST&quot;wrk.body = &quot;&#123;\&quot;firstKey\&quot;: &#x27;somedata&#x27;, \&quot;secondKey\&quot;: &#x27;somedata&#x27;&#125;&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;application/json&quot;

FAQwhy content-length is 0 with POST method by curlThe reason should be the content of the file used by POST contains zero, as curl first reads the whole file to memory, then calculate the content_length with strlen(file content), hence if the content of the file contains ‘\0’, the content_length !&#x3D; file size, unexpected. here is a example
# file content is all &#x27;\0&#x27;!!$ dd if=/dev/zero of=data.txt bs=1M count=1# actually, curl just reads the data, but not send it, the Content_Length: 0$ curl -X POST -d &quot;@data.txt&quot; http:/www.test.com# FIX it, generate a file without &#x27;\0&#x27; or with given repeated content like$ yes &#x27;this is a test&#x27; | head -c 1K &gt; 1K.file$ yes &#x27;this is a test&#x27; | head -c 1M &gt; 1M.file$ curl -X POST -d &quot;@1K.file&quot; http:/www.test.com
send http over unix-socket# if http server listens on unix socket# --unix-socket (Added in 7.40.0)$ curl --version$ curl --unix-socket /var/run/tes.sock http://debug/api

IOUse dd to test io benchmark, as it’s an easy tool to use
# write to another block device like nbd here$ dd if=/dev/zero of=/dev/nbd124 bs=1M count=1000# write testing$ dd if=/dev/zero of=out.disk bs=1M count=1000# with direct io for writing$ dd if=/dev/zero of=out.disk bs=1M count=1000 oflag=direct# read testing$ dd if=out.disk of=/dev/null bs=1M count=1000# check static io stats or runtime io$ iostat$ iotop# fio is better than dd# -------------------------------------------------------------------------------# --rw=str# read: sequential reads# write: sequential writes# randread: random reads# randwrite: random writes# rw: sequential mix of reads and writes# randrw: random mix of reads and writes# -------------------------------------------------------------------------------# --numJobs=int# The number of threads spawned by the test. By default, each thread is reported separately. To see the results for all threads as a whole, use --group_reporting.# -------------------------------------------------------------------------------# --iodepth=int# Number of I/O units to keep in flight against the file. That is the amount of outstanding I/O for each thread.# -------------------------------------------------------------------------------# --runtime=int#  The amount of time the test will be running in seconds.# -------------------------------------------------------------------------------# --time_based# If given, run for the specified runtime duration even if the files are completely read or written. The same workload will be repeated as many times as runtime allows.# -------------------------------------------------------------------------------$ fio --name=fiotest --filename=./test1 --size=1Gb --rw=randread --bs=4K --direct=1 --numjobs=1 --ioengine=libaio --iodepth=1 --group_reporting --runtime=60 --time_based$ fio --name=fiotest --filename=./test1 --size=1Gb --rw=write --bs=4K --direct=1 --numjobs=1 --ioengine=libaio --iodepth=1 --group_reporting --runtime=60 --time_based$ fio --name=fiotest --filename=./test1 --size=1Gb --rw=rw --bs=4K --direct=1 --numjobs=1 --ioengine=libaio --iodepth=1 --group_reporting --runtime=60 --time_based

Ref
tcpdump manpage
tcpdump advanced filters
tcprelay manpage
sendip manpage
ab manpage
wrk manpage
vegeta
jmeter
locust
fio doc
fio example

]]></content>
      <categories>
        <category>traffic</category>
        <category>tool</category>
      </categories>
      <tags>
        <tag>traffic sender</tag>
        <tag>traffic capture</tag>
        <tag>http benchmark</tag>
        <tag>io benchmark</tag>
      </tags>
  </entry>
  <entry>
    <title>c-language-basic</title>
    <url>/2023/08/24/c-language-basic/</url>
    <content><![CDATA[C
变量的声明和定义在c语言中是不同的，声明不开辟空间，而是告诉编译器该变量在其他地方定义了，而定义则是要给变量开辟空间
c语言中式没有引用类型
虽然C语言中有const，但是const不可以修饰函数属性, const 只能修饰变量和参数
C 不支持函数重置，不同的函数必须使用不同的函数名！！！



printf%o  八进制整数(无论char,short,int, long ）%x  16进制整数(无论char,short,int, long ）%d  有符号10机制输出（singed char, short, int 都可以使用）%u  无符号10机制输出（unsigned char, unsigned short, unsigned int 都可以使用）%ld 有符号长整数%lu 无符号长整数%f  浮点数输出%c  字符输出，输出表面字符，而不是字符的对应值!%s  字符串输出// NO %l at all!!!// print uint64_t#define __STDC_FORMAT_MACROS#include &lt;inttypes.h&gt;uint64_t i = 10;int64_t n = 7;printf(&quot;%&quot;PRId64&quot;\n&quot;, n);printf(&quot;%&quot;PRIu64&quot;\n&quot;, i);
C99 printf format
pointer
void*是无类型的指针，该类指针可以和其他指针完成相互转换! 但是它(void*)无法进行指针运算
不同类型的指针转化是有危险的！！可能导致数据的丢失和非预期的访问

pointer pointerint a, b, c;int* p[3]; //pointer arrayp[0] = &amp;a;p[1] = &amp;b;p[2] = &amp;c;typedef void (*ftype)(int a); //ftype 是一个函数指针类型void hello(int a);ftype pfun = hello;// 可变参数的函数指针#ifdef __cplusplustypedef void (*PTR)(...);#else//和无参函数指针一样typedef void (*PTR)();#endif

const with pointerconst int a = 12;              //(这是定义一个只读类型的变量！)const char *p; === char const *p;   //那么p指向的内存区的内容不可以更改, 但是p可以指向不同内存区.int*const p = &amp;a;             //p初始化以后不可更改(定义的时候必须初始化)！但p所指向的内容可以更改````### pointer operation![](https://cyun.tech/images/C/pointer_ops.png)```cint a[2];int *p = a;   // p+1 == address(a) + 4      ----&gt;sizeof(int) == 4int **pp = &amp;p; // pp + 1 == address(p) + 8    ----&gt;sizeof(int*) == 8
p+1就是下一个元素的地址，p是int*,因此下一个元素就是在上个元素的地址上加+sizeof(int).
指针的减法表示的地址之间的该类型元素的个数，而不是地址之间的字节数
int a[3];&amp;a[2]-&amp;a[0] = 2 // 而不是2*sizeof(int)

NULL and 0空指针和0是不同的，但是当一个变量是指针变量的时候，指针变量和0的赋值，比较操作编译器会把0转化为NULL也就是说下面的代码编译后的结果是相同的
int * p;//一样if (p!=NULL)if (p!=0) //一样p = 0;p = NULL;

integer typeOn major 32-bit platforms:

int is 32 bits
long is 32 bits as well
long long is 64 bits


On major 64-bit platforms:

int is 32 bits
long is either 32 or 64 bits
long long is 64 bits as well


Explicit type


int32_t, int64_t, int16_t, int8_t
u_int32_t, u_int64_t, u_int16_t, u_int8_t

range有符号数是有符号位的，无符号数是没有符号位的！不同类型的变量，长度可能是不同的，因此它所表示的数值范围也是有限制的，而同一种类型的变量，根据是否有无符号，范围也是不同的下面列举几种常见类型的大小.
类型		长度			     无				有char		1				255				-128---127short		2				65535			-32768—32767int			4				4294967295		-2147483648---2147483647

符号影响了不同类型的表示范围，而更为严重的是，编译器在比较和赋值的时候可能会有类型的隐式转化！！
what is the reason for explicitly declaring L or UL for long valuesWhen a suffix L or UL is not used ,compiler uses the first type that can contain the constant from a list (see details in C99 standard, clause 6.4.4:5. For a decimal constant, the list is int, long, long long).
As a consequence, most of the times, it is not necessary to use the suffix. It does not change the meaning of the program. It does not change the meaning of your example initialization of x for most architectures, although it would if you had chosen a number that could not be represented as a long long.
There are a couple of circumstances when the programmer may want to set the type of the constant explicitly. One example is when using a variadic function:
printf(&quot;%lld&quot;, 1LL); // correctprintf(&quot;%lld&quot;, 1);   // undefined behavior 1 type is int!! but use lld format.
A common reason to use a suffix is ensuring that the result of a computation doesn’t overflow. Two examples are:
long x = 10000L * 4096L;unsigned long long y = 1ULL &lt;&lt; 36;
In both examples, without suffixes, the constants would have type int and the computation would be made as int. In each example this incurs a risk of overflow. Using the suffixes means that the computation will be done in a larger type instead, which has sufficient range for the result.
As Lightness Races in Orbit puts it, the litteral’s suffix comes before the assignment. In the two examples above, simply declaring x as long and y as unsigned long long is not enough to prevent the overflow in the computation of the expressions assigned to them.
Another example is the comparison x &lt; 12U where variable x has type int. Without the U suffix, the compiler types the constant 12 as an int, and the comparison is therefore a comparison of signed ints.
int x = -3;printf(&quot;%d\n&quot;, x &lt; 12); // prints 1 because it&#x27;s true that -3 &lt; 12

With the U suffix, the comparison becomes a comparison of unsigned ints. “Usual arithmetic conversions” mean that -3 is converted to a large unsigned int:
printf(&quot;%d\n&quot;, x &lt; 12U); // prints 0 because (unsigned int)-3 is large

In fact, the type of a constant may even change the result of an arithmetic computation, again because of the way “usual arithmetic conversions” work.
Integral Promotion(char,short,enum----&gt;promoted to int or uint)
A character, a short integer, or an integer bit-field, all either signed or not, or an object of enumeration type, may be used in an expression wherever an integer may be used. If an int can represent all the values of the original type, then the value is converted to int; otherwise the value is converted to unsigned int. This process is called integral promotion.
有无符号的char,short,位域整数，枚举当使用在需要整数的地方的时候，会进行类型的提升，值的大小决定了到底转化为有符号int,还是无符号int.
char,short,enum----&gt;promoted to int or uint
short a = 0x7fff;short b = 2;short c = (a*b)/(short)2; a*b提升为整数 (中间值都是int or unsigned int)printf (&quot;%x\n&quot;, a); //==0x7fff// gcc -o test  a.c -g  -Wconversion

short a = 1;short b = 2;short c = a+b; //类型转换。

Integral ConversionsAny integer is converted to a given unsigned type by finding the smallest non-negative value that is congruent to that integer, modulo one more than the largest value that can be represented in the unsigned type. In a two’s complement representation, this is equivalent to left-truncation if the bit pattern of the unsigned type is narrower, and to zero-filling unsigned values and sign-extending signed values if the unsigned type is wider.When any integer is converted to a signed type, the value is unchanged if it can be represented in the new type and is implementation-defined otherwise.
整数(short,char,int,unsigned,long)转换

转换为有符号的类型，如果值能够用新类型表示，那么值不变，如果值太大，那么结果是未定义的，根据具体的实现，  然后把最高位当符号为看待。然后就可以给你补码规则知道具体的值。
转换为无符号的类型，如果新类型比较宽，用符号位填充，如果新类型比较‘窄’，截取最左端！也就是丢弃左端！！保留低字节位，而不管big-endian和little-endian。例如：char var = 0xff12; //在big-endian和little-endian的结果都是0x12// 然后 没有了符号位，计算新的结果。

Arithmetic ConversionsMany operators cause conversions and yield result types in a similar way. The effect is to bring operands into a common type, which is also the type of the result. This pattern is called the usual arithmetic conversions. 
如果决定转化的目标类型: 按照下面顺序

First, if either operand is long double, the other is converted to long double. 
Otherwise, if either operand is double, the other is converted to double. 
Otherwise, if either operand is float, the other is converted to float.

integer operation must has the same type!!!

Otherwise, the integral promotions are performed on both operands; then, if either operand is unsigned long int, the other is converted to unsigned long int. 
Otherwise, if one operand is long int and the other is unsigned int, the effect depends on whether a long int can represent all values of an unsigned int; if so, the unsigned int operand is converted to long int; if not, both are converted to unsigned long int. 
Otherwise, if one operand is long int, the other is converted to long int. 
Otherwise, if either operand is unsigned int, the other is converted to unsigned int. 
Otherwise, both operands have type int。

examples有符号和无符号的数，运算和比较的话，有符号数要向无符号数转化！
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main()&#123;    int a = -1;    if (a &gt; 12u) // 12u type: unsigned int    &#123; // a转化为无符号数  a prompted to unsigned int        printf(&quot;greater\n&quot;);    &#125;    // 同符号数运算结果仍然是同符号数    unsigned int c = 1;    unsigned int d = 2;    if (c - d &gt; 0) &#123; // 0提升为无符号数！                     //说明c-d&gt;0 也不能保证c是大于d的！        printf(&quot;greater\n&quot;);    &#125;    int e = -1;    unsigned short f = 1;    if (e &lt; f) &#123; // f prompted to int!!!        printf(&quot;e is smaller than f\n&quot;);    &#125;&#125;

greater
greater
e is smaller than f

constantWays to define

常量字符
常量字符串
enum定义的常量（其中的值类型是int) can be used to define array
define定义的常量(可以为常量数也可以是字符串) can be used to define array
const 定义的常量数和字符串(其实是变量，只读属性，can NOT be used to define array)const char * p = &quot;hello&quot;enum day&#123;mon=2, tus=1,wed&#125;;#define 	A	12#define	B 	&quot;hello&quot;enum &#123;size=10&#125;const int c_size = 10; // actually c_size is a variable with read only attribute!!!!int main()&#123;int a[size];//rightint b[c_size];//wrong&#125;

type of constant
integer constant: The type of an integer constant depends on its form, value and suffix
If it is not suffixed and decimal, it has the first of these types in which its value can be represented: int, long int, unsigned long int

If it is not suffixed, octal or hexadecimal, it has the first possible of these types: int, unsigned int, long int, unsigned long int

If it is suffixed by u or U, then unsigned int, unsigned long int

If it is suffixed by l or L, then long int, unsigned long int

If an integer constant is suffixed by UL, it is unsigned long



枚举常量类型是int
字符常量类型是char

variablestatic var
初始化为 0
static定义的变量的存储空间是开辟在 data segment if initialized by user, otherwise in BSS segment, 只有在程序退出的时候才消失
如果定义在函数内，反复调用也不会重新初始化。

var address变量是有类型的，而不同的类型的长度也是不同的，因此当为一个变量分配多个字节的时候（显然这些字节是连续的！），该用哪个字节的地址表示变量的地址呢，显然最合理的就是用地址最小的那个字节！
int a;

那么a的地址是A，想要取得所有分片(one byte one slot)内的内容，就需要按照字节访问， 方法就是定义一个char*，指向a
char * p = (char*)&amp;a
这样就可以按照字节访问一个int每个byte了.
struct按照常理，结构体的长度就是所有变量的长度之和，但是为了对存取快捷，硬件也规定了不同类型的地址是有规律的！例如short虚拟地址必须是2的倍数，int的虚拟地址必须是4的倍数，因此导致了编译器会对结构体做调整，使其变长了，虽然浪费了空间，但是提高了内存的存取速度！
每种类型的都有其对齐模数(k),也就是该类型变量的地址必须是k的整数倍，而基本类型的对齐模数一般都是其本身长度 结构体类型的对齐模数一般则是所有成员中最大的(基本类型)成员对齐模数。对齐模数和地址紧密相关.
x86-64
paddingstruct ms&#123;char c;//pading hereint b;&#125;;```由于struct ms能够保证变量的地址是必须是max(sizeof(char),sizeof(int))==4的倍数，但是也要保证内部变量的地址满足其自身要求。因此需要在 两成员之间 填充3字节！，保证b的地址为4的倍数，因此sizeof(struct ms)==8```cstruct ms1&#123;int b;char c;//类型的模数是关键！//pading here&#125;;

还是按照上述分析对于单个变量而言，情况满足，那么sizeof(struct ms1)&#x3D;&#x3D;5?,其实并非如此，因为当定义数组的时候标准规定任何类型(包括自定义结构类型)的数组所占空间的大小一定等于一个单独的该类型数据的大小乘以数组元素的个数，也就是数组的每个元素都是紧密相邻的。
struct ms1 array[2];
对于第2个元素，显然无法保证b的地址为4的倍数！因此结构体需要在尾部填充！那么尾部该填充多少呢？一般而言在中间对齐之后，如果结果不是对齐模数的倍数则提升到倍数就可以了。也就是结构体的长度一定是对齐模数的倍数!
对齐模数编译器是可以配置的，因此结构体的长度是和操作系统和编译器相关的
bit of member规定位域不能跨同类型的两个地址.
struct test&#123;int a:30; int b:3; // in next 4 bytes, not same with a to avoid cross two variables for b&#125;;// sizeof(struct test) == 8struct test&#123;int a:30; int b:1; // in the same 4 bytes as a&#125;;// sizeof(struct test) == 4

functionargument passing参数中的变量都是实参的一个副本!!
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void print(char *str)&#123;    str++; //str is another variable(address are different with p) that points to hello as well!!    // *str = *(str+1); as str points to readonly memory, so this will cause segment fault!!!&#125;int main()&#123;    // p points to hello    char * p = &quot;hello&quot;;    print(p);    printf(&quot;%s\n&quot;, p); // result: hello    return 0;&#125;

hello

variable parametersWhenever a function is declared to have an indeterminate number of arguments, in place of the last argument you should place an ellipsis (which looks like ‘…’), so, int a_function (int x, ... ); would tell the compiler the function should accept however many arguments that the programmer uses, as long as it is equal to at least one, the one being the first, x.

va_start, which initializes the list

va_start is a macro which accepts two arguments, a va_list and the name of the variable that directly precedes the ellipsis (“…”).


va_arg, which returns the next argument in the list, 

va_arg takes a va_list and a variable type, and returns the next argument in the list in the form of whatever variable type it is told. It then moves down the list to the next argument, Note that you need to know the type of each argument–that’s part of why printf requires a format string.


va_end, which cleans up the variable argument list.


#include &lt;stdarg.h&gt;#include &lt;stdio.h&gt; /* this function will take the number of values to average   followed by all of the numbers to average */// the first parameter can be any type, in case you have a way to know how many parameters passed in.// see below example.double average ( int num, ... )&#123;    va_list arguments;                         double sum = 0;     /* Initializing arguments to store all values after num */    va_start ( arguments, num );   // num is the name of the list!!!             /* Sum all the inputs; we still rely on the function caller to tell us how     * many there are */    for ( int x = 0; x &lt; num; x++ )            &#123;        sum += va_arg ( arguments, double ); // must know type for each argument!!!    &#125;    va_end ( arguments );                  // Cleans up the list     return sum / num;                      &#125; int main()&#123;    /* this computes the average of 13.2, 22.3 and 4.5 (3 indicates the number of values to average) */    printf( &quot;%f\n&quot;, average ( 3, 12.2, 22.3, 4.5 ) );    /* here it computes the average of the 5 values 3.3, 2.2, 1.1, 5.5 and 3.3 */    printf( &quot;%f\n&quot;, average ( 5, 3.3, 2.2, 1.1, 5.5, 3.3 ) );&#125;

13.000000
3.080000

#include &lt;stdarg.h&gt;#include &lt;stdio.h&gt;void print(const char* format, ...)&#123;    va_list arguments;    va_start(arguments, format);    int ival;    char* sval;    // support format takes 2 chars: like %d, %s    while (*format) &#123;        if (*format == &#x27;%&#x27;) &#123;            format++;            switch (*format) &#123;            case &#x27;d&#x27;:                ival = va_arg(arguments, int);                printf(&quot;%d\n&quot;, ival);                break;            case &#x27;s&#x27;:                sval = va_arg(arguments, char*);                printf(&quot;%s\n&quot;, sval);                break;            &#125;        &#125;        format++;    &#125;    va_end(arguments); // Cleans up the list&#125;int main()&#123;    print(&quot;%d %s&quot;, 12, &quot;hello&quot;);    return 0;&#125;

12
hello

operator priority and combination括号第一，单目2，乘除余3，加减4，移5，系6，逻于7（一股细流落于齐）位8，问9，赋逗到底(位8是与异位，问号，赋值，逗到底)单右，双左问同右*s++，*s--, get the value, then pointer + or -*++s，*--s, point+ or -, then get the value.

NOTE
Always use () for clear
bit operation左移&lt;&lt;和右移&gt;&gt;也会移动符号位, 有符号的正数可能变成负数, &amp;，|，~是对数的每一位都做这样的操作，因此它是不区分符号位的！！
因此要想&amp;，|，~操作的话就必须知道数在计算机中的表示

对应无符号数补码就是其原码
而对于正整数补码也是其原码！
而对于负整数补码取绝对值的原码，然后对该原码取反，然后加1，就是负数的补码！

简单起见，实际应用中 &amp;， |， ~ 只操作无符号数
bit move
操作数必须是integer！如果不是进行integral promotion.

The result is undefined if the right operand is negative, or greater than or equal to the number of bits in the left expression&#39;s type, the type of the result is that of the promoted left operand.
移位是位操作，每位都要移动包括符号位。左移都是填充0，右移对应有无符号数是不同的，无符号数右移填充0，有符号数右移可能填充符号位【一般这么做】，也可能填充0！移位后的表达式的类型是左侧操作数提升后的类型.
&amp; and |
操作数必须是integer！如果不是进行integral promotion.

&amp; 按位与，常见的操作就是把某些位置0

| 按位或，常见的操作就是把某些位置1

~ 按位取反！


FAQwhat does include header  doCopy all contents of the file to which includes it when building a .c file. To avoid multiple copy of it, define header file like this. 
#ifndef IPNET_CONFIG_H#define IPNET_CONFIG_H# contents#endif

Can we define(not declare) variable and function in header file as it’s copied to each .c file?
Yes, you can, but it’s not a usual way, as it may cause conflict or multiple definitions if you don’t handle it properly.
example which causes multiple definition
#test.h#ifndef TEST_H#define TEST_Hvoid say() &#123;&#125;#endif

# In a.c#include “test.h” # copy test.h# test.h is copied when building a.o as no defined before

# In b.c#include “test.h” # copy test.h here# test.h is copied when building b.o as no defined before

Makefile
program:    a.o b.o #multiple definition as say() defined in a.o and b.o!    # as both a.o and b.o define say(), multiple definitions.


Another example
test.h#ifndef TEST_H#define TEST_Hvoid say() &#123;&#125;#endif
In a.cvoid say(int b) &#123;&#125;
In b.c#include “test.h”  copy test.h here


Makefile
program:    a.o b.o #definition conflict as say() defined in a.o and b.o! but it&#x27;s different!

Guide line

If you define variable and function in header, make sure, add static keyword and the function is short

evaluation orderFunction calls, nested assignment statements, and increment and decrement operators cause side effects, some variable is changed as a by-product of the evaluation of an expression. In any expression involving side effects, there can be subtle dependencies on the order in which variables taking part in the expression are updated.
evaluation order is not defined for arithmetic operator(like +, -, *, &#x2F;) and function arguments.
operandC，does not specify the order in which the operands of an operator are evaluated. (exceptions are &amp;&amp;, ||, ?:, and ‘,’) For example, in a statement like
x = f() + g();
f may be evaluated before g or vice versa; thus if either f or g alters a variable on which the other depends, x can depend on the order of evaluation, it’s hard to say the value of x.
function argumentThe order in which function arguments are evaluated is not specified, so the statement below.
printf(&quot;%d %d\n&quot;, ++n, power(2, n)); /* WRONG */
It can produce different results with different compilers, depending on whether n is incremented before power is called.
++i and i++i++ is post-increment and ++i is pre-increment. Post-increment means that the previous value is returned after incrementing the object. pre-increment means that the object is incremented and then returned. Either way, the object is incremented when its expression is evaluated before any other next code.
, and ? expressionThe value of , is the last expression but ? if condition is true, use value is first expression, otherwise the second expression.
d = (a, b, c);e = a? b: c;

NOTE  

? , has lowest priority then , + - * / % which is from low to high
, execute from left to right for each expression

int a = 1;int b = 2;int c;c = a, b;    // c == ac = (a, b);  // c == bc = a &gt; b? 10: 20;c = a &gt; b? 10: 20 + 30;  // + has the high priority c== 20 + 30c = a &gt; b? 10: (20 + 30); // c == 20 + 30, same as above    c = 2 + a &gt; b? 10: 20; // + has high priority NOT----&gt;2 + (a&gt;b? 10 : 20)c = (2 + a) &gt; b? 10: 20;

ref
C tutorial
C FAQ

]]></content>
      <categories>
        <category>c</category>
        <category>language</category>
      </categories>
      <tags>
        <tag>c language</tag>
      </tags>
  </entry>
  <entry>
    <title>character_encoding</title>
    <url>/2021/04/20/character-encoding/</url>
    <content><![CDATA[OverviewWhat is a character encoding? In the end, when people do things with computers, they tend to work in text forms, whether that’s programs, or whether that is some other input that they give to the computer. It’s usually text. Text is conceptually a list of characters. That’s what separates it from random images on paper. The idea with character encoding is that the computers definitely would prefer numbers. We take these characters, we assign them numbers, integers. Then we figure out a way to transcribe those integers into a list of bytes. That whole process of going from text to a list of bytes is known as encoding. The reverse step is known as decoding. For example, this would be the standard ASCII approach to this. Hello is a five letter word, we will split that into five separate characters. Each of these characters is being assigned a number. At least in this case, we take these numbers and say, each of these numbers corresponds to 1 byte in the final output. Once you start working with more characters, that system breaks down because when you say each character is 1 byte, then you’re stuck with 256 characters. That doesn’t work for more complicated use cases like Chinese characters.

ASCIIThe simplest version that you can do this is ASCII. At least, historically, it’s the most important one of the first character encodings that came into existence. The idea is, we take about 127 characters. Not all of these are printable characters that you could see on paper, and we assign each of the numbers. These are the decimal and hexadecimal values that we give them. We say each of these values will just be encoded as a single byte in the final output. I will use hexadecimal representations a lot. It doesn’t really matter because the exact values don’t matter. ASCII is a 7-bit character encoding. It covers most use cases that appear in the English languages and languages that use a similar alphabet, which aren’t all that many. That’s pretty much it. There’s not a lot else that you can do with it, which is frustrating when you do want to support other languages.

ISO 8859-*, and Windows code pagesThe first step towards making ASCII work for other languages is to extend it. The idea behind a lot of the character encodings that came next, in particular, the ISO 8859-*, and Windows code pages. ASCII is 7-bit, which means we have another 128 characters available. We’re just going to create a lot of character encodings that covers some specific languages. For example, there’s Latin-1, which stands for ISO 8859-1, but rolls over the tongue a bit more nicely. That is for Western languages, you can write Spanish, French with it, languages like that, German too. Then there’s other encodings for other languages, like there’s the Cyrillic variant in that standard. There’s the Cyrillic Windows code page. These are not necessarily compatible. There’s an example where there’s characters in two different character encodings for the same language, so both for Cyrillic languages. If you encode something as one of them and decode as another, it will come out as garbage. That was also not a great situation overall. This doesn&#39;t really cover, for example, Chinese character use case.
GBKThis is one example of a character encoding that was used for Chinese characters is GBK. The idea is, 128 extra characters, that’s not enough. If there&#39;s a character that&#39;s not ASCII, with the upper bit set, then the next byte will also count towards this character. It’s either 1 byte ASCII or 2 byte Chinese character. That gives you about 30,000 more characters, which is still not enough to cover all of the Chinese characters that exist, but it’s practical enough.
UNICODEWhat we ended up with is Unicode, which is not an encoding. It is a character set, which says, each of the characters which I chose because it has a non-ASCII character in it, to each of these characters, we assign the number, and that is Unicode, mapping text(character) to number. That should ideally cover all the use cases that were previously covered by other encodings. Then we specify an actual encoding, and there are multiple of those, which define how to translate these integers into byte sequences, like there’s UTF-8, UTF-16, UTF-32, and quite a few others. UTF-8 and UTF-16 are the most important ones.
The way that Unicode characters are usually spelled out is U+ and then four hex digits, or five sometimes if the characters don&#39;t fit into the four hex digit range. That is how you specify, this is the character I’m talking about, does not specify how it is encoded. The numbering is compatible with Latin-1. The first 256 Unicode characters are the 256 Latin-1 characters. The maximum number that one can have is larger than 1 million, so we’ll have a little more than 1 million characters in total available for Unicode. Hopefully, that’s enough for the future, we’ll see. Right now there’s no issue with that. It also includes emoji, which is something that the Unicode standard is famous for these days.
UTF-8The most common character encoding that is used with Unicode is UTF-8. It’s a variable length encoding. The higher the character number is, the longer the byte sequence is in which it is encoded. In particular, it&#39;s ASCII compatible. The ASCII characters in UTF-8 are the ASCII characters as they are in code of ASCII, which is very nice. It’s also a nice property of using the scheme. These particular byte sequences don’t really have to worry about how the actual bits are encoded. If there is something broken, some invalid byte in there when you decode it, that won’t break decoding of the rest of the string.
UTF-16UTF-16, which is 2-byte code units, so 65,000 characters that can be encoded in a single 2-byte unit. Ones that do not fit into that range, they’re split into two separate pairs of code units. Because it uses 2 bytes, there are two different variants. I don’t know if you are familiar with that. There’s generally, little endian machines and big endian machines. The little endian ones put the low byte first and then the higher value byte, big endian is the reverse situation. Most modern processors use little endian.
REF
character-encoding
encoding

]]></content>
      <categories>
        <category>char</category>
        <category>encoding</category>
      </categories>
      <tags>
        <tag>encoding</tag>
      </tags>
  </entry>
  <entry>
    <title>c-language-tips</title>
    <url>/2020/01/02/c-language-tips/</url>
    <content><![CDATA[Overviewparameter passingx86 always uses stack to pass function parameter, while x86-64 always uses registers to pass function if possible(as it has more registers than x86)

ESP (Extended Stack Pointer), always points to the top of the stack. RSP(x86-64)

EBP (Extended Stack Base pointer), like a stack boundary for a function, all stack variables are offset from it.RBP(x86-64), EBP bottom of stack which has the highest address.

push

decrease ESP(get a slot), save value to stack(slot, 4 bytes or 8 bytes etc)


pop

get the value, increase ESP





stack change when call a function on x86-64
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int foobar(int a, int b, int c)&#123;    int xx = a + 2;    int yy = b + 3;    int zz = c + 4;    int sum = xx + yy + zz;    return xx * yy * zz + sum;&#125;int main()&#123;    return foobar(77, 88, 99);&#125;

On x86-64, arguments passed into foobar() use registers, while local variables of that function, along with some other data, are going to be stored on the stack when foobar is called. This set of data on the stack is called a frame for this function. 


push IP into stack(called by CPU at runtime,you can not see this in assembly)
push rbp(parent) into stack
set rbp with rsp
pass parameter into register


after call foobar

push local variable into stack
calculate
pop rbp
pop IP

Note: One process just has one stack, not each for a function, but xx,yy,zz location may be different based on complier or os arch, before calling a function, CPU will put the next ins(IP) onto stack.
x86-64 assembly
4:a.c           **** int foobar(int a, int b, int c)  5:a.c           **** &#123; 11 0000 55       		pushq	%rbp 14 0001 4889E5   		movq	%rsp, %rbp 16 0004 897DEC   		movl	%edi, -20(%rbp)----&gt;get parameter from register, save to local stack 17 0007 8975E8   		movl	%esi, -24(%rbp) 18 000a 8955E4   		movl	%edx, -28(%rbp)  6:a.c           ****     int xx = a + 2; 20 000d 8B45EC   		movl	-20(%rbp), %eax 21 0010 83C002   		addl	$2, %eax 22 0013 8945F0   		movl	%eax, -16(%rbp)---&gt; save result to xx  7:a.c           ****     int yy = b + 3; 24 0016 8B45E8   		movl	-24(%rbp), %eax 25 0019 83C003   		addl	$3, %eax 26 001c 8945F4   		movl	%eax, -12(%rbp)---&gt; save result to yy  8:a.c           ****     int zz = c + 4; 28 001f 8B45E4   		movl	-28(%rbp), %eax 29 0022 83C004   		addl	$4, %eax 30 0025 8945F8   		movl	%eax, -8(%rbp)----&gt; save result to zz  9:a.c           ****     int sum = xx + yy + zz; 32 0028 8B55F0   		movl	-16(%rbp), %edx 33 002b 8B45F4   		movl	-12(%rbp), %eax 34 002e 01C2     		addl	%eax, %edx ---&gt; edx has result xx + yy 35 0030 8B45F8   		movl	-8(%rbp), %eax 36 0033 01D0     		addl	%edx, %eax 37 0035 8945FC   		movl	%eax, -4(%rbp)--&gt;save result to sum 10:a.c           ****  11:a.c           ****     return xx * yy * zz + sum; 39 0038 8B45F0   		movl	-16(%rbp), %eax 40 003b 0FAF45F4 		imull	-12(%rbp), %eax 41 003f 0FAF45F8 		imull	-8(%rbp), %eax 42 0043 89C2     		movl	%eax, %edx 43 0045 8B45FC   		movl	-4(%rbp), %eax 44 0048 01D0     		addl	%edx, %eax 12:a.c           **** &#125; ... 14:a.c           **** int main() 15:a.c           **** &#123; 58 004c 55       		pushq	%rbp-----&gt;save rbp 61 004d 4889E5   		movq	%rsp, %rbp--&gt;move rbp points to rsp 16:a.c           ****     return foobar(77, 88, 99); 64 0050 BA630000 		movl	$99, %edx----&gt;use register to pass parameter 64      00 65 0055 BE580000 		movl	$88, %esi 65      00 66 005a BF4D0000 		movl	$77, %edi 66      00 67 005f E8000000 		call	foobar 67      00 17:a.c           **** &#125; 68              		.loc 1 17 0 69 0064 5D       		popq	%rbp

function vs macroBoth are used to replace similar code that are always used in different places, macro makes writing code simple and easy to read, but the assembly code is not reduced after used macro while function has the same advantages as Macro does, and the assembly code is small, takes small disk space and small memory when loaded after used function. but the disadvantages is that calling a function takes some time if it&#39;s not a inline function. below are some general rules to obey when use function or macro.
rules to obey
large similar use function
 here I say similar not same, same code is easy to define a function, but similar code may have some trouble, similar code here the logical process is same, but the variable may be different TYPE, as function parameter only has one TYPE, so it’s not easy for this case, the solution for this is to abstract the same part for each of TYPE, access the same part of it in the function, if TYPES have no same parts(members), define each of a tinny function(different part depends on type)(void*) for each of the TYPE, call the tinny function in the common function, so that common function(with void*) can replace the &#39;similar code&#39; and easy to read and extend for new TYPES.


// A and B function have the same logical but only small part is different.void tiny_a(void *v) &#123;&#125;void A(int a) &#123;    common(a, type)&#125;void tiny_b(void *v) &#123;&#125;void B(char b) &#123;    common(b, type)&#125;void common(void* v, type) &#123;    //same logical    if (type == A) &#123;        tiny_a(v); //special for A    &#125;    if (type == B) &#123;        tiny_b(v); // special for B    &#125;    //same logical&#125;


small similar code two choices depending on the code
similar code accesses the same type of variable, use inline function(can use macro as well), 
similar code accessed the different types of variables use Macro



tips for using macro
you can pass parameter from one macro to another.

# used to get the string of “passed_value” in the body

##used to link parameter with other in the body

you can NOT use # and ## during parameter passing, there is no effect

do NOT define variable inside macro while used outside of it

macro is only effect from the beginning to the end of that file, but if you define it in *.h, as you know *.h will be copied to the file(*.c) who includes it


difference between ## and # in macro
// &#x27;#&#x27; used to get the string of the parameter, you can pass hello without &quot;&quot; but #msg return the value of &quot;hello&quot; !!!#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define print(msg) printf(&quot;msg: %s\n&quot;, #msg)int main() &#123;    print(hello);    print(&quot;hello&quot;);&#125;

msg: hello
msg: “hello”
file: a.c// &#x27;##&#x27; used to link tokens, normally, parameter will replace token in the body of macro// but how to identify a token in the body of macro// a token is separated by whitespace or ; or others(parameter) after C compiler parsed.#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define print(msg) \&#123;\    printf(&quot;%s\n&quot;, b_msg);\   // here msg is NOT replaced.    printf(&quot;%s\n&quot;, b_##msg);\ // msg is replaced with hello    printf(&quot;%s\n&quot;, b_##msg##_a);\&#125;int main() &#123;        print(hello);&#125;

root@dev:/tmp# gcc -o t a.ca.c: In function ‘main’:a.c:6:20: error: ‘b_msg’ undeclared (first use in this function)     printf(&quot;%s\n&quot;, b_msg);\                    ^a.c:12:9: note: in expansion of macro ‘print’         print(hello);         ^a.c:6:20: note: each undeclared identifier is reported only once for each function it appears in     printf(&quot;%s\n&quot;, b_msg);\                    ^a.c:12:9: note: in expansion of macro ‘print’         print(hello);         ^a.c:7:20: error: ‘b_hello’ undeclared (first use in this function)     printf(&quot;%s\n&quot;, b_##msg);\                    ^a.c:12:9: note: in expansion of macro ‘print’         print(hello);         ^a.c:8:20: error: ‘b_hello_a’ undeclared (first use in this function)     printf(&quot;%s\n&quot;, b_##msg##_a);\                    ^a.c:12:9: note: in expansion of macro ‘print’         print(hello);

ifdef&#x2F;if vs macromacro and ifdef are executed before compiling(**precompile**) but macro is more earlier than ifdef, so first taking replacement with macro, then ifdef.

how to identify a token(token separated by whitespace) that will be replaced by macro.
function name
parameter
lvalue&#x2F;rvalue



if Token is equal(same with macro parameter) with macro, replace it with macro!!!
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define HELLO hello#ifdef HELLO#define NUM 12#endifvoid hello(int a)&#123;    printf(&quot;hello %d \n&quot;, a);&#125;int main()&#123;    HELLO(NUM);    return 0;&#125;


what can be used as a condition in ifdef

literal object(integer operation), number 12 or char ‘H’ , if you use variable, its evaluation is always 0
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;//it&#x27;s ok as replacement take first like 12 &gt;10#define NUM 12#if NUM &gt; 10#define var1 10#endif//as we never define name so name is 0!! var3 is defined#if name!=&#x27;H&#x27;#define var3 30#endif//as we never define name and name1, so they always 0, var4 is defined#if name==name1#define var4 40#endifint main()&#123;    printf(&quot;%d %d %d %d\n&quot;, var1, var3, var4);    return 0;&#125;

Free memory when process exitsIt depends on the operating system. The majority of modern (and all major) operating systems will free memory when process ends.
But relying on this is bad practice and it is better free it explicitly. The issue isn’t just that your code looks bad. You may decide you want to integrate your small program into a larger, long running one. Then a while later you have to spend hours tracking down memory leaks.
Relying on a feature of an operating system also makes the code less portable.
Even if your OS (not all do that) frees memory at exit, there are some reasons to free it explicitly.

it’s good manner
it adds symmetry, so code looks better
if someone takes this code and place it in a program that uses the libraries only in a small part of his runtime, the resources should be free when not needed.
if you are looking for bad memory leaks, your debugger won’t find these unimportant ones.
OS does not automatically free some resources at exit, like devices (sensors, scanners…),  temporary files, shared memory.

Send file descriptor to another processFile descriptor can be sent only by AF_LOCAL or AF_UNIX, as during sending, Actually the fd number is not transmitted(it&#39;s local for that process), but file descriptor instance(file struct in kernel) is transmitted in skb(scm data), so that receiver can add the transfered file descriptor instance to its process with new fd in its scope, so that after transmission, the file descriptor can be accessed by sender and receiver, actually both linked to it, in most case, the sender will close the fd at its side, hence only receiver accessed that file descriptor.
Steps to take

Giving SOL_SOCKET to cmsg_level of a struct cmsghdr
Giving SCM_RIGHTS to cmsg_type of the struct cmsghdr
Placing the file descriptor at last of the struct cmsghdr
Assigning the struct cmsghdr to msg_control of a struct msghdr
Passing the struct msghdr to sendmsg(2) with the Unix domain socket

Using CMSG_DATA is easy to get a pointer to last of the struct cmsghdr for 3
Client
#include &lt;fcntl.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/un.h&gt;#include &lt;sys/wait.h&gt;#include &lt;sys/socket.h&gt;#define handle_error(msg)   \    do &#123;                    \        perror(msg);        \        exit(EXIT_FAILURE); \    &#125; while (0)static void send_fd(int socket, int* fds, int n) // send fd by socket&#123;    struct msghdr msg = &#123; 0 &#125;;    struct cmsghdr* cmsg;    char buf[CMSG_SPACE(n * sizeof(int))], dup[256];    memset(buf, &#x27;\0&#x27;, sizeof(buf));    struct iovec io = &#123; .iov_base = &amp;dup, .iov_len = sizeof(dup) &#125;;    msg.msg_iov = &amp;io;    msg.msg_iovlen = 1;    msg.msg_control = buf;    msg.msg_controllen = sizeof(buf);    cmsg = CMSG_FIRSTHDR(&amp;msg);    cmsg-&gt;cmsg_level = SOL_SOCKET;    cmsg-&gt;cmsg_type = SCM_RIGHTS;    cmsg-&gt;cmsg_len = CMSG_LEN(n * sizeof(int));    memcpy((int*)CMSG_DATA(cmsg), fds, n * sizeof(int));    if (sendmsg(socket, &amp;msg, 0) &lt; 0)        handle_error(&quot;Failed to send message&quot;);&#125;intmain(int argc, char* argv[])&#123;    int sfd, fds[1];    struct sockaddr_un addr;    if (argc != 2) &#123;        fprintf(stderr, &quot;Usage: %s &lt;file-name1&gt;\n&quot;, argv[0]);        exit(1);    &#125;    // use AF_LOCAL to transfer fd    sfd = socket(AF_UNIX, SOCK_STREAM, 0);    if (sfd == -1)        handle_error(&quot;Failed to create socket&quot;);    memset(&amp;addr, 0, sizeof(struct sockaddr_un));    addr.sun_family = AF_UNIX;    strncpy(addr.sun_path, &quot;/tmp/fd-pass.socket&quot;, sizeof(addr.sun_path) - 1);    fds[0] = open(argv[1], O_RDONLY);    if (fds[0] &lt; 0)        handle_error(&quot;Failed to open file 1 for reading&quot;);    else        fprintf(stdout, &quot;Opened fd %d in parent\n&quot;, fds[0]);    // connect with UNIX server to send    if (connect(sfd, (struct sockaddr*)&amp;addr, sizeof(struct sockaddr_un)) == -1)        handle_error(&quot;Failed to connect to socket&quot;);    send_fd(sfd, fds, 1);    fprintf(stdout, &quot;Sleep 1000s after sent fd\n&quot;);    sleep(1000);    exit(EXIT_SUCCESS);&#125;

Server
#include &lt;fcntl.h&gt;#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/un.h&gt;#include &lt;sys/wait.h&gt;#include &lt;sys/socket.h&gt;#define handle_error(msg)   \    do &#123;                    \        perror(msg);        \        exit(EXIT_FAILURE); \    &#125; while (0)static int* recv_fd(int socket, int n)&#123;    int* fds = malloc(n * sizeof(int));    struct msghdr msg = &#123; 0 &#125;;    struct cmsghdr* cmsg;    char buf[CMSG_SPACE(n * sizeof(int))], dup[256];    memset(buf, &#x27;\0&#x27;, sizeof(buf));    struct iovec io = &#123; .iov_base = &amp;dup, .iov_len = sizeof(dup) &#125;;    msg.msg_iov = &amp;io;    msg.msg_iovlen = 1;    msg.msg_control = buf;    msg.msg_controllen = sizeof(buf);    // get transferred file descriptor    if (recvmsg(socket, &amp;msg, 0) &lt; 0)        handle_error(&quot;Failed to receive message&quot;);    cmsg = CMSG_FIRSTHDR(&amp;msg);    memcpy(fds, (int*)CMSG_DATA(cmsg), n * sizeof(int));    return fds;&#125;intmain(int argc, char* argv[])&#123;    ssize_t nbytes;    char buffer[256];    int sfd, cfd, *fds;    struct sockaddr_un addr;    int i;    sfd = socket(AF_UNIX, SOCK_STREAM, 0);    if (sfd == -1)        handle_error(&quot;Failed to create socket&quot;);    if (unlink(&quot;/tmp/fd-pass.socket&quot;) == -1 &amp;&amp; errno != ENOENT)        handle_error(&quot;Removing socket file failed&quot;);    memset(&amp;addr, 0, sizeof(struct sockaddr_un));    addr.sun_family = AF_UNIX;    strncpy(addr.sun_path, &quot;/tmp/fd-pass.socket&quot;, sizeof(addr.sun_path) - 1);    if (bind(sfd, (struct sockaddr*)&amp;addr, sizeof(struct sockaddr_un)) == -1)        handle_error(&quot;Failed to bind to socket&quot;);    // listen on given address    if (listen(sfd, 5) == -1)        handle_error(&quot;Failed to listen on socket&quot;);    // get one connection    cfd = accept(sfd, NULL, NULL);    if (cfd == -1)        handle_error(&quot;Failed to accept incoming connection&quot;);    fds = recv_fd(cfd, 1);    for (i = 0; i &lt; 1; ++i) &#123;        fprintf(stdout, &quot;Reading from passed fd %d\n&quot;, fds[i]);        while ((nbytes = read(fds[i], buffer, sizeof(buffer))) &gt; 0)            // read file and write it to stdout            write(1, buffer, nbytes);        *buffer = &#x27;\0&#x27;;    &#125;    printf(&quot;sleep 1000 after read file\n&quot;);    sleep(1000);    free(fds);    if (close(cfd) == -1)        handle_error(&quot;Failed to close client socket&quot;);    return 0;&#125;

IPC
Pipe, socketpair, socket(AF_LOCAL)
signal, eventfd
shared memory, file, need lock

pipeOnly for parent process—-&gt;child process(can be used by different processes as well by send one fd to process), it’s stream-oriented

It’s a one-way communication mechanism, with one end opened for reading(fd[0]) and the other end for writing(fd[1]). Therefore, parent and child need to agree on which way to use the pipe, from parent to child or the other way around. A pipe is also a stream communication mechanism, that is all messages sent through the pipe are placed in order,when readers asks for a certain number of bytes from this stream, he is given as many as bytes as are available, up to the amount of request, Note that these bytes may have come from the same call to write() or from several calls to write() which are concatenated.
socketpairOnly for parent process&lt;—-&gt;child process, it can be stream or datagram

Two-way communication mechanism, an extension of pipe.
named pipeany process&lt;—-&gt;any process
int mkfifo(const char *pathname, mode_t mode)
A FIFO special file is similar to a pipe, except that it is created in a different way.  Instead of being an anonymous communications channel, a FIFO special file is entered into the filesystem by calling mkfifo().make sure delete it by your application(call remove() in your application, otherwise it’s left on system
One side
void main()&#123;    int fd, n;    char buf[BUFFSIZE];    mkfifo(&quot;fifo_x&quot;, 0666);    // create file on disk you can see it    //prw-r--r--   1 root root    0 Aug  5 16:50 fifo_x        if ( (fd = open(&quot;fifo_x&quot;, O_WRONLY)) &lt; 0)        err(&quot;open&quot;)    while( (n = read(STDIN_FILENO, buf, BUFFSIZE) ) &gt; 0) &#123;        if ( write(fd, buf, strlen(buf)) != strlen(buf)) &#123;             err(&quot;write&quot;);        &#125;    &#125;    close(fd);&#125;


the other
void main()&#123;    int fd, n;    char buf[BUFFSIZE];    if ( (fd = open(&quot;fifo_x&quot;, O_RDONLY)) &lt; 0)        err(&quot;open&quot;)    while( (n = read(fd, buf, BUFFSIZE) ) &gt; 0) &#123;        if ( write(STDOUT_FILENO, buf, n) != n) &#123;             exit(1);        &#125;    &#125;    close(fd);&#125;

Two-way communication mechanism.
AF_LOCAL(AF_UNIX) socketany process&lt;—-&gt;any process
Unix domain allows communication between any two processes that are in same machine.
signalany process&lt;—-&gt;any process within same machine
# send signal to process with kill() APIint kill(pid_t pid, int signal);
tiny, only signal number sent, can not take payload
eventfdit’s fd but used for event counter, parent&lt;—-&gt;child
between threads or between process to notify something happens by write a counter into kernel and walkup the reader to handle things.
uint64_t u;efd = eventfd(0, 0);switch(fork()) &#123;    case 0: //child        u = 10;        write(efd, &amp;u, sizeof(u));    default:       read(efd, &amp;u, sizeof(uint64_t));&#125;

tiny, can only take integer payload
share memoryAny process&lt;—&gt;any process on same machine, most of time a lock is needed.
system-v styleit uses key to identify the named shared memory, so that other process can access the shared memory, but can also without key for private memory for parent/child, as the shared memory is created before fork(), so both see it.
//one processkey_t key;key = 1000;shmget(key, SHMSZ, IPC_CREAT | 0666));// create shared memory with flag: IPC_CREAT//another processkey_t key;key = 1000;shmget(key, SHMSZ, 0666);//attach to the shared memory identified by key 1000


Private share memory parent&#x2F;child

shmget() with IPC_PRIVATE as the key
IPC_PRIVATE isn&#39;t a flag field but a key_t type.  If this special value is used for key, the system call ignores everything but the least significant 9 bits of shmflg and  creates a new shared memory segment (on success). The name choice IPC_PRIVATE was perhaps unfortunate, IPC_NEW would more clearly show its function. Both child and parent see the same memory, they always use same memory!!!
posix styleIntend to share memory between unrelated processes. One process creates the file, then other process opens it and maps it for sharing (think it as a normal file, actually it&#39;s an identifier of memory)
it uses special file to identify the share memory located at /dev/shm
//one processfd = shm_open(&quot;test&quot;, O_CREAT | O_RDWR, 0666);//create a file at /dev/shm/test, O_CREAT// set the shared memory sizeaddr = mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE,            MAP_SHARED, fd, 0);// the othershm_fd = shm_open(&quot;test&quot;, O_RDONLY, 0666);//attach to itaddr = mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE,                                MAP_SHARED, fd, 0);
As shm_open() creates a file descriptor, so user must call shm_unlink() to close it(close fd, decrease the reference counter) 
Warning
if process does not call shm_unlink, when fd is closed, reference count is not decreased!!! so the shared memory may be never freed at all.

share memory between parent&#x2F;child

without shm_open(), use MAP_ANONYMOUS flag and fd(-1) as parent&#x2F;child all know the shared memory
//before fork()    addr = mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE,                MAP_SHARED | MAP_ANONYMOUS, -1, 0);


for mmap() with MAP_PRIVATE

Create a private copy-on-write mapping.  Updates to the mapping are not visible to other processes mapping the same file, and are not carried through to the underlying file. It is unspecified whether changes made to the file after the mmap() call are visible in the mapped region
That means each process has its own copy of it if it writes to shared memory, other process does not see changed part!!!!
MultiplexingThe poll() API performs the similar API as the existing select() API. The only difference between these two APIs is the interface provided to the caller.
The select() API requires that the application pass in a set of bits in which one bit is used to represent each descriptor number(for example if we only create a socket with fd 1000, the first argument for select is 1000+1, and kernel will create 1001 structure in kernel and unnecessary loop!!!, even if we only create one socket). When descriptor numbers are very large, it can overflow the 30KB allocated memory size, forcing multiple iterations of the process. This overhead can adversely affect performance.

The poll() API allows the application to pass an array of structures rather than a set of bits. Because each pollfd structure can contain up to 8 bytes, the application only needs to pass one structure for each descriptor (we created), even if descriptor numbers are very large.

Both poll() and select() require the full list of file descriptors to watch on each invocation. The kernel must then walk the list of each file descriptor to be monitored. When this list grows large, it may contain hundreds or even thousands of file descriptors walking the list on each invocation becomes a scalability bottleneck.

select needs to set monitor fd for each call

select limits fd number 1024, not true for poll

even with one fd&#x3D;1000, select has to iterate from 0–1000 times, not true for poll

poll needs set monitor fd only once!!

poll no limits fd number

if only with one fd&#x3D;1000, poll iterates only once, it only iterates all the monitored fd to see if it has event.



epoll circumvents this problem by decoupling the monitor registration from the actual monitoring. One system call initializes an epoll context, another adds monitored file descriptors to or removes them from the context, and a third performs the actual event wait.

epoll needs set monitor fd only once!!
epoll no limits fd number
it only iterates the monitored fd which has event, fast.

selectOne API: select()
select(fdmax+1, &amp;read_fds, NULL, NULL, NULL)for(i = 0; i &lt;= fdmax; i++) &#123;    // check which one has read event&#125;// fdmax the max fd descriptor we monitors// read_fds: 1024 bit sets, set the fd bit if we want to monitor it.
For each loop

tell the max fd and fds that monitors
if event happens, check all from 0--max to know event on which fd(both in kernel and user)

The major disadvantages include:

select modifies the passed fd_sets so none of them can be reused. Even if you don’t need to change anything, such as if one of descriptors received data and needs to receive more data, a whole set has to be either recreated again or restored from a backup copy via FD_COPY. And this has to be done each time the select is called.

To find out which descriptors raised the events you have to manually iterate through all the descriptors in the set and call FD_ISSET on each one of them. When you have 2,000 of those descriptors and only one of them is active, likely, the last one, you’re wasting CPU cycles each time you wait.

Max number of supported fd 1024, Did I just mention 2,000 descriptors? Well, select cannot support that much. At least on Linux. The maximum number of the supported descriptors is defined by the FD_SETSIZE constant, which Linux happily defines as 1024. And while some operating systems allow you to hack this restriction by redefining the FD_SETSIZE before including the sys&#x2F;select.h, this is not portable. Indeed, Linux would just ignore this hack and the limit will stay the same.

Can not close fd if it&#39;s in select() now, You cannot modify the descriptor set from a different thread while waiting. Suppose a thread is executing the code above. Now suppose you have a housekeeping thread which decided that sock1 has been waiting too long for the input data, and it is time to cut the cord. Since this socket could be reused to serve another paying working client, the housekeeping thread wants to close the socket. However the socket is in the fd_set which select is waiting for. Now what happens when this socket is closed? man select has the answer, and you won’t like it. The answer is, “If a file descriptor being monitored by select() is closed in another thread, the result is unspecified”. 

Can&#39;t send data on fd if it&#39;s in select() now Same problem arises if another thread suddenly decides to send something via sock1. It is not possible to start monitoring the socket for the output event until select returns.

select puts extra burden on you when filling up the descriptor list to calculate the largest descriptor number and provide it as a function parameter.


is there is any reason to use select at all?

The first reason is portability. select has been around for ages, and you can be sure that every single platform around which has network support and nonblocking sockets will have a working select implementation while it might not have poll at all. And unfortunately I’m not talking about the tubes and ENIAC here; poll is only available on Windows Vista and above which includes Windows XP – still used by the whooping 34% of users as of Sep 2013 despite the Microsoft pressure. Another option would be to still use poll on those platforms and emulate it with select on those which do not have it; it is up to you whether you consider it reasonable investment.

The second reason is more exotic, and is related to the fact that select can – theoretically – handle the timeouts with in the one nanosecond precision, while both poll and epoll can only handle the one millisecond precision. This is not likely to be a concern on a desktop or server system, which clocks doesn’t even run with such precision, but it may be necessary on a realtime embedded platform while interacting with some hardware components. Such as lowering control rods to shut down a nuclear reactor – in this case, please, use select to make sure we’re all stay safe!


The case above would probably be the only case where you would have to use select and could not use anything else. However if you are writing an application which would never have to handle more than a handful of sockets (like, 200), the difference between using poll and select would not be based on performance, but more on personal preference or other factors.
pollOne API: poll()
poll(ufds, 2, 3500);/* ufds     array of fds monitor * 2        the array size (how many fds we want to monitor) * 3500     3.5s */

For each loop

tell only the fds monitors not max fd as select does.
if event happens, check all monitors to see event on which fd(both in kernel and user), iterate all and compare to find the fd that has events.

poll was mainly created to fix the pending problems select had, so it has the following advantages over it:

There is no hard limit on the number of descriptors poll can monitor, so the limit of 1024 does not apply here.

It does not modify the data passed in the struct pollfd data. Therefore it could be reused between the poll() calls as long as set to zero the revents member for those descriptors which generated the events. The IEEE specification states that “In each pollfd structure, poll() shall clear the revents member, except that where the application requested a report on a condition by setting one of the bits of events listed above, poll() shall set the corresponding bit in revents if the requested condition is true“. However in my experience at least one platform did not follow this recommendation, and man 2 poll on Linux does not make such guarantee either (man 3p poll does though).

It allows more fine-grained control of events comparing to select. For example, it can detect remote peer shutdown without monitoring for read events.


poll still has a few issues which need to be kept in mind:

Like select, it is still not possible to find out which descriptors have the events triggered without iterating through the whole list and checking the revents. Worse, the same happens in the kernel space as well, as the kernel has to iterate through the list of file descriptors to find out which sockets are monitored, and iterate through the whole list again to set up the events.

Like select, it is not possible to dynamically modify the set or close the socket which is being polled


poll should be your preferred method even over epoll if the following is true:

You need to support more than just Linux, and do not want to use epoll wrappers such as libevent (epoll is Linux only);

Your application needs to monitor less than 1000 sockets at a time (you are not likely to see any benefits from using epoll);

Your application needs to monitor more than 1000 sockets at a time, but theconnections are very short-lived (this is a close case, but most likely in this scenario you are not likely to see any benefits from using epoll because the speedup in event waiting would be wasted on adding those new descriptors into the set – see below)

Your application is not designed the way that it changes the events while another thread is waiting for them (i.e. you’re not porting an app using kqueue or IO Completion Ports).


epollTwo APIs: epoll_ctl() and epoll_wait()
For each loop

no need to tell fds monitors as kernel keeps it from another API
if event happens, no need to check to see event on which fd, as epoll() returns only the fd that has events.

epoll has some significant advantages over select/poll both in terms of performance and functionality:

epoll returns only the list of descriptors which triggered the events. No need to iterate through 10,000 descriptors anymore to find that one which triggered the event!

You can attach meaningful context to the monitored event instead of socket file descriptors. In our example we attached the class pointers which could be called directly, saving you another lookup.

You can add sockets or remove them from monitoring anytime, even if another thread is in the epoll_wait function. You can even modify the descriptor events. Everything will work properly, and this behavior is supported and documented. This gives you much more flexibility in implementation.

Since the kernel knows all the monitoring descriptors, it can register the events happening on them even when nobody is calling epoll_wait. This allows implementing interesting features such as edge triggering

It is possible to have the multiple threads waiting on the same epoll queue with epoll_wait(), something you cannot do with select&#x2F;poll. In fact it is not only possible with epoll, but the recommended method in the edge triggering mode.


epoll is not a “better poll”, and it also has disadvantages when comparing to poll:

Changing the event flags (i.e. from READ to WRITE) requires the epoll_ctl syscall, while when using poll this is a simple bitmask operation done entirely in userspace. Switching 5,000 sockets from reading to writing with epoll would require 5,000 syscalls and hence context switches (as of 2014 calls to epoll_ctl still  could not be batched, and each descriptor must be changed separately), while in poll it would require a single loop over the pollfd structure.

Each accepted socket needs to be added to the set, and same as above, with epoll it has to be done by calling epoll_ctl which means there are two required syscalls per new connection socket instead of one for poll. If your server has many short-lived connections which send or receive little traffic, epoll will likely take longer than poll to serve them.

epoll is exclusively Linux domain, and while other platforms have similar mechanisms, they are not exactly the same ,edge triggering, for example, is pretty unique (FreeBSD’s kqueue supports it too though).

High performance processing logic is more complex and hence more difficult to debug, especially for edge triggering which is prone to deadlocks if you miss extra read&#x2F;write.


Therefore you should only use epoll if all following is true:

Your application runs a thread poll which handles many network connections by a handful of threads. You would lose most of epoll benefits in a single-threaded application, and most likely it won’t outperform poll.

You expect to have a reasonably large number of sockets to monitor (at least 1,000); with a smaller number epoll is not likely to have any performance benefits over poll and may actually worse the performance;

Your connections are relatively long-lived; as stated above epoll will be slower than poll in a situation when a new connection sends a few bytes of data and immediately disconnects because of extra system call required to add the descriptor into epoll set;

Your app depends on other Linux-specific features (so in case portability question would suddenly pop up, epoll wouldn’t be the only roadblock), or you can provide wrappers for other supported systems. In the last case you should strongly consider libevent.


epoll Triggering modes
epoll provides both edge-triggered and level-triggered modes. In edge-triggered mode, a call to epoll_wait will return only when a new event is enqueued with the epoll object, you should receive all data when event happens, otherwise, next call epoll_wait will block if no new data,  while in level-triggered mode, epoll_wait will return as long as the condition holds.
For instance, if a pipe registered with epoll has received data, a call to epoll_wait will return, signaling the presence of data to be read. Suppose, the reader only consumed part of data from the buffer. In level-triggered mode, further calls to epoll_wait will return immediately, as long as the pipe’s buffer contains data to be read. In edge-triggered mode, however, epoll_wait will return only once new data is written to the pipe.
edge-triggered mode must read them all if read event happens

how do I know all data is read for this time(read event)?

n = recv(fd, buf, buflen)if (n == 0) &#123;    //end of file    close(fd);&#125;if (n == -1) &#123;   if (error == EAGAIN) &#123;   //no data to read, !!!read them all!!!   &#125; else (error == EINTR) &#123;   //interrupted when block   &#125; &#125;if (n &lt; buflen) &#123;// !!!read them all!!!&#125;

Multi-threadpthread_cleanup_pop and pthread_cleanup_pushAs multi-threads share the address namespace, if a resources is shared by multi-thread for writing, a mutex lock is needed for this, something like this in all threads, but in some case one thread may call pthread_cancel() to abort another thread, if that thread just gets lock but did not run unlock, it will be terminated, no chance to run unlock, hence causes other threads waiting on the lock which will never be freed,deadlock. so we can add a callback that will be called even abort signal  by pthread_cleanup_pop and pthread_cleanup_push.
pthread_mutex_lock(&amp;mutex);// do_something()pthread_mutex_unlock(&amp;mutex);

pthread_cleanup_push added a callback that will be called even receives abort signal, so we can unlock the mutex inthe callback.
the callback executes in three cases.

The thread exits (that is, calls pthread_exit()).
The thread acts upon a cancellation request.
The thread calls pthread_cleanup_pop() with a non-zero execute argument

// void pthread_cleanup_push(void (*routine)(void*), void *arg); pthread_cleanup_push(some_clean_func,...)pthread_mutex_lock(&amp;mutex);// do_somethingpthread_mutex_unlock(&amp;mutex);pthread_cleanup_pop(0); # remove the callback, never call it.

pthread_joinIn multiple threads, one thread may depend on another thread, for instance, A must execute after B quits, or before A quits we must quit B firstly, that’s why pthread_join(wait another thread exits, blocked caller) comes in. one typical case:
In main thread, we create a new thread, if no pthread_join, the two threads may run across, if main thread quits before the new thread (new thread still some work to do), the left work can’t be done, because when the main thread quits, all resources will be freed by OS, the new thread will exit as well!
#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;void *start(void *str)&#123;    int i;    for (i = 0; i &lt; 10; ++i)    &#123;        sleep(2);        printf( &quot;This in the thread : %d\n&quot; , i );    &#125;    return NULL;&#125; int main()&#123;    pthread_t pth;    int i;    int ret = pthread_create(&amp;pth, NULL, start, (void *)(i)); //start to run now    pthread_join(pth, NULL); //wait until thread exits    for (i = 0; i &lt; 10; ++i)    &#123;        sleep(1);        printf( &quot;This in the main : %d\n&quot; , i );    &#125;         return 0;&#125;// If without pthread_join(pth, NULL), the output is unpredictable!!!! Because you// don’t know when the main thread will exit.

thread-safe vs multi-safe vs signal-safelockless queue&#x2F;stackIn order to achieve lockless, there are two main points.

use CMPXCHG(x86) directive atomic operation
optimize your code and data structure, to compress your code to access critical area in one statement or check and set flags in one ins, then access critical area to block from user level.

what does CMPXCHG do
//simulate it in C codeint val_compare_and_swap ( int *memory_location, int expected_value, int new_value) &#123;   int old_value = *memory_location;  if (old_value == expected_value)     *memory_location = new_value;  return old_value;&#125;bool bool_compare_and_swap ( int *memory_location, int expected_value, int new_value) &#123;   int old_value = *memory_location;  if (old_value == expected_value) &#123;     *memory_location = new_value;      return true;  &#125;  return false;&#125;

Thanks to gcc, we no need to write assembly code to do it, as it provides us two functions with written in assembly(CMPXCHG)
GCC built-in
bool __sync_bool_compare_and_swap (type *ptr, type oldval, type newval, ...)type __sync_val_compare_and_swap (type *ptr, type oldval, type newval, ...)


lockless stack
struct node &#123;    int val;    struct node *next;&#125;;struct list &#123;    struct node *head;    void push(int val) &#123;        struct node *p = malloc(sizeof(struct node));        if (p) &#123;            p-&gt;val = val;            while(1) &#123;                p-&gt;next = head;                //check if head is changed by another thread, if not switch                //otherwise, do another loop.                if (__sync_bool_compare_and_swap(&amp;head, p-&gt;next, p)) &#123;                    break;                &#125;            &#125;        &#125;    &#125;&#125;;

glibc pthread mutex Semaphores and futexBefore introduce futex(fast userspace mutex), let’s see how pthread_mutex implement in earlier days, when you call pthread_mutex_lock(), it calls another sytem call whick will do below things in kernel. 

check if the lock available 
if unavailable, sleep the caller.

what’s the problem with such solution? let’s say if the lock is not used frequently, we still need to go to kernel to check and get the lock. but for check and get the lock we can move it to user space, so that if lock is not used highly, we do not need to go to kernel as no need to sleep the process which must be done by kernel. this is the core concept that futex does, futex includes two parts one part is in glibc that checks the lock, the other part is in kernel when lock not available, call a system api(futex()) to sleep the caller, futex can also wake a process based on parameter
//futex 的逻辑可以用如下C语言表示int val = 0;void lock()&#123;    int c    if ((c = cmpxchg(val, 0, 1)) != 0) &#123;        if (c != 2)            c = xchg(val, 2);        while (c != 0) &#123;            futex_wait((&amp;val, 2);            c = xchg(val, 2);        &#125;    &#125;&#125;       void unlock()&#123;       if (atomic_dec(val) != 1)        futex_wake(&amp;val, 1);&#125;

futex(uaddr, FUTEX_WAKE, 1)
futex(uaddr, FUTEX_WAIT, 1)
From kernel 2.5.7(2002), glic uses futex to implement pthread_mutex and semaphores, pthread_join() also calls futex to sleep the caller
get thread id#include &lt;sys/types.h&gt;#include &lt;sys/syscall.h&gt;    pid_t tid = syscall(SYS_gettid);printf(&quot;%d&quot;, tid);

conditionCondition used like this
//thread 1:    pthread_mutex_lock(&amp;mutex);    while (!condition) // condition is var protected by mutex while cond is pthread_cond_t        pthread_cond_wait(&amp;cond, &amp;mutex); // it&#x27;s always block!!! and mutex is freed        // when wake up next time mutex is gained again!!!    /* do something that requires holding the mutex and condition is true */    pthread_mutex_unlock(&amp;mutex);thread2:    pthread_mutex_lock(&amp;mutex);    /* do something that might make condition true */    pthread_cond_signal(&amp;cond);    pthread_mutex_unlock(&amp;mutex);

Multi-processwait() vs waitpid()Always call wait in parent process, otherwise child process will never die(destroyed by os), without wait() call, exited child is in zombie state, memory is freed, but task struct is not freed.
zombie process
When a process exits, OS frees all memory for it and closes all fds, only left task_struct which has meta data of exited process like process id, exit state, cpu usage etc, then OS sends SIGCHILD to its parent, before parent call wait or waitpid, the exited process is a zombie, if parent never calls wait&#x2F;waitpid, it’s always zombie, after parent calls wait&#x2F;waitpid, the only left task_struct is freed.
The wait() system call suspends execution of the current process until one of its children terminates.  The call wait(&amp;status) is equivalent to: waitpid(-1, &amp;status, 0), but waitpid support non-block mode if no child exited by setting option.

WNOHANG     return immediately if no child has exited.

The waitpid() system call suspends execution of the current process until a child specified by pid argument has changed state(terminated or stopped).
waitpid(pid, &amp;status, WIFEXITED)
waitpid(pid, &amp;status, WIFSTOPPED)

wait() api
zombie process

Always use waitpid() as it’s powerful,it can return when [any one] or ]particular one] or ]no one exited] while wait() always block, wakeup only when any child quits
execve()exec() familiy executes the program referred to by pathname.  This causes the program that is currently being run by the calling process to be replaced with a new program, with newly initialized stack, heap, and (initialized and uninitialized) data segments, except below:

File descriptors open except for those whose close-on-exec flag (FD_CLOEXEC) is set
Process ID
Parent process ID
Process group ID
Session membership
Real user ID
Real group ID
Current working directory
File mode creation mask
Process signal mask
Pending signals

In short
You can think it roughly all are replaced except keeping the file descriptors, signal, pid, uid so make sure close these opened file, there are two ways to do this.

One way

Add flag when open a file(socket) with O_CLOEXEC


The other way

after open, use fcntl() to set a flag FD_CLOEXEC



demo for each function
Mix process and thread, call fork in thread?Should never call fork in multi-thread as there are lots of non-obvious problems that you can’t detect them easily.
The non-obvious problem in this approach(mix them) is that at the moment of the fork(2) call some threads may be in critical sections of code, doing non-atomic operations protected by mutexes. In the child process the threads just disappears and left data half-modified without any possibility to “fix” them, there is no way to say what other threads were doing and what should be done to make the data consistent. Moreover: state of mutexes is undefined, they might be unusable and the only way to use them in the child is to call pthread_mutex_init() to reset them to a usable state. It’s implementation dependent how mutexes behave after fork(2) was called. On my Linux machine locked mutexes are locked in the child.
malloc, syslog(), printf() use lock internally, be careful with them in multi-thread or multi-process.
More details, refer to think-twice-before-using-tem
pipe vs pipe2 vs socketpair and dup vs dup2pipe(fd[2])&#x2F;pipe2(fd[2], flag) create a unidirection channel with two fds(fd[0] for read, while fd[1] for write), pipe2 gives more control when creating the channel like set NON_BLOCK etc.
pipe2() is to avoid race conditions by taking the O_CLOEXEC | O_NONBLOCK, https://man7.org/linux/man-pages/man2/open.2.html
socketpair(domain, proto, fd[2]) is similar to pipe, actually, it&#39;s extension of pipe, it&#39;s two-way communication.
dup(oldfd)&#x2F;dup2(oldfd, newfd) make duplicate the fd, hence dup2 lets you provide the new fd while dup() pick the smallest unused fd
dup(oldfd) will duplicate the oldfd, the return value is new fd(dup always pick the smallest fd unused)
dup2(oldfd, newfd) takes two fd, oldfd must be created before dup2, it will use newfd points to oldfd, if newfd is open, it will silently close it before reuse it.
typical use for pipe
parent creates a pipe, then fork child, child&#x2F;parent use pipe for one-way communication.
int fd[2];pipe(fd);int pid = fork();if(pid) &#123;    //parent read    close(fd[1]);    read(fd[0]);&#125; else &#123;    //child write    close(fd[0]);    write(fd[1]);&#125;


typical use for dup&#x2F;dup2 along with pipe
int fd[2];pipe(fd);int pid = fork();if (pid) &#123;    //parent read    close(fd(1));    read(fd(0));    close[fd[0]);&#125; else &#123;    //child write    close(fd[0])    //redirect STDOUT, hence parent will get the output from execv!!!    dup2(fd[1], STDOUT_FILENO);    execlp(&quot;grep&quot;, &quot;grep&quot;, &quot;pipe&quot;,NULL);    close(fd[1]);&#125;

little-endian and big-endian“Little Endian” means that the low-order byte of the number is stored in memory at the lowest address, and the high-order byte at the highest address. (The little end comes first.) For example, a 4 byte Int
`Byte3 Byte2 Byte1 Byte0`

will be arranged in memory as follows: 
Base Address+0   Byte0Base Address+1   Byte1Base Address+2   Byte2Base Address+3   Byte3

Intel processors (those used in PC’s) use “Little Endian” byte order.“Big Endian” means that the high-order byte of the number is stored in memory at the lowest address, and the low-order byte at the highest address. (The big end comes first.) Our LongInt, would then be stored as: 
Base Address+0   Byte3Base Address+1   Byte2Base Address+2   Byte1Base Address+3   Byte0
Motorola processors (those used in Mac’s) use “Big Endian” byte order.
daemonizeTo daemonize, there are two ways to do it.

Make itself as daemon1. parent -&gt; fork (child)2. parent exit3. init take orphan child                                                   
It’s daemon, want to daemonize its child(forked from me)1. parent -&gt;fork(child)2. child -&gt; fork(grandchild)3. child exit4. init take orphan grandchild

gccgcc provides lots of built-in keywords like inline, typeof etc for better use.
Note: while -ansi and the various -std options disable certain keywords (asm, inline typeof) in such case, use__asm__, __inline__, __typeof__ when -ansi or -std is enable during compiling.
typeofget type of variable then define internal variable in Macro, this is what ‘typeof’ is always used.
//a and b are two pointerstypeof(int*) a, b;//a is a pointer while b is inttypeof(int) *a, b;


inlineGCC does not inline any functions without optimizing(-o0) unless you specify the ‘always_inline’ attribute for the function.

suggest inline, inline or not determined by compilerinline void foo() &#123;&#125;
force inlineinline void foo()__attribute__((always_inline)) &#123;&#125;

Note: inline keyword is added at function definition not declaration
What does compiler do if it inlines that function?
Actually for a inline function, compiler will repace the function call with extended code, that means there is no function call happens for inline function, fast but with more code, but there are exceptions here, if we assign inline function to a function pointer, compiler will create function(even it’s inline) for it with an address, otherwise, if this no function in assemble code for inline function. 
inline in dot c file
if you define inline function in dot c and want to call it in another dot c, compiler will create a function address for inline function as well, another dot c sees it as a normal function, but in the file it defines, it&#39;s a inline. function.
inline in dot h file
if you define it in dot h, as dot h will be copied to the c file who includes it, so that there are several copied of this inline function like it&#39;s defined in that dot c file so it&#39;s inlined for dot c file. 
For short shared function, define it as inline in dot h file with static keyword
static used in headerOne important thing that must know is that for header file it will be copied to dot c file who includes it. that means if you static int b = 12 in a header file, if it’s included by two dot c files, it has the same effect that two dot c files define static int b = 12 for themself.
//File q7a.h:static int err_code = 3;void printErrCode(void);//File q7a.c:#include &lt;stdio.h&gt;#include &quot;q7a.h&quot;void printErrCode(void)&#123;    printf (&quot;%d\n&quot;, err_code);&#125;//File q7main.c:#include &quot;q7a.h&quot;int main(void)&#123;    err_code = 5;    printErrCode();    return 0;&#125;
$gcc –Wall –c q7a.c –o q7a.o$gcc –Wall –c q7main.c –o q7main.o$gcc q7main.o q7a.o –o q7$./q7the output is 3

attribute of function__attribute__ can be used only in declaration! here only list common attributes that may be used in your daily life.

deprecated: indicate you’d better not use me as I’m deprecated, if used, warning will appear&#x2F;
constructor: run before main
destructor:  run after main
section: specify the section that I’m defined
warn_unused_result: warning if result is not used
noinline: prevent me from being considered for inlining.
always_inline: force inline
ununsed: prevent printing warning if not used.

void fun() __attribute__((deprecated));
attribute of variable
aligned: This attribute specifies a minimum alignment for the variable or structure field, measured in bytes, if type default alignment is large than this value, use default. alignment&#x3D;max(default, aligned)


Whenever you leave out the alignment factor in an aligned attribute specification, the compiler automatically sets the alignment for the declared variable or field to the largest alignment which is ever used for any data type on the target machine you are compiling for. Doing this can often make copy operations more efficient, because the compiler can use whatever instructions copy the biggest chunks of memory when performing copies to or from the variables or fields that you have aligned this way.The aligned attribute can only increase the alignment; but you can decrease it by specifying packed as well


packed: The packed attribute specifies that a variable or structure field should have the smallest possible alignment–one byte for a variable, and one bit for a field, alignment&#x3D;min(default, packed)
deprecated: see function
section: see function
unused: see function

attribute of type
aligned: same as variable
packed: specifying this attribute for struct and union types is equivalent to specifying the packed attribute on each of the structure or union members

struct T1&#123;    char a;    short b;    char c;    int d __attribute__((packed));&#125;;//sizeof(struct T1)==10    struct T2&#123;    char a;    short b;    char c;    int d;&#125; __attribute__((packed)); // like each member __attribute__((packed))//sizeof(struct T2)==8

deprecated: see variable.

typedef int T1 __attribute__((deprecated));
built-in function
the most&#x2F;less significant bit

#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main()&#123;	unsigned int val = 0b1100; // binary	printf (&quot;there are %d zero before the fist bit 1 for 1100\n&quot;,__builtin_clz (val));	printf (&quot;there are %d zero after the fist bit 1 for 1100\n&quot;,__builtin_ctz (val));	return 0;&#125;

tipsshould add void to function parameter who has no parameter?Yes, it’s better do that as if compiling with -Werror=strict-prototypes, it will show error if function did not add void to function that does not need parameter.
void hello(void);
Eliminate unused warningIn some case, you do not want to comment unsed variable out , but avoid compiling warning, or avoid warning for unused return value.

(void)unsed_var;
(void)function();

use do{}while(0) if want to use block(var) in macro// group macro in block with do/while, limit var scope if defined in macro// to avoid conflict with caller.#define hello(msg) \do &#123; \    int a = 12; \    printf(&quot;%d %s&quot;, a, msg); \&#125; while(0) \int main() &#123;    hello(&quot;boy&quot;);    return 0;&#125;

dynamic argument in macroWhen the macro is invoked, all the tokens in its argument list […], including any commas, become the variable argument. This sequence of tokens replaces the identifier VA_ARGS in the macro body wherever it appears.
#define ENUMS(name, ...) \    static const char *name[] = &#123;__VA_ARGS__&#125;;int main() &#123;    ENUMS(tests, &quot;a&quot;, &quot;b&quot;)    printf(&quot;%s, %s\n&quot;, tests[0], tests[1]);\&#125;

see assembly with source codegcc -c -g -Wa,-a,-ad a.c &gt; test.lst
show convertion warning when compile codegcc -Wconversion
cacheline alignedIn some case, we want a variable(struct) cache line aligned, so that CPU can read them once(cpu read memory cache line every time), so it’s better put related field(if access a, will access b soon, a, b are related) together in a cache line, check struct layout with pahole
# install it from rpm or from source with below link$ yum install dwarve# usage$ cat test.c#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdbool.h&gt;struct T1&#123;    char a;    short b;    char c;    int d __attribute__((packed));&#125;;int main() &#123;    struct T1 t;    return 0;&#125;# must compile it with -g option.$ gcc -o test -g test.c# show all structs$ pahole -V test# show particular one$ pahole -V -C T1 teststruct T1 &#123;        char                       a;                    /*     0     1 */        /* XXX 1 byte hole, try to pack */        short int                  b;                    /*     2     2 */        char                       c;                    /*     4     1 */        int                        d;                    /*     5     4 */        /* size: 10, cachelines: 1, members: 4 */        /* sum members: 8, holes: 1, sum holes: 1 */        /* padding: 1 */        /* last cacheline: 10 bytes */&#125;;

Two typical cache line size: 32 bytes or 64 bytes.
# get cache line size of given cpu$cat /sys/devices/system/cpu/cpu0/cache/index1/coherency_line_size $getconf LEVEL1_DCACHE_LINESIZE

bit shiftMost of time, bit shift for unsigned integer.
left shift: always pad with 0  
right shift:

unsigned operator, padding with 0
signed operator, padding with flag bit(for positive 0, 1 for negative)

send&#x2F;recv  API differencespairs of API for sending &#x2F;recving data.
&#39;connected&#39; socket  can be [STREAM, DGGRAM who called connect()]

read&#x2F;readv: read one buffer&#x2F;several buffers on fd [file like API]
write&#x2F;writev: write with one buffer&#x2F;several on ‘connected fd [file like API]
recv&#x2F;send: with one buffer on ‘connected’ socket
recvfrom&#x2F;sendto: with on buffer on ‘unconnected’ socket
recvmsg&#x2F;sendmsg: with several buffers on ‘connected’ socket

In the kernel side, all mapped to sendmsg and recvmsg of kernel version.
STREAM vs DatagramStream communication implies several things, communication takes place across a connection between two sockets, the communication is reliable, error-free, and no message boundaries are kept, reading from a stream may result in reading the data sent from one or several calls to write() or only part of the data from single call, if there is not enough room for the entire message, or if not all data from a large message has been transferred, the protocol implementing such a style will retransmit message received with errors, it will also return error if one tries to send a message after the connection has been broken.
Datagram communication doesn’t use connection, each message is addressed individually, if the address is correct, it will generally be received, although this is not guaranteed, the individual datagram will be kept separate when they are read, that is message boundaries(from user level not from kernel level) are preserved.
TCP-----byte--------stream-----send receive---------------------in order --- -------------may partialUDP----message------datagram—--send—receive---------------------not guarantee(may lost)---full messageLike TCP:  sender: AA  BB----recver: A   AB    B	 UDP:  sender: AA  BB---recver:  BB        AA


when call sendto(), there are two limitation, one is the socket sendbufer, the other is 
MAX_MESSAGE_SIZE(udp payload)==65507==65535(max ip length)-20(ip header)-8(udp header) but ip packet will may be fragment due to MTU(1500), so you can send max udp message(not include udp header) is 65507, fragment always happens at IP layer for UDP if possible(not happend when GSO is enabled, in that case, IP fragment happens when sending to driver after go through network stack), but for TCP, there is no IP fragment at sender machine as TCP already split into smaller message before call IP layer(GSO/TSO is another case), but if socket send buffer is not enough to hold it, sendto() will blocked until kernel send some message and free the room.
TCP bi-directionFor a tcp connection, it’s bi-direction, that means both sides can send and recv data from the other, but you can close one direction, to make one side as reader, the other side as writer only. shutdown() allows you to only stop data transmission in a certain direction, while data transmission in one direction continues. If you can close the write operation of a socket and allow to continue to accept data on the socket until all data is read.
More control about TCP close
int shutdown (int sockfd, int how);There are three ways of how:SHUT_RD (0): Turn off the read function on sockfd. This option will not allow sockfd to read.SHUT_WR (1): Turn off the write function of sockfd, this option will not allow sockfd to write.SHUT_RDWR (2): Turn off the read and write function of sockfd.

add a wrapper for a function who defined in dynamic librarythis is used when need to add a wrapper for a function that’s defined in dynamic library(so that you don’t see the source code of it!!) like C dynamic library or other dynamic library (dynamic library!!!!!!!!)

step 1: create you own dynamic library   1.1 program wrapper library   void* malloc(size_t size)   {       void ret;       static void (realmalloc(size_t size)) &#x3D; NULL;       if (realmalloc &#x3D;&#x3D; NULL) {           realmalloc &#x3D; dlsym(RTLD_NEXT, “malloc”); &#x2F; function that will be wrapped       }       ret &#x3D; realmalloc(size);       !!!!add trace here!!!!!!!!!!!!       return ret;   }   1.2 compile it into wrapper.so

step 2: create your program like before, see nothing happens   ptr &#x3D; malloc(64);

setp 2: load it, must load your dynamic library before any other dynamic libray!!!  LD_PRLOAD&#x3D;.&#x2F;wrapper.so .&#x2F;program  (wrapper.so is your dynamic library that wrapps   the dynamic library(like C dynamic library)


NULL vs 0when evaluate the value of NULL, it’s 0, but if you assign NULL to non-pointer, it&#39;s a warning, but a has value 0!
int a = NULL? 1: 2; //now a is 2!!!if (NULL == 0) &#123;    printf(&quot;equal&quot;);//printed&#125;

function pointervoid hello(void(*h)())&#123;    // h is function pointer var    h();&#125;// BY is a function pointer typetypedef void (*BY)();

sizeof vs strlenstrlen returns memory byte size when it sees &#39;\0&#39; while sizeof calculates the size of bytes that the variable takes.
char *p = &quot;a&quot;; //sizeof(p)==8 (x86-64), strlen(p)==1int p[] = &#123;1, 2, 3&#125;; //sizeof(p)==12 even p is a pointer!!!

// char *p and char p[] behave same when used in parameter.void test(char p []) // same as char *p!!!&#123;    printf(&quot;%d\n&quot;, sizeof(p));&#125;

what about char p[0] in a structstruct prot &#123;    int a    char p[0]; //always as a label for dynamic size allocated after a!!! take no memory    // struct prot * p = malloc(sizeof(struct prot) + 100);&#125;;sizeof(struct prot) == 4!!!

what about function in a structstruct prot &#123;    int a    int hello() &#123;printf(&quot;hello&quot;);&#125; //define a function inside a struct&#125;;sizeof(struct prot) == 4!!!

shared variable between source(global variable)
define it at xxx.c

int global_v = 10;

export it at xxx.h others can include such header, like other x.c written with extern int global_v as well.extern int global_v;

Can we return a local pointer variable from a functionActually you should NOT do this, as it’s not a good way as if local pointer points to memory on the stack after function call, it will be freed!!!
char * f1 ()&#123;    char * p = &quot;abc&quot;;    return p; // as &#x27;abc&#x27; is literal, so its memory is not on stack!!&#125;char * f2 ()&#123;    char p[] = &quot;cde&quot;;    return p;&#125;int main()&#123;    char *str;    str = f1 ();    printf (&quot;%s\n&quot;, str); //we still see right value as str points to the memory which is not on stack!!!    str = f2 ();    printf (&quot;%s\n&quot;, str); //segment fault!    return 0;&#125;

type length and overflowchar c = 0x40; //64char x = 64;char b = 2;printf(&quot;%d\n&quot;,(char)(x*b)); // the highest bit is flag(0(+), 1(-1)), result: -128printf(&quot;%d\n&quot;, c&lt;&lt;3);       // result: 512 as %d is used, like int tmp = c&lt;&lt;3printf (&quot;%d\n&quot;, (char)(c&lt;&lt;3)); //truncted result: 0

what does volatile really meanvolatile is a keyword to prevent compiler optimizing(like cache value, reorder etc) for the described variable. its value can be changed by code outside the scope of current code at any time. The system always reads the current value of a volatile object from the memory location rather than keeping its value in temporary register at the point it is requested, even if a previous instruction asked for a value from the same object
it may be used to

describe an object corresponding to a memory-mapped input&#x2F;output port(driver also does)
global object accessed by an asynchronously interrupting function(ISR, signal handler), ISR(Signal handler) + Thread
global object accessed by two threads Thread + Thread

// test.c#include &lt;pthread.h&gt;#include &lt;signal.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;volatile int flag = 1;void thread1_func(void* args)&#123;    printf(&quot;child thread %lu  is running \n&quot;, pthread_self());    while (1) &#123;        if (!flag) &#123;            printf(&quot;break happen, thread %lu exiteds\n&quot;, pthread_self());            break;        &#125;    &#125;&#125;void thread2_func(void* args)&#123;    printf(&quot;child thread %lu  is running \n&quot;, pthread_self());    while (1) &#123;        sleep(10);        printf(&quot;change flag to 0 in thread\n&quot;);        flag = 0;        break;    &#125;&#125;void install_signal(int signo, void (*handler)(int signo))&#123;    struct sigaction act;    bzero(&amp;act, sizeof act);    sigfillset(&amp;act.sa_mask);    act.sa_handler = handler;    if (sigaction(signo, &amp;act, NULL) != 0) &#123;        printf(&quot;install signal failed&quot;);    &#125;&#125;void sig_ctrl_c(int signo)&#123;    printf(&quot;thread %lu  received sigint \n&quot;, pthread_self());    printf(&quot;change flag to 0 in signal handler\n&quot;);    flag = 0;&#125;int main()&#123;    pthread_t id1;    pthread_t id2;    install_signal(SIGINT, sig_ctrl_c);    if (pthread_create(&amp;id1, NULL, (void*)thread1_func, NULL) != 0) &#123;        printf(&quot;create thread failed\n&quot;);    &#125;    if (pthread_create(&amp;id2, NULL, (void*)thread2_func, NULL) != 0) &#123;        printf(&quot;create thread failed\n&quot;);    &#125;    // should join until children exits    while (1);    return 0;&#125;// MUST compile with 03// $gcc -o test test.c  -lpthread -O3

ISRwhen ISR always mapps port&#x2F;io&#x2F;device register from device exported, we should get its value from memory not cache register as it can be changed by DMA out scope of processor.
how dynamic library is searched when runningOld way, when program starts, it checks below path for dynamic library searching

directories from LD_LIBRARY_PATH;
directories from &#x2F;etc&#x2F;ld.so.conf(used for additional library)
&#x2F;lib64;
&#x2F;usr&#x2F;lib64.

But check path is slow, hence to improve the speed, A cache is used which is located at /etc/ld.so.cache which stores library and its path, fast.
So new way, dynamic library is searched with below

$LD_LIBRARY_PATH
&#x2F;etc&#x2F;ld.so.cache

ld.so.cache is generated after scanning /etc/ld.conf.conf, /lib64 and /usr/lib64, ldconfig is a tool to do this which is called after each dynamic library is installed by yum/apt.
# if you manually remove or copy a library, you need to rebuild the cache(base) [root@dev ]# ldconfig# check /etc/ld.so.cache content(base) [root@dev ]# ldconfig -p538 libs found in cache `/etc/ld.so.cache&#x27;        p11-kit-trust.so (libc6,x86-64) =&gt; /lib64/p11-kit-trust.so        libzstd.so.1 (libc6,x86-64) =&gt; /lib64/libzstd.so.1        libz.so.1 (libc6,x86-64) =&gt; /lib64/libz.so.1        libz.so (libc6,x86-64) =&gt; /lib64/libz.so        libyaml-0.so.2 (libc6,x86-64) =&gt; /lib64/libyaml-0.so.2        libxtables.so.10 (libc6,x86-64) =&gt; /lib64/libxtables.so.10        libxshmfence.so.1 (libc6,x86-64) =&gt; /lib64/libxshmfence.so.1        libxml2.so.2 (libc6,x86-64) =&gt; /lib64/libxml2.so.2        libxdot.so.4 (libc6,x86-64) =&gt; /lib64/libxdot.so.4        libxcb.so.1 (libc6,x86-64) =&gt; /lib64/libxcb.so.1        libxcb.so (libc6,x86-64) =&gt; /lib64/libxcb.so        libxcb-xvmc.so.0 (libc6,x86-64) =&gt; /lib64/libxcb-xvmc.so.0        libxcb-xvmc.so (libc6,x86-64) =&gt; /lib64/libxcb-xvmc.so        libxcb-xv.so.0 (libc6,x86-64) =&gt; /lib64/libxcb-xv.so.0

how dynamic library is searched when compilingpkg-config is a tool to check dependencies for a library, it outputs version, header path, libs of that library, so that someone who uses this library passes these to compiler for building.
pkg-config gets all these information by checking xxx.pc from several paths, so that if a library wants to be managed by pkg-config, it must proivde a xxx.pc file at some path.
# get default search paths for pkg-config$pkg-config --variable pc_path pkg-configchange search paths for pkg-config$ export PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig:/new/path# list all known packages$pkg-config --list-all##################### example ###########################$pkg-config --modversion gnutls3.3.29$pkg-config --libs  gnutls-L/usr/lib64 -lgnutls$pkg-config --cflags gnutls-I/usr/include/p11-kit-1

REF
c c++ programming
linux library howto
shared memory
epoll
select vs pool vs epoll

]]></content>
      <categories>
        <category>c</category>
        <category>tips</category>
      </categories>
      <tags>
        <tag>c language</tag>
      </tags>
  </entry>
  <entry>
    <title>css_basic</title>
    <url>/2020/07/09/css-basic/</url>
    <content><![CDATA[CSSCSS like a painter, it styles your house to make it beautiful, it allows you to create rules that specify how the content of an element should appear(for box: background, border, margin, padding, width, height etc, TEXT: font, color), these setting are html element styles, but set outside of html for better management, that means you need to know

what to be set (html element style property)
how to set it (css syntax)



html element’s styles(border, color, font) can be set with below order(the first one have the highest priority)

inline, inside each element(&lt; color&gt;)[used for just one element]

  &lt;body style=&quot;background-color: linen&quot;&gt;&lt;h1&gt;This is a Heading&lt;/h1&gt;&lt;p&gt;This is a paragraph.&lt;/p&gt;&lt;p&gt;This is another paragraph.&lt;/p&gt;&lt;/body&gt;


internal css(style at header)[used for just one page]
external (css)[used for whole website, many pages can share the same css file]
browser default setting

how css workscss associates style rules with html elements by SELECTOR DECLARATION
/* * selector &#123;property: value&#125; */h1, h2&#123; color:yellow;&#125;
two ways to apply CSS
external
&lt;head&gt;  &lt;link href=&quot;path&quot; type=&quot;text/css&quot; rel=&quot;stylesheet&quot; /&gt;  //rel points to the *.css file&lt;/head&gt;

internal
&lt;head&gt;  &lt;style type=&quot;text/css&quot;&gt;    h1, h2 &#123;      color: yellow;    &#125;  &lt;/style&gt;&lt;/head&gt;

Always use external for production env, the reason is:

multiple pages can share the same css
decouple css with html code, easy for maintenance
client only needs to request file once if shared by pages, fast

More rules matched, which one should be used?

if two selector are identical, that latter one, that means

if has inline, always use it
else see the position of external and internal, use later one
else use browser default.


if one selector is more specific than the others, the more specific rule applies

special add !important after property value, just this value used for this property!!


CSS selectorSimple Selectors

CSS CombinatorsThere are four different combinators in CSS:

descendant selector (space), son, grandson etc
child selector (&gt;), direct child, only son
adjacent sibling selector (+), has same parent
general sibling selector (~), has same parent


CSS Pseudo-classes


active state means mouse clicked not freed state
checked state for input element like check box etc
enabled state for input element as well.
focus state for input element when clicked, cursor is there
invalid state for input element when invalid value, like email but input is not
out-of-range state for input element when value is out of range, like number with range(1,10), input 11
first-child and first-of-type both say direct child
nth-child(n) and nth-of-type(n) both say direct child as n index, index from 1
only-child and only-of-type both say direct child

Pseudo-class always combines with selectors, here are some example
div:hover p &#123;  // when mouse moves over a div, all descendant p changes display.  display: block;&#125;a.highlight:hover &#123;  // a tag with &#x27;highlight&#x27; class when mouse moves over, change it&#x27;s color  color: #ff0000;&#125;

Pseudo ElementsIt’ used to insert content or change first-line or first-letter, the content can be a string or url, more details about the content, refer to content format

Attribute Selectors

Similar attribute selector

[attribute|&#x3D;value] 	[lang|&#x3D;en] 	Selects all elements with a lang attribute value starting with “en”en must be attribute value, hence match a whole word

[attribute^&#x3D;value] 	a[href^&#x3D;”https”] 	Selects every &lt;a&gt; element whose href attribute value begins with “https”https can be part of attribute value, part of word

[attribute&#x3D;value] 	[title&#x3D;flower] 	Selects all elements with a title attribute containing the word “flower”flower must be attribute value, hence match a whole word

[attribute*&#x3D;value] 	a[href*&#x3D;”w3schools”] 	Selects every &lt;a&gt; element whose href attribute value contains the substring “w3schools”w3schools can be part of attribute value, part of word


Note: regular pattern ^, $, * just do matching as a string, |, ~ matching with meaning, it must be an attribute value.
CSS propertyCSS Property Groups

Color
Background and Borders
Basic Box
Flexible Box
Text
Text Decoration
Fonts
Writing Modes
Table
Lists and Counters
Animation
Transform
Transition
Basic User Interface
Multi-column
Paged Media
Generated Content
Filter Effects
Image&#x2F;Replaced Content
Masking
Speech
Marquee

This is just groups, not specific property, all properties refer to css reference
Backgrounddiv &#123;  background-color: green;  // not transparent  opacity: 0.3;&#125;
Background can be set with an image, but the image may be not so big to fill the full content of view, hence we can set ‘repeat’ or ‘position’, ‘scroll’ for the image.
background-sizeThe background-size can be specified in lengths, percentages, or by using one of the two keywords: contain or cover.

Without background-size: img stays its original size, if area is large, some part is not cover, if area is small, the overflow part is hidden
cover: the cover keyword scales the background image so that the content area is completely covered by the background image, so part of image may out of area.
contain: the contain keyword scales the background image to be as large as possible (but both its width and its height must fit inside the content area), the part of area may be not covered at all.

body &#123;  background-image: url(&quot;img_tree.png&quot;);  background-size: cover;  background-repeat: no-repeat; //repeat-x, repeat-y  background-attachment: scroll; //move when you scroll your view, another value, fixed.  background-position: right top;&#125;/* Shorthand for background, write a single property to include all values with fixed order    background-color    background-image    background-repeat    background-attachment    background-position  It does not matter if one of the property values is missing, as long as the other ones are in this order*/body &#123;  background: url(&quot;img_tree.png&quot;) no-repeat scroll right top;&#125;/*The CSS background-origin property specifies where the background image is positioned.The property takes three different values:    border-box - the background image starts from the upper left corner of the border    padding-box - (default) the background image starts from the upper left corner of the padding edge    content-box - the background image starts from the upper left corner of the content*/
Note: you can set background for every element like body, div, input etc, it’s only applied to that element!!
BorderBorder has four sides, top, right, bottom, left, you can set properties like ‘style’, ‘width’, ‘radius’ for each side separately or define two(top, bottom pair, right, left pair) or four side as same. but with Shorthand, the four sides must be same!!!

border-xxx-style border-style
border-xxx-width border-width
border-xxx-color border-color
border-radiusdiv &#123;  border-top-style: solid;  border-width: 10px 10px;  border-color: red;  border-radius: 5px;&#125;/* Shorthand does NOT include radius with below order   border-width   border-style (required)   border-color*/div &#123;  border: 10px solid red;  border-radius: 5px;&#125;
Margin

Margin can be set for each side separately with specific property, or set with one property(Shorthand), the values:

auto - the browser calculates the margin(only works for block element)
length - specifies a margin in px, pt, cm, etc.
% - specifies a margin in % of the width of the containing element(mostly % of its parent’s width)
inherit - specifies that the margin should be inherited from the parent element

Top and bottom margins of elements are sometimes collapsed into a single margin that is equal to the largest of the two margins.This does not happen on left and right margins! Only top and bottom margins!
p &#123;  // margin-top, margin-xx  //Shorthand with top, right, bottom left order  margin: 25px 50px 75px 100px;&#125;
The width of an element does not include padding, borders, or margins by default. but you can include border, padding with  box-sizing: border-box;
For auto, the left margin receives a share of the unused horizontal space, as determined mainly by the layout mode that is used. If the values of margin-left and margin-right are both auto, the calculated space is evenly distributed. if only margin-left set, the unused horizontal space added as margin-left,  move the item right align, more details refer to margin auto.
PaddingThe CSS padding properties are used to generate space around an element’s content, inside of any defined borders, it has the same format as margin.
p &#123;  // padding-top, padding-xx  //Shorthand with top, right, bottom left order  padding: 25px 50px 75px 100px;&#125;
Width and HeightThe height and width properties are used to set the height and width of an element.
The height and width properties do not include padding, borders, or margins. It sets the height&#x2F;width of the area inside the padding, border,and margin of the element.
The height and width properties may have the following values:

auto - This is default. The browser calculates the height and width
length - Defines the height&#x2F;width in px, cm etc.
% - Defines the height&#x2F;width in percent of the containing block, special for img
inherit - The height&#x2F;width will be inherited from its parent value

Note: For img if the max-width property is set to 100%, the image will scale down if it has to, but never scale up to be larger than its original size. same thing for video.
The max-width property is used to set the maximum width of an element, the element size scroll when window changes, it’s respective, but width has fixed size, hence on small window, some of the element may be out of window.
OutLineAn outline is a line that is drawn around elements, OUTSIDE the borders, to make the element “stand out”, it has the same format as border does, ‘style’, ‘color’, ‘width’. but can’t set each side!!!
p &#123;  // NO outline-xxx-width like border  // outline-width: 5px;  //Shorthand  outline: 5px solid red;  // outline offset from border  outline-offset: 5px;&#125;
TextText has several properties you can set by CSS like color, alignment, decoration, transformation, space, shadow.

The color property is used to set the color of the text.
The text-align property is used to set the horizontal alignment of a textjustify: each line is stretched so that every line has equal width
The text-transform property is used to specify uppercase and lowercase letters in a text
The text-indent property is used to specify the indentation of the first line of a text
The text-shadow property adds shadow to text.horizontal shadow  and the vertical shadowp &#123;  background-color: white;  color: black;  text-align: left;//&quot;justify&quot;, each line is stretched so that every line has equal width, and the left and right margins are straight (like in magazines and newspapers)  vertical-align: middle;  text-decoration: underline;  text-transform: uppercase; //capitalize  text-indent: 10px;  letter-spacing: 2px;  word-spacing: 3px;  line-height: 2;  text-shadow: 2px 2px red;&#125;
FontThe CSS font properties define the font family, boldness, size, and the style of a text

p &#123; font-style: italic; font-weight: bold; font-size: 30px; /* font-size: 2.5em; 40px/16=2.5em */&#125;/*The font property is a shorthand property for:    font-style    font-variant    font-weight    font-size/line-height    font-family*/p &#123;  font: italic bold 30px;&#125;
IconThe simplest way to add an icon to your HTML page, is with an icon library, such as Font Awesome, Bootstrap Icons, Google Icon
...&lt;script src=&quot;https://kit.fontawesome.com/a076d05399.js&quot;&gt;&lt;/script&gt;...&lt;i class=&quot;fas fa-cloud&quot;&gt;&lt;/i&gt;
list CSSThe list-style-image property specifies an image as the list item marker.
ul.a &#123;  list-style-type: circle; // or use an image list-style-image: url(&#x27;sqpurple.gif&#x27;);&#125;ul.b &#123;  list-style-type: square;&#125;

position relatedNormal flow(default): every block-level element(start with new line) appears on a new line, causing each item to appear lower down the page than previous one, even if you specify the width of the boxes and there is space for two elements to sit side-by-side, they will not appear next to each other, this is the default behavior!

Relative positionThis moves an element from the position it would be in normal flow, shift it to the top, right, bottom, or left of where it would have been placed, this doesn&#39;t affect the position of surrounding elements, they stay in the position they would be in normal flow(relative—relative to where they would be in normal flow when set offset top, right, bottom, left), if no shift set, relative position has no difference with default.
div.relative &#123;  position: relative;  left: 10px; // move left from normal flow  border: 3px solid #73AD21;&#125;

Absolute positionThis positions the element in [relation to its containing element], It’s taken out of normal flow, it could affect sibling elements, as it has the fixed position in the container.Positioned relative to the nearest positioned ancestor (instead of positioned relative to the viewport, like fixed)However; if an absolute positioned element has no positioned ancestors, it uses the document body, A &quot;positioned&quot; element is one whose position is anything except static

Fixed positionThis is a form of absolute positioning that positions the elements in relation to the browser window!!! as opposed to the containing element! they don’t move when user scrolls up or down the page!(absolute—the browser window!!!) it’s always fixed to the window, out of flow, can affect any element in the view!!!


z-indexWhen positioned with three ways, element may overlap each other,t he z-index property specifies the stack order of an element, an element with greater stack order is always in front of an element with a lower stack order.
Note: z-index only works on positioned elements (position: absolute, position: relative, position: fixed), default 0
Media QueriesMedia queries can be used to check many things, such as:

width and height of the viewport
orientation (is the tablet&#x2F;phone in landscape or portrait mode?)
resolution

it returns feature of given media type, then we use that result do some expression to define different values of element.
//When a media query is true, the corresponding style sheet or style rules are applied, following the normal cascading rules@media not|only mediatype and (expressions) &#123;  CSS-Code;&#125;//example:/* When the width is between 600px and 900px OR above 1100px - change the appearance of &lt;div&gt; */@media screen and (max-width: 900px) and (min-width: 600px), (min-width: 1100px) &#123;  div.example &#123;    font-size: 50px;    padding: 50px;    border: 8px solid black;    background: yellow;  &#125;&#125;

mediatype:

screen
print

features

height 	The viewport height
max-height 	The maximum height of the display area, such as a browser window
min-height 	The minimum height of the display area, such as a browser window
min-resolution 	The minimum resolution of the device, using dpi or dpcm
max-resolution 	The maximum resolution of the device, using dpi or dpcm
max-width 	The maximum width of the display area, such as a browser window
min-width 	The minimum width of the display area, such as a browser window
width 	The viewport width

VariableVariables in CSS should be declared within a CSS selector that defines its scope. For a global scope you can use either the :root or the body selector.The variable name must begin with two dashes (–) and is case sensitive!
:root &#123;  /* global variable */  --main-bg-color: coral;&#125;#div1 &#123;  --local-color: red;  background-color: var(--main-bg-color);  color: var(--local-color);&#125;
REF
css w3school tutorial
css cheatsheet

]]></content>
      <categories>
        <category>web</category>
        <category>css</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>css</tag>
        <tag>css selector</tag>
        <tag>css property</tag>
      </tags>
  </entry>
  <entry>
    <title>css_faq</title>
    <url>/2020/07/09/css-faq/</url>
    <content><![CDATA[FAQAlign element center&#x2F;center
Align block element center with margin:auto the browser assigns half the remaining space to margin-left and the other half to margin-right, this is only works for block element so &lt;img&gt; not working, you need to change it display: block or display:inline-block.

Align text center
.center &#123;  margin: auto;  width: 50%;&#125;.text &#123;  text-align: center;&#125;
inline, block, inline-block differenceCompared to display: inline, the major difference is that display: inline-block allows to set a width and height on the element, it’s always set for real inline elment that wants width or height.


Compared to display: block, the major difference is that display: inline-block does not add a line-break after the element, so the element can sit next to other elements
Note: for real inline element, you can set padding, border, background, but cant NOT set width and height.
what happens if content is too big to fix in the container?The CSS overflow property controls what happens to content that is too big to fit into an area.
The overflow property specifies whether to clip the content or to add scrollbars when the content of an element is too big to fit in the specified area.
The overflow property has the following values:

visible - Default. The overflow is not clipped. The content renders outside the element&#39;s box
hidden - The overflow is clipped, and the rest of the content will be invisible
scroll - The overflow is clipped, and a scrollbar is always added to see the rest of the content at X&#x2F;Y.
auto - Similar to scroll, but it adds scrollbars only when necessary.

web layoutfloat property used for what?The float property is used for positioning and formatting content e.g. let an image float left to the text in a container, the float limited inside the conainer!!!.
The float property can have one of the following values:

left - The element floats to the left of its container in the same line
right - The element floats to the right of its container
none - The element does not float (will be displayed just where it occurs in the text). This is default
inherit - The element inherits the float value of its parent

In its simplest use, the float property can be used to wrap text around images. The image as a child of p tag or sibling of p have different effect. it’s floated inside the container!!!
Another good example, float makes the &lt;li&gt; sit side by side in the same line.
li &#123;  float: left;&#125;

displayThe display property specifies if&#x2F;how an element is displayed, every HTML element has a default display value depending on what type of element it is. The default display value for most elements is block or inline.
A block-level element always starts on a new line and takes up the full width available (stretches out to the left and right as far as it can), we can prevent this by setting width property to prevent it from stretching out to the edges of its container.
&lt;div&gt;&lt;h1&gt; - &lt;h6&gt;&lt;p&gt;&lt;form&gt;&lt;header&gt;&lt;footer&gt;&lt;section&gt;&lt;li&gt;&lt;td&gt;&lt;th&gt;

An inline element does not start on a new line and only takes up as much width as necessary
&lt;span&gt;&lt;a&gt;&lt;img&gt;&lt;input&gt;&lt;i&gt;&lt;b&gt;&lt;cite&gt;&lt;strong&gt;

As mentioned, every element has a default display value. However, you can override this, Setting the display property of an element only changes how the element is displayed, NOT what kind of element it is. So, an inline element with display: block; is not allowed to have other block elements inside it but can set width, height the fake block!!!.
li &#123;  display: inline;&#125;


display: none, take no space from layout, seem no element
visibility:hidden; also hides an element. but it still takes the space(width and height) of layout, just can’t see it!!!
width: 0 and overflow: hidden, can’t see the element as well, but the element still takes(height) of layout. just can’t see it!!!

FlexFlexbox is made for one dimensional layouts, there are two concepts here, flex container and flex items, flex container is a block element, any element with display: flex, it will behaves like a box starts a new line, scretch full width of it&#39;s container, all flex items behaves like inline-block, its width equals its content length, they sit one by one in one direction, but they are flexible, that means their width can grow(should set flex-grow) if there is free space left in the flex container or their width can shrink if there is less space in the conatiner. ther width are not fixed, even you set it explicitly. but there is the default behavior, you can change the default behavior by setting proper property.
As the width are not fixed, what’s the final value of flex items

initial value(width) = equal to content length or width if set explicitly.
may grow or shrink if allowed to the final width

flex container property setting
.flex_container &#123;  display: flex;  // default: row  // others: row-reverse, column, colum-reverse  flex-direction: row;  //default: nowrap: all items always in the same line  flex-wrap: nowrap  //OR shorthand for above two  flex-flow: row nowrap;  // align-items specifies the default alignment for items inside the flexible container.  // default: scretch  align-items: center;&#125;
grow and shrink on each flex itemThe flex-grow property specifies how much the item will grow relative to the rest of the flexible items inside the same container, it grows only when there is free space left after the initial value.
.flex_item &#123;  //default: auto, width == content length.  //others: flex-basis: 100px, flex-basis: 20%(20% container&#x27;s width)  flex-basis: auto;  //default: no grow, shrink euqaly, make on item grow, other not  // the grow element will take the rest space of its container!!  flex-grow: 1;  flex-shrink: 1;  //shorthand for above three with below order  //1. flex-grow  //2. flex-shrink  //3. flex-basis  // default: 0 1 auto  // others: flex:auto    == 1 1 auto  //         flex:initial == 0 1 auto  //         flex:none    == 0 0 auto    flex: 0 1 auto;&#125;.flex_item_large &#123;  //default: auto, width == content length.  //others: flex-basis: 100px, flex-basis: 20%(20% container&#x27;s width)  flex-basis: auto;  //2 times grows tha others  //let&#x27;s take an example, 60px free space, two flex items, flex items grow, the large one gains 40px, the other one gains 20px. same thing for shrink.  flex-grow: 2;  flex-shrink: 2;&#125;

More examples refer to Flex example;
Main difference between display: flex; and display: inline-flex; is that display: inline-flex; will make the flex container an inline element while its content maintains its flexbox properties.
ALign of flex

align-items: specifies the default alignment for items inside the flexible container vertically
justify-content: aligns the flexible container’s items when the items do not use all available space on the main-axis (horizontally)
align-content: modifies the behavior of the flex-wrap property. It is similar to align-items, but instead of aligning flex items, it aligns flex lines only valid with wrap property set, as in that case more lines can happen for flex items.
align-self: specifies the alignment for the selected item inside the flexible container, the flexible container’s align-items property.

It only works for the direct children(which is called flex items), it does not affect the elements inside each items
&lt;div class=&quot;flex&quot;&gt;&lt;!--flex has three flex items  1. div.inner  2. &lt;span&gt;  3. &lt;i&gt;  4. #Text with yes--&gt;  &lt;div class=&quot;inner&quot;&gt;    Hello  &lt;/div&gt;  &lt;span&gt;World&lt;/span&gt;  &lt;i class=&quot;fas fas-book&quot;&gt;&lt;/i&gt;  Yes&lt;/div&gt;

Wrap means if flex item width is over the container’s width, the item starts with a new line, by default no wrap that means each item will shrink to hold the item in the same line.
Note: Use the align-self property of each item to override the align-items property
can fixed item be flex item?element with position: fixed itself can NOT be flex item, but Can be flex container. if you check flex layout with developer tool, you will see such item is not flex item!!! that means flex setting not work for such item!!!
GridGrid is made for two dimensional layouts, Row and Column. Grid will give you more flexibility, you can control how row and column display. each row is spilitted into columns, the direct children(grid item) fall into each column automatically.
Grid Container
.grid-container &#123;  display: grid;  //default no gaps for row and column  grid-row-gap: 10px;  grid-column-gap: 20px;    //OR shorthand, if same value grid-gap 10px;  grid-gap: 10px 20px;&#125;

Explicity container
.grid-container &#123;  display: grid;  // explicit 4 columns  //The size(width) of the columns is determined by the size of the container and on the size of the content of the items in the column  grid-template-columns: auto auto auto auto;  //OR only depends its content  grid-template-columns: max-content max-content max-content max-content;  //OR fix length repeat(4, 100px) or repeat(4, 10%)  grid-template-columns: 100px 100px 100px 100px;  //OR  grid-template-columns: repeat(4, 10%);  //Specify the first two row size(height), it&#x27;s tempalte, that means  // the second row in real may not there but if it&#x27;s there, it has 300px height.  grid-template-rows: 100px 300px;&#125;
As explicit container only says part of column’s width and part of row’s height.
.grid-container &#123;  display: grid;  //Each &lt;flex&gt;-sized track takes a share of the remaining space in proportion to its flex factor  //the last two columns take all rest space in proportion  grid-template-columns: 100px 100px 2fr 1fr;  //the last columns take all rest space!!  grid-template-columns: 100px 100px 100px 1fr;  //if we have 9 grid items, the 9th items starts a new row, we did not say the height for that row  //it use default row height and default column width which is defined below.  grid-template-rows: 100px 300px;  //set default row height value for implicit grid row, column in that row use default width set by grid-auto-columns  grid-auto-rows: 50px;  //implicit tracks: This can happen either by explicitly positioning into a column that is out of range, or by the auto-placement algorithm creating additional columns.  grid-auto-columns: 200px;  grid-auto-columns: 100px 150px 390px;&#125;
dynamic columnsAbove the grid has fixed column number, even there is free space left in the row, but auto-fill keyword creates as many tracks as fit into the grid container without causing the grid to overflow it, same thing for auto-fit except that after grid item placement it will only create as many tracks(columns) as needed and any empty repeated track collapses.
.grid &#123;  display: grid;  grid-template-columns: repeat(auto-fit, 100px);  grid-gap: 20px;&#125;

Grid itemBy default each grid item takes one column from one row but you can change the default behavior to let one item to take more columuns and more rows.
.item1 &#123;  //item1 takes two rows starting from row 1  grid-row: 1 / span 2;  //item1 starts from row 1, end row3(not include), two rows  grid-row: 1 / 3;  //item1 takes two columns starting from colum 1  grid-column: 1 /span 2;  //span 2 columns, the start depends on location.  grid-column: span 2;  //item1 starts from column 1, end column3(not include), two columns  grid-column: 1 / 3;  //OR  //shorthand for above  // grid-area: grid-row-start / grid-column-start /grid-row-end /grid-column-end;  grid-area: 1 / 1 / span 2 / span 2;&#125;

Most of time, we use named area to do this, as it’s easy to understand.
//NOTE: NO &quot;&quot; quotes for each area name!!!.item1 &#123; grid-area: header; &#125;.item2 &#123; grid-area: menu; &#125;.item3 &#123; grid-area: main; &#125;.item4 &#123; grid-area: right; &#125;.item5 &#123; grid-area: footer; &#125;.grid-container &#123;  display: grid;  //as you can see six columns  //header takes 5 columns + unamed columns  //menu takes 1 column + across two rows  grid-template-areas:    &#x27;header header header header header .&#x27;    &#x27;menu main main main right right&#x27;    &#x27;menu footer footer footer footer footer&#x27;;&#125;

Note
Flex and Grid are note standalone, you can use Flex inside Grid, combine them, also Flex can achieve grid system this’s boostrap4 does.
Bootstrap grid systemBootstrap3 uses float&#x2F;width and more hacks to achieve its grid system, from bootstrap4, it uses CSS flex box to do this, as flex is widely supported by most browsers, CSS Grid is more powerful, but not all browsers support it, but someday in future, after it’s widely supported, bootstrap may change its grid system to use CSS Grid.
CSS grid is currently not an official standard (it is a W3C Candidate Recommendation) although it has been adopted by most major browsers, as of October 2017, Chrome, Firefox, Safari and Edge all support CSS grid without vendor prefixes.[3][4][5] IE 10 and 11 support CSS grid but with an outdated specification. On mobile, all modern browsers support CSS grid except for Opera Mini and UC Browser.
Bootstrap grid vs CSS grid for the same layoutresponsive layout

&lt;body&gt;  &lt;div class=&quot;container&quot;&gt;    &lt;div class=&quot;row&quot;&gt;      &lt;div class=&quot;col-xs-12 header&quot;&gt;...&lt;/div&gt;    &lt;/div&gt;    &lt;div class=&quot;row&quot;&gt;      &lt;div class=&quot;col-xs-12 col-md-6 col-lg-4 navigation-menu&quot;&gt;...&lt;/div&gt;      &lt;div class=&quot;col-xs-12 col-md-6 col-lg-8 main-content&quot;&gt;...&lt;/div&gt;    &lt;/div&gt;    &lt;div class=&quot;row&quot;&gt;      &lt;div class=&quot;col-xs-12 footer&quot;&gt;...&lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/body&gt;

CSS Grid is more cleaner
&lt;body&gt;  &lt;div class=&quot;container&quot;&gt;    &lt;div class=&quot;header&quot;&gt;...&lt;/div&gt;    &lt;div class=&quot;navigation-menu&quot;&gt;...&lt;/div&gt;    &lt;div class=&quot;main-content&quot;&gt;...&lt;/div&gt;    &lt;div class=&quot;footer&quot;&gt;...&lt;/div&gt;  &lt;/div&gt;&lt;/body&gt;
....container &#123;  display: grid;  grid-template-columns: repeat(12, 1fr);&#125;.header &#123;  grid-column: span 12;&#125;@media screen and (max-width: 768px) &#123;  .navigation-menu &#123;    grid-column: span 6;  &#125;  .main-content &#123;    grid-column: span 6;  &#125;&#125;@media screen and (max-width: 480px) &#123;  .navigation-menu &#123;    grid-column: span 12;  &#125;  .main-content &#123;    grid-column: span 12;  &#125;&#125;.footer &#123;  grid-column: span 12;&#125;

Browser extensions to CSSCSS defines the standard way(property name, value) that all browser should support, but some browsers support it in their own way that’s the extension for, browser extensions are only valid for that particular type of browser, if you only use the standard CSS property, the style may not take effect in some version of browser, so check that if browser supports it at canIuse;




Firefox
Chrome
Microsoft(IE)



prefix
-moz-xxx
-webkit-xxx
-ms-xxx


@mixin border-radius($radius) &#123;    -webkit-border-radius: $radius;       -moz-border-radius: $radius;        -ms-border-radius: $radius;            border-radius: $radius;  &#125;
how to change the cursor when it moves to element&lt;span style=&quot;cursor:pointer&quot;&gt;pointer&lt;/span&gt;&lt;br&gt;&lt;span style=&quot;cursor:wait&quot;&gt;wait&lt;/span&gt;&lt;br&gt;

input with iconinput[type=text] &#123;  background-color: white;  background-image: url(&#x27;searchicon.png&#x27;);  background-position: 10px 10px;  background-repeat: no-repeat;  padding-left: 40px;&#125;//Animated Search Inputinput[type=text] &#123;  width: 100px;  transition: width 0.4s ease-in-out;&#125;input[type=text]:focus &#123;  width: 100%;&#125;
Note: In real world, you don’t need to write property for each browser as it could be done by tool like PostCSS;
how to make a round or ovallet’s say we have an element like div or img how to make it a round or oval, use border-radius: 50%
.img &#123;    display: inline-block;    width: 100px;    height: 100px;    // as width == height, so it&#x27;s round, if width != height, it&#x27;s an oval    border-radius: 50%;&#125;div &#123;    background-color: red;    border-radius: 50%;&#125;
set multiple imgs as backgroundFor some case, you may want to set multiple images as a background for an element, CSS allows us to do this and set peroperties for each imge seperately. The different background images are separated by commas, and the images are stacked on top of each other, where the first image is closest to the viewer, if overlap happens, you see the first image for overlap part.
#example1 &#123;  // set each img seperately  background-image: url(img_flwr.gif), url(paper.gif);  background-position: right bottom, left top;  background-repeat: no-repeat, repeat;&#125;//ShortHand#example1 &#123;  background: url(img_flwr.gif) right bottom no-repeat, url(paper.gif) left top repeat;&#125;

gradients only for color!!!CSS gradients let you display smooth transitions between two or more specified colors
#grad &#123;  background-image: linear-gradient(to right, red , yellow);&#125;
How to set shadow?There are two kind of shadowns text-shadow and box-shadow;
h1 &#123;  text-shadow: 2px 2px red;&#125;// box shadowdiv &#123;  box-shadow: 10px 10px grey;&#125;

how to use web font which is not installed on client computer.@font-face &#123;  // use font-face to define a font-family which refer to web font  font-family: myFirstFont;  // client(browser will download the font automatically  src: url(sansation_light.woff);&#125;div &#123;  font-family: myFirstFont;&#125;

transition&#x2F;transform&#x2F;animationtransformCSS transforms allow you to move, rotate, scale, and skew elements based on the center of element itself.

2D transforms: X, Y
3D transforms: X, Y, Z

Transform performs on any element.  
Both 2D and 3D use the same property, but 2D only support X,Y function, 3D added Y function.

The translate() method moves an element from its current position (according to the parameters given for the X-axis and the Y-axis)
The rotate() method rotates an element clockwise or counter-clockwise according to a given degree
The scale() method increases or decreases the size of an element (according to the parameters given for the width and height)
The skew() method skews an element along the X and Y-axis by the given angles.
The matrix() method combines all the 2D transform methods into one, the parameters are as follow: matrix(scaleX(),skewY(),skewX(),scaleY(),translateX(),translateY())

.div &#123;  // 2 times width, 20deg skew, move 100px on X-axis, 50 px on Y-axis  transform: matrix(2, 20deg, 20deg, 0.5, 100px, 50px);  //rotate is only for 2D, not a combine with rotateX and rotateY which is provided by 3D  transform: rotate(10deg); //rotate clockwise  transform: rotateX(10deg); //rotate on X-axis&#125;

transitionCSS transitions allows you to change property values smoothly, over a given duration. you have to specify

allowed property(default all)
when it happens
how long it keepsAt last set the value change to trigger the transition

div &#123;  width: 100px;  //allowed  transition-property: width;  //how long  transition-duration: 2s;  //effect others: ease, ease-in, ease-out, ease-in-out  transition-timing-function: linear;  //delay before happen  transition-delay: 1s;  //shorthand  transition: width 2s linear 1s;&#125;div:hover &#123;  width: 200px;&#125;//Change Several Property Valuesdiv &#123;// multiple properties must be in signle line, that means at most one transition property set.  transition: width 2s, height 4s;&#125;div:hover &#123;  width: 100px;  height: 200px;&#125;

animationAn animation lets an element gradually change from one style to another. the style includes several properitesYou can change as many CSS properties you want, as many times you want
div &#123;  width: 100px;  height: 100px;  background-color: red;  position: relative;  //animation setting  animation-name: example;  animation-duration: 5s;  animation-timing-function: linear;  animation-delay: 2s;  animation-iteration-count: infinite; // can set 3 times  animation-direction: alternate;  //Shorthand  animation: example 5s linear 2s infinite alternate;&#125;@keyframes example &#123;  // styles for each frame  0%   &#123;background-color:red; left:0px; top:0px;&#125;  25%  &#123;background-color:yellow; left:200px; top:0px;&#125;  50%  &#123;background-color:blue; left:200px; top:200px;&#125;  75%  &#123;background-color:green; left:0px; top:200px;&#125;  100% &#123;background-color:red; left:0px; top:0px;&#125;&#125;

negative value in top&#x2F;left&#x2F;right&#x2F;bottom and margin.div &#123;  //the offset if relative to its normal position.  //move left margin of element to left direction 5px: move close to left element  margin-left: -5px;  //if the right element besides it at same row, the right element move left 5px  margin-right: -5px;  //move top margin of element to top direction 5px: move close to up element  margin-top: -5px;  //the closest down element besides it, the down element moves up 5px  margin-bottom: -5px;&#125;

top&#x2F;left&#x2F;right&#x2F;bottom value

If position: absolute; or position: fixed; - the left property sets the left edge of an element to a unit to the left of the left edge of its nearest positioned ancestor

If position: relative; - the left property sets the left edge of an element to a unit to the left&#x2F;right of its normal position.


The value can be

length 	Sets the left edge position in px, cm, etc. Negative values are allowed
% 	    Sets the left edge position in % of containing element. Negative values are allowed, 10% if container is 100px, 10% means 10px!!!

div.b &#123;  position: absolute;  // 0px means edge of its container, -10px means move left 10px of its container&#x27;s left edge  left: -10px;  width: 100px;  height: 120px;  border: 3px solid blue;&#125; 
what’s the difference for width and max-widthwidth has a fixed length, no matter the viewpoint, if viewpoint is small, scroll bar is addedmax-width: it limits the max length of the element when brower is large, but when viewpoint is small, no scroll bar!
hence same value for width and max-width, max-width is responsive.
how to define large buttonAcutally button size is determined by font-size, if font is large, the button is large as well. but you can set its width like this
.button1 &#123;width: 250px;&#125;// depends on container size.button2 &#123;width: 50%;&#125;.button3 &#123;width: 100%;&#125;&#125;

how to resize element by dragThe resize property specifies if (and how) an element should be resizable by the user.
div &#123;  // other values: both, vertical  resize: horizontal;  overflow: auto;&#125;

beatiful tooltips&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;style&gt;.tooltip &#123;  position: relative;  display: inline-block;  border-bottom: 1px dotted black;&#125;.tooltip .tooltiptext &#123;  visibility: hidden;  width: 120px;  background-color: black;  color: #fff;  text-align: center;  border-radius: 6px;  padding: 5px 0;  position: absolute;  z-index: 1;  bottom: 150%;  left: 50%;  margin-left: -60px;&#125;.tooltip .tooltiptext::after &#123;  content: &quot;&quot;;  /* the parent must set position with relative, otherwise the offset top, left will used body as ancestor */  position: absolute;  top: 100%;  left: 50%;  margin-left: -5px;  border-width: 5px;  border-style: solid;  border-color: black transparent transparent transparent;&#125;.tooltip:hover .tooltiptext &#123;  visibility: visible;&#125;&lt;/style&gt;&lt;body style=&quot;text-align:center;&quot;&gt;&lt;h2&gt;Top Tooltip w/ Bottom Arrow&lt;/h2&gt;&lt;div class=&quot;tooltip&quot;&gt;Hover over me  &lt;span class=&quot;tooltiptext&quot;&gt;Tooltip text&lt;/span&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;

debug css by firefoxThe Rules view lists all the rules that apply to the selected element, ordered from most-specific to least-specific, some from itself, some from its parent. see below
cross-line with attribute mean it’s overwritten!

A warning icon appears next to unsupported or invalid styles. This can help you understand why certain styles are not being applied:
rule display
px&#x2F;em&#x2F;rem&#x2F;vw&#x2F;fr
px: fixed size
em: relative to it’s parent
vw: 1vw &#x3D; 1% of viewport width. If the viewport is 50cm wide, 1vw is 0.5cm. font-size is responsive!!!child &#123;  font-size: 2em; //relative to parent&#x27;s font-size  width: 2em; //relative to itself font-size&#125;
rem: relative to root

Note: font-size: 16px by default
&lt;style&gt;div &#123;  font-size: 40px;  width: 10em; /* 400px */  height: 10em;  border: solid 1px black;&#125;p &#123;  font-size: 0.5em; /* 20px */  width: 10em; /* 200px */  height: 10em;  border: solid 1px red;&#125;span &#123;  font-size: 0.5em;  width: 10em;  height: 10em;  border: solid 1px blue;  display: block;&#125;&lt;/style&gt;&lt;div&gt;  我是父元素div  &lt;p&gt;    我是子元素p    &lt;span&gt;我是孙元素span&lt;/span&gt;  &lt;/p&gt;&lt;/div&gt;

css useful snippet//“Responsive” mediaimg, video, audio &#123;  max-width: 100%;  height: auto;&#125;//Box sizinghtml &#123;  box-sizing: border-box;&#125;*, *::before, *::after &#123;  box-sizing: inherit;&#125;//.🦄 &#123;  display: flex;  align-items: center;  justify-content: center;&#125;body &#123;  display: grid;  place-items: center;  height: 100vh;&#125;.foo &#123;  position: absolute;  top: 50%;  left: 50%;  transform: translate (-50%, -50%);&#125;

what’s value for css property.As you know that property can be inherited from its parent, but not ALL properties support this, that means for a property that’s not set explicitly from inline style, internal style, external style, its value depends on what type it’s
Value of a property which is Not set explicitly

default value (non-inherited properity)
its parent value (inherited property)

default value for css
Actually, most of properties can be NOT inheritid.Inherited Property

color
font-size
font-family
font-weight
font-xxx
list-style
list-xxx
visibility
text-indent
text-transform
text-align
text-shadow
:hover

height and width for an elementdefaut valueIt depends on the type of elmenent, auto means different
inline element you can’t set height and width

height: auto means the height of its content
width: auto means the length of its content

block element you can set height and width

height: auto means the height of it’s content
width: auto means scratch the full of its container

element with position: fixed like inline but without containerAs fixed element out of document flow, its width and height equal its content length if not set explicitlyotherwise belong to user setting.div with position: fixed will not start a new line and scretch its container, as it out of document flow.
explicit settingexplicit setting has the highest priority than others, element always belongs to it if set.
div.a &#123;    height: 100px;&#125;
Note: position property can effect height and width as well if set in pairs
div.a &#123;    position: fixed;    top: 0;    bottom: 0;    //the height is the entire viewport    //same thing for left and width as well!!!&#125;div.b &#123;    position: fixed;    top: 0;    bottom: 0;    height: 100px;    //bottom not working, start from top 0 with height 100px(which has high priority)&#125;
how to use relative and absolute and fixed
static 	Default value. Elements render in order, as they appear in the document flow
absolute 	The element is positioned relative to its first positioned (not static) ancestor element, it must have a parent with(fixed, relative, sticky)
The element is positioned relative to the browser window
The element is positioned relative to its normal position, so “left:20px” adds 20 pixels to the element’s LEFT position
The element is positioned based on the user’s scroll position

Note: the absolute and fixed element behaves like inline element not scretch at all even it’s block element. but relative still remains
Ref
flex-grid good cases
css flex and grid

]]></content>
      <categories>
        <category>web</category>
        <category>css</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title>css_reponsive_web</title>
    <url>/2020/07/30/css-reponsive-web/</url>
    <content><![CDATA[Responsive Web DesignResponsive web design is broken down into three main components, including flexible layouts, media queries, and flexible media. 

flexible layoutsThe first part, flexible layouts, is the practice of building the layout of a website with a flexible grid, capable of dynamically resizing to any width. Flexible grids are built using relative length units, most commonly percentages or em units. These relative lengths are then used to declare common grid property values such as width, margin, or padding.
Relative length units, specifically related to the viewport size of the browser or device. These new units include vw, vh, vmin, and vmax

vw viewports width
vh viewports height
vmin Minimum of the viewport’s height and width 
vmax Maximum of the viewport’s height and width

.container &#123;    width: 100vw;    height: 80vmax;&#125;
media queriesflexible mediaRules
Do NOT use large fixed width elements
Do NOT let the content rely on a particular viewport width to render well
Use CSS media queries to apply different styling for small and large screens
Always Design for Mobile(small device) First, This will make the page display faster on smaller devices
Hide Elements With Media Queries(hide something for small device)
Different Images for Different DevicesExample.menu &#123;  // use percent of container not fixed width like width: 500px etc  width: 25%;  float: left;&#125;.main &#123;  width: 75%;  float: left;&#125;// If the max-width property is set to 100%, the image will scale down if it has to, but never scale up to be larger than its original size, same thing for video as well.img &#123;  max-width: 100%;  height: auto;&#125;

Mobile First
/* For mobile phones: */[class*=&quot;col-&quot;] &#123;  width: 100%;&#125;@media only screen and (min-width: 768px) &#123;  /* For desktop: */  .col-1 &#123;width: 8.33%;&#125;  .col-2 &#123;width: 16.66%;&#125;  .col-3 &#123;width: 25%;&#125;  .col-4 &#123;width: 33.33%;&#125;  .col-5 &#123;width: 41.66%;&#125;  .col-6 &#123;width: 50%;&#125;  .col-7 &#123;width: 58.33%;&#125;  .col-8 &#123;width: 66.66%;&#125;  .col-9 &#123;width: 75%;&#125;  .col-10 &#123;width: 83.33%;&#125;  .col-11 &#123;width: 91.66%;&#125;  .col-12 &#123;width: 100%;&#125;&#125;

Typical device
 /* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) &#123;...&#125;/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) &#123;...&#125;/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) &#123;...&#125;/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) &#123;...&#125; ]]></content>
      <categories>
        <category>web</category>
        <category>css</category>
      </categories>
      <tags>
        <tag>css</tag>
        <tag>responsive web design</tag>
      </tags>
  </entry>
  <entry>
    <title>css_sass_basic</title>
    <url>/2020/07/09/css-sass-basic/</url>
    <content><![CDATA[SASS


convert sass scss less to css$npm install -g less$lessc xx.less &gt; xx.css$npm install -g sass$sass xx.scss xx.css$sass xx.sass xx.csswatch mode for s single scss$sass --watch xx.scss:xx.csswatch mode for a dir$sass --watch a/scss:b/css
sass syntaxThere are keywords and special characters in scsseach keyword use @keyword like

@mixin: define mixin
@use: import module
@extend: inheritance
@inlcude: call mixin
@function: define a function
@if&#x2F;@else&#x2F;@for&#x2F;@while&#x2F;@each&#x2F;@return control

special character

% placeholder
&amp; access ancestors selector
$ variable
# link string and variable if varialbe not used as the value of another propertyvariablevariable can be color, number, text.

/* global variable definition, var must start with $ as prefix */$mainColor: #963;$siteWidth: 1024px;$borderStyle: dotted;body &#123;  /* local vairable */  $mainColor: #100;  /* use variable, local var wins */  color: $mainColor;  width: $siteWidth;&#125;head &#123;  /* use global one */  color: $mainColor;&#125;

mixinsmixin is used to replace several properites, not one for variable, use @include to contain the mixin
@mixin error($bW: 2px) &#123;  border: $bW;  color: red;&#125;.login-error &#123;  //@include error() is ok also;  @include error;&#125;.logout-error &#123;  @include error(10px);&#125;

selector nestingnested selector is child of parent, when converted to css, a space is added between parent and child but for pesudo class and pesudo element they must no space, hence in these two case, we should use &amp; to refer its parent directly.
section &#123;  margin: 10px;&#125;nav &#123;  height: 25px;&#125;section &#123;  margin: 10px;  nav &#123;    height: 25px;  &#125;&#125;//converted to csssection &#123;  margin: 10px;&#125;//a space between section and nav selectorsection nav &#123;  height: 25px;&#125;

section &#123;  margin: 10px;  nav &#123;    height: 25px;    a &#123;      color: #0982c1;      /* &amp; 引用父选择器  section nav a       with   &amp;: section nav a:hover       without&amp;: section nav a :hover   --&gt;not working for hover due to extra space.       */      &amp;:hover &#123;        text-decoration: underline;      &#125;    &#125;  &#125;&#125;

Inheritanceextend to inherits all properties from its parent
%block_base &#123;  margin: 10px 5px;  padding: 2px;&#125;p &#123;  //extend base class  @extend %block_base  border: 1px solid #eee;&#125;

Operationsmust add space before and after operation
$bW: 2px;$double_bW: $bW * 2;

color functionbuilt-in color functions
lighten($color, 10%); /* 返回的颜色在$color基础上变亮10% */darken($color, 10%); /* 返回的颜色在$color基础上变暗10% */saturate($color, 10%); /* 返回的颜色在$color基础上饱和度增加10% */desaturate($color, 10%); /* 返回的颜色在$color基础上饱和度减少10% */grayscale($color); /* 返回$color的灰度色*/complement($color); /* 返回$color的补色 */invert($color); /* 返回$color的反相色 */mix($color1, $color2, 50%); /* $color1 和 $color2 的 50% 混合色*/
Except color functions, scss also has other built-in functions for number, string, list, map etc, refer to built-in function.
importImport another scss file at compile time, at last(after generate css) you only see one csss
//old way@import &#x27;base&#x27;// find base.scss at current folder@use &#x27;base&#x27;;
if&#x2F;elsep &#123;  @if 1 + 1 == 2 &#123;    border: 1px solid;  &#125;  @if 5 &lt; 3 &#123;    border: 2px dotted;  &#125;  @if null &#123;    border: 3px double;  &#125;&#125;$type: monster;p &#123;  @if $type == ocean &#123;    color: blue;  &#125; @else if $type == matador &#123;    color: red;  &#125; @else if $type == monster &#123;    color: green;  &#125; @else &#123;    color: black;  &#125;&#125;//note: number and string use same operator &quot;==&quot; and no () for condition;
loop/* include 3, 1, 2, 3 *//* @for..from..through to iterate a range of number */@for $i from 1 through 2 &#123;.item-#&#123;$i&#125; &#123; width: 2em * $i; &#125;&#125;//converted to css as you can see #&#123;$i&#125; used to link string and variable!!!.item-1 &#123;	width: 2em;&#125;.item-2 &#123;	width: 4em;&#125;//@each ...in bot work for list and map@each $var in &lt;list&gt;&#123;&#125;@each $animal in puma, sea-slug, egret, salamander &#123;    .#&#123;$animal&#125;-icon &#123;        background-image: url(&#x27;/images/#&#123;$animal&#125;.png&#x27;);    &#125;&#125;$i: 6;@while $i &gt; 0 &#123;    .item-#&#123;$i&#125; &#123; width: 2em * $i; &#125;    $i: $i - 2;&#125;

list&#x2F;map built-inlist items can be seperated by space, commalist functions

length($list)
join($list1,$list2,[$separator]) return a new list, the orignal unchanged
append($list,$value,[$separator]), return a new list, the orignal unchanged

//define a list with different separators(can only be comma or space!!!)$linkColor: #08c #333; //第一个值为默认值，第二个鼠标滑过值$linkColor: #08c,#333; //第一个值为默认值，第二个鼠标滑过值a &#123;  color: nth($linkColor, 1);  &amp;:hover &#123;    color: nth($linkColor, 2);  &#125;  //append a new element and assign to itself, no quote for green  $linkColor: append($linkColor, green);  button &#123;    color: nth($linkColor, 3);  &#125;&#125;

map functions

map-get($map, $key)
map-keys($map)
map-values($map)//define a map$headings: (  h1: 2em,  h2: 1.5em,  h3: 1.2em);@each $header, $size in $headings &#123;  #&#123;$header&#125; &#123;    font-size: $size;  &#125;&#125;button &#123;    font-size: map-get($headings, h1);&#125;

placeholder%base &#123;    // if %base is not used, this part will be not converted to css file!!    color: red;&#125;

function@function size() &#123;  @return 12px;&#125;.body &#123;  //() is a must to call a function  font-size: size();&#125;

ref
sass less stylus
sass basic
sass tutorial
scss document

]]></content>
      <categories>
        <category>web</category>
        <category>css</category>
      </categories>
      <tags>
        <tag>css</tag>
        <tag>scss</tag>
        <tag>sass</tag>
      </tags>
  </entry>
  <entry>
    <title>debugging application</title>
    <url>/2019/09/27/debugging-application/</url>
    <content><![CDATA[debug applicationHere are list of frequent commands that are used in daily life and some tips that may be useful to debug application

tested only on centos7 platform
show dynamic library used by an applicationWay 1Way 2$ ldd app_binaryoutput:linux-vdso.so.1 &#x3D;&gt;  (0x00007fff76bcd000)libopenvswitch.so.1 &#x3D;&gt; &#x2F;usr&#x2F;lib&#x2F;libopenvswitch.so.1 (0x00007f19c83c2000)libboost_system.so.1.54.0 &#x3D;&gt; &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libboost_system.so.1.54.0 (0x00007f19c81be000)…
$ readelf -d app_binary | grep NEEDEDoutput:0x0000000000000001 (NEEDED)             Shared library: [libopenvswitch.so.1]0x0000000000000001 (NEEDED)             Shared library: [libboost_system.so.1.54.0]…
then find the library path$ locate libopenvswitch.so.1&#x2F;usr&#x2F;lib&#x2F;debug&#x2F;usr&#x2F;lib&#x2F;libopenvswitch.so.1.0.0&#x2F;usr&#x2F;lib&#x2F;libopenvswitch.so.1&#x2F;usr&#x2F;lib&#x2F;libopenvswitch.so.1.0.0
how application searches library

directories listed in the LD_LIBRARY_PATH environment variable (DYLD_LIBRARY_PATH on OSX)
directories listed in the executable’s rpath($chrpath -l app)
directories on the system search path, which (on Linux at least) consists of the entries in &#x2F;etc&#x2F;ld.so.conf plus &#x2F;lib and &#x2F;usr&#x2F;lib.



create and use a static librarycreate static lib from object files$ ar cru libopenvswitch.a stream-ssl.o stream.olater on you may want to add a new object in the lib$ ar crs libopenvswitch.a foo.o
use it in your application$ gcc -o app main.c &#x2F;xx&#x2F;libopenvswitch.aOR$ gcc -o app main.c -lopenvswitch -L&#x2F;path&#x2F;to&#x2F;openvswitch
when adding library in gcc command line the order is important, let&#39;s say if A.a depends on B.a, the command should be $ gcc -o app main.c A.a B.a
show all objects in static library$ ar -t libopenvswitch.aextract object file from static library$ ar -xv libopenvswitch.a stream-ssl.o


create and use a dynamic library$ gcc -fPIC -g foo.c$ gcc -shared -o libfoo.so foo.o
Use it later on by this way$ gcc -o app main.c libfoo.soOr$ gcc -app main.c -lfoo -L&#x2F;path&#x2F;to&#x2F;foo


show symbols in dynamics and static library$ readelf -s &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libssl.so$ readelf -Ws &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libssl.a$ readelf -Ws stream-ssl.o-W&#x3D;&#x3D;–wide means show full name of the long symbol


check process threadshow all processes(not thread) of a given program$ pidof program_name
-s     Single shot - this instructs the program to return only one pid$ pidof -s program_name
-x     Scripts too - this causes the program to also return process id’s of shells running the named scripts$ pidof -x shell_scripts
-o omit pid Tells pidof to omit processes with that process id.The special pid %PPID can be used to name the parent process of the pidof program, in other words the calling shell or shell script.example:
if pidof -o %PPID -x “abc.sh”&gt;&#x2F;dev&#x2F;null; then  echo “Process already running”fi
show threads in a process$ ps -Lf $pidOr see from &#x2F;proc$ ls &#x2F;proc&#x2F;$pid&#x2F;task


do everything in memoryAs in some case, you have large memory, you can run all commands in memory, it&#39;s fast, but also note these fileswill be lost after reboot.
For example compile kernel in memory$ tar xjvf linux.tar.gz -C &#x2F;dev&#x2F;shm$ cd &#x2F;dev&#x2F;shm&#x2F;linux$ cp &#x2F;boot&#x2F;config-4.3.3-4.3.y.20151215.ol7.x86_64 .config$ make oldconfig &amp;&amp; make -j32$ make -j32 modules_install$ make install


debug dynamic library info when runs a program$ LD_DEBUG=help lsValid options for the LD_DEBUG environment variable are:libs        display library search pathsreloc       display relocation processingfiles       display progress for input filesymbols     display symbol table processingbindings    display information about symbol bindingversions    display version dependenciesscopes      display scope informationall         all previous options combinedstatistics  display relocation statisticsunused      determined unused DSOshelp        display this help message and exit$ LD_DEBUG=libs ls$ LD_DEBUG=all ls

useful commands in binutilsaddr2line, ar, gprof, nm, objdump, readelf, gcov, strip, ranlib, size, strings
ranlib: generates an index to the contents of an archive(below will generate symbol index for the archive but actually, it&#39;s not need as &#39;ar&#39; already takes care of it, you can use &#39;nm -s xxx.a&#39; to check the symbol before running ranlib)$ ranlib xxx.a
size: list the section size of an object or archive file
strings: list printable strings from files(note strings only return printable characters, strings displays all strings that are at least four characters in length in the files but can change with -n)strings is always used to get string from Binary file!!!
$ strings &#x2F;bin&#x2F;ls | grep Copyright
show c++ symbol with namespace$ nm –demangle xxx.a(–demangle shows symbol like namespace::Builder::hello(int); that is readable for human not ABXxxhello33x which is really stored by compiler)output format for nm
A: Archive symbol (from an archive file)B: BSS segment (uninitialized data)C: Common symbol (a type of BSS)D: Data segment (initialized data)G: Global symbol (for dynamic symbols, typically in shared objects)I: Indirect symbol (used in dynamic linking)N: Debugging section (symbol with no defined value)R: Read-only data segmentS: Stack segmentT: Text segment (code)U: Undefined symbol (a symbol that is referenced but not defined in this object file)W: Weak symbol (a symbol that can be overridden by a definition with the same name)t: Local (static) text (code) symbold: Local (static) data symbol


process affinitytask state: ZOMBIE(if child exits, it enters this STATE, waiting parent to read it state, after parent read it by wait() etc, child resource(task_struct) is freed)Make sure parent call wait() for its child process when they exit, otherwise resource is leak in kernel as said above!priority:        nice is given when create a process to calculate the static priority        while scheduling is based on dynamic priority (effective_prio) which considers        sleep time and static prioritycommand    show nice value    #ps -axl    default priority(20)&lt;-------&gt;nice(0)    run a program with adjustment nice value    (nice value for ls ==default_nice_value(0)+(-10)    #nice -n -10 ls    change nice value of processes    #renice -n 10 -p 1203    retrieve task cpu affinity    #taskset -p 1203     cpu7    cpu6  cpu5   cpu4 cpu3  cpu2  cpu1  cpu0       +      +     +      +    +     +     +     +       |      |     |      |    |     |     |     |       +------+-----+-----------+-----+-----+-----+       |                   |                      |       |   1111(f)         |       0000(0)        |       +   cpu mask        +       cpu mask       +    Let&#x27;s say we have 8 cpus, core id    begins from 0-7, cpu mask is bits    each bit represent a cpu, 1/0    #taskset -p 1223    0x03    affinity mask 0x03 means it runs only on cpu0, cpu1    c means use 1, 2, 3 format for cpu not 0x format!!    #taskset -cp 1223    pid 1223&#x27;s current affinity list: 0,    retrieve task cpu affinity, Only runs on process #0, set it cpu bit with 1    #taskset -p 0x01 1223    OR    #taskset -cp 0 1223CPU bandwidth    /proc/sys/kernel/sched_rt_period_us:    The scheduling period that is equivalent to 100% CPU    bandwidth    /proc/sys/kernel/sched_rt_runtime_us:    A global limit on how much time realtime scheduling may    use.

IRQ affinity settingsee cpu affinity for IRQ$ cat &#x2F;proc&#x2F;irq&#x2F;145&#x2F;smp_affinityf(0x1111)$ cat &#x2F;proc&#x2F;irq&#x2F;145&#x2F;smp_affinity_list0-3(cpu0, cpu1, cpu2, cpu3)


trace process by stracestrace&#x2F;ltrace&#x2F;ptrace&#x2F;trussshow how much time process spent on system call or library, (the library is C library like, memset, fgets etc)
ptrace is a system calllong ptrace(enum __ptrace_request request, pid_t pid, void *addr, void *data);
strace and ltrace are based on ptrace to implement their featurestrace&#x2F;truss is used to trace system call and signal when it occursltrace is used to trace a process&#39;s library call initially
truss is similar to strace, but it&#39;s for UNIX, not linux
strace is tool we selected for linux#mount -t procfs proc &#x2F;proc#strace -f  -tt -o vim.strace vim
&#x2F;*attach a process to see what system (lib) is calling *&#x2F;#strace -p pid
#strace -f -p pid(monitor parent and all its children, this is what -f option does)
#strace -o filename(save result to a file)#strace -T(show time spent on system call)#strace -t(show time of day)#strace -s 1024(max sie of string to print)#strace -e trace&#x3D;nanosleep     -e trace&#x3D;network     -e trace&#x3D;file     -e trace&#x3D;desc     -e trace&#x3D;signal
(trace a particular event)7) strace -f(trace child as well)8) strace -c(Count time, calls, and errors for each system call and report a summary on program exit)
ltrace also has options above!!!!!
so the usually use case is$ strace -f -c -o result vim
show time, spending time for system call$ strace -f -t -T -o result vimroot@manager:~# strace -f -c vim

% time     seconds  usecs/call     calls    errors syscall------ ----------- ----------- --------- --------- ---------------- 21.81    0.000935           3       309           read 15.35    0.000658           2       383       127 stat  9.96    0.000427           3       142         5 open  9.68    0.000415           6        65           munmap  6.02    0.000258           2       113           mmap  5.25    0.000225           2       145           close  4.15    0.000178           2        86        80 openat  3.48    0.000149           6        25           write  3.15    0.000135           4        36           mprotect  2.61    0.000112           2        59           select  2.59    0.000111           1       160           fcntl  2.15    0.000092           1       106           fchdir  2.05    0.000088           2        37           brk  1.94    0.000083           1        56           chdir  1.70    0.000073           1        83           fstat  1.24    0.000053           1        56           getcwd  1.19    0.000051           1        56           lseek  1.10    0.000047           4        12           getdents  1.03    0.000044           2        21        20 access  0.84    0.000036          36         1           unlink  0.63    0.000027           1        25           ioctl  0.42    0.000018           1        24           rt_sigaction  0.30    0.000013           7         2         2 connect  0.23    0.000010          10         1           rename  0.19    0.000008           4         2         2 statfs  0.16    0.000007           1         7           rt_sigprocmask  0.16    0.000007           4         2           socket  0.16    0.000007           7         1           fchown  0.12    0.000005           1         6           getuid  0.09    0.000004           4         1           sysinfo  0.05    0.000002           1         2           getrlimit  0.05    0.000002           2         1         1 futex  0.02    0.000001           1         1           execve  0.02    0.000001           1         1           uname  0.02    0.000001           1         2           umask  0.02    0.000001           1         1           sigaltstack  0.02    0.000001           1         1           arch_prctl  0.02    0.000001           1         1           set_tid_address  0.02    0.000001           1         1           set_robust_list------ ----------- ----------- --------- --------- ----------------100.00    0.004287                  2033       237 total

show shared memory on the systemshow ipc shared memory$ ipcsshow posix shared memory$ ls &#x2F;dev&#x2F;shm


only show mem&#x2F;cpu usage of a particular pid$ ps -p 13687  -o %mem&#x3D;$ ps -p 13687  -o %cpu&#x3D;


show processes who opened a file or dirwho open the dir only(top level)$ lsof $dirsearch for all open instances of directory s and the files and directories it contains at its top level$ lsof +d $ssearch for all open instances of directory D and all the files and directories it contains to its complete depth$ lsof +D $s
only this file, who opens the file$ lsof $file


force a running process to generate a core dumpgcore(from gdb) does NOT kill the process$ gcore  $pid(usually to see the snapshot of the process memory)
kill the process as send abort to the process$ kill -SIGABRT $pid


signalA few days ago, i landed upon unix signals that lead to process termination. I guess i was trying to remember the signals generated in linux when one presses Ctrl+Z and Ctrl+C. Memory did not serve me at that moment and i decided to look these up, one more time. I realized that having a consolidated book which explains these terms clearly is better than searching loads of webpages. I did the later since i had kept my unix os book away from my reach.To my disappointment, there was no single link that listed out all differences in an orderly fashion.
Hence, in this post, i wish to delineate these terms by consolidating my findings from stackoverflow, wikipedia and other unix internals websites. Here it goes:
SIGKILL: Terminates a process immediately. This signal cannot be handled (caught), ignored or blocked. (The “kill -9” command in linux generates the same signal).
SIGTERM: Terminates a process immediately. However, this signal can be handled, ignored or caught in code. If the signal is not caught by a process, the process is killed. Also, this is used for graceful termination of a process. (The “kill” command in linux if specified without any signal number like -9, will send SIGTERM)
SIGINT: Interrupts a process. (The default action is to terminate gracefully). This too, like, SIGTERM can be handled, ignored or caught. The difference between SIGINT and SIGTERM is that the former can be sent from a terminal as input characters. This is the signal generated when a user presses Ctrl+C. (Sidenote: Ctrl+C denotes EOT(End of Transmission) for (say) a network stream)
SIGQUIT: Terminates a process. This is different from both SIGKILL and SIGTERM in the sense that it generates a core dump of the process and also cleans up resources held up by a process. Like SIGINT, this can also be sent from the terminal as input characters. It can be handled, ignored or caught in code. This is the signal generated when a user presses Ctrl+.
SIGSTP: Suspends a process. This too, can be handled, ignored or blocked. Since it does not terminate the process, the process can be resumed by sending a SIGCONT signal. This signal can be generated by pressing Ctrl+Z. (Sidenote: Ctrl+Z stands for substitute character which indicates End-of-File in DOS)
SIGHUP: (From Wikipedia): Hangs up a process when the controlling terminal is disconnected. This especially relates to modem&#x2F;dial in connections. A process has to explicitly handle this signal for it to work. A good use is to “poke” a process and letting the process (as defined by the programmer) decide what to do with the signal is described here. Hence, SIGHUP can be handled, ignored or caught. This is the signal generated when a user presses Ctrl+D.
Some time, you may want to hang a process for low response
$ kill -STOP $pid$ kill -SIGCONT $pid


show environment variables of a running process.use gdb, environ is a global variable
environment variables of this process, it’s the final place, absolute right!attach to that processgdb&gt; p environgdb&gt;environ[0]gdb&gt;environ[1]gdb&gt;environ[2]…gdb&gt;environ[x]
check &#x2F;proc&#x2F;PIDxx&#x2F;environ
it only shows the environment variables when process launches, if you addednew environment variable by setenv()[C function], it doesn’t have one thatadded after process launched!!!


check process name when knows pid$ cat &#x2F;proc&#x2F;$pid&#x2F;status


memory leak&#x2F; overflow toolUse tools:valgrind and AddressSanitizer


shutdown() vs close() shutdown() is useful for deliniating when you are done providing a request to a server using TCP.  A typical use is to send a request to a server followed by a shutdown().  The server will read your request followed by an EOF (read of 0 on most unix implementations).  This tells the server that it has your full request.  You then go read blocked on the socket.  The server will process your request and send the necessary data back to you followed by a close.  When you have finished reading all of the response to your request you will read an EOF thus signifying that you have the whole response.The shutdown(s, how) call causes all or part of a full-duplex connection on the socket associated with sockfd to be shut down.- If how is SHUT_RD, further receptions will be disallowed.    - No FIN sent, only kernel set the state of socket NOT reading(the peer does not know), should not read any more    - if read on SHUT_RD, -1(ECONNRESET) reset by peer, or end of file if peer close(fd).- If how is SHUT_WR, further transmissions will be disallowed.    - cause FIN sent to peer and the peer recv EOF()(peer should not send any more, as its peer close reading)`the peer can send data even after receiving EOF`    - if write again on SHUT_WR socket, exception `SIGPIPE, Broken pipe`.- If how is SHUT_RDWR,further receptions and transmissions will be disallowed.    - Both action above- close() can do SHUT_RDWR and tell kernel to free socket resourceUsage:- shutdown + close()(only for resource free)- close() only

RESET TCP from applicationif application sent RST(RST flag set) on a tcp socket, the socket will go into Closed state imediately(no FIN sent), the peer that receives RST packet will go into peer Closed state as well, no need ACK for RST packet, this is the quick way to close tcp conenction and free port.
struct linger linger;linger.l_onoff = 1;linger.l_linger = 0;if (setsockopt(fd, SOL_SOCKET, SO_LINGER,               (const void *) &amp;linger, sizeof(struct linger)) == -1)&#123;    /* log error */&#125;//RST packet is sent when call close()close(fd);

install debuginfo of kernel or application on centos# debuginfo of kernel(kernel is not built by yourself)$debuginfo-install -y kernel-$(uname -r)# debuginfo of application or library installed by yum$cp CentOS-Debuginfo.repo /etc/yum.repos.d/$yum update$debuginfo-install libgcc-4.8.5-44.el7.x86_64

show stack for give process(thread)# show task of the group$pstack 11289# only print this process itself$/proc/11289/stack# or generate a core without kill this process$gcore $pid

pkg-configpkg-config is a tool to check dependencies for a library, it outputs version, header path, libs of that library, so that someone who uses this library passes these to compiler for building.
pkg-config gets all these information by checking xxx.pc from several paths, so that if a library wants to be managed by pkg-config, it must proivde a xxx.pc file at some path.

&#x2F;usr&#x2F;lib64&#x2F;pkgconfig&#x2F;glib.pc
prefix=/usrexec_prefix=/usrlibdir=/usr/lib64includedir=/usr/includeName: GLibDescription: C Utility LibraryVersion: 1.2.10Libs: -L$&#123;libdir&#125; -lglibCflags: -I$&#123;includedir&#125;/glib-1.2 -I$&#123;libdir&#125;/glib/include 


# get default search paths for pkg-config$pkg-config --variable pc_path pkg-config# change search paths for pkg-config$ export PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig:/new/path# list all known packages$pkg-config --list-all##################### example ###########################$pkg-config --modversion gnutls3.3.29$pkg-config --libs  gnutls-L/usr/lib64 -lgnutls$pkg-config --cflags gnutls-I/usr/include/p11-kit-1$cat /usr/lib64/pkgconfig/gnutls.pc # Process this file with autoconf to produce a pkg-config metadata file.# Copyright (C) 2004-2012 Free Software Foundation, Inc.# Copying and distribution of this file, with or without modification,# are permitted in any medium without royalty provided the copyright# notice and this notice are preserved.  This file is offered as-is,# without any warranty.# Author: Simon Josefssonprefix=/usrexec_prefix=/usrlibdir=/usr/lib64includedir=/usr/includeName: GnuTLSDescription: Transport Security Layer implementation for the GNU systemURL: http://www.gnutls.org/Version: 3.3.29Libs: -L$&#123;libdir&#125; -lgnutlsLibs.private: /usr/lib64/libz.so     -lp11-kit    -ltspi -lgmpRequires.private: nettle, hogweed, libtasn1, p11-kit-1, zlibCflags: -I$&#123;includedir&#125;##################### example ###########################
ld finding library# when for linking, ld is used, show paths checked for a library$ ld -lopenvswitch --verbose

get hardware meta$dmidecode# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 2.8 present.11 structures occupying 539 bytes.Table at 0x000F70E0.Handle 0x0000, DMI type 0, 24 bytesBIOS Information	Vendor: SeaBIOS	Version: 1.10.2-1.el7	Release Date: 04/01/2014	Address: 0xE8000	Runtime Size: 96 kB	ROM Size: 64 kB	Characteristics:		BIOS characteristics not supported		Targeted content distribution is supported	BIOS Revision: 0.0Handle 0x0100, DMI type 1, 27 bytesSystem Information	Manufacturer: Red Hat	Product Name: KVM	Version: RHEL 7.4.0 PC (i440FX + PIIX, 1996)	Serial Number: Not Specified	UUID: Not Settable	Wake-up Type: Power Switch	SKU Number: Not Specified	Family: Red Hat Enterprise Linux...####################### PCIE Slot information ============================# get free pci-e slot, lanes of each slot# then you can install devices into these slots$dmidecode --type slot...Handle 0x0902, DMI type 9, 17 bytesSystem Slot Information	Designation: PCIe Slot 4 ----&gt; like a description	Type: x16 PCI Express 3  ----&gt; 16 lanes PCIE 3.0	Current Usage: Available ----&gt; this slot is free, can be inserted with pcie device	Length: Long	ID: 4	Characteristics:		3.3 V is provided		PME signal is supportedHandle 0x0903, DMI type 9, 17 bytesSystem Slot Information	Designation: PCIe Slot 5 ----&gt; like a description	Type: x8 PCI Express 3 x16---&gt; electrical 8 lanes but x16 (physical slot), [think it as fake x16]	Current Usage: In Use     ---&gt; this slot is in use	Length: Long	ID: 5	Characteristics:		3.3 V is provided		PME signal is supported	Bus Address: 0000:86:00.0# In some linux kernel, the type may not contain pcie generation info# then check the pcie generation from link rate$lspci -s 0000:86:00.0 -vv | egrep  &quot;LnkCap|LnkSta&quot;		LnkCap:	Port #0, Speed 8GT/s, Width x8, ASPM L0s, Exit Latency L0s &lt;2us, L1 &lt;4us		LnkSta:	Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-# LinkCap(Link Capabilities Register): Speed 8GT/s(Data transfer rate)， Width x8(lanes), is the capability supported by slot on motherboard# LinkSta(Link Status Register): Speed 8GT/s, Width x8, is the result(current speed) after negotiation with pcie device inserted on this slot...####################### PCIE Slot information ============================
lspci outputlspci only shows pci(e) devices which means in-use pci(e) slot, more detail about pci device by lspci and setpci to query and set pci registers
# list all pci devices$ls -al /sys/bus/pci/deviceslrwxrwxrwx 1 root root 0 Aug 29 17:59 0000:3b:12.0 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:12.0...# We can break the device string &quot;0000:3b:12.0&quot; down as follows:# 0000 : PCI domain (each domain can contain up to 256 PCI buses)# 3b   : the bus number the device is attached to # 12   : the device number # .0   : PCI device function# To get additional information about the device, we can change into the 0000:04:00.0 directory and execute our favorite pager to display one or more pseudo-device entries:# show number instead of name$lspci -s 3b:12.0 -n3b:12.0 0100: 1af4:1042# Field 1 : 3b:12.0 : bus number (3b), device number (12) and function (0) # Field 2 : 0100    : device class # Field 3 : 1af4    : vendor ID # Field 4 : 1042    : device ID# Or more easy way and more details$lspci -s 3b:12.0 -nmvDevice:	3b:12.0Class:	0100Vendor:	1af4Device:	1042SVendor:	1172SDevice:	0001NUMANode:	0# To convert the identifiers to human-readable strings, we can look up the identifiers in the PCI ID repository: http://pci-ids.ucw.cz/# Field 2 : 0100   : class 0200 is listed as a &quot;SCSI storage controller&quot;# Field 3 : 1af4   : vendor ID 1af4 is listed as the &quot;Red Hat, Inc.&quot; # Field 4 : 1000   : device ID 1042 is listed as a &quot;Virtio 1.0 block device&quot;$ lspci3b:12.0 SCSI storage controller: Red Hat, Inc Virtio block device...
coredump without symbol tableAs in production env, symbol table is stripped to reduce the binary size, but external symbol table is generated and stored somewhere usually, so that when core happens, it’s esay for us to know why.
but what about no external symbol what we can do?Stack analysis: Analyze the stack trace in the core dump to understand the sequence of function calls leading up to the crash(use virtual address and objdump -d binary to find the assembly code, with the assembly code, try to map the source). This can provide insights into the program’s execution flow and potentially identify the source of the problem.
Register analysis: Examine the register values in the core dump to understand the state of the program at the time of the crash. This can help you identify any abnormal values or conditions that may have caused the crash.
Keep in mind that without symbols, the information obtained from the core dump analysis will be limited. It will be difficult to pinpoint the exact cause of the crash or understand the specific code paths leading to it.
network relatedenable forwarding in kernelopen &#x2F;etc&#x2F;sysctl.confenable net.ipv4.ip_forward&#x3D;1enable ipv6 forwarding set net.ipv6.conf.all.forwarding&#x3D;1then restart sysctl service
Actually, all variables in &#x2F;etc&#x2F;sysctl.conf will be applied to corresponding kernel variables through &#x2F;proc or sysctl APINote:  proc for sysctl is at  &#x2F;proc&#x2F;sys$ cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward


IP address and RouteIP AddressDisplaying existing addressesip [-6] addr show [dev ]
Add an IP addressip [-6] addr add &#x2F; dev ip -6 addr add 2001:0db8:0:f101::1&#x2F;64 dev eth0
Removing an IP addressip [-6] addr del &#x2F; dev ip -6 addr del 2001:0db8:0:f101::1&#x2F;64 dev eth0
IP Routeshow ip routes on all interfaceip [-6] route showshow ip routes on particular interfaceip [-6] route show dev eth0
Add an IP route through a gatewayip [-6] route add &#x2F; via  [dev ]ip -6 route add 2000::&#x2F;64 via 2001:0db8:0:f101::1
Removing an IP route through a gatewayip [-6] route del &#x2F; via  [dev ]ip -6 route del 2000::&#x2F;3 via 2001:0db8:0:f101::1
Add an IP route through an interfaceip [-6] route add &#x2F; dev  metric 1ip -6 route add 2000::&#x2F;3 dev eth0 metric 1
Removing an IP route through an interfaceip [-6] route del &#x2F; dev ip -6 route del 2000::&#x2F;3 dev eth0


Deep into route tableActually, kernel supports 255 route tables with priority, the two mainused ones are local table and main table, local table can’t be modified, controlled by kernelwhile main table is the default table when you use tools to add&#x2F;delete&#x2F;create route, they are in main table
which table should we use when packet comes in?First check local table, if not match, check main table, DO NOT check other table if nopolicy(ip rule) is configure, otherwise, check other table based on rule, skip local and main
For example, tag skb with mark 1, if skb with mark 1, lookup table 100
iptables -t mangle -A PREROUTING -j MARK –set-mark 1ip rule add fwmark 1 lookup 100ip route add local 0.0.0.0&#x2F;0 dev lo table 100
Show routes in different tables(default is main)ip -6 route showip -6 route show table mainip -6 route show table allip -6 route show table localip -6 route show table 100


lets explain the output fields for each route with ipv4 as example
$ ifconfigdocker0   Link encap:Ethernet  HWaddr 02:42:32:26:c0:ce          inet addr:172.17.0.2  Bcast:172.17.255.255  Mask:255.255.0.0          inet6 addr: fe80::42:32ff:fe26:c0ce/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:27408 errors:0 dropped:0 overruns:0 frame:0          TX packets:30149 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0          RX bytes:1717078 (1.7 MB)  TX bytes:67951999 (67.9 MB)eth0      Link encap:Ethernet  HWaddr 00:50:56:91:d7:c1          inet addr:10.107.60.8  Bcast:10.117.7.255  Mask:255.255.252.0          inet6 addr: fe80::250:56ff:fe91:d7c1/64 Scope:Link          inet6 addr: fc00:10:117:7:250:56ff:fe91:d7c1/64 Scope:Global          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:20424652 errors:1753 dropped:2707017 overruns:0 frame:0          TX packets:2390188 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000          RX bytes:9346260108 (9.3 GB)  TX bytes:32523341970 (32.5 GB)lo        Link encap:Local Loopback          inet addr:127.0.0.1  Mask:255.0.0.0          inet6 addr: ::1/128 Scope:Host          UP LOOPBACK RUNNING  MTU:65536  Metric:1          RX packets:754204 errors:0 dropped:0 overruns:0 frame:0          TX packets:754204 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1          RX bytes:74578327 (74.5 MB)  TX bytes:74578327 (74.5 MB)(base) root@dev:~/$ ip route showdefault via 10.117.7.253 dev eth0 onlink# default gateway 10.117.7.253 output eth010.117.4.0/22 dev eth0  proto kernel  scope link  src 10.117.6.8# scope link(local network 10.117.4.0/22), src 10.117.6.8 eth0&#x27;s ip172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.2# scope link(local network 172.17.0.0/16), src 172.17.0.2 docker0&#x27;s ip#local network, match subnet.(base) root@dev:~/# ip route show table localbroadcast 10.117.4.0 dev eth0  proto kernel  scope link  src 10.117.6.8local 10.117.6.8 dev eth0  proto kernel  scope host  src 10.117.6.8# local mapped to RT_LOCAL(kernel) eth0, scope host-&gt;0 hop, myselfbroadcast 10.117.7.255 dev eth0  proto kernel  scope link  src 10.117.6.8broadcast 127.0.0.0 dev lo  proto kernel  scope link  src 127.0.0.1local 127.0.0.0/8 dev lo  proto kernel  scope host  src 127.0.0.1local 127.0.0.1 dev lo  proto kernel  scope host  src 127.0.0.1# local mapped to RT_LOCAL(kernel) lo, scope host-&gt;0 hop, myselfbroadcast 127.255.255.255 dev lo  proto kernel  scope link  src 127.0.0.1broadcast 172.17.0.0 dev docker0  proto kernel  scope link  src 172.17.0.2local 172.17.0.2 dev docker0  proto kernel  scope host  src 172.17.0.2# local mapped to RT_LOCAL(kernel) docker0, scope host-&gt;0 hop, myselfbroadcast 172.17.255.255 dev docker0  proto kernel  scope link  src 172.17.0.2# local address, match 255.255.255.255, exact match
prevent route missing after rebootThere are two ways to do this

add static route
# Centos# /etc/sysconfig/network-scripts/route-eth0 created if not theredefault via 10.10.10.1 dev eth010.117.0.0/16 via 10.117.1.1 dev eth0# from terminal restart network service$sudo service network restart

save route table and restore it after reboot
$sudo ip route save &gt;dump# after reboot$sudo ip route restore &lt;dump



show arp entry# incomplete means request is sent and no reply$ arp -nAddress                  HWtype  HWaddress           Flags Mask            Iface172.17.0.6                       (incomplete)                              docker0172.17.0.2                       12:64:0b:38:ea:49                         docker010.226.134.65            ether   1c:ab:34:33:81:84   C                     eth0127.0.0.3                ether   02:00:00:12:34:05   CM                    loopback-279# if you delete an entry, it will be marked incomplete, you still see such entry$ arp -d 172.17.0.2$ arp -nAddress                  HWtype  HWaddress           Flags Mask            Iface172.17.0.6                       (incomplete)                              docker0172.17.0.2                       (incomplete)                              docker010.226.134.65            ether   1c:ab:34:33:81:84   C                     eth0127.0.0.3                ether   02:00:00:12:34:05   CM                    loopback-279Flags:C-----&gt;entry is completedM------&gt;Permanent entries, user added.

save tcpdump captured packet to file$ tcpdump -i eth0 -s 0 -w &#x2F;home&#x2F;lzq&#x2F;pt.pcap-s limits the size of packet-o indicates no limition!


packet shows as tcp segment of a reassembled pdu in wiresharkThis is due to application message(PDU) is larger than MSS, tcp splits it into several tcp segments, but tcp segment has no Fragment flag, how does wireshark know several tcp segments belong to same PDU?
wireshark thinks if several tcp segments have the same ACK number but different sequence numbers, they belong to same PDU, refer to 


get two sides of established STREAM socketSTREAM socket can be over TCP or unix, for TCP, it’s easy to get it, here we foucus on UNIX STREAM socket with ss command.
State      Recv-Q Send-Q Local Address:Port              Peer Address:Port                local process who uses local address          # -t for tcp# -u for udp# -w for Raw# -x for unix# -p show process name and id$ss -tp | grep libvirtdESTAB      0      0      172.17.0.2:55758                172.17.0.3:16508                 users:((&quot;libvirtd&quot;,pid=5056,fd=22))# for unix established socket# stream client side: Peer Address is *, Port is unix socket inode, same thing for local address and port as well# stream server side: Peer Address is *, Port is unix socket inode, while local address is unix path, port is unix socket inode# as you can see virsh use unix socket(254243) to connect peer who uses 254245 unix socket$ss -xp | grep virshu_str  ESTAB      0      0       * 254243                * 254245                users:((&quot;virsh&quot;,pid=11049,fd=6))# it&#x27;s clear# virsh connects with libivrt on path /var/run/libvirt/libvirt-sock $ss -xp | grep 254245u_str  ESTAB      0      0      /var/run/libvirt/libvirt-sock 254245                * 254243                users:((&quot;libvirtd&quot;,pid=5056,fd=21))u_str  ESTAB      0      0       * 254243                * 254245                users:((&quot;virsh&quot;,pid=11049,fd=6))

show all sockets$ netstat -a -t     #all tcp  sockets$ netstat -a -u     #all udp  sockets$ netstat -a -w     #all raw  sockets$ netstat -a -x     #all unix sockets$ netstat -tulpn    #l means listening p with program nameNote: if -a is not present, it will only display socket in established state
Show all files opened by a process and -P(show number not name for host, port)$ lsof -p pid -Pshow which process opens a socket on particular port# lsof -i:80


check&#x2F;set mtu or MAC$ ifconfig eth1$ ifcofnig eth1 mtu 1500
temporary$ ifconfig eth0 down$ ifconfig eth0 hw ether 00:80:48:BA:d1:30$ ifconfig eth0 up
permanentlyCentos7edit &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0    MACADDR&#x3D;02:01:02:03:04:0


socket bind to no-local addressif you bind() normally, the ip address must be one of the host’s address
bind can be usedin two cases:

server call socket&#x2F;bind&#x2F;listen&#x2F;accpet, that means only the dst is for bind address, it will accept it, otherwise no.
client call bind, socket&#x2F;bind&#x2F;send, that means client uses the bind address as the source address when sending packet.

but for some case, you want to bind an address which is no-local(none of the host address) which is always need for loadbalancer server!
global setting, so that every application can bind non local address$ echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_nonlocal_bind
enable it on a socket of the processIn programe code, call setsocketopt(IP_TRANSPARENT, 1), then$ setcap CAP_NET_ADMIN program(before program runs)
IP_TRANSPARENT (since Linux 2.6.24)Setting this boolean option enables transparent proxying onthis socket.  This socket option allows the callingapplication to bind to a nonlocal IP address and operate bothas a client and a server with the foreign address as the localendpoint.
NOTE: for receiving non-local address that requires routing should be set up in a way that packets going to the foreign address are routed through the TProxy box (i.e., the system hosting the application that employs the IP_TRANSPARENT socket option), make sure packet is not dropped before reaching tproxy module in kernel
Enabling this socket option requires superuser privilege(the CAP_NET_ADMIN capability).


source port selectionif not set source port explicitly by bind(), source port selected from below range$ cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range32768	60999
change the source port range$ sysctl -w  net.ipv4.ip_local_port_range&#x3D;1024 4095


create vlan interface and check its real devAs vlan interface is a virtual device, so it must attach to a ‘real’ device, ‘real’ does not mean it must be a physical, it could be another virtual device as well, when send traffic on the vlan interface, it sends out to ‘real’ device after adding vlan id, call ‘real’ device’s dev_queue_xmit().
# must provide &#x27;real&#x27; device when creating vlan interface, two vlan interfaces can point to same &#x27;real&#x27; deivce.$ ip link add link ens192 name ens192.100 type vlan id 100$ ip link set dev ens192.100 up$ ip -d link show ens192.100446: ens192.100@ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000    link/ether 00:50:56:b2:01:75 brd ff:ff:ff:ff:ff:ff promiscuity 0    vlan protocol 802.1Q id 100 &lt;REORDER_HDR&gt; addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535# ens192.100@ens192  &#x27;real device&#x27; ens192# vlan protocol 802.1Q id 100$ ip link add link ens192 name ens192.200 type vlan id 200$ ip link set dev ens192.200 up$ ip -d link show ens192.200447: ens192.200@ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000    link/ether 00:50:56:b2:01:75 brd ff:ff:ff:ff:ff:ff promiscuity 0    vlan protocol 802.1Q id 200 &lt;REORDER_HDR&gt; addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535# ens192.200@ens192  &#x27;real device&#x27; ens192# vlan protocol 802.1Q id 200

check tcp fastopen cookieWhen tcp fastopen is enabled both on client and server side, client sends cookie request as TCP option, server sends cookie as TCP option as well, client saves it at kernel with key(dstip: port), when next TCP SYN will send the cookie if for same dst:port
# check cookie saved by kernel$ sudo ip tcp_metrics10.10.10.10 age 4.764sec cwnd 10 rtt 110us rttvar 188us fo_mss 65495 fo_cookie a427a77724e9229f source 10.117.6.2# fo_cookie is sent by server and save at client in kernel# check cookie only for dst 10.10.10.10$ sudo ip tcp_metrics 10.10.10.10# flush cookie$ sudo ip tcp_metrics flush$ sudo ip tcp_metrics flush 10.10.10.10
NOTE when TCP fastopen is enabled(setsockopt), use send() not connect() to setup TCP connection
How to know if a network interface is tap, tun, bridge or physicalethtool -i tunOrTapDeviceName

In case of a TAP device we will get: “bus-info: tap”.
In case of a TUN device we will get: “bus-info: tun”.

$ ethtool -i vnet0driver: tunversion: 1.6firmware-version: expansion-rom-version: bus-info: tapsupports-statistics: nosupports-test: nosupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: no

bind irq(s) of ethx to specific cpuirq binding
do it by yourself

get irq of the ethx
set affinity of each irq

use vendor script

Mellanox: &#x2F;usr&#x2F;sbin&#x2F;set_irq_affinity_bynode.sh socket ethN
Chelsio: &#x2F;sbin&#x2F;t4_perftune.sh

# get irq of eth0, for ethx that support multiple queues, for each queue there is a irq number$grep eth0-TxRx /proc/interrupts | awk &#x27;&#123;printf &quot;  %s\n&quot;, $1&#125;&#x27;103:104:105:106:107:108:109:110:111:112:113:114:115:116:117:118:...# cpu 0 for irq 103$echo 1 &gt; /proc/irq/103/smp_affinity

check rss of ethx#+++++++++++++++++++++++++++++ One Way++++++++++++++++++++++++++++++++++++++++# if rss is enabled, ethx should have multiple queues# with hardware rss $ls /sys/class/net/eth0/queues/rx-0   rx-12  rx-16  rx-2   rx-23  rx-27  rx-30  rx-34  rx-38  rx-41  rx-45  rx-49  rx-52  rx-56  rx-6   rx-7  tx-1   tx-13  tx-17  tx-20  tx-24  tx-28  tx-31  tx-35  tx-39  tx-42  tx-46  tx-5   tx-53  tx-57  tx-60  tx-8rx-1   rx-13  rx-17  rx-20  rx-24  rx-28  rx-31  rx-35  rx-39  rx-42  rx-46  rx-5   rx-53  rx-57  rx-60  rx-8  tx-10  tx-14  tx-18  tx-21  tx-25  tx-29  tx-32  tx-36  tx-4   tx-43  tx-47  tx-50  tx-54  tx-58  tx-61  tx-9rx-10  rx-14  rx-18  rx-21  rx-25  rx-29  rx-32  rx-36  rx-4   rx-43  rx-47  rx-50  rx-54  rx-58  rx-61  rx-9  tx-11  tx-15  tx-19  tx-22  tx-26  tx-3   tx-33  tx-37  tx-40  tx-44  tx-48  tx-51  tx-55  tx-59  tx-62rx-11  rx-15  rx-19  rx-22  rx-26  rx-3   rx-33  rx-37  rx-40  rx-44  rx-48  rx-51  rx-55  rx-59  rx-62  tx-0  tx-12  tx-16  tx-2   tx-23  tx-27  tx-30  tx-34  tx-38  tx-41  tx-45  tx-49  tx-52  tx-56  tx-6   tx-7# no rss$ls /sys/class/net/eth0/queues/rx-0 tx-0# with RSS, there are multiple irqs, for each irq there are two queues rx-x and tx-x#+++++++++++++++++++++++++++++ Another Way++++++++++++++++++++++++++++++++++++++++$ethtool -l eth0Channel parameters for eth0:Pre-set maximums:RX:		0TX:		0Other:		0Combined:	1 --------------------------&gt; one queue, no RSSCurrent hardware settings:RX:		0TX:		0Other:		0Combined:	1$ethtool -l eth0Channel parameters for eth0:Pre-set maximums:RX:		0TX:		0Other:		1Combined:	63-------------------------&gt;63 queueus(63 rx and 63 tx), RSS supported by hardware!!!Current hardware settings:RX:		0TX:		0Other:		1Combined:	63

get bus info for a network device like eth0################# use ethtool=====================# you can also get bus info, driver info etc$ethtool -i eth0driver: mlx5_core      --------------------&gt; driver infoversion: 5.6-1.0.3firmware-version: 16.33.1048 (MT_0000000241)expansion-rom-version: bus-info: 0000:4b:00.0 --------------------&gt; bus info heresupports-statistics: yes...################ check /sys filesystem ===========$grep PCI_SLOT_NAME /sys/class/net/*/device/uevent | grep eth0/sys/class/net/eth0/device/uevent:PCI_SLOT_NAME=0000:4b:00.0# check device for this (it&#x27;s physical function)$lspci -D | grep 0000:4b:00.00000:4b:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5]# show more details about this device$lspci -vv -s 0000:4b:00.04b:00.0 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5] ---------------------&gt;Mellanox Connect-5	Subsystem: Mellanox Technologies Device 0052	Physical Slot: 19                                                          ----------------------&gt;physical Slot    ...	Interrupt: pin A routed to IRQ 18                                          ----------------------&gt; IRQ	NUMA node: 0                                                               ----------------------&gt; Numa node it belongs to	Region 0: Memory at 9c000000 (64-bit, prefetchable) [size=32M]	Expansion ROM at 99000000 [disabled] [size=1M]    ....	Capabilities: [180 v1] Single Root I/O Virtualization (SR-IOV)             -----------------------&gt;Caps like SR-IOV		IOVCap:	Migration-, Interrupt Message Number: 000		IOVCtl:	Enable- Migration- Interrupt- MSE- ARIHierarchy+		IOVSta:	Migration-		Initial VFs: 127, Total VFs: 127, Number of VFs: 0, Function Dependency Link: 00		VF offset: 2, stride: 1, Device ID: 1018		Supported Page Size: 000007ff, System Page Size: 00000001		Region 0: Memory at 00000000a5f00000 (64-bit, prefetchable)		VF Migration: offset: 00000000, BIR: 0    ...	Kernel driver in use: mlx5_core                                           -------------------------&gt; driver in use	Kernel modules: mlx5_core####################### another example##################################$ethtool -i enp75s17f2np0driver: mlx5_coreversion: 5.6-1.0.3firmware-version: 16.33.1048 (MT_0000000241)expansion-rom-version: bus-info: 0000:4b:11.2....$lspci -D | grep 0000:4b:11.20000:4b:11.2 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5 Virtual Function]----&gt; VF# show only given pci device$lspci -vv -s 0000:4b:11.24b:11.2 Ethernet controller: Mellanox Technologies MT27800 Family [ConnectX-5 Virtual Function]  -----&gt; Mellanox VF	Subsystem: Mellanox Technologies Device 0052	NUMA node: 0                                                                                 ------&gt; no physical slot,no IRQ, numa node it blongs to	Region 0: [virtual] Memory at 9e900000 (64-bit, prefetchable) [size=1M]    ...    Capabilities: [9c] MSI-X: Enable+ Count=6 Masked-                                            -------&gt; MSI-X with six msx-irqs        Vector table: BAR=0 offset=00002000        PBA: BAR=0 offset=00003000	Kernel driver in use: mlx5_core	Kernel modules: mlx5_core

create binding interface through sysfs#first check all existing bindings$modprobe --first-time bonding$cat /sys/class/net/bonding_masters# create binding interface$echo +bond0 &gt;/sys/class/net/bonding_masters(after this #ifconfig -a to check or /sys/class/net/bond1 created)# add slaves to binding interface$ifconfig eth0 down$ifconfig eth1 down$echo +eth0 &gt;/sys/class/net/bond1/bonding/slaves$echo +eth1 &gt;/sys/class/net/bond1/bonding/slaves# configure binding interface like mode and its address$echo active-backup &gt;/sys/class/net/bond1/bonding/mode$ifconfig bond1 192.168.100.1 netmask 255.255.255.0# enable bonding interface$ifconfig bond1 up# (these will bring up all slaves in it)# NOTE: remove in revert order with &quot;-&quot; prefix
icmp ping is ok but tcp connect failsTcp connection fails with ‘NO route to host’ while ping is ok, it’s probably packet is dropped by firewall.
# --- Way1: use tcpdump to see where packet dropped# 04:10:28.906424 IP dev-162 &gt; 22.7.73.161: ICMP host dev-162 unreachable - admin prohibited filter, length 68 -----&gt; as you can seee &quot;admin prohibited&quot; means firewall drops it.# --- Way2: check iptables rulessudo iptables -L -v# --- Way3: check firewalld(firwalld can use iptables or nftable as backend)# you can not see firewall ruels if it uses nftable as backend.$ sudo systemctl status firewalld# To see the current configuration, including allowed services and ports, use:$ sudo firewall-cmd --list-all# For a more detailed view of all active zones and their settings, use:$ sudo firewall-cmd --get-active-zones$ sudo firewall-cmd --get-default-zone# list rich rules$ sudo firewall-cmd --list-rich-rules]]></content>
      <categories>
        <category>linux</category>
        <category>command</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>linux command</tag>
      </tags>
  </entry>
  <entry>
    <title>distribute_HA_keepalived</title>
    <url>/2021/06/15/distribute-HA-keepalived/</url>
    <content><![CDATA[IntroductionLoad balancing is a method of distributing IP traffic across a cluster of real servers, providing one or more highly available virtual services. When designing load balanced topologies, it is important to account for the availability of the load balancer itself as well as the real servers behind it.
Keepalived provides frameworks for both load balancing and high availability. The load balancing framework relies on the well-known and widely used Linux Virtual Server (IPVS) kernel module, which provides Layer 4 load balancing. Keepalived implements a set of health checkers to dynamically and adaptively maintain and manage load balanced server pools according to their health. 
high-availability is achieved by the Virtual Router Redundancy Protocol (VRRP). VRRP is a fundamental brick for router failover. In addition, Keepalived implements a set of hooks to the VRRP finite state machine providing low-level and high-speed protocol interactions. In order to offer fastest network failure detection, Keepalived implements the Bidirectional Forwarding Detection (BFD) protocol. VRRP state transition can take into account BFD hints to drive fast state transition. Keepalived frameworks can be used independently or all together to provide resilient infrastructures.
In short, Keepalived provides two main functions:

Health checking for LVS systems
Implementation of the VRRPv2 stack to handle load balancer failover

In this article, we only talk about high-availability， load balancer failover.
Inside keepalivedVRRPThe Virtual Router Redundancy Protocol (VRRP) is a computer networking protocol that provides for automatic assignment of available Internet Protocol (IP) routers to participating hosts. This increases the availability and reliability of routing paths via automatic default gateway selections on an IP subnetwork.
The protocol achieves this by creation of virtual routers, which are an abstract representation of multiple routers, i.e. Primary&#x2F;Active and Secondary&#x2F;Standby routers, acting as a group. The virtual router is assigned to act as a default gateway of participating hosts, instead of a physical router. If the physical router that is routing packets on behalf of the virtual router fails, another physical router is selected to automatically replace it. The physical router that is forwarding packets at any given time is called the Primary&#x2F;Active router.
VRRP provides information on the state of a router, not the routes processed and exchanged by that router.
Physical routers within the virtual router must communicate within themselves using packets with multicast IP address 224.0.0.18(newly implementation support unicast heartbeat to peers) and IP protocol number 112.
Routers have a priority of between 1 and 254 and the router with the highest priority will become the Primary&#x2F;Active. The default priority is 100.
Elections of Primary&#x2F;Active routers  
A failure to receive a multicast packet from the Primary&#x2F;Active router for a period longer than three times the advertisement timer causes the Secondary&#x2F;Standby routers to assume that the Primary&#x2F;Active router is dead. The virtual router then transitions into an unsteady state and an election process is initiated to select the next Primary&#x2F;Active router from the Secondary&#x2F;Standby routers. This is fulfilled through the use of multicast packets.
Secondary/Standby router(s) are only supposed to send multicast packets during an election process. One exception to this rule is when a physical router is configured with a higher priority than the current Primary/Active, which means that on connection to the network it will preempt the Primary/Active status. This allows a system administrator to force a physical router to the Primary&#x2F;Active state immediately after booting, for example when that particular router is more powerful than others within the virtual router. The Secondary&#x2F;Standby router with the highest priority becomes the Primary&#x2F;Active router by raising its priority above that of the current Primary&#x2F;Active. It will then take responsibility for routing packets sent to the virtual gateway’s MAC address. In cases where Secondary&#x2F;Standby routers all have the same priority, the Secondary&#x2F;Standby router with the highest IP address becomes the Primary&#x2F;Active router.
All physical routers acting as a virtual router must be in the same local area network (LAN) segment(newly implementation support unicast). Communication within the virtual router takes place periodically. This period can be adjusted by changing advertisement interval timers. The shorter the advertisement interval, the shorter the black hole period, though at the expense of more traffic in the subnet.
Once the new master has been elected, it sends out a “gratuitous ARP.”, every host has an ARP table that ties IP addresses to Ethernet addresses. A gratuitous ARP is an unsolicited message with an IP address to Ethernet address mapping. All hosts receiving the gratuitous ARP update their tables, which effectively means that the virtual IP address is owned by a new device on the network.
Note that whether we use VRRP in multicast or unicast mode, we are not using UDP&#x2F;IP or TCP&#x2F;IP. VRRP is its own protocol on top of IP that is independent of either of those
keepalived casesEven keepalived supports nodes located at different subnet, but the best choice is to run them at same subnet.
different subnets nodes  
# VRRP advertisements ordinarily go out over multicast. This # configuration paramter causes keepalived to send them# as unicasts. This specification can be useful in environments# where multicast isn&#x27;t supported or in instances where you want# to limit which devices see your VRRP announcements. The IP# address(es) can be IPv4 or IPv6, and indicate the real IP of# other members.unicast_peer &#123;    10.5.132.122&#125;

nodes are at same subnet, no across router

Two nodes runs keepalived
# Ubuntunode1# apt-get install -y keepalivednode2# apt-get install -y keepalived# Centos7node1# yum install -y keepalivednode2# yum install -y keepalived# node1 keepalived confnode1# cat /etc/keepalived/keepalived.confglobal_defs &#123;                                                                      notification_email &#123;                                                               jason_lkm@163.com   &#125;                                                                               notification_email_from keepalived@cyun.tech                           smtp_server 192.168.100.1                                                      smtp_connect_timeout 30                                                         router_id LVS_DEVEL                                                          &#125;vrrp_instance VI_1 &#123;  state MASTER  interface eth0  virtual_router_id 51  priority 100  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 12345  &#125;  virtual_ipaddress &#123;    10.117.5.123 dev eth0    #10.117.5.111 dev eth0    # should be nginx VIP(it should be public IP in production), nginx should run at this node as well    # (nginx can listen on any address even it&#x27;s not local or exist on interface)  &#125;&#125;node1# service keepalived restart#node2 keepalived node2# cat /etc/keepalived/keepalived.confglobal_defs &#123;                                                                      notification_email &#123;                                                              jason_lkm@163.com   &#125;                                                                               notification_email_from keepalived@cyun.tech                           smtp_server 192.168.100.1                                                      smtp_connect_timeout 30                                                         router_id LVS_DEVEL                                                          &#125;vrrp_instance VI_1 &#123;  state BACKUP  interface eth0  virtual_router_id 51  priority 80  advert_int 1  authentication &#123;    auth_type PASS    auth_pass 12345  &#125;  virtual_ipaddress &#123;    10.117.5.123 dev eth0  &#125;&#125;node2# service keepalived restart


vrrp_instance defines an individual instance of the VRRP protocol running on an interface.
state defines the initial state that the instance should start in, but may not be final state due to master selection algorithm.
interface defines the interface that VRRP runs on.
virtual_router_id is the unique identifier, should be same for the all nodes.
priority is the advertised priority used for master&#x2F;slave election.
advert_int specifies the frequency that advertisements are sent at (1 second, in this case).
authentication specifies the information necessary for servers participating in VRRP to authenticate with each other. In this case, a simple password is defined.
virtual_ipaddress defines the IP addresses (there can be multiple) that VRRP is responsible for.

If you’re using a host-based firewall, such as firewalld or iptables, then you need to add the necessary rules to permit IP protocol 112 traffic.
Debug keepalived# check virtual ip configured or not on master$ ip addr show eth0$ service keepalived status$ tcpdump -i eth0 vrrp# if virtual IP is not local$ sysctl -w net.ipv4.ip_nonlocal_bind=1# run keepalived in forground and detail logs!!!$ keepalived -d -D -l -n
split-brainIn a highly available (HA) system, when the &quot;heartbeat&quot; linking the two nodes is disconnected, the HA system, which was originally a whole and coordinated in action, splits into two independent individuals. Since they lost contact with each other, they thought it was the other party that had malfunctioned. The HA software on the two nodes is like a “brain splitter”. If they compete for &quot;shared resources&quot; and compete for &quot;application services&quot;, serious consequences will occur-or if the shared resources are divided and the &quot;services&quot; on both sides will not&#96;. Coming; or both “services” are up, but at the same time reading and writing “shared storage”, resulting in data corruption (common errors such as online logs polled by the database).
Two active nodes, same virtual IP configured at differetn nodes
Why it happens

The heartbeat link between the pair of highly available servers fails, which prevents normal communication. If the heartbeat line is broken (including broken, aging).
Because the network card and related drivers are broken, IP configuration and conflict problems (network card direct connection).
Due to the failure of the equipment connected between the heartbeat cables (network card and switch).
There is a problem with the arbitration machine (using the arbitration scheme).
The iptables firewall is turned on on the high availability server to block the transmission of heartbeat messages.
In the same VRRP instance in the Keepalived configuration, if the virtual_router_id parameter settings on both ends are inconsistent, split-brain problems can also occur.
vrrp instance names are inconsistent and their priorities are the same

avoid it
Add redundant heartbeat wires, for example: double-line wires (heartbeat wires are also HA), to minimize the occurrence of “split brain”

Enable disk lock. The serving party locks the shared disk, and when the “split brain” occurs, let the other party completely “snatch away” the shared disk resources. But there is also a problem with using locked disks. If the party occupying the shared disk does not actively “unlock” it, the other party will never get the shared disk. In reality, if the service node suddenly crashes or crashes, it is impossible to execute the unlock command. The backup node cannot take over shared resources and application services. So someone designed a “smart” lock in HA. That is: the party that is serving only enables the disk lock when it finds that the heartbeat line is all disconnected (the peer end is not detected). Usually it is not locked.

Set up an arbitration mechanism. For example, set the reference IP (such as the gateway IP). When the heartbeat line is completely disconnected, both nodes ping the reference IP. If they fail, the breakpoint is at the local end. Not only the “heartbeat”, but also the external “service” of the local network link is broken, even if the application service is started (or continued) is useless, then actively give up competition and let the end that can ping the reference IP to start the service . More secure, the party that cannot ping the reference IP simply restarts itself to completely release the shared resources that may be occupied.

Script detection and alarm


The last two are commonly used in production env
troubleshootingCheck why  

first make sure, config is correct, check /etc/keepalived/keepalived.conf
check route is ok
check iptables to allow vrrp

REF
Keepalived User Guide
VRRP wiki
VRRP Example

]]></content>
      <categories>
        <category>distribute</category>
        <category>keepalived</category>
      </categories>
      <tags>
        <tag>distribute</tag>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title>distribute_zookeeper</title>
    <url>/2021/04/23/distribute-zookeeper/</url>
    <content><![CDATA[OverviewApache ZooKeeper is basically a distributed(cluster) coordination service for managing a large set of hosts. Coordinating and managing the service in the distributed environment is really a very complicated process. Apache ZooKeeper, with its simple architecture and API, solves this issue. ZooKeeper allows the developer to focus on the core application logic without being worried about the distributed nature of the application.
Apache ZooKeeper is basically a service that is used by the cluster to coordinate between themselves and maintain the shared data with the robust synchronization techniques.
Apache ZooKeeper is itself a distributed application providing services for writing the distributed application.

Distributed ApplicationA distributed application is the application that can run on the multiple systems in a network simultaneously by coordinating among themselves in order to complete the specific task in an efficient manner. The distributed application can complete the complex and the time-consuming tasks in minutes as compared to the non-distributed application that will take hours to complete the task.
The distributed application uses the computing capabilities of all the machines involved. We can further reduce the time to complete the task by configuring the distributed application to run on more nodes in the cluster.
A distributed application consists of two parts, that is, Server and Client application. The server applications are actually distributed and they have a common interface so that the clients can connect to any server in a cluster and get the same result. The client applications were the tools for interacting with the distributed application.
Benefits of Distributed Applications

Reliability − If the single node or the few systems fails, then it doesn’t make the whole system fail.
Scalability − We can easily increase the performance when needed by adding more machines with the minor changes in the application configuration without any downtime.
Transparency − It hides the complexity of the entire system and depicts itself as a single entity or application.

ZookeeperThe various services provided by Apache ZooKeeper are as follows

Naming service − This service is for identifying the nodes in the cluster by the name. This service is similar to DNS, but for nodes.
Configuration management − This service provides the latest and up-to-date configuration information of a system for the joining node.
Cluster management − This service keeps the status of the Joining or leaving of a node in the cluster and the node status in real-time.
Leader election − This service elects a node as a leader for the coordination purpose.
Locking and synchronization service − This service locks the data while modifying it. It helps in automatic fail recovery while connecting the other distributed applications such as Apache HBase.
Highly reliable data registry − It offers data availability even when one or a few nodes goes down.

Architecture

Server: The server sends an acknowledge when any client connects. In the case when there is no response from the connected server, the client automatically redirects the message to another server.

Client: Client is one of the nodes in the distributed application cluster. It helps you to accesses information from the server. Every client sends a message to the server at regular intervals that helps the server to know that the client is alive.

Leader: One of the servers is designated a Leader. It gives all the information to the clients as well as an acknowledgment that the server is alive. It would performs automatic recovery if any of the connected nodes failed.

Follower: Server node which follows leader instruction is called a follower.

Client read requests are handled by the correspondingly connected Zookeeper server
The client writes requests are handled by the Zookeeper leader.


Ensemble&#x2F;Cluster: Group of Zookeeper servers which is called ensemble or a Cluster. You can use ZooKeeper infrastructure in the cluster mode to have the system at the optimal value when you are running the Apache.


Writes in ZookeeperIn Zookeeper, all the writes go through the Master(leader) node. Due to this all the writes are sequential. While performing the write operation in Zookeeper, each of the servers which are attached to that client persists data along with the master. This updates all the servers(follower, leader etc) about the data. This also means that we cannot make concurrent writes. The guarantee for linear writes can be problematic if we use Zookeeper for writing dominant workload.
Reads in ZookeeperZookeeper is best at reads. Reads can be concurrent. In Zookeeper, concurrent reads are performed as each client is attached to a different server and all the clients can read data from the servers simultaneously. It may sometimes happen that the client may have an outdated view. This gets updated within a little time.
Data Model

The zookeeper data model follows a Hierarchal namespace where each node is called a ZNode. A node is a system where the cluster runs.

Key Znode features you need to know:

Znodes can store data and have children Znode at same time
It can store information like the current version of data changes in Znode, transaction Id of the latest transaction performed on the Znode.
Each znode can have its access control list(ACL), like the permissions in Unix file systems. Zookeeper supports: create, read, write, delete, admin(set&#x2F;edit permissions) permissions.
Znodes ACL supports username&#x2F;password-based authentication on individual znodes too.
Clients can set a watch on these Znodes and get notified if any changes occur in these znodes.
These change&#x2F;events could be a change in znodes data, change in any of znodes children, new child Znode creation or if any child Znode is deleted under the znode on which watch is set.

The main purpose of the Zookeeper data model is:

To maintain the synchronization in a zookeeper cluster
To explain the metadata of each Znode.

Node Types in Zookeeper
Persistence ZnodePersistence Znode are the nodes that stay alive even when the client who created the node is disconnected. All the server nodes in the ensemble assume themselves to be the Persistence Znodes, To remove these Znodes, you need to delete them manually(use delete operation)

Ephemeral ZnodeThe Ephemeral Znode are the nodes that stay alive until the client is alive or connected to them. They die when the client gets disconnected. Ephemeral Znode are not allowed to have children. They play an important role in the leader elections.Zookeeper clients keep sending the ping request to keep the session alive. If Zookeeper does not see any ping request from the client for a period of configured session timeout, Zookeeper considers the client as dead and deletes the client session and the Znode created by the client.

Sequential ZnodeSequential Znode can be either the Persistence Znode or the Ephemeral Znode. While creating a new Sequential Znode, the ZooKeeper sets the path of the Znode by attaching the 10 digit sequence number to the original name. This znode plays an important role in the Locking and Synchronization


WatchesZookeeper, a watch event is a one-time trigger which is sent to the client that set watch. It occurred when data from that watch changes. watch allows clients to get notifications when znode changes. read operations like getData(), getChidleren(), exist have the option of setting a watch.
Watches are ordered, the order of watch events corresponds to the order of the updates. A client will able to see a watch event for znode before seeing the new data which corresponds to that znode.Access Control list
ZNode ACLZookeeper uses ACLs to control access to its znodes. ACL is made up of a pair of (Scheme: id, permission)
Build in ACL schemes:

world: has a single id, anyone
auth: Not use any id, It represents any authenticated user
digest: use a username: password
host: Allows you to use client’s hostname as ACL id identity
IP: use the client host IP address as ACL id identity

ACL Permissions:

CREATE
READ
WRITE
DELETE
ADMIN

E.x. (IP: 192.168.0.0/16, READ)
Session

Before executing any request, it is important that the client must establish a session with service
All operations clients are sent to service are automatically associated with a session
The client may connect to any server in the cluster. But it will connect to only a single server
The session provides “order guarantees”. The requests in the session are executed in FIFO order
The main states for a session are 1) Connecting, 2) Connected 3) Closed 4) Not Connected.Leader SelectionWe will discuss three algorithms for the leader election.

Approach 1:
A client(any server belonging to the cluster) creates a persistent znode /election in Zookeeper.
All clients add a watch to /election znode and listen to any children znode deletion or addition under /election znode.
Now each server joining the cluster will try to create an ephemeral znode /leader under node /election with data as hostname, ex: node1.domain.com
Since multiple servers in the cluster will try to create znode with the same name(/leader), only one will succeed, and that server will be considered as a leader.
Once all servers in the cluster completes above step, they will call getChildren(“/election”) and get the data(hostname) associated with child znode “/leader”, which will give the leader’s hostname.
At any point, if the leader server goes down, Zookeeper will kill the session for that server after the specified session timeout. In the process, it will delete the node /leader as it was created by leader server and is an ephemeral node and then Zookeeper will notify all the servers that have set the watch on /election znode, as one of the children has been deleted.
Once all server gets notified that the leader is dead or leader’s znode(/leader) is deleted, they will retry creating “/leader” znode and again only one server will succeed, making it a new leader.
Once the /leader node is created with the hostname as the data part of the znode, zookeeper will again notify all servers (as we have set the watch in step 2).
All servers will call getChildren() on “/election” and update the new leader in their memory.

The problem with the above approach is, each time &#x2F;leader node is deleted,Zookeeper will send the notification to all servers and all servers will try to write to zookeeper to become a new leader at the same time creating a herd effect. If we have a large number of servers, this approach would not be the right idea.Ways to avoid, herd effect could be:(i) by restricting the number of servers that take part in the election and allow only a few servers to update &#x2F;election znodeOR(ii) by using sequential znode, which I will explain in the next approach.
Approach 2: Using Ephemeral Sequential Znode
A client(any server belonging to the cluster) creates a persistent znode /election.
All clients add a watch to /election znode and listen to any children znode deletion or addition under /election znode.
Now each server joining the cluster will try to create an ephemeral sequential znode /leader-&lt;sequential number&gt; under node /election with data as hostname, ex: node1.domain.com
Let’s say three servers in a cluster created znodes under /election, then the znode names would be:
/election/leader-00000001
/election/leader-00000002
/election/leader-00000003
Znode with least sequence number will be automatically considered as the leader.
Once all server completes the creation of znode under /election, they will perform getChildren(“/election”) and get the data(hostname) associated with least sequenced child node “/election/leader-00000001”, which will give the leader hostname.
At any point, if the current leader server goes down, Zookeeper will kill the session for that server after the specified session timeout. In the process, it will delete the node “/election/leader-00000001” as it was created by the leader server and is an ephemeral node and then Zookeeper will send a notification to all the server that was watching znode /election.
Once all server gets the leader’s znode-delete notification, they again fetch all children under /election znode and get the data associated with the child znode that has the least sequence number(/election/leader-00000002) and store that as the new leader in its own memory.

In this approach, we saw, if an existing leader dies, the servers are not sending an extra write request to the zookeeper to become the leader, leading to reduce network traffic.But, even with this approach, we will face some degree of herd effect we talked about in the previous approach. When the leader server dies, notification is sent to all servers in the cluster, creating a herd effect.But, this is a design call that you need to take. Use approach 1 or 2, if you need all servers in your cluster to store the current leader’s hostname for its purpose.If you do not want to store current leader information in each server&#x2F;follower and only the leader needs to know if he is the current leader to do leader specific tasks. You can further simplify the leader election process, which we will discuss in approach 3.
Approach 3: Using Ephemeral Sequential Znode but notify only one server in the event of a leader going down.
Create a persistent znode /election.
Now each server joining the cluster will try to create an ephemeral sequential znode /leader-&lt;sequential number&gt; under node /election with data as hostname, ex: node1.domain.com
Let’s say three servers in a cluster created znodes under /election, then the znode names would be:
/election/leader-00000001
/election/leader-00000002
/election/leader-00000003
Znode with least sequence number will be automatically considered as a leader.
Here we will not set the watch on whole/election znode for any children change(add/delete child znode), instead, each server in the cluster will set watch on child znode with one less sequence.
The idea is if a leader goes down only the next candidate who would become a leader should get the notification.
So, in our example:
- The server that created the znode /election/leader-00000001 will have no watch set.
-The server that created the znode /election/leader-00000002 will watch for deletion of znode /election/leader-00000001
-The server that created the znode /election/leader-00000003 will watch for deletion of znode /election/leader-00000002
Then, if the current leader goes down, zookeeper will delete the node /election/leader-00000001 and send the notification to only the next leader i.e. the server that created node /election/leader-00000002

That’s all on leader election logic. These are simple algorithms. There could be a situation when you want only those servers to take part in a leader election which has the latest data if you are creating a distributed database.In that case, you might want to create one more node that keeps this information, and in the event of the leader going down, only those servers that have the latest data can take part in an election.
Distributed LocksSuppose we have “n” servers trying to update a shared resource simultaneously, say a shared file. If we do not write these files in a mutually exclusive way, it may lead to data inconsistencies in the shared file.
We will manipulate operations on znode to implement a distributed lock, so that, different servers can acquire this lock and perform a task.
The algorithm for managing distributed locks is the same as the leader election with a slight change.
Instead of the /election parent node, we will use /lock as the parent node.
The rest of the steps will remain the same as in the leader election algorithm. Any server which is considered a leader is analogous to server acquiring the lock.
The only difference is, once the server acquires the lock, the server will perform its task and then call the delete operation on the child znode it has created so that the next server can acquire lock upon delete notification from zookeeper and perform the task.

Use casesGroup Membership&#x2F;Managing Cluster state
In Zookeeper it is pretty simple to maintain group membership info using persistent and ephemeral znodes. I will talk about a simple case where you want to maintain information about all servers in a cluster and what servers are currently alive.
We will use a persistent znode to keep track of all the servers that join the cluster and zookeeper’s ability to delete an ephemeral znodes upon client session termination will come handy in maintaining the list of active&#x2F;live servers.
Create a parent znode /all_nodes, this znode will be used to store any server that connects to the cluster.

Create a parent znode /live_nodes, this znode will be used to store only the live nodes in the cluster and will &lt;store ephemeral child znodes&gt;. If any server crashes or goes down, respective child ephemeral znode will be deleted.

Any server connecting to the cluster will create &lt;a new persistent znode&gt; under /all_nodes say /node1.domain.com. Let’s say another two node joins the cluster. Then the znode structure will look like:
/all_nodes/node1.domain.com
/all_nodes/node2.domain.com
/all_nodes/node3.domain.com

You can store any information specific to the node in znode’s data
Any server connecting to the cluster will create &lt;a new ephemeral znode&gt; under /live_nodes say /node1.domain.com. Let’s say another two-node joins the cluster. Then the znode structure will look like:
/live_nodes/node1.domain.com
/live_nodes/node2.domain.com
/live_nodes/node3.domain.com

Add a watch for any change in children of /all_nodes. If any server is added or deleted to/from the cluster, all server in the cluster needs to be notified.

Add a watch for any change in children of /live_nodes. This way all servers will be notified if any server in the cluster goes down or comes alive.

deployAs for demo only, we start only one zookeeper instance(standalone mode), in production env, it’s better start a zookeeper cluster which may have 3, 5, odd nodes for performance and HA.
# ubuntu18$ sudo apt install openjdk-8-jdk-headless# Centos7$ yum install -y java-11-openjdk-headless$ wget https://apachemirror.wuchna.com/zookeeper/zookeeper-3.6.3/apache-zookeeper-3.6.3-bin.tar.gz$ tar xvf apache-zookeeper-3.6.3-bin.tar.gz$ cd apache-zookeeper-3.6.3-bin$ cp conf/zoo_sample.cfg conf/zoo.cfg$ bin/zkServer.sh start# check info$ bin/zkServer.sh status$ bin/zkServer.sh/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /home/ubuntu/zookeeper/apache-zookeeper-3.6.3-bin/bin/../conf/zoo.cfgClient port found: 2181. Client address: localhost. Client SSL: false.Mode: standalone
client CLI command$ bin/zkCli.sh....# create persistence znode[zk: localhost:2181(CONNECTED) 0] create /jasoncreate /jason &#x27;hello&#x27;# create Sequential znode[zk: localhost:2181(CONNECTED) 0] create -s /node &#x27;hi node1&#x27;Created /node0000000005# create Ephemeral znode[zk: localhost:2181(CONNECTED) 0] create -e /node &#x27;tmp&#x27;Created /node# update a znode[zk: localhost:2181(CONNECTED) 0] set /node &#x27;update tmp&#x27;# create a child znode[zk: localhost:2181(CONNECTED) 0]  create /node/server1 &#x27;s1&#x27;Ephemerals cannot have children: /node/server1[zk: localhost:2181(CONNECTED) 0]  create /jason/server1 &#x27;s1&#x27;Created /jason/server1# list child znode[zk: localhost:2181(CONNECTED) 0] ls /jason[server1]# stats of znode[zk: localhost:2181(CONNECTED) 0] get /jasonhello[zk: localhost:2181(CONNECTED) 31] get -s  /jasonhellocZxid = 0x15ctime = Fri Apr 23 06:10:35 UTC 2021mZxid = 0x15mtime = Fri Apr 23 06:10:35 UTC 2021pZxid = 0x1dcversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 5numChildren = 1# remove a specified znode and recursively all its children.[zk: localhost:2181(CONNECTED) 0] deleteall /jason# add a watcher and get its value[zk: localhost:2181(CONNECTED) 0] get -w /jason\[zk: localhost:2181(CONNECTED) 0] removewatches  /jason# set ACL[zk: localhost:2181(CONNECTED) 63] setAcl /jason world:anyone:rw[zk: localhost:2181(CONNECTED) 64] getAcl /jason&#x27;world,&#x27;anyone: rw

Three nodes clustereach node must install zookeeper with cfg
node1  zoo.cfg  
tickTime=2000initLimit=10syncLimit=5dataDir=/zookeeper/dataclientPort=2181server.1=11.117.4.56:2888:3888server.2=11.117.4.57:2888:3888server.3=11.117.4.58:2888:3888$ mkdir /zookeeper/data$ echo 1 &gt; /zookeeper/data/myid

node2 zoo.cfg  
zoo.cfg:tickTime=2000initLimit=10syncLimit=5dataDir=/zookeeper/dataclientPort=2181server.1=11.117.4.56:2888:3888server.2=11.117.4.57:2888:3888server.3=11.117.4.58:2888:3888$ mkdir /zookeeper/data$ echo 2 &gt; /zookeeper/data/myid

node3 zoo.cfg  
zoo.cfg:tickTime=2000initLimit=10syncLimit=5dataDir=/zookeeper/dataclientPort=2181server.1=11.117.4.56:2888:3888server.2=11.117.4.57:2888:3888server.3=11.117.4.58:2888:3888$ mkdir /zookeeper/data$ echo 3 &gt; /zookeeper/data/myid

FAQ
why we need more than one zookeeper servers?

Two main reason: high availability and performance


why is zookeeper server nubmer odd?

because even and odd have the same high availability.


how does zookeeper handle inconsistence?

it handles the inconsistency of data by atomicity, using ZAB protocol



REF
tutorial
zk design
use cases
C bindings

]]></content>
      <categories>
        <category>distribute</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>distribute</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-core-tech-aufs</title>
    <url>/2019/10/11/docker-core-tech-aufs/</url>
    <content><![CDATA[AUFSIntroduction - advanced multi layered unification filesystem
Aufs is a stackable unification filesystem such as Union fs which unifies several directories and provides a merged single directory.

overlay2 is the preferred storage driver, for all currently supported Linux distributions, and requires no extra configuration.
aufs was the preferred storage driver for Docker 18.06 and older, when running on Ubuntu 14.04 on kernel 3.13 which had no support for overlay2

AUFS is not merged into linux main branch, only ubuntu&#x2F;debian support it!!!


Core ConceptHere is an example to merge two dirs into a single one, let’s explain it in details.
# mount -t aufs -o br=/tmp/dir1:/tmp/dir2 none /tmp/uniondir

This is the basic usage for aufs, more options refer to each dir is called branch, the dir has order, the first one call top branch(br0), the later called bottom branch(brX)
Order is important as

if same file(with path) exists in different branches, the top branch wins
Only top branch has RW attribute, others has RO if not set explicitly

AUFS create&#x2F;modify&#x2F;deletecheck aufs is supported or not
# grep aufs /proc/filesystemsnodev   aufs
then mount two dirs into single one
# mount -t aufs -o br=/tmp/dir1:/tmp/dir2 none /tmp/udirhere is the output after mount# tree ..|-- dir1|   -- file1|-- dir2|   -- file2-- udir    |-- file1    -- file2
Create file# touch udir/file3# tree ..|-- dir1|   |-- file1|   -- file3|-- dir2|   -- file2-- udir    |-- file1    |-- file2    -- file3
As you can see new file is created at RW branch
modify filemodify file from RW branch
# echo &#x27;f1&#x27; &gt;&gt;udir/file1# cat dir1/file1f1# cat udir/file1f1# tree ..|-- dir1|   |-- file1|   -- file3|-- dir2|   -- file2-- udir    |-- file1    |-- file2    -- file3
file is updated both in aufs and it’s source
modify file from RO branch
# echo &#x27;f2&#x27; &gt;&gt;udir/file2# cat udir/file2f2# cat dir2/file2 (unchanged original file2 is empty)# tree ..|-- dir1|   |-- file1|   |-- file2 ----------&gt;copied file2|   -- file3|-- dir2|   -- file2-- udir    |-- file1    |-- file2    -- file3
file is coped to RW branch, the original one unchanged
Delete filedelete file from RWFile is deleted from disk as well
delete file from RO branch
# tree -a ..|-- dir1|   |-- file1|   |-- .wh..wh.aufs|   |-- .wh..wh.orph|   -- .wh..wh.plnk|-- dir2|   -- file2-- udir    |-- file1    -- file2# rm udir/file2  ----&gt;remove file2 from aufs# tree -a ..|-- dir1|   |-- file1|   |-- .wh.file2  --------------&gt;whiteout file is created|   |-- .wh..wh.aufs|   |-- .wh..wh.orph|   -- .wh..wh.plnk|-- dir2|   -- file2-- udir    -- file1    ----------------&gt;can not see file2 from udir# rm dir1/.wh.file2  --------&gt;remove whiteout file for file2# tree -a ..|-- dir1|   |-- file1|   |-- .wh..wh.aufs|   |-- .wh..wh.orph|   -- .wh..wh.plnk|-- dir2|   -- file2-- udir    |-- file1    -- file2   ------&gt;file2 is back after remove whiteout file
Note: actually file is hided, so that you can&#39;t see it from udiras a whiteout file is created in the RW branch to tell aufs does not show the file which is deleted
check mount point and info/tmp/udir must exist before mount# mount -t aufs -o br=/tmp/dir1:/tmp/dir2 none /tmp/udir# mount | grep aufsnone on /tmp/udir type aufs (rw,relatime,si=226e11c4af0fb920) ---&gt; si is the id for thi mountuse this si to check the mount info for it.# ls -l /sys/fs/aufs/si_226e11c4af0fb920/total 0-r--r--r-- 1 root root 4096 Oct 14 11:05 br0-r--r--r-- 1 root root 4096 Oct 14 11:05 br1-r--r--r-- 1 root root 4096 Oct 14 11:05 brid0-r--r--r-- 1 root root 4096 Oct 14 11:05 brid1-r--r--r-- 1 root root 4096 Oct 14 11:05 xi_path# cat /sys/fs/aufs/si_226e11c4af0fb920/br0/tmp/dir1=rw         ---&gt;source dir# cat /sys/fs/aufs/si_226e11c4af0fb920/br1/tmp/dir2=ro         ---&gt;source dir]]></content>
      <categories>
        <category>docker</category>
        <category>core-tech</category>
      </categories>
      <tags>
        <tag>aufs</tag>
        <tag>filesystem</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-core-tech-chroot</title>
    <url>/2019/10/11/docker-core-tech-chroot/</url>
    <content><![CDATA[ChrootIntroductionEvery process/command in Linux&#x2F;Unix like systems has a current working directory called root directory. chroot changes the root directory for current running process as well as its children.
it creates a virtualized environment in a Unix(linux) operating system, separating it from the main operating system’s directory structure. This process essentially generates a confined space with its own root directory, to run software programs. This virtual environment runs separately from the main operating system&#39;s root directory.
Any software program run in this environment can only access files within its own directory tree. It cannot access files outside of that directory tree. This confined virtual environment is often called a "chroot jail".


Use cases

Privilege separation for unprivileged process such as Web-server or DNS server.
Setting up a test environment.
Run programs or ABI in-compatibility programs without crashing application or system. different apps in the same system can use different libraries, it’s what container image needs.
System recovery.
Reinstall the bootloader such as Grub or Lilo.
Password recovery – Reset a forgotten password and more.

Example to use chrootTwo ways to change root directory

chroot() system call(binary with CAP_SYS_CHROOT capability)
chroot linux command (super user)

let&#39;s use chroot as the example to show how we should do on linux
# format:# command is the command(in new root) that will run after chroot, if not provided /bin/sh by default$ chroot /path/to/new/root command (must run with super user)
Note: in order to run this command in new root dir, the new root dir must have such command and its dependency before chroot !!!

$ mkdir /tmp/new_root$ mkdir -p /tmp/new_root/&#123;bin,lib64,lib/x86_64-linux-gnu&#125;$ cp /bin/bash /tmp/new_root/bin# check bash depends$ ldd /bin/bash        linux-vdso.so.1 =&gt;  (0x00007ffc6cd67000)        libtinfo.so.5 =&gt; /lib/x86_64-linux-gnu/libtinfo.so.5 (0x00007f9bf4c85000)        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f9bf4a81000)        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9bf46b7000)        /lib64/ld-linux-x86-64.so.2 (0x00007f9bf4eae000)# Copy required libs to new root$ cp /lib/x86_64-linux-gnu/libtinfo.so.5  /lib/x86_64-linux-gnu/libdl.so.2 /tmp/new_root/lib/x86_64-linux-gnu$ cp /lib/x86_64-linux-gnu/libc.so.6 /tmp/new_root/lib/x86_64-linux-gnu$ cp /lib64/ld-linux-x86-64.so.2 /tmp/new_root/lib64$$ chroot /tmp/new_root /bin/bashbash-4.3#bash-4.3# ls\command not foundbash-4.4# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin
 here we just copy bash, if you run ls, you need to copy ls and it dependency as well
]]></content>
      <categories>
        <category>docker</category>
        <category>core-tech</category>
      </categories>
      <tags>
        <tag>chroot</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-core-tech-cgroups</title>
    <url>/2019/10/14/docker-core-tech-cgroups/</url>
    <content><![CDATA[cgroups provides a mechanism for aggregating/partitioning sets of tasks, and all their future children, into hierarchical groups with specialized behavior(under resource controller) like

Resource Limiting (i.e. not to exceed a memory limit) mostly used
Prioritization (i.e. groups may have larger share of CPU)
Isolation (i.e. isolate GPUs for particular processes)
Accounting (i.e. monitor resource usage for processes)
Control (i.e. suspending and resuming processes)



Definitions:

A cgroup associates a set of tasks with a set of parameters(control resource) for oneor more subsystems.

A subsystem is a module that makes use of the task groupingfacilities provided by cgroups to treat groups of tasks inparticular ways. A subsystem is typically a “resource controller” thatschedules a resource or applies per-cgroup limits, but it may beanything that wants to act on a group of processes, e.g. avirtualization subsystem.

A hierarchy is a set of cgroups arranged in a tree, such thatevery task in the system is in exactly one of the cgroups in thehierarchy, and a set of subsystems(CPU, Memory); each subsystem has system-specificstate attached to each cgroup in the hierarchy.  Each hierarchy hasan instance of the cgroup virtual filesystem associated with it.


cgroup pseudo filesystem
Here are most cgroups(subsys) supported by linux

cpuset – assigns tasks to individual CPUs and memory nodes in a cgroup
cpu – schedules CPU access to cgroups(how much cpu used and how long)
cpuacct – reports CPU resource usage of tasks of a cgroup
memory – set limits on memory use and reports memory usage for a cgroup, normal page memory not huge page
hugetlb - allows to use virtual memory pages of large sizes and to enforce resource limits on these pages
devices – allows or denies access to devices (i.e. gpus) for tasks of a cgroup
freezer – suspends and resumes tasks in a cgroup
net_cls – tags network packets in a cgroup to allow network traffic priorities
blkio – tracks I&#x2F;O ownership, allowing control of access to block I&#x2F;O resources



Rules for cgroup

Each subsystem (memory, CPU…) has a hierarchy (tree)
Hierarchies are independent
(the trees for e.g. memory and CPU can be different)


Each process belongs to exactly 1 node in each hierarchy
(think of each hierarchy as a different dimension or axis)


Each hierarchy(subsystem) starts with 1 node (the root)

# like for cpu subsystem  root node is at# tasks # attach a task(thread) and show list of threads# cgroup.procs # show list of processes$ls /sys/fs/cgroup/cpu/cgroup.clone_children  cpuacct.stat       cpuacct.usage_percpu       cpuacct.usage_sys   cpu.cfs_quota_us  docker/            release_agent  user.slice/cgroup.procs           cpuacct.usage      cpuacct.usage_percpu_sys   cpuacct.usage_user  cpu.shares        init.scope/        system.slice/cgroup.sane_behavior   cpuacct.usage_all  cpuacct.usage_percpu_user  cpu.cfs_period_us   cpu.stat          notify_on_release  tasks

All processes initially belonged to the root of each hierarchy
Each node &#x3D; group of processes(sharing the same resources)

NOTE

All subdirs shared the setting of its parent, total amount should not exceed its parent, even tasks in subdir not present in parent’s tasks, see example below.

# NOTE: remove a cgroup dir using: rmdir not rm -rf$ mkdir /sys/fs/cgroup/cpu/agent# total two CPU$ echo 100000 &gt;/sys/fs/cgroup/cpu/agent/cpu.cfs_period_us$ echo 200000 &gt;/sys/fs/cgroup/cpu/agent/cpu.cfs_quota_us$ cat /sys/fs/cgroup/cpu/agent/tasks# no tasks here even each subdirs has its own task!!!$ mkdir /sys/fs/cgroup/cpu/agent/sub1$ cat /sys/fs/cgroup/cpu/agent/sub1/cpu.cfs_period_us100000$ cat /sys/fs/cgroup/cpu/agent/sub1/cpu.cfs_quota_us-1$ cat /sys/fs/cgroup/cpu/agent/sub1/tasks9877$ mkdir /sys/fs/cgroup/cpu/agent/sub2$ cat /sys/fs/cgroup/cpu/agent/sub2/cpu.cfs_period_us100000$ cat /sys/fs/cgroup/cpu/agent/sub2/cpu.cfs_quota_us$ cat /sys/fs/cgroup/cpu/agent/sub2/tasks9853

cpu subsystemsys path: /sys/fs/cgroup/cpu, it’s used for cpu state(usage), weight and limitation for tasks attached to this group, here are several key parameters of this subsystem.

cpu.shares(relative weight to other cpu cgroups)

it’s relative weight to other cgroups under cpu subsystem, when access to host CPU cycles, by default it’s 1024, it works only when other cgroups have tasks attached and competing for host CPU cycles, say cgroupA has task A with default cpu.shares, cgroupB has task B with cpu.shares 2048, if both taskA and taskB are running, taskB get host CPU cycles twice than taskA, while if taskB is slept or quits, taskA will get all host CPU cycles, as no others are competing with it.As it’s relative weight, It does not guarantee or reserve any specific CPU access.!!!


cpu.cfs_period_us

specifies a period of time in microseconds not millisecond(ms) (represented here as “us”) for how regularly a cgroup’s access to CPU resources should be reallocated. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. The upper limit of the cpu.cfs_quota_us parameter is 1 second(1000 milliseconds) and the lower limit is 1000 microseconds(1 milliseconds)


cpu.cfs_quota_us and cpu.cfs_period_us(hard limit for tasks in the group)

CPU quota control how much CPU time tasks in cgroup can use during a period, cpu_quota is the number of microseconds of CPU time, cpu period is counting unit for cpu quota, they should work together to limit the upper bound(max cpu) for that cgroup(tasks in it). by default period is 100000 us, while quota is -1(no limit). if quota is 100000(&#x3D;&#x3D;cpu period), like tasks in this group can use one host CPU, but if quota is 200000 larger than period, like tasks in this groups can use two Host CPUs.



NOTE

cgroup controls resource to all tasks in the same group, say for cpu quota and cpu period, counting all tasks for the same cpu use, not per task
child process forked from task in the cgroup, obey the same limitation with its parent(child task is added parent cgroup automatically)
even quota is two CPU, but if only one task in the cgroup without multi-thread on, at most one CPU is used!!!!

cpu.shares and cpu.cfs_quota_us, cpu.cfs.period_us can work together!!! 
host has two cpus, both taskA and taskB are always running, no other task is running for ideal.

taskA in groupA with share 1024, 1 cpu(cpu.cfs_period_us 100000 cfs_quota_us 100000)
taskB in groupB with share 2048, 0.5cpu(cpu.cfs_period_us 100000 cfs_quota_us 50000)
for each period, host provides 2*100000, taskB can get 0.67 due to share setting, but taskB has hard limit 0.5cpu, hence after task B get 0.5cpu, it’s paused, so that taskA continues to run, after one period, taskA gets 1 cpu, taskB gets 0.5cpu(another 0.5cpu is idea), but taskB gets 0.5cpu before taskA gets its 0.5cpu



warn


taskA in groupA with share 1024, 1 cpu(cpu.cfs_period_us 100000 cfs_quota_us 100000)
taskB in groupB with share 1024, 0.5cpu(cpu.cfs_period_us 100000 cfs_quota_us 50000)
for each period, host provides 2*100000, taskA and taskB gets its 0.5cpu at the same, then taskB paused, taskA continues to run, after one period, taskA gets 1 cpu, taskB gets 0.5cpu(another 0.5cpu is idea).




docker parameters related
# let&#x27;s say there are four CPUS on host, if all containers are running CPU intensive workload# first container takes one CPU# second container takes one CPU# third container takes two CPU# but if third container quits or sleep, the other two both take two CPUS !!!$ docker run -it --rm  --cpu-shares 1024 ubuntu /bin/bash$ docker run -it --rm  --cpu-shares 1024 ubuntu /bin/bash$ docker run -it --rm  --cpu-shares 2048 ubuntu /bin/bash# --cpu-period &amp; --cpu-quota# CPU quota (cpu_quota) is a feature of Linux Control Groups (cgroup). CPU quota control `how much CPU time a container can use`, `cpu_quota is the number of microseconds of CPU time` a container can use `per cpu_period`.# cpu_quota allows setting an `upper bound on the amount of CPU time a container gets`. Linux enforces the limit even if CPU time is available. Quotas can hinder utilization while `providing a predictable upper bounds on CPU time.`# one Host CPU for this docker$ docker run -it --rm  --cpu-period 100000 --cpu-quota 100000 ubuntu /bin/bash# --cpus=&lt;value&gt;# Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus=&quot;1.5&quot;, the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period=&quot;100000&quot; and --cpu-quota=&quot;150000&quot;. short way for cpu-period and cpu-quota# In order to set cpu_quota correctly, you need to know how many cpu can be used by container, then set cpu_quota and cpu_period correctly.$ docker run -it --rm  --cpus=1.5 ubuntu /bin/bash# Same as below$ docker run -it --rm  --cpu-period=100000 --cpu-quota=150000 ubuntu /bin/bash

memory subsystemThe memory subsystem generates automatic reports on memory resources used by the tasks in a cgroup, and sets limits on memory use of those tasks.
$ls /sys/fs/cgroup/memory/cgroup.clone_children  memory.force_empty              memory.kmem.tcp.limit_in_bytes      memory.memsw.failcnt             memory.oom_control          memory.use_hierarchycgroup.event_control   memory.kmem.failcnt             memory.kmem.tcp.max_usage_in_bytes  memory.memsw.limit_in_bytes      memory.pressure_level       notify_on_releasecgroup.procs           memory.kmem.limit_in_bytes      memory.kmem.tcp.usage_in_bytes      memory.memsw.max_usage_in_bytes  memory.soft_limit_in_bytes  release_agentcgroup.sane_behavior   memory.kmem.max_usage_in_bytes  memory.kmem.usage_in_bytes          memory.memsw.usage_in_bytes      memory.stat                 system.slicemachine.slice          memory.kmem.slabinfo            memory.limit_in_bytes               memory.move_charge_at_immigrate  memory.swappiness           tasksmemory.failcnt         memory.kmem.tcp.failcnt         memory.max_usage_in_bytes           memory.numa_stat                 memory.usage_in_bytes       user.slice

let’s focus on memory limitation parameters memory.limit_in_bytes, memory.memsw.limit_in_bytes, memory.oom_control, memory.soft_limit_in_bytes, memory.swappiness

memory.limit_in_bytes  sets the maximum amount of user memory (including file cache). If no units are specified, the value is interpreted as bytes. However, it is possible to use suffixes to represent larger units(k&#x2F;K, m&#x2F;M, g&#x2F;G). echo 1G &gt; /cgroup/memory/lab1/memory.limit_in_bytes

memory.memsw.limit_in_bytes  sets the maximum amount for the sum of memory and swap usage. If no units are specified, the value is interpreted as bytes, However, it is possible to use suffixes to represent larger units(k&#x2F;K, m&#x2F;M, g&#x2F;G).
  It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter:, This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted. memory.limit_in_bytes = 2G and memory.memsw.limit_in_bytes = 4G for a certain cgroup will allow processes in that cgroup to allocate 2 GB of memory and, once exhausted, allocate another 2 GB of swap only.

memory.soft_limit_in_bytes  enables flexible sharing of memory. Under normal circumstances, control groups are allowed to use as much of the memory as needed, constrained only by their hard limits set with the memory.limit_in_bytes parameter. However, when the system detects memory contention or low memory, control groups are forced to restrict their consumption to their soft limits(reclaim memory).If lowering the memory usage to the soft limit does not solve the contention, cgroups are pushed back as much as possible to make sure that one control group does not starve the others of memory. Note that soft limits take effect over a long period of time, since they involve reclaiming memory for balancing between memory cgroups.

memory.oom_control  contains a flag (0 or 1) that enables or disables the Out of Memory killer for a cgroup. If enabled (0), tasks that attempt to consume more memory than they are allowed are immediately killed by the OOM killer. The OOM killer is enabled by default in every cgroup using the memory subsystem; to disable it, write 1 to the memory.oom_control file.
  When the OOM killer is disabled, tasks that attempt to use more memory than they are allowed are paused until additional memory is freed.

memory.swappiness  sets the tendency of the kernel to swap out process memory used by tasks in this cgroup instead of reclaiming pages from the page cache. This is the same tendency, calculated the same way, as set in &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness for the system as a whole. The default value is 60. Values lower than 60 decrease the kernel&#39;s tendency to swap out process memory, values greater than 60 increase the kernel&#39;s tendency to swap out process memory, and values greater than 100 permit the kernel to swap out pages that are part of the address space of the processes in this cgroup.
  Note that a value of 0 does not prevent process memory being swapped out; swap out might still happen when there is a shortage of system memory because the global virtual memory management logic does not read the cgroup value. To lock pages completely, use mlock() instead of cgroups. 
  NOTE:  Increasing this value will make the system more inclined to utilize swap space, leaving more memory free for caches.  Decreasing this value will make the system less inclined to swap, and may improve application responsiveness.
  Tuning vm.swappiness incorrectly may hurt performance or may have a different impact between light and heavy workloads. Changes to this parameter should be made in small increments and should be tested under the same conditions that the system normally operates.


NOTE
if soft_limit_in_bytes is unlimited while limit_in_bytes is set, when processes in this group reach  limit_in_bytes, kernel will try to swap some memory of the processes to disk, so that it may be below the limit, but if there is no swap left of the system or it reaches memsw.limit_in_bytes, in these case, one of the process(high score) should be killed by OOM killer.
cgroup subsystem kernelHere is the source code for each subsystem
cpuset_subsys - defined in kernel/cpuset.c.freezer_subsys - defined in kernel/cgroup_freezer.c.mem_cgroup_subsys - defined in mm/memcontrol.c; Aka memcg - memory control groups.blkio_subsys - defined in block/blk-cgroup.c.net_cls_subsys - defined in net/sched/cls_cgroup.c ( can be built as a kernel module)net_prio_subsys - defined in net/core/netprio_cgroup.c ( can be built as a kernel module)devices_subsys - defined in security/device_cgroup.c.perf_subsys (perf_event) - defined in kernel/events/core.chugetlb_subsys - defined in mm/hugetlb_cgroup.c.cpu_cgroup_subsys - defined in kernel/sched/core.ccpuacct_subsys - defined in kernel/sched/core.c


css_set:(set of groups from different subsystems)When a process forks, the child will be in all the same cgroups that the parent is in. While either process could be moved around, they very often are not. This means that it is quite common for a collection of processes (and the threads within them) to all be in the same set of cgroups. To make use of this commonality, the struct css_set exists. It identifies a set of cgroups (css stands for “cgroup subsystem state”) and each thread is attached to precisely one css_set. All css_sets are linked together in a hash table so that when a process or thread is moved to a new cgroup, a pre-existing css_set can be reused, if one exists with the required set of cgroups
cgroupv2Why cgroupv2(kernel&gt;&#x3D;4.5)?There was a lot of criticism and issues about the implementation of cgroups, which seems to present a number of inconsistencies and a lot of chaos. For example, when creating subgroups (cgroups within cgroups), several cgroup controllers propagate parameters to their immediate subgroups, while other controllers do not. Or, for a different example, some controllers use interface files (such as the cpuset controller’s clone_children) that appear in all controllers even though they only affect one. also due to different subsystem(cpu, memory, blkio), limitation only affect within that subsystem, hence when buffer io is enabled(enabled by default), write&#x2F;read only happens at page cache, blkio who works at block layer, does not know it, hence can not limit disk io correctly! but with cgroupv2, as memory, blkio can be enabled at same group, so blkio with v2 can works as expected!!!
The biggest change to cgroups in v2 is a focus on simplicity to the hierarchy. Where v1 used independent trees for each controller (such as &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;GROUPNAME and &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;GROUPNAME), v2 will unify those in &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;GROUPNAME. In the same vein, if Process X joins &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;test, every controller enabled for test will control Process X
For example, in cgroups v2, memory protection is configured in four files:

memory.min: this memory will never be reclaimed.
memory.low: memory below this threshold is reclaimed if there’s no other reclaimable memory in other cgroups.
memory.high: the kernel will attempt to keep memory usage below this configuration.
memory.max: if memory reaches this level the OOM killer (a system used to sacrifice one or more processes to free up memory for the system when all else fails) is invoked on the cgroup

cgroupv1 vs cgroupv2
cgroupv2 usage and redhat cgroupv2 guideline
FAQcheck cgroups of a given process# cat /proc/$pid/cgroup
change a cgroup of a given processchange cpu cgroup of given process
# you can create a sub group under a subsystem or under another group# tasks # attach a task(thread) and show list of threads# cgroup.procs # show list of processes$echo $pid &gt; /sys/fs/cgroup/cpu/$sub_group/cgroup.procs
check root node of particular cgroup(subsystem)# ls /sys/fs/cgroup/xx
create cgroup(sub group of a given cgroup subsystem)create a cpu control group, you can create cgroup alone without process, later on add process to it# cd /sys/fs/cgroup/cpu/# mkdir test_cgafter this system will add attributes files(control files) automatically, then you can changeparameters for this group to control cpu etc
the created cgroup will be deleted after reboot!!!, when created, there is no process in this group
when you delete a cgroup, all its processes are moved to their parent group.
run a program in a given cgroup$cgexec -g controllers:path_to_cgroup command arguments # must create cgroup before!!!# path: -g memory:test/sub$cgexec -g memory:test stress-ng -m 1 --vm-bytes 128M -t 10s --metrics-brief$cgexec -g memory:test -g cpu:test stress-ng -m 1 --vm-bytes 128M -t 10s --metrics-brief]]></content>
      <categories>
        <category>docker</category>
        <category>core-tech</category>
      </categories>
      <tags>
        <tag>cgroups</tag>
        <tag>resource controller</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-core-tech-ns</title>
    <url>/2019/10/14/docker-core-tech-ns/</url>
    <content><![CDATA[IntroductionNamespaces provide processes with their own view of the system, limit what you can see(and therefore use)


Overview
There are multiple namespaces(no cgroup namespace actually):

pideach PID namespace has it own numbering(start at 1), when PID 1 goes away, the whole namespace is killed. Even you run process in a PID namespace, root pid namespace still sees it, but the pid number is different in both namespaces, but others non-root pid namespace does NOT see it.
netprocesses within a given network namespace get their own private network stack, including:
network interfaces
routing tables
iptables rules
sockets(inet socket, NOT unix socket)network interface belongs to exactly one net namespace, same for (inet)socket, the newly create net namespace only contains a loopback, no others, when a network space is deleted, all its movable network devices are moved back to the default network namespace, while unmovable devices(device who have NETIF_F_NETNS_LOCAL in their features) and virtual devices are not moved to the default network namespace


mnt(mount)processes can have ‘private’ mounts, mounts&#x2F;unmounts in that mount namespace are invisible to the rest of the system
utsgethostname&#x2F;sethostname can be different at uts namespace, you can change hostname in this namespace, but not affect others(namespace)
ipc(rarely used)Allow a process to have its own
IPC semaphores
IPC message queues
IPC shared memory


userAllows to map UID&#x2F;GID(that means you can see mapped uid&#x2F;gid in user namespace, you can’t know the real pid&#x2F;gid in container with this mapping)
UID 0-1999 in container C1 is mapped to UID 10000-11999 on host(security improvement)



mount namespaceMount namespace provides isolation of the list of mounts seen by the processes in each namespace instance.  Thus, the processes in each of the mount namespace instances will see distinct single-directory hierarchies.
A new mount namespace is created using either clone(2) or unshare(2) with the CLONE_NEWNS flag.  When a new mount namespace is created, its mount list is initialized as follows:

If the namespace is created using clone(2), the mount list of the child’s namespace is a copy of the mount list in the parent process’s mount namespace.
If the namespace is created using unshare(2), the mount list of the new namespace is a copy of the mount list in the caller’s previous mount namespace.

Subsequent modifications to the mount list (mount(2) and umount(2)) in either mount namespace will not (if MS_PRIVATE is used[default]) affect the mount list seen in the other namespace, this is controlled by propagation types.
propagation types

MS_SHAREDThis mount shares events with members of a peer group(parent process). Mount and unmount events immediately under this mount will propagate to the other mounts that are members of the peer group.  Propagation here means that the same mount or unmount will automatically occur under all of the other mounts in the peer group.  Conversely, mount and unmount events that take place under peer mounts will propagate to this mount.

MS_PRIVATE(default)This mount is private; it does not have a peer group. Mount and unmount events do not propagate into or out of this mount.

MS_SLAVEMount and unmount events propagate into this mount from a (master) shared peer group.  Mount and unmount events under this mount do not propagate to any peer.


Note that a mount can be the slave of another peer group while at the same time sharing mount and unmount events with a peer group of which it is a member.  (More precisely, one peer group can be the slave of another peergroup.)

MS_UNBINDABLEThis is like a private mount, and in addition this mount can’t be bind mounted.  Attempts to bind mount this mount (mount(2) with the MS_BIND flag) will fail.

When a recursive bind mount (mount(2) with the MS_BIND and MS_REC flags) is performed on a directory subtree, any bind mounts within the subtree are automatically pruned(i.e., not replicated) when replicating that subtree to produce the target subtree.
kernel vieweach namespace is identified by an inode(unique), if two processes are in the same namespace if they see the same inode for equivalent namespace types.

FAQhow to create different namespaces
namespaces are created with the clone() system call(with extra flags when creating a new process), when the last process of a namespace exits, it’s destroyed automatically by kernel(but can be preserved)
process can ‘join’ a namespace by setns()

clone flags for namespace

CLONE_NEWNS
CLONE_NEWNET
CLONE_NEWPID
CLONE_NEWIPC
CLONE_NEWUTS
CLONE_NEWUSER

All needs CAP_SYS_ADMIN except CLONE_NEWUSER
how to check namespaces of given process$ ls -al /proc/$pid/nsdr-x--x--x 2 root root 0 Oct 25 16:24 .dr-xr-xr-x 9 root root 0 Oct 11 22:15 ..lrwxrwxrwx 1 root root 0 Oct 25 16:24 ipc -&gt; ipc:[4026531839]lrwxrwxrwx 1 root root 0 Oct 25 16:24 mnt -&gt; mnt:[4026531840]lrwxrwxrwx 1 root root 0 Oct 25 16:24 net -&gt; net:[4026531956]lrwxrwxrwx 1 root root 0 Oct 25 16:24 pid -&gt; pid:[4026531836]lrwxrwxrwx 1 root root 0 Oct 25 16:24 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Oct 25 16:24 uts -&gt; uts:[4026531838]

how to communicate between network namespacesuse veth pair or unix socket
how to check if device is netns local or notif device with NETIF_F_NETNS_LOCAL, it’s not allowed to move between network namespace; example of this device

loopback, vxlan, pp, bridge
use ethtool to check if device is set or not.

$ ethtool -k eth0 | grep localnetns-local: off [fixed]$ ethtool -k lo | grep localnetns-local: on [fixed]
how to check mounted point of the system# this is the real mounts from kernel$ cat /proc/mounts# it just read file from /etc/mtab$ mount
how to run a program in another namespace from shellThere are two commands for this.

unshare to run command in new namespace or existing namespace
nsenter to run command in another process&#39;s namespace[existing namespace]

unshare [options] &lt;program&gt; [&lt;argument&gt;...]Options: -m, --mount[=&lt;file&gt;]      unshare mounts namespace -u, --uts[=&lt;file&gt;]        unshare UTS namespace (hostname etc) -i, --ipc[=&lt;file&gt;]        unshare System V IPC namespace -n, --net[=&lt;file&gt;]        unshare network namespace -p, --pid[=&lt;file&gt;]        unshare pid namespace -U, --user[=&lt;file&gt;]       unshare user namespace -f, --fork                fork before launching &lt;program&gt;     --mount-proc[=&lt;dir&gt;]  mount proc filesystem first (implies --mount) -r, --map-root-user       map current user to root (implies --user)     --propagation slave|shared|private|unchanged                           modify mount propagation in mount namespace -s, --setgroups allow|deny  control the setgroups syscall in user namespaces# without namespace option provided, it uses the same as its parent!!!# PRIVATE mount namespace# after copied mount list from parent, they will never affect each other$ unshare -m /bin/bash# shared mount namespace# after copied mount list from parent, they will always affect each other# mount on one, will also mount automatically on peer!!!$ unshare --propagation shared -m /bin/bash
network namespace command list# show all named net ns$ ip netns list# create netns$ ip netns add ns1$ ls /var/run/netns# move eth0 to ns1 if it&#x27;s movable$ ip link set dev eth0 netns ns1# run command in netns$ ip netns exec ns1 bash# then$ ifconfig -a# show all processes joined a netns$ ip netns pids ns1# check which netns a process belongs to$ ip netns identify $pid# delete a namespace$ ip netns delete ns1# how two netns communicate(ping with each other or send() packet through socket)# use veth pair, two virtual net devices(like a pipe), put them at different netns.$ ip link add name v1 type veth peer name v1_peer# check veth pair$ ip -d link show v1443: v1@v1_peer: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000    link/ether 76:01:28:8f:09:4f brd ff:ff:ff:ff:ff:ff promiscuity 0 --&gt;  veth addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 $ ip -d link show v1_peer442: v1_peer@v1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000    link/ether aa:12:df:8a:ce:4c brd ff:ff:ff:ff:ff:ff promiscuity 0 --&gt;  veth addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 # @v1 means its peer name, veth means veth pair$ ip link set dev v1 netns ns1$ ip link set dev v1_peer netns ns2$ ip netns exec ns1 ifconfig v1 192.168.1.1/24 up$ ip netns exec ns2 ifconfig v1_peer 192.168.1.2/24 up# now the two netns can ping each other$ ip netns exec ns1 bash$ ping 192.168.1.2# how process runs in a netns communicates with internet.# One way:# 1. move a physical ethx into that namespace# # The other(mostly used)# create a tunnel(veth pairs)between the netns and default ns, then create a bridge that contains a physical interface and one end of the tunnel that in root netns, the other side is in netns.# # create bridge and add a physical device to it$ sudo brctl addbr br0$ ifconfig eth0 0.0.0.0$ sudo brctl addif br0 eth0$ ifconfig br0 up$ dhclient br0# add veth paris and add one end to the bridge$ sudo ip link add name veth1 type veth peer name veth1_peer$ sudo ifconfig veth1 up$ sudo brctl addif br0 veth1# add peer to netns and get ip from bridge$ sudo ip link set dev veth1_peer netns ns1$ sudo ip netns exec ns1 bash$ dhclient veth1_peer (inside netns)# show all network namespaces(named and unnamed)# -------- show named netns --------$ ip netns listns1# -------- show unnamed netns --------$ lsns --type=net        NS TYPE NPROCS   PID USER     NETNSID NSFS                           COMMAND4026532008 net     686     1 root  unassigned                                /usr/lib/systemd/systemd --switched-root --system --deserialize 174026533573 net       9 26128 jaluo          0 /run/docker/netns/e18a6fbd7cae bash]]></content>
      <categories>
        <category>docker</category>
        <category>ns</category>
      </categories>
      <tags>
        <tag>namespace</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-create-image</title>
    <url>/2019/10/14/docker-create-image/</url>
    <content><![CDATA[Create docker imageThere are several ways to create an image, you can create it by

$ docker commit   # from an existing container
$ docker load     # from a saved tar package
$ docker build    # from a Dockerfile or from standard input




Here I only say how to create an image from Dockerfile, for other ways, please refer to docker command that I posted earlier.
Basically speaking, Dockerfile is a sequence of instructions in order, docker parses these instructions one by one created layer when necessary, at last generates an image with these layers and conf.
Here is an overview of all instructions that docker provided, I will explain some of them later on with more details.

instructionsHere is just a basic usage of some commands, details refer to 

FROM: Sets the base image for subsequent
MAINTAINER: set the author field of the generated images(deprecated use LABEL instead)
RUN: Execute commands in a new layer on top of the current image and commit the result
CMD: Allow only one CMD as the start command when container starts
LABEL: Add metadata to an image
EXPOSE: Informs container runtime that the container listens on the specified network ports at runtime, it’s just declaration
ENV: Sets an environment variable
ADD: Copy new files or directories or remote file URLs from into the filesystem of the container, it can process tar, deb etc uncompressed then copy
COPY: Copy new files or directories into the filesystem of the container
ENTRYPOINT: Allows you to configure a container that will run as an executable
VOLUME: Creates a mount point and marks it as holding externally mounted volumes from native host or other containers

Here is an example of Dockerfile, see comments for more details
# from base image to create newFROM ubuntu:18.04LABEL maintainer=&quot;jason_lkm@163.com&quot;LABEL version=&quot;1.0&quot;# remember to delete intermediate files for aptRUN apt-get update &amp;&amp; apt-get install -y \    automake \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; apt-get clean# set start commandCMD [&quot;echo&quot;, &quot;hello&quot;]# if run docker with docker -P(no parameter for it), map random port from host to 80(EXPOSE here)# if docker doesn&#x27;t run with -P, here is an description at all!!!!EXPOSE 80/udp# better use &#x27;-p&#x27; to explicitly map host port to docker port when docker run# docker run -p 8080:80 nginx # map host 8080 to container 80, will create a DNAT rule on host#    0     0 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8080 to:172.17.0.1:80ENV NAME=&quot;jason&quot; EMAIL=&quot;jason@163.com&quot;# ADD COPY support pattern matchingADD ./test.tar /testCOPY ./*.txt /COPY ./b.c /# create volume for each container automatically, remember to delete volume# when remove that docker# /data is dst pathVOLUME [&quot;/data&quot;]

There are several rules that you must obey to avoid troubles
Write one CMD in Dockerfile, if many, the last on takes effect
Better to use CMD or ENTRYPOINT, not both, if both used, CMD will be passed to ENTRYPOINT as parameter
CMD may be overwritten by command from cli $docker run nginx &#x2F;bin&#x2F;bash, &#x2F;bin&#x2F;bash overwrites CMD in nginx image
Remove unnecessary files when do apt-get
Avoid RUN apt-get upgrade and dist-upgrade



NOTE: instruction in Dockerfile may be overwritten by parameter from cli, like ENTRYPOINT may be overwritten by docker run –entrypoint, so check what conf container uses by$docker inspect $container_id(name) 
how build worksWhen you issue a docker build command, the current working directory is called the build context,  all recursive contents of files and directories in the current directory are sent to the Docker daemon, but only the ones used by Dockerfile is copied to image, other is not used!!!, docker client(issuer) and docker daemon can be at different machines, in that case, make sure there is no unnecessary files under current dir, as all files will be sent to docker daemon, it may take long timer if the current dir is large, after docker daemon receives all and parses Dockerfile and execute each instruction one by one with order, create layer if necessary or set conf for this image, at last save image to the local registry of machine where docker daemon is running.
Actually before it runs, docker daemon created a temporary for this image and chroot to it, so that the destination path(ADD ./test.tar /) in Dockefile is for the new rootfs

build an imagefrom a file  
$ lsDockerfile$ docker build --no-cache -t $image_name:$tagname .# it will read Dockerfile from current dir# and send all files/dirs to docker daemon

from standard input  
# t is image name$ docker build -t tcpdump - &lt;&lt;EOFFROM ubuntuRUN apt-get update &amp;&amp; \apt-get install -y tcpdump &amp;&amp; \apt-get install -y net-tools &amp;&amp; \rm -rf /var/lib/apt/lists/* &amp;&amp; \apt-get cleanCMD /bin/bashEOF

check info about image
check instructions used by an image
$ docker history $image_id(name) --no-trunc$ docker history $image_id(name)IMAGE               CREATED             CREATED BY                                      SIZE                COMMENToutput:5a9061639d0a        26 hours ago        /bin/sh -c #(nop)  CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon…   0B&lt;missing&gt;           26 hours ago        /bin/sh -c #(nop)  STOPSIGNAL SIGTERM           0B&lt;missing&gt;           26 hours ago        /bin/sh -c #(nop)  EXPOSE 80                    0B&lt;missing&gt;           26 hours ago        /bin/sh -c ln -sf /dev/stdout /var/log/nginx…   22B

check instructions with full content of an image
cat /var/lib/docker/image/aufs/imagedb/content/sha256/5a9061639d0aeca4b13f8e18b985eea79e55168969d069febdb6723993ebba7d  | python -m json.tooloutput:...&quot;history&quot;:[        &#123;            &quot;created&quot;: &quot;2019-10-17T04:43:59.291372925Z&quot;,            &quot;created_by&quot;: &quot;/bin/sh -c ln -sf /dev/stdout /var/log/nginx/access.log     &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log&quot;        &#125;,        &#123;            &quot;created&quot;: &quot;2019-10-17T04:43:59.546729941Z&quot;,            &quot;created_by&quot;: &quot;/bin/sh -c #(nop)  EXPOSE 80&quot;,            &quot;empty_layer&quot;: true        &#125;,        &#123;            &quot;created&quot;: &quot;2019-10-17T04:43:59.807233891Z&quot;,            &quot;created_by&quot;: &quot;/bin/sh -c #(nop)  STOPSIGNAL SIGTERM&quot;,            &quot;empty_layer&quot;: true        &#125;,        &#123;            &quot;created&quot;: &quot;2019-10-17T04:44:00.059327535Z&quot;,            &quot;created_by&quot;: &quot;/bin/sh -c #(nop)  CMD [\&quot;nginx\&quot; \&quot;-g\&quot; \&quot;daemon off;\&quot;]&quot;,            &quot;empty_layer&quot;: true        &#125;]
As you can see some instructions Did NOT create layer at all, see ‘empty_layer’ field.

check the config for an image
$ docker images$ docker inspect 5a9061639d0a# this will show what&#x27;s the cmd, env, entrypoint, volume, rootfs etc for this image

]]></content>
      <categories>
        <category>docker</category>
        <category>image</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>image</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-faq</title>
    <url>/2019/10/18/docker-faq/</url>
    <content><![CDATA[FAQFrequently asked questions about Docker.


how to capture packet for a containerThere are two ways to capture packet for a given container
way1: capture in the given container
go to that container, use tcpdump(install it first if not available) in it.
required:

container has tcpdump
container no tcpdump but can install tcpdump on it from local or internet(change the container)

way2: capture in another container which shares the same net with the given container
In some case, you can’t change the given container or it’s complex to install tcpdump on the given container, so create a template container which has tcpdump installed and shared the same network with given container.
$ docker build -t tcpdump - &lt;&lt;EOFFROM ubuntuRUN apt-get update &amp;&amp; apt-get install -y tcpdump &amp;&amp; \apt-get install -y net-tools &amp;&amp; \rm -rf /var/lib/apt/lists/* &amp;&amp; apt-get cleanCMD /bin/bashEOF# create an image called tcpdump with tcpdump installed# then create a container with this image shares network with another container# rm means container will be destroy when exit.# 99c1bd6b342c is container ID$ docker run --rm -it --net=container:99c1bd6b342c tcpdump# this will create unnamed container from image tcpdump and shared network with the container that you want to capture packet.


how to deploy docker env on lots of machine?Use Docker Machine, docker machine provides a client to deploy docker env on lots machine(windows, mac, linux).
whats dockerd, docker-containerd,docker-containerd-shim, docker-runc
docker-container-shim runs in host, not in container, while, entrypoint is the first process runs in container
NOTE: latest version, runc and shim merged into one binary: containerd-shim-runc-v2
docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI
2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用
$ dockerdocker                  docker-containerd-ctr   dockerddocker-containerd       docker-containerd-shim  docker-runc

dockerdocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。

dockerddockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：

接收并处理 docker client 发送的请求
管理所有的 docker 容器有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。


containerdcontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的.


containerd 的主要功能有：    - 容器生命周期管理    - 镜像管理    - 存储管理    - 容器网络接口及网络管理    - 日志管理

containerd-shimcontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：&#x2F;var&#x2F;run&#x2F;docker&#x2F;libcontainerd&#x2F;containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。
$ ps -ef...docker-containerd-shim 0d55f781ae78a903e68fe6b7941e78c82ca4362b550ca5e7dfc522c113d29226 /var/run/docker/libcontainerd/0d55f781ae78a903e68fe6b7941e78c82ca4362b550ca5e7dfc522c113d29226 docker-runc
  其主要作用是：  它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)  即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的  向 containerd 报告容器的退出状态  有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。

runCrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。


# even without docker, dockerd, docker-containerd, we still can run a container using runC, here is an example to run busybox:$ mkdir /container$ cd /container/$ mkdir rootfs# 准备容器镜像的文件系统,从 busybox 镜像中提取$ docker export $(docker create busybox) | tar -C rootfs -xvf -$ ls rootfs/bin  dev  etc  home  proc  root  sys  tmp  usr  var# 有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，# 包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成$ docker-runc spec$ lsconfig.json  rootfs$ docker-runc run simplebusybox    #启动容器$ lsbin   dev   etc   home  proc  root  sys   tmp   usr   var
rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，
docker daemon uses registry mirror and open tcp socketcreate file &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;override.conf then restart service with systemctl restart docker
[Service]ExecStart=ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 \          --registry-mirror https://registry.docker-cn.com

push local image to docker hubFirst you must have a docker hub account like xxx, and tag your image with this format.
xxx/yyy, xxx is you account id, yyy can be any string
$ docker tag local_image xxx/yyy$ docker tag local_image xxx/yyy:new_tag$ docker login$ docker push xxx/yyy# other people can download from docker hub$ docker pull xxx/yyy

what’s the most used repo for docker.
docker hub:   https://hub.docker.com/
google hub:   https://console.cloud.google.com/gcr/images/google-containers/GLOBAL

how to get value of a particular attribute$ docker inspect --format=&#x27;&#123;&#123;.HostConfig.CpuQuota&#125;&#125;&#x27; $container_id

how to map a host block device to container$ docker run --device=/dev/sdb:/dev/xvda -it ubuntu /bin/bash# /dev/sdb is host device# /dev/xvda is device in container# default permission: rwm(read, write, create)# with permission$ docker run --device=/dev/sdb:/dev/xvda:r -it ubuntu /bin/bash

how to limit IO for a containercgroups to accomplish this
Refer to docker io throttle, only supports DirectIO due to blkio cgroup limitation.
# these two policies can work together, not it only supports DirectIO, not buffer io--blkio-weight=0                Block IO weight (relative weight) accepts a weight value between 10 and 1000, default weight for each device--blkio-weight-device=&quot;&quot;        Block IO weight (relative device weight, format: DEVICE_NAME:WEIGHT), override default weight, example: --blkio-weight-device &quot;/dev/sda:100&quot;--device-read-bps=[]            Limit read rate (bytes per second) from a device--device-read-iops=[]           Limit read rate (IO count per second) from a device--device-write-bps=[]           Limit write rate (bytes per second) to a device--device-write-iops=[]          Limit write rate (IO count per second) to a device$ docker run -it --rm --device-write-bps /dev/sda:1mb ubuntu /bin/bash# /dev/sda is host device, it limits the rate of this container if it accesses /dev/sda$ docker run -it --rm --device-write-iops /dev/sda:10 ubuntu /bin/bash# blkio-weight is relative weight, that means if only one processA(docker process or other) with weight 100 access /dev/sda, it uses the 100% IO bandwidth,# if another processB access /dev/sda at same time who belongs to another blkio group with weight 200, 1/3 bandwidth for processA, 2/3 IO bandwith for processB# test inside each container with directIO(container)# time dd if=/dev/zero of=test.out bs=1M count=1024 oflag=direct

limit cpu of a containercgroups to accomplish this
--cpu-shares , -cSet this flag to a value greater or less than the default of 1024 to increase or reduce the container’s weight, and give it access to a greater or lesser proportion of the host machine’s CPU cycles. This is only enforced when CPU cycles are constrained. When plenty of CPU cycles are available, all containers use as much CPU as they need. In that way, this is a soft limit. It prioritizes container CPU resources for the available CPU cycles. # !!!It does not guarantee or reserve any specific CPU access.!!!# let&#x27;s say there are four CPUS on host, if all containers are running CPU intensive workload# first container takes one CPU# second container takes one CPU# third container takes two CPU# but if third container quits or sleep, the other two both take two CPUS !!!$ docker run -it --rm  --cpu-shares 1024 ubuntu /bin/bash$ docker run -it --rm  --cpu-shares 1024 ubuntu /bin/bash$ docker run -it --rm  --cpu-shares 2048 ubuntu /bin/bash--cpu-period &amp; --cpu-quotaCPU quota (cpu_qota) is a feature of Linux Control Groups (cgroup). CPU quota control `how much CPU time a container can use`, `cpu_quota is the number of microseconds of CPU time` a container can use `per cpu_period`. For example configuring:cpu_quota to 50,000cpu_period to 100,000The container will be allocated 50,000 microseconds per 100,000 microsecond period. `A bit like (see below) the use of 0.5 CPUs`. Quota can be greater than the period. For example:    cpu_quota to 200,000    cpu_period to 100,000Now the container can use `200,000 microseconds of CPU time every 100,000 microseconds`. To use the CPU time there will either need to be multiple processes in the container, or a multi-threaded process. `This configuration is a bit like (see below) having 2 CPUs`.cpu_quota allows setting an `upper bound on the amount of CPU time a container gets`. Linux enforces the limit even if CPU time is available. Quotas can hinder utilization while `providing a predictable upper bounds on CPU time.`--cpuset-cpusLimit the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use, if you have more than one CPU. The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU).--cpus=&lt;value&gt; 	Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus=&quot;1.5&quot;, the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period=&quot;100000&quot; and --cpu-quota=&quot;150000&quot;. shortway for cpu-period and cpu-quota# In order to set cpu_quota correctly, you need to know how many cpu can be used by container, then set cpu_quota and cpu_period correctly.$ docker run -it --rm  --cpuset-cpus 0-1 --cpus=1.5 ubuntu /bin/bash# OR$ docker run -it --rm  --cpuset-cpus 0-1 --cpu-period=100000 --cpu-quota=150000 ubuntu /bin/bash# only alow cpu0 and cpu1 to run this container$ docker run -it --rm  --cpuset-cpus 0-1 ubuntu /bin/bash

expose Nvidia GPU to a container# must install nvidia runtime first on host$ apt-get install nvidia-container-runtime# run from image ubuntu with nvidia-smi command as entrypoint$ docker run -it --rm --gpus device=GPU-3a23c669-1f69-c64e-cf85-44e9b07e7a2a ubuntu nvidia-smi

limit memory used by a containercgroups to accomplish this
# options followed by a suffix of b, k, m, g, to indicate bytes, kilobytes, megabytes, or gigabytes-m or --memory= The maximum amount of memory the container can use. If you set this option, the minimum allowed value is 6m (6 megabyte).# !!!it&#x27;s just cgroup limitation, it does not reserved such memory for a container!!!$ docker run -it --rm  -m 100M ubuntu /bin/bash$ docker run -it --rm  -m 10G ubuntu /bin/bash# update docker memory or cpu when it&#x27;s running$docker update --memory 123289600 --memory-swap 123289600 ubuntu$docker update --cpus 1 ubuntu

check stats for container# it will show container memory limit and how much it&#x27;s used now$ docker stats $container_idCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O5ed                 12.44%              104.8 MB / 104.9 MB   99.92%              4.861 kB / 648 B    9.138 GB / 10.16 GB

run container automatically when dockerd start# run your container with  --restart=always$ docker run --restart=always -it ubuntu /bin/bash

expose a port from containeriptable rule to accomplish this
# in this way you when you access host port, it redirect to container port# host port: 6000# container port: 22$ docker run --restart=always -p 6000:22 -it ubuntu /bin/bash

start a container with given ip and hostname# check subnet of bridge that container uses$ docker network ls$ docker network inspect bridge# set with hostname and ip$ docker run --restart=always -p 6000:22 --hostname test --ip 172.16.0.2 -it ubuntu /bin/bash

change docker store &#x2F;var&#x2F;lib&#x2F;docker to other dir1. Modify /lib/systemd/system/docker.service to tell docker to use our own directory    instead of default /var/lib/docker. In this example, I am using /p/var/lib/docker      Apply below patch.   $ diff -uP -N /lib/systemd/system/docker.service.orig /lib/systemd/system/docker.service   --- /lib/systemd/system/docker.service.orig	2018-12-05 21:24:20.544852391 -0800   +++ /lib/systemd/system/docker.service	2018-12-05 21:25:57.909455275 -0800   @@ -10,7 +10,7 @@    # the default is not to use systemd for cgroups because the delegate issues still    # exists and systemd currently does not support the cgroup feature set required    # for containers run by docker  -ExecStart=/usr/bin/dockerd -H unix://  +ExecStart=/usr/bin/dockerd -g /p/var/lib/docker -H unix://   ExecReload=/bin/kill -s HUP $MAINPID   TimeoutSec=0   RestartSec=22. Stop docker service   $ systemctl stop docker3. Do daemon-reload as we changed docker.service file      $ systemctl daemon-reload4. rsync existing docker data to our new location      $ rsync -aqxP /var/lib/docker/ /p/var/lib/docker/5. Start docker service      $ systemctl start docker

inside docker permission denied to run some command# run docker with --privileged=true and must run with /usr/sbin/init$ docker run --restart=always -p 6000:22 --hostname test --ip 172.16.0.2 --privileged=true -it ubuntu /usr/sbin/init

retrieve docker run command from container# use docker inspect to get all parameter of run$ pip install runlike$ runlike -p  centos7_orgdocker run \        --name=centos7_org \        --hostname=1cde10a63a09 \        --mac-address=02:42:ac:11:00:02 \        --env=PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \        --restart=always \        --label=&#x27;org.label-schema.license=GPLv2&#x27; \        --label=&#x27;org.label-schema.build-date=20201113&#x27; \        --label=&#x27;org.label-schema.schema-version=1.0&#x27; \        --label=&#x27;org.label-schema.vendor=CentOS&#x27; \        --label=&#x27;org.opencontainers.image.created=2020-11-13 00:00:00+00:00&#x27; \        --label=&#x27;org.opencontainers.image.title=CentOS Base Image&#x27; \        --label=&#x27;org.opencontainers.image.licenses=GPL-2.0-only&#x27; \        --label=&#x27;org.label-schema.name=CentOS Base Image&#x27; \        --label=&#x27;org.opencontainers.image.vendor=CentOS&#x27; \        --runtime=runc \        -t \        centos:7 \        /bin/bash

run df&#x2F;free&#x2F;cpu inside containerAs there is no namespace for memory, cpu, hence when you run free, cat /proc/cpuinfo, it shows information about the host!!!, it’s not one that container can use, what you see is not true, it’s not what container can use, it’s also true, it’s true for host.
but df show mount information while container has its own mnt namespace, so it sees devices of its own not host.
$ docker exec -it centos bash# it&#x27;s host memory[root@423eb7c3e9a5] # free -h              total        used        free      shared  buff/cache   availableMem:          7.8Gi       538Mi       6.9Gi       8.0Mi       390Mi       7.0GiSwap:         7.9Gi          0B       7.9Gi# it&#x27;s host fs[root@423eb7c3e9a5 opt]# df -hFilesystem               Size  Used Avail Use% Mounted onoverlay                   50G   23G   28G  46% /tmpfs                     64M     0   64M   0% /devtmpfs                    3.9G     0  3.9G   0% /sys/fs/cgroupshm                       64M     0   64M   0% /dev/shm/dev/vg1/lv1              89M  4.9M   84M   6% /opt/dev/mapper/centos-root   50G   23G   28G  46% /etc/hoststmpfs                    3.9G     0  3.9G   0% /proc/asoundtmpfs                    3.9G     0  3.9G   0% /proc/acpitmpfs                    3.9G     0  3.9G   0% /proc/scsitmpfs                    3.9G     0  3.9G   0% /sys/firmware# it&#x27;s host cpu info[root@423eb7c3e9a5 opt]# cat /proc/cpuinfo processor       : 0vendor_id       : GenuineIntelcpu family      : 6model           : 165model name      : Intel(R) Core(TM) i7-10700T CPU @ 2.00GHzstepping        : 5cpu MHz         : 1992.000cache size      : 16384 KBphysical id     : 0siblings        : 8core id         : 0cpu cores       : 8apicid          : 0initial apicid  : 0fpu             : yesfpu_exception   : yescpuid level     : 22wp              : yesflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc eagerfpu pni pclmulqdq ssse3 cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase avx2 invpcid rdseed clflushopt md_clear flush_l1d arch_capabilitiesbogomips        : 3984.00clflush size    : 64cache_alignment : 64address sizes   : 39 bits physical, 48 bits virtualpower management:...# check disk size that can be used by container, please check https://cyun.tech/docker-persist-data# all limits you have with containers using something like the overlay2 filesystem are inherited from the parent filesystem. # Since docker does everything under /var/lib/docker, your available disk space on that filesystem are the same as the limits you&#x27;ll see inside of a container.# check cpu and memory can be used for this container, run docker inspec$ docker inspect centos | grep Cpuset89:            &quot;CpusetCpus&quot;: &quot;0-1&quot;,90:            &quot;CpusetMems&quot;: &quot;&quot;,$ docker inspect centos | grep Memory76:            &quot;Memory&quot;: 2147483648,

limit rootfs size per containerBy default, docker sees the same disk size(while RW layer sits) as host and can use it as well, but in some case we want to limit storage size used by container, --storage-opt size=xxx can do this, but limit to some storage driver devicemapper, btrfs, overlay2, windowsfilter and zfs  For the devicemapper, btrfs, windowsfilter and zfs drivers, user cannot pass a size less than the Default BaseFS Size. For the overlay2 storage driver, the size option is only available if the backing fs is xfs and mounted with the pquota mount option. Under these conditions, user can pass any size less than the backing fs size.
XFS supports disk quotas by user, by group, and by project. Project disk quotas allow you to limit the amount of disk space on individual directory hierarchies. You can configure both hard and soft limits on the number of disk blocks (or disk space), and the number of inodes, which limit the number of files a user can create. Quotas do not apply to the root user.
You must first enable quotas for users, groups, and/or projects by using a mount option when mounting for the XFS file system. After enabling quotas, use the xfs_quota command to set limits to view quota information.
# enable disk project quotas on /var$ cat /etc/fstab.../dev/mapper/centos_dev-var /var                     xfs     rw,pquota        0 0 ...# report the overall quota state information:$ xfs_quota -x -c stateUser quota state on /var (/dev/mapper/centos_dev-var)  Accounting: OFF  Enforcement: OFF  Inode: #20328 (5 blocks, 5 extents)Group quota state on /var (/dev/mapper/centos_dev-var)  Accounting: OFF  Enforcement: OFF  Inode: #227342 (1 blocks, 1 extents)Project quota state on /var (/dev/mapper/centos_dev-var)  Accounting: ON  Enforcement: ON  Inode: #227342 (1 blocks, 1 extents)Blocks grace time: [7 days]Inodes grace time: [7 days]Realtime Blocks grace time: [7 days]# show quota$ xfs_quota -x -c &#x27;report -h&#x27; /varProject quota on /var (/dev/mapper/centos_dev-var)                        Blocks              Project ID   Used   Soft   Hard Warn/Grace   ---------- --------------------------------- #0           2.2G      0      0  00 [------]$ docker run -it --storage-opt size=10G fedora /bin/bash$ xfs_quota -x -c &#x27;report -h&#x27; /varProject quota on /var (/dev/mapper/centos_dev-var)                        Blocks              Project ID   Used   Soft   Hard Warn/Grace   ---------- --------------------------------- #0           2.2G      0      0  00 [------]#2             8K    10G    10G  00 [------]#3             8K    10G    10G  00 [------]# what quota does# initialize project with ID: 100$ mkdir -p /data/volumes/xfs32m/5m$ xfs_quota -x -c &#x27;project -s -p /data/volumes/xfs32m/5m 100&#x27; /data/volumes/xfs32m# set a 5M quota on project, id=100$ xfs_quota -x -c &#x27;limit -p bsoft=5m bhard=5m 100&#x27; /data/volumes/xfs32m

change storage driverRefer to storagedriver
Get Pid of container# Pid we see from host, inside container, it&#x27;s Pid 1!!!$docker inspect --format &#123;&#123;.State.Pid&#125;&#125; $container_id$docker inspect --format &#123;&#123;.State.Pid&#125;&#125; $container_name16755

enter docker namespace without docker execThe first process of container, its parent is containerd-shim-runc-v2, but if you run docker exec -it $docker bash, its parent is containerd-shim-runc-v2 as well.
$docker inspect --format &#123;&#123;.State.Pid&#125;&#125; $container_name16755              /proc/pid/ns/mnt    the mount namespace              /proc/pid/ns/uts    the UTS namespace              /proc/pid/ns/ipc    the IPC namespace              /proc/pid/ns/net    the network namespace              /proc/pid/ns/pid    the PID namespace              /proc/pid/ns/user   the user namespace              /proc/pid/root      the root directory# mount, uts, ipc, net, pid, user namespace$nsenter -t 16755 --mount --net --uts --ipc --pid --user --root /bin/bash# same as$docker exec -it $container_name /bin/bash

dockerd settingkeep container running when docker service restarting
$ cat /etc/docker/daemon.json &#123;    &quot;live-restore&quot;: true&#125;


prevent docker to create default bridge network
$ cat /etc/docker/daemon.json &#123;    &quot;bridge&quot;: &quot;none&quot;&#125;


change log level for dockerd
$ cat /etc/docker/daemon.json &#123;    &quot;log-level&quot;: &quot;debug&quot;&#125;

Get log of dockerd
$ journalctl -xu docker.service

change data directory for dockerddefault it’s &#x2F;var&#x2F;lib&#x2F;docker on Linux.
$ cat /etc/docker/daemon.json &#123;   &quot;data-root&quot;: &quot;/mnt/docker-data&quot;&#125;


run dockerd in forground with profiling$ /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --shim docker-containerd-shim --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --runtime docker-runc --pprof-address 127.0.0.1:5151 --debug

update docker config after created it# build-dev is container&#x27;s name$docker inspect --format=&quot;&#123;&#123;.Id&#125;&#125;&quot; build-dev6b667579c2a963767bb97b5ed35e4d56ca9a428da8b9fd067fac14b25712048a# stop docker service, otherwise, edit config.v2.json will be lost as docker will rewrite this file if edited outside!!!$service stop docker$vim /var/lib/docker/containers/6b667579c2a963767bb97b5ed35e4d56ca9a428da8b9fd067fac14b25712048a/config.v2.json$service start docker

show disk usage of container# NOTE: this list virtual size and real size# virtual size = real size + image size# real size is only RW layer size$ docker ps --size
updating conf of existing containerhow to set a pre-existing docker container’s restart policy
# docker update has very limit options(for mem, cpu, restart only)$ docker update  --restart=always jason-dev

Ref
docker run parameter
docker cpu, memory, gpu parameter
docker run options

]]></content>
      <categories>
        <category>docker</category>
        <category>command</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-frequently-used-commands</title>
    <url>/2019/10/11/docker-frequently-used-commands/</url>
    <content><![CDATA[Docker CLIBefore going into docker cli, let’s see how docker client, daemon and registry works, here is an overview of these parts and how they work with each other.


As you can see client, daemon, registry are three different parts, they can run in one machine or different machines, by default, daemon listens on Unix socket, CLI connects with that socket, registry can be your company registry or docker official hub, 
You can use docker cli or call REST API to communicate with docker daemon, docker cli is just a wrapper of rest api
All docker commands are available here 
ContainerFrequently used one
# run with bridge mode(vethpair) with specific address$ docker run --name mycentos --restart=always -p 6000:22 --hostname test --ip 172.16.0.2 --privileged=true -itd centos /usr/sbin/init# run with host network$ docker run --name mycentos --restart=always -p 6000:22 --hostname test --net=host  --privileged=true -itd centos /usr/sbin/init


Run a container in interactive mode with tty
# docker run = docker create + docker start# -t: Allocate a pseudo-TTY, When set to true Docker can allocate a pseudo-tty and attach to the standard input of any container# -i: Keep STDIN open even if not attached.$ docker run -it nginx bash# this will pull nginx(image) and create a container and start it with bash command# Note: bash overwrite CMD provide in nginx so nginx daemon is not started in this docker$ docker run --rm -it nginx bashcontrol + C# --rm means this docker will be removed when it exited$ docker ps -a # will not show it as it&#x27;s removed

Run a container in detached mode or non-detached mode
# detach(run in background) then later on you attach to it$ docker run --name my-nginx -d nginx$ docker attach $container_id# OR$ docker run nginx# both will run nginx image from CMD provided in docker image CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon&quot;]# as docker run will create container if not exist(user docker create) so it&#x27;s also# has lots of args like create provide https://docs.docker.com/engine/reference/commandline/run/
Check start logs of a specific container
$ docker logs $container_id(name)# if docker failed to write such log, check syslog as well# /var/log/syslog
List containers and its disk usage
$ docker ps # only active containers$ docker ps -a # all containers even it&#x27;s stopped.$ docker ps --size$ docker stats # show running stats CPU, MEM, IO, NET
create&#x2F;stop&#x2F;start&#x2F;restart&#x2F;rm container
$ docker create --name my-nginx nginx$ docker start $container_id(name)$ docker stop $container_id(name)$ docker restart $container_id(name)$ docker rm $container_id(name)# Actually create provides lots of args when create a docker to set it config# like volume, mount points, cpu, entrypoint, CMD, env etc# https://docs.docker.com/engine/reference/commandline/create/
Run a new command in an running container
$ docker exec -it $container_id(name) bash
copy file in&#x2F;out container
$ docker cp $container_id(name):/home/text.txt /root/text.txt$ docker cp /root/text.txt $container_id(name):/home/text.txt

inspect a container
$ docker inspect $container_id

Image
pull an image from registry$ docker pull nginx:latest
build an imageDetails refer to Dockfile inside
check history of an image$ docker history $image_id# OR to show history full description$ docker history $image_id --no-trunc
list images$ docker images
remove image(s)$ docker rmi $image_id
tag an image$ docker tag $image_id $new_image_tag # will create a new image$ docker images
create an image from a running container$ docker commit $container_id $new_image_name # add the image to local repo# OR export it to a local file$ docker export -o new_image.tar $container_id$ docker import new_image.tar $new_image_name:latest
backup&#x2F;restore an image# save an image to tar file$ docker save nginx:latest &gt;/root/back.tar # must be absolute path with repo and tag later on when load# still have such info$ docker load &lt;/root/back.tar # must be absolute path
Network$ docker network ls$ docker network inspect $network_id

Registry$ docker search xxx # search image from registry
Volume$ docker volume ls$ docker volume inspect $volume_name$ docker volume create $volume_name$ docker volume rm $volume_name# volume can be used at docker create or docker run
Others
check docker info$ docker infoClient: Context:    default Debug Mode: false Plugins:  app: Docker App (Docker Inc., v0.9.1-beta3)  buildx: Docker Buildx (Docker Inc., v0.8.2-docker)  scan: Docker Scan (Docker Inc., v0.17.0)Server: Containers: 0  Running: 0  Paused: 0  Stopped: 0 Images: 0 Server Version: 20.10.15 Storage Driver: overlay2                ------important  Backing Filesystem: xfs  Supports d_type: true  Native Overlay Diff: true  userxattr: false Logging Driver: json-file              -------important Cgroup Driver: cgroupfs Cgroup Version: 1 ... Docker Root Dir: /var/lib/docker        -----important Debug Mode: false Registry: https://index.docker.io/v1/ ...$ docker version

]]></content>
      <categories>
        <category>docker</category>
        <category>command</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker command</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-network</title>
    <url>/2019/10/14/docker-network/</url>
    <content><![CDATA[Docker networkOverviewDocker’s networking subsystem is pluggable using drivers. Several drivers exist by default, and provide core networking functionality


bridge: The default network driver. If you don’t specify a driver, this is the type of network you are creating. Bridge networks are usually used when your applications run in standalone containers that need to communicate, will use a separate network namespace for this container, add veth pairs, one in the bridge, the other in the container.

host: For standalone containers, remove network isolation between the container and the Docker host, and use the host’s networking, use same network namespace(root) with host directly.

overlay and macvlan

none: For this container, disable all networking. Usually used in conjunction with a custom network driver. will use a separate network namespace but only with loopback interface.


More details about docker network, refer to 
Bridge modeAs host and none driver are simple, host uses network namespace with host, while none uses a separate namespace but only with loopback interface, host driver has high performance but less isolation, none for customer defining it own network.
So here let’s explain Bridge driver with more details and see how traffic goes in container and goes out with ping command in docker.

From simplicity, ignore arp, dns, $ ping baidu  from container.
Request packet out:

First container checks its routing table, it sees that for baidu(arp, dns ignore),it should send packet to gateway 172.17.0.2(docker0[each bridge has such virtual device] which is in root network namespace), as eth0(in container) and docker0 are in same subnet, so the request packet is sent by container with below info through vethpair(eth0 in container, the other end in bridge).

src mac: eth0(in container) macdst mac: docker0 macsrc ip: eth0(container) ip: 172.17.0.1dst ip: baidu&#x27;s ip


The packet goes directly to vethx（as it’s the peer of eth0 in container)
When packet reaches vethx, bridge CAM table is used to search which port should be sent to by checking the dst mac, as dst mac is docker0 mac, so packet is sent to docker0 without any change(because it&#39;s bridge(switch)).
When docker0 receives this packet with dst mac is itself, packet goes up(bypass the bridge) as it’s local, PREROUTING hook is called at IP layer for each table with priority(conntrack-&gt;nat), as no connection track for this request now, so conntrack table does not match, then check if any rule matches in nat table, still no match here. hence looking up routing table after PREROUTING.

there are raw mangle tables in PREROUTING hook, we did not check that because it&#39;s empty, just ignore it for simple, only check nat table at PREROUTING

By looking up routing table with dst ip(baidu’s ip), the default route is matched with gateway 10.117.7.253, out iface eth0(host), then goes to FORWARD phase(received packet that’s not for me, forward it out), check rules in that HOOK point.

The first rule in FORWARD(filter table) matches, jump to DOCKER-ISOLATION-STAGE-1

Also the first rule in DOCKER-ISOLATION-STAGE-1 matches, jump to DOCKER-ISOLATION-STAGE-2

The second rule in DOCKER-ISOLATION-STAGE-2 matches with target RETURN, so goes back to DOCKER-ISOLATION-STAGE-1, the second rule in DOCKER-ISOLATION-STAGE-2 matches with target RETURN, goes back to FORWARD chain, the fourth rule in FORWARD matches with target ACCEPT, FORWARFD hooks is done.


In FORWARD hook, we only check filter table, but there are other table: mangle as well, as it&#39;s empty, ignore checking it for simple

Now packet reaches POSTROUTING HOOK at IP layer, the first rule matches with target MASQUERADE(special SNAT with output interface ip), create connection at conntrack table after SNAT, then packet gos out on eth0(host) with below info.

In POSTROUTING Hook, we only check nat table as well, but there are other tables: mangle as well, as it&#39;s empty, ignore checking it for simple
src mac: eth0(in host) macdst mac: gateway mac(10.117.7.253 mac)src ip : eth0(in host) ip  ---&gt;because SNAT(MASQUERADE)dst ip: baidu ip

For short, eth0(container)—–&gt;docker0(host)—routing—-postrouting SNAT(outgoing ip of interface)——physical eth0


Reply packet IN:when reply packet comes back with below info
src mac: gateway mac(10.117.7.253)dst mac: eth0(in host) macsrc ip: baidu ipdst ip: eth0 (host) ip

Only ip and port for connection track entry, no mac address
$ conntrack -Ltcp      6 431982 ESTABLISHED src=10.226.143.201 dst=172.17.0.2 sport=2806 dport=8080 src=172.17.0.2 dst=10.226.143.201 sport=8080 dport=2806 [ASSURED] mark=0 use=1tcp      6 196299 ESTABLISHED src=127.0.0.1 dst=127.0.0.1 sport=49816 dport=46457 src=127.0.0.1 dst=127.0.0.1 sport=46457 dport=49816 [ASSURED] mark=0 use=1

As the dst mac is eth0’s mac, you can image packet bypass bridge(as dst mac is local),  then the next step is to check hooks in PREROUTING, as we already created a connection at conntrack table, hence we found that entry before checking rule in nat table(PREROUTING), as we found the connection track, skip nat table rules in PREROUTING(most for DNAT), As we found the connection in conntrack, do DNAT for the packet, after DNAT, packet with below info, actually the mac is not important now, as it reaches IP layer(PREROUTING).
src mac: gateway macdst mac: eth0(host) macsrc ip: baidu ipdst ip: eth0(in container) ip(172.17.0.1)


After PREROUTING(DNAT), looks up routing table with above infothe third route matches, result: out iface docker0, then goes to FORWARD HOOKS(as dst ip is none of host’s port, forwarding)

At FORWARD HOOK, the first rule matches, jump to DOCKER-ISOLATION-STAGE-1

At DOCKER-ISOLATION-STAGE-1, the second rule matches with target RETURN, hence goes back to FORWARD, the second rule in FORWARD matches with target ACCEPT(terminated here)

Check POSTROUTING rules, no one matches

Goes down neighbor system neigh_xmit() which calls dev_hard_header() to add skb’s mac address with below information, then call dev_queue_xmit() which calls dev-&gt;ndo_start_xmit()(here dev is docker0)


src mac: docker0 macdst mac: eth0(container)172.17.0.1 macsrc ip: baidu ipdst ip: eth0(container)172.17.0.1

Now packets goes into bridge(through docker0-&gt;ndo_start_xmit==br_dev_xmit), as for docker0(management port), its ndo_start_xmit is br_dev_xmit which flood(for multicast, broadcast identified by dst mac) to all ports or forward packet to the specific port through which we see the dst mac, so here packet is forward to vethxx(update skb-&gt;dev = vethxx which is docker0), call its dev-&gt;ndo_start_xmit() which is veth_xmit() which sends packet to the other end(its peer), so eth0(in container) receives it. All right, it’s done.



command to set docker networkThis is parameter for docker run to set network for a container
--network=&quot;bridge&quot; : Connect a container to a network

‘bridge’: create a network stack on the default Docker bridge

‘none’: no networking

‘container:&lt;name|id&gt;’: reuse another container’s network stack

‘host’: use the Docker host network stack

‘&lt;network-name&gt;|&lt;network-id&gt;’: connect to a user-defined network


As you can see docker can use network of another docker that means they share the same network(same network namespace, no new veth pair for this container as it shares with another container which already has one!!!
use default bridge networkBy default, docker creates three networks, if you do not set network when you run a container bridge is used, you can check details about each network
$ docker network lsNETWORK ID          NAME                DRIVER              SCOPEd40edaa25285        bridge              bridge              local22a73a59ecee        host                host                locald812e787ee49        none                null                local$ docker network inspect bridge[    &#123;        &quot;Name&quot;: &quot;bridge&quot;,        &quot;Id&quot;: &quot;d40edaa25285c66d856252d9396eaa03167b108e2a00c80a41ec73ddf90bfbae&quot;,        &quot;Created&quot;: &quot;2019-10-18T17:30:49.803348286+08:00&quot;,        &quot;Scope&quot;: &quot;local&quot;,        &quot;Driver&quot;: &quot;bridge&quot;,        &quot;EnableIPv6&quot;: false,        &quot;IPAM&quot;: &#123;            &quot;Driver&quot;: &quot;default&quot;,            &quot;Options&quot;: null,            &quot;Config&quot;: [                &#123;                    &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;,                    &quot;Gateway&quot;: &quot;172.17.0.2&quot; #gateway for this bridge which docker0 in root namespace                &#125;            ]        &#125;,        &quot;Internal&quot;: false,        &quot;Attachable&quot;: false,        &quot;Ingress&quot;: false,        &quot;ConfigFrom&quot;: &#123;            &quot;Network&quot;: &quot;&quot;        &#125;,        &quot;ConfigOnly&quot;: false,        # containers who uses this bridge and it&#x27;s ip and mac        &quot;Containers&quot;: &#123;            &quot;b10936e9e3f7a8703e8360ffb994e1736455d54ef77bd72f32c99d8be3573550&quot;: &#123;                &quot;Name&quot;: &quot;nervous_lamport&quot;,                &quot;EndpointID&quot;: &quot;04dead82abb38c42c55c547c77ac10cf65701528c5ac5b05576b2d910fcc2e2f&quot;,                &quot;MacAddress&quot;: &quot;02:42:ac:11:00:01&quot;,                &quot;IPv4Address&quot;: &quot;172.17.0.1/16&quot;,                &quot;IPv6Address&quot;: &quot;&quot;            &#125;        &#125;,        &quot;Options&quot;: &#123;            &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;,            &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;,            &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot;        &#125;,        &quot;Labels&quot;: &#123;&#125;    &#125;]$ ifconfig docker0docker0   Link encap:Ethernet  HWaddr 02:42:32:26:c0:ce          inet addr:172.17.0.2  Bcast:172.17.255.255  Mask:255.255.0.0          inet6 addr: fe80::42:32ff:fe26:c0ce/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:21131 errors:0 dropped:0 overruns:0 frame:0          TX packets:21649 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0          RX bytes:1212359 (1.2 MB)  TX bytes:66638333 (66.6 MB)# show bridge info with brctl command$ brctl showbridge name     bridge id               STP enabled     interfacesdocker0         8000.02423226c0ce       no              vethd0a3d97# check veth pair one in host, the other is in container# vethd0a3d97 is the veth pair which is in the host bridge# the peer is @if44(index is 44) which is in container$ ip link45: vethd0a3d97@if44: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default    link/ether 62:75:74:03:97:c1 brd ff:ff:ff:ff:ff:ff link-netnsid 0$ docker exec -it b10936e9e3f7 ip link44: eth0@if45: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default    link/ether 02:42:ac:11:00:01 brd ff:ff:ff:ff:ff:ff link-netnsid 0# check the default route for this container, as you can see gateway is 172.17.0.2(docker0)$ docker exec -it b10936e9e3f7 netstat -nrlKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         172.17.0.2      0.0.0.0         UG        0 0          0 eth0
Note:

docker network is separated from container, you can create it and rm it
veth paris are created and removed automatically when start or stop a container

Let’s create another container which still uses default bridge.
$ brctl showbridge name     bridge id               STP enabled     interfacesdocker0         8000.02423226c0ce       no              veth7e4c2a0                                                        vethd0a3d97#check veth pairs$ ip link45: vethd0a3d97@if44: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default    link/ether 62:75:74:03:97:c1 brd ff:ff:ff:ff:ff:ff link-netnsid 051: veth7e4c2a0@if50: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default    link/ether 6e:8a:8e:f8:ac:8c brd ff:ff:ff:ff:ff:ff link-netnsid 1$ docker network inspect bridge[    &#123;        &quot;Name&quot;: &quot;bridge&quot;,        &quot;Id&quot;: &quot;d40edaa25285c66d856252d9396eaa03167b108e2a00c80a41ec73ddf90bfbae&quot;,        &quot;Created&quot;: &quot;2019-10-18T17:30:49.803348286+08:00&quot;,        &quot;Scope&quot;: &quot;local&quot;,        &quot;Driver&quot;: &quot;bridge&quot;,        &quot;EnableIPv6&quot;: false,        &quot;IPAM&quot;: &#123;            &quot;Driver&quot;: &quot;default&quot;,            &quot;Options&quot;: null,            &quot;Config&quot;: [                &#123;                    &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;,                    &quot;Gateway&quot;: &quot;172.17.0.2&quot;                &#125;            ]        &#125;,        &quot;Internal&quot;: false,        &quot;Attachable&quot;: false,        &quot;Ingress&quot;: false,        &quot;ConfigFrom&quot;: &#123;            &quot;Network&quot;: &quot;&quot;        &#125;,        &quot;ConfigOnly&quot;: false,        &quot;Containers&quot;: &#123;            &quot;1128a3a6f4d0c3fe7f910253b368ec33f49e931765231f89a31bfa27174f2a19&quot;: &#123;                &quot;Name&quot;: &quot;laughing_sanderson&quot;,                &quot;EndpointID&quot;: &quot;2fa47920e37e3b5956327a5a3d500b55d51743003f9a895a850f170d0bfc033f&quot;,                &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;,                &quot;IPv4Address&quot;: &quot;172.17.0.3/16&quot;,                &quot;IPv6Address&quot;: &quot;&quot;            &#125;,            &quot;b10936e9e3f7a8703e8360ffb994e1736455d54ef77bd72f32c99d8be3573550&quot;: &#123;                &quot;Name&quot;: &quot;nervous_lamport&quot;,                &quot;EndpointID&quot;: &quot;04dead82abb38c42c55c547c77ac10cf65701528c5ac5b05576b2d910fcc2e2f&quot;,                &quot;MacAddress&quot;: &quot;02:42:ac:11:00:01&quot;,                &quot;IPv4Address&quot;: &quot;172.17.0.1/16&quot;,                &quot;IPv6Address&quot;: &quot;&quot;            &#125;        &#125;,        &quot;Options&quot;: &#123;            &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;,            &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;,            &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;,            &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot;        &#125;,        &quot;Labels&quot;: &#123;&#125;    &#125;]

use customized bridge network# let create a bridge network with my_bridge from docker cli$ docker network create --driver bridge my_bridge35e2bf6f7cfce89c07c5ba6493c47fb1561be53c4e5d5f1364907a3b549c281a# show docker network$ docker network  lsNETWORK ID          NAME                DRIVER              SCOPEd40edaa25285        bridge              bridge              local22a73a59ecee        host                host                local35e2bf6f7cfc        my_bridge           bridge              locald812e787ee49        none                null                local# show system bridge$ brctl showbridge name     bridge id               STP enabled     interfacesbr-35e2bf6f7cfc         8000.0242e911394e       no# run a container with my_bridge$ docker run -it -d --net my_bridge  ubuntu:tool983f5af3b0e39093e599d8cc1f169be8fd94e76b4c4dbbee5abadc41634ef6af$ brctl showbridge name     bridge id               STP enabled     interfacesbr-35e2bf6f7cfc         8000.0242e911394e       no              veth83832ea# inspect my_bridge$ docker network inspect my_bridge[    &#123;        &quot;Name&quot;: &quot;my_bridge&quot;,        &quot;Id&quot;: &quot;35e2bf6f7cfce89c07c5ba6493c47fb1561be53c4e5d5f1364907a3b549c281a&quot;,        &quot;Created&quot;: &quot;2019-10-22T10:57:28.647218559+08:00&quot;,        &quot;Scope&quot;: &quot;local&quot;,        &quot;Driver&quot;: &quot;bridge&quot;,        &quot;EnableIPv6&quot;: false,        &quot;IPAM&quot;: &#123;            &quot;Driver&quot;: &quot;default&quot;,            &quot;Options&quot;: &#123;&#125;,            &quot;Config&quot;: [                &#123;                    &quot;Subnet&quot;: &quot;172.18.0.0/16&quot;,                    &quot;Gateway&quot;: &quot;172.18.0.1&quot; # gateway is different with docker0(another bridge)                &#125;            ]        &#125;,        &quot;Internal&quot;: false,        &quot;Attachable&quot;: false,        &quot;Ingress&quot;: false,        &quot;ConfigFrom&quot;: &#123;            &quot;Network&quot;: &quot;&quot;        &#125;,        &quot;ConfigOnly&quot;: false,        &quot;Containers&quot;: &#123;            &quot;983f5af3b0e39093e599d8cc1f169be8fd94e76b4c4dbbee5abadc41634ef6af&quot;: &#123;                &quot;Name&quot;: &quot;frosty_murdock&quot;,                &quot;EndpointID&quot;: &quot;9f6c861df34fa5901ba460d411dd0cb38df545c58126d920d8479dc72911d472&quot;,                &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;,                &quot;IPv4Address&quot;: &quot;172.18.0.2/16&quot;,                &quot;IPv6Address&quot;: &quot;&quot;            &#125;        &#125;,        &quot;Options&quot;: &#123;&#125;,        &quot;Labels&quot;: &#123;&#125;    &#125;]# each bridge has a virtual device with bridge name(system name not bridge name used by docker)$ ifconfigbr-35e2bf6f7cfc Link encap:Ethernet  HWaddr 02:42:e9:11:39:4e          inet addr:172.18.0.1  Bcast:172.18.255.255  Mask:255.255.0.0          inet6 addr: fe80::42:e9ff:fe11:394e/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:0 errors:0 dropped:0 overruns:0 frame:0          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0          RX bytes:0 (0.0 B)  TX bytes:1186 (1.1 KB)# check gw for the container$ docker exec -it 983f5af3b0e3 netstat -nrlKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         172.18.0.1      0.0.0.0         UG        0 0          0 eth0]]></content>
      <categories>
        <category>docker</category>
        <category>network</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-persist-share-data</title>
    <url>/2019/10/14/docker-persist-data/</url>
    <content><![CDATA[Persisting DataOverviewSometimes you want to share data between container or you want to persist data even docker is deleted, there there two ways for it.

bind mount
volume


Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker(volume is dir on host but managed by docker itself) 
Volumes have several advantages over bind mounts:  

Volumes are easier to back up or migrate than bind mounts.
You can manage volumes using Docker CLI commands or the Docker API.
Volumes work on both Linux and Windows containers.
Volumes can be more safely shared among multiple containers.
Volume drivers let you store volumes on remote hosts or cloud providers, to encrypt the contents of volumes, or to add other functionality.
New volumes can have their content pre-populated by a container.


Ignore tmpfs(in memory), it&#39;s for non-persisting data, data will be disappear if docker is stop or restart
Note: volume dir is at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;, controlled by docker cli, so volume is independent of container, even you delete container, volume is still there if not delete explicitly by cli
docker option for persisting dataThere are two options you can use to do this, one is -v(–volume ), the other is –mount, new user should use –mount

–mount: Consists of multiple key-value pairs, separated by commas and each consisting of a &lt;key&gt;=&lt;value&gt; tuple. The –mount syntax is more verbose than -v or –volume, but the order of the keys is not significant, and the value of the flag is easier to understand.

The type of the mount, which can be bind, volume, or tmpfs. This topic discusses volumes, so the type is always volume.
The source of the mount. For named volumes, this is the name of the volume. For anonymous volumes, this field is omitted. May be specified as source or src.
The destination takes as its value the path where the file or directory is mounted in the container. May be specified as destination, dst, or target.
The readonly option, if present, causes the bind mount to be mounted into the container as read-only.
The volume-opt option, which can be specified more than once, takes a key-value pair consisting of the option name and its value.



–mount needs docker version &gt;&#x3D;17.06, check it $docker version
bind mount# For type=bind, must specify the source(from host)$ docker run -d \  -it \  --name devtest \  --mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app \  nginx:latest

volume$ docker volume create my-vol$ docker volume ls$ docker volume inspect my-vol[    &#123;        &quot;CreatedAt&quot;: &quot;2023-09-26T11:25:18+08:00&quot;,        &quot;Driver&quot;: &quot;local&quot;,        &quot;Labels&quot;: &#123;&#125;,        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;,        &quot;Name&quot;: &quot;my-vol&quot;,        &quot;Options&quot;: &#123;&#125;,        &quot;Scope&quot;: &quot;local&quot;    &#125;]# use an existing volume so that two container can share data by volume id# Container 1$ docker run -d \  --name devtest1 \  --mount src=myvol,dst=/app \  nginx:latest# Container 2$ docker run -d \  --name devtest2 \  --mount source=myvol,target=/app \  nginx:latest# OR# use non-existing volume(without source)# in this case docker will create a volume automatically# but this volume will not deleted automatically when container is deleted# Container 1$ docker run -d \  --name devtest1 \  --mount target=/app \  nginx:latest# Container 2 use volumes same as Container 1$ docker run -d \  --name devtest2 \  --volumes-from devtest1 \  nginx:latest# ----------------------------$ docker inspect devtest1$ docker stop devtest1$ docker rm devtest1# need to remove volume explicitly!!!$ docker volume rm my-vol# 
volume should be deleted manually even if it’s created automatically sometime 
mount block device into container# create a raw disk$ dd if=/dev/zero of=/tmp/loop.raw bs=1M count=100# setup raw disk as a block device(/dev/loop0)$ losetup /dev/loop0 /tmp/loop.raw# create disk and format with fs $ pvcreate /dev/loop0$ vgcreate vg1 /dev/loop0$ lvcreate --size 90M --name lv1 vg1$ mkfs.xfs /dev/vg1/lv1$ docker run --rm -it --mount=&#x27;type=volume,dst=/opt,volume-driver=local,volume-opt=type=xfs,volume-opt=device=/dev/vg1/lv1&#x27; centos bash

tmpfstmpfs mounts only for linux. When you create a container with a tmpfs mount, the container can create files outside the container’s writable layer
This is useful to `temporarily store` sensitive files that you don’t want to persist in either the host or the container writable layer.

Unlike volumes and bind mounts, you can’t share tmpfs mounts between containers
$ docker run -d \  -it \  --name tmptest \  --mount type=tmpfs,destination=/app \  nginx:latest

Volume size(mount) and Container size(RW Layer)There is no limitation for volume when creating it, you can’t set limit for it, the size of volume is determined by the disk(block) of host it resides. same thing for tmpfs as well, its size is determined by host tmpfs system(default it’s size half of total memory). volume, bind mount, tmpfs, block device appear same in the container(dst directory), that means inside a container, you see a directory, the size or data that can be stored at that directory depends on the size of block on host. but if you use fdisk, lsblk inside the container, you see the host block size, not the size can be used by the container.
The storage size can be used by container has two parts.

volume size determined by host block size, check volume source docker inspect, then check which block as source
writable layer determined by host block where the writable layer exists(default /var/lib/docker, check which block for /var)

REF
docker volume
docker bind mount
docker tmpfs mount

]]></content>
      <categories>
        <category>docker</category>
        <category>persist-data</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>volume</tag>
        <tag>bind</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-storage-driver</title>
    <url>/2019/10/11/docker-storage-driver/</url>
    <content><![CDATA[Storage driverOverviewDocker supports many storage drivers like below, but if you work on ubuntuuse AUFS if linux kernel version is less than 4.0 otherwise use overlay(overlay2) as it has better performance.

Note: AUFS is not accepted by linux kernel(not in main) but it&#39;s initially default storage driver for docker for ubuntu&#x2F;debian.
how docker uses AUFSAufs merges different dirs into single one, basic usage, refer to Aufs-basic, here let’s say how aufs works for docker.
Actually, Aufs($ mount -t aufs xx) only happens when you create a container(event it's not started), at that time docker creates a RW layer(called container layer), then mounts different layers[container layer + image layers] into single dir, when you start this container, chroot to this single dir as the root dir for it, that's it.

Note: this mount point(single dir) and RW layer are deleted only when the container is removed.
&#x2F;var&#x2F;lib&#x2F;docker changes when created containerlet's take a real example on ubuntu to see how it works, in order to understand it correctly, it's better read how image stored on ubuntu firstly.

Initially, we only have one image, ubuntu:latest which has four layers that’s where we begin.
$ docker ps -aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES$ docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEubuntu              latest              2ca708c1c9cc        3 weeks ago         64.19 MB

later on we create a container with this image
$ docker create  --name myubuntu  ubuntu:latestba2377cb603f8c5ef44619af1bb8734a870139939913777896438e34facd3a3e$ docker ps -aCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMESba2377cb603f        ubuntu:latest       &quot;/bin/bash&quot;         About a minute ago   Created                                 myubuntu

Let’s see what changes for each docker directory.
aufs&#x2F;diff change 
Before created$ ls -l /var/lib/docker/aufs/difftotal 16drwxr-xr-x  6 root root 4096 Oct 14 09:50 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602ddrwxr-xr-x 21 root root 4096 Oct 14 09:50 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7drwxr-xr-x  3 root root 4096 Oct 14 09:50 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cfdrwxr-xr-x  3 root root 4096 Oct 14 09:50 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6After created$ ls -l /var/lib/docker/aufs/difftotal 24drwxr-xr-x  4 root root 4096 Oct 14 10:15 0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47drwxr-xr-x  6 root root 4096 Oct 14 10:15 0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47-init# above are two new added layers(RW layer + init Layer) for this container# RW layer is empty# init layer is system related, some system required infoaufs/diff/0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47-init/|-- dev|   |-- console|   |-- pts|   `-- shm`-- etc    |-- hostname    |-- hosts    |-- mtab -&gt; /proc/mounts    `-- resolv.confdrwxr-xr-x  6 root root 4096 Oct 14 09:50 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602ddrwxr-xr-x 21 root root 4096 Oct 14 09:50 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7drwxr-xr-x  3 root root 4096 Oct 14 09:50 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cfdrwxr-xr-x  3 root root 4096 Oct 14 09:50 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6

aufs&#x2F;layer change 
Before created$ ls -l /var/lib/docker/aufs/layers/total 12-rw-r--r-- 1 root root 130 Oct 14 09:50 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602d-rw-r--r-- 1 root root   0 Oct 14 09:50 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7-rw-r--r-- 1 root root 195 Oct 14 09:50 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cf-rw-r--r-- 1 root root  65 Oct 14 09:50 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6# After created$ ls -l /var/lib/docker/aufs/layers/total 20-rw-r--r-- 1 root root 330 Oct 14 10:15 0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47-rw-r--r-- 1 root root 260 Oct 14 10:15 0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47-init# Above are two new added layers relationship files-rw-r--r-- 1 root root 130 Oct 14 09:50 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602d-rw-r--r-- 1 root root   0 Oct 14 09:50 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7-rw-r--r-- 1 root root 195 Oct 14 09:50 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cf-rw-r--r-- 1 root root  65 Oct 14 09:50 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6


aufs&#x2F;mnt change 
# Before created$ ls -l /var/lib/docker/aufs/mnt/total 16drwxr-xr-x 2 root root 4096 Oct 14 09:50 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602ddrwxr-xr-x 2 root root 4096 Oct 14 09:50 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7drwxr-xr-x 2 root root 4096 Oct 14 09:50 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cfdrwxr-xr-x 2 root root 4096 Oct 14 09:50 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6# After created$ ls -l /var/lib/docker/aufs/mnt/total 24drwxr-xr-x 2 root root 4096 Oct 14 10:15 0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47drwxr-xr-x 2 root root 4096 Oct 14 10:15 0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47-init# Above is mount point for RW layer and init layer, it&#x27;s only mounted when docker runs here just create# an empty directory!!!!drwxr-xr-x 2 root root 4096 Oct 14 09:50 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602ddrwxr-xr-x 2 root root 4096 Oct 14 09:50 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7drwxr-xr-x 2 root root 4096 Oct 14 09:50 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cfdrwxr-xr-x 2 root root 4096 Oct 14 09:50 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6$ mount -t aufs -o br=aufs/diff/0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47:aufs/diff/0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47-init xxx none aufs/mnt/0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47# mount happens only container runs!!!!# Before container runroot@dev:/var/lib/docker# tree aufs/mnt/0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47/aufs/mnt/0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47/0 directories, 0 files# After container runs


containers&#x2F; change 
# Before createdroot@dev:/var/lib/docker# ls -l containers/total 0# After createdroot@dev:/var/lib/docker# ls -l containers/total 4drwx------ 2 root root 4096 Oct 14 10:15 ba2377cb603f8c5ef44619af1bb8734a870139939913777896438e34facd3a3e# it stores conf and runtime info for this container

image&#x2F;aufs&#x2F;layerdb&#x2F;mounts change 
# Before createdroot@dev:/var/lib/docker# ls -l image/aufs/layerdb/mounts/total 0# After createdroot@dev:/var/lib/docker# ls -l image/aufs/layerdb/mounts/total 4drwxr-xr-x 2 root root 4096 Oct 14 10:15 ba2377cb603f8c5ef44619af1bb8734a870139939913777896438e34facd3a3eroot@dev:/var/lib/docker# tree image/aufs/layerdb/mounts/ba2377cb603f8c5ef44619af1bb8734a870139939913777896438e34facd3a3e/image/aufs/layerdb/mounts/ba2377cb603f8c5ef44619af1bb8734a870139939913777896438e34facd3a3e/|-- init-id|-- mount-id`-- parentroot@dev:/var/lib/docker# cat image/aufs/layerdb/mounts/ba2377cb603f8c5ef44619af1bb8734a870139939913777896438e34facd3a3e/mount-id0d6eb9abafaca00679acf48400264dbb2704a996ae2781a615809ec672859a47# it stores the mount point(mount-id) for this container
That&#39;s almost the changes from storage view when you create a container.
]]></content>
      <categories>
        <category>docker</category>
        <category>storage-driver</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker storage</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-tools</title>
    <url>/2023/07/10/docker-tools/</url>
    <content><![CDATA[Toolsdocker composeDocker Compose is a tool for running multi-container applications on Docker defined using the Compose file format. A Compose file is used to define how one or more containers that make up your application are configured. Once you have a Compose file, you can create and start your application with a single command: docker compose up, A commpose file defines your application, a componse file has several services, each service is one container!


You can imagine docker compose as a wrapper for docker command but supports operating containers at application level, meaning start&#x2F;stop application, no matter how many container it has, it just makes the work earlier.
docker compose vs docker-compose: docker-compose is orignal tool this the target mentioned above, but docker-compose is written in Python while most Docker developments are in Go, so they decided to recreate docker compose project in Go with the same and more features, docker compose will eventually replace docker-compose, but no timeline for that yet, so if you’r new to this, use docker compose.
Install the docker compose, please refer to guide to install docker compose
Usagewhen you run docker compose command, it scans a file named docker-compose.yml from current directory if not specified by user.
Usage:  docker compose [OPTIONS] COMMANDDefine and run multi-container applications with Docker.Options:      --dry-run                    Execute command in dry run mode  -f, --file stringArray           Compose configuration files  ...Commands:  build       Build or rebuild services  config      Parse, resolve and render compose file in canonical format  cp          Copy files/folders between a service container and the local filesystem  create      Creates containers for a service.  down        Stop and remove containers, networks  events      Receive real time events from containers.  exec        Execute a command in a running container.  images      List images used by the created containers  kill        Force stop service containers.  logs        View output from containers  ls          List running compose projects  pause       Pause services  port        Print the public port for a port binding.  ps          List containers  pull        Pull service images  push        Push service images  restart     Restart service containers  rm          Removes stopped service containers  run         Run a one-off command on a service.  start       Start services  stop        Stop services  top         Display the running processes  unpause     Unpause services  up          Create and start containers  version     Show the Docker Compose version information

docker-compose.yml example
version: &#x27;2&#x27;services:  app:    image: node:latest    container_name: app_main    restart: always    command: sh -c &quot;yarn install &amp;&amp; yarn start&quot;    ports:      - 8000:8000    working_dir: /app    volumes:      - ./:/app    environment:      MYSQL_HOST: localhost      MYSQL_USER: root      MYSQL_PASSWORD:      MYSQL_DB: test  mongo:    image: mongo    container_name: app_mongo    restart: always    ports:      - 27017:27017    volumes:      - ~/mongo:/data/dbvolumes:  mongodb:

$lsdocker-compose.yml  Dockerfile$cat DockerfileFROM hello-world$cat docker-compose.ymlversion: &#x27;2&#x27;services:  hello_world:    # build this container from image directly    image: hello-world  hello-world-local:    # build this container from Dockerfile    build:      context: .$docker compose ls -aNAME                STATUS              CONFIG FILEScompose             exited(2)           /tmp/compose/docker-compose.yml$docker compose ps -aNAME                          IMAGE                       COMMAND             SERVICE             CREATED              STATUS                          PORTScompose-hello-world-local-1   compose-hello-world-local   &quot;/hello&quot;            hello-world-local   About a minute ago   Exited (0) About a minute ago   compose-hello_world-1         hello-world                 &quot;/hello&quot;            hello_world         About a minute ago   Exited (0) About a minute ago   


Ref
awesome-compose-projects

]]></content>
      <categories>
        <category>docker</category>
        <category>command</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>docker-image-container-on-disk</title>
    <url>/2019/10/11/docker-image-container-on-disk/</url>
    <content><![CDATA[Image and Container ConceptThe major difference between a container(runtime) and an image(static) is the top writable layer. All writes to the container that add new or modify existing data are stored in this writable layer. When the container is deleted, the writable layer is also deleted. The underlying image layers remain unchanged.
Because each container has its own writable container layer, and all changes are stored in this container layer, multiple containers can share access to the same underlying image and yet have their own data state.



Image layers
Init layer(there is Init layer between image layer and container layer on ubuntu actually) which is system related info like &#x2F;dev host etc.
Container layer(R&#x2F;W layer)

Image on disk &#x2F;var&#x2F;lib&#x2F;dockerLet’s see what’s the file layout for image and container. here is the layout for it
Below output is the file layout after create a  container(official ubuntu image)
docker&#x2F;aufs directory$ ls /var/lib/docker/aufsdiff  layers  mnt

diff&#x2F;: content of each layer(image layer and init  layer and container layer)
layers&#x2F;: As layer has relationship, file in it shows all layers it based on
mnt&#x2F;: As in diff&#x2F;, it only shows content of each layer, here it shows content of all layers it based on(actually it’s aufs mount point(single dir)).

Note: without container runs, all are empty!when container runs only the contain layer mount point has content!!!
root@dev:/var/lib/docker# tree -L 2 aufsaufs|-- diff|   |-- 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602d|   |-- 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7|   |-- 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cf|   |-- 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6|   |-- afc8986c55fb3ec896070d3f475a61ababf6e5e458107e7d8d449df6b53055cf|   `-- afc8986c55fb3ec896070d3f475a61ababf6e5e458107e7d8d449df6b53055cf-init|-- layers|   |-- 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602d|   |-- 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7|   |-- 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cf|   |-- 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6|   |-- afc8986c55fb3ec896070d3f475a61ababf6e5e458107e7d8d449df6b53055cf|   `-- afc8986c55fb3ec896070d3f475a61ababf6e5e458107e7d8d449df6b53055cf-init`-- mnt    |-- 268f3ca66142b2b9791504df67c604a5f0bf331064f4e1f899fcdd33686b602d    |-- 38fd587650e8a0befc2a177395a2afa380abcd32ac78316c6a38ea6b01bdadd7    |-- 39d5b8fbe7446545c8e982bc3c2e0a0e955a81c99a88a85121252800406c88cf    |-- 9bd9ca976269998def585a7848e180207f88cdec9e95b11e930532cfe84780e6    |-- afc8986c55fb3ec896070d3f475a61ababf6e5e458107e7d8d449df6b53055cf    `-- afc8986c55fb3ec896070d3f475a61ababf6e5e458107e7d8d449df6b53055cf-init

docker&#x2F;containers directoryroot@dev:/var/lib/docker# tree -L 2 containers/containers/`-- 1eb906e718d8271061ac894972533234af4c563364bd7522f482b1d427095270    |-- 1eb906e718d8271061ac894972533234af4c563364bd7522f482b1d427095270-json.log    |-- config.v2.json    |-- hostconfig.json    |-- hostname    |-- hosts    |-- resolv.conf    |-- resolv.conf.hash    `-- shm

containers&#x2F;: includes the config and runtime info about this container etc

docker&#x2F;imageimage/`-- aufs    |-- imagedb    |   |-- content    |   |   `-- sha256    |   |       `-- 2ca708c1c9ccc509b070f226d6e4712604e0c48b55d7d8f5adc9be4a4d36029a ---&gt;image id    |   `-- metadata    |       `-- sha256    |-- layerdb    |   |-- mounts    |   |   `-- 1eb906e718d8271061ac894972533234af4c563364bd7522f482b1d427095270   ----&gt;container id    |   |       |-- init-id    |   |       |-- mount-id                                                       ----&gt;mount point for this container    |   |       `-- parent    |   |-- sha256    |   |   |-- 5308e2e4a70bd4344383b8de54f8a52b62c41afb5caa16310326debd1499b748    |   |   |   |-- cache-id                                                       ---&gt;local id for this layer aufs/diff/cache-id    |   |   |   |-- diff                                                           ---&gt;id of this layer    |   |   |   |-- parent    |   |   |   |-- size    |   |   |   `-- tar-split.json.gz                                              --&gt;content for this layer    |   |   |-- a1aa3da2a80a775df55e880b094a1a8de19b919435ad0c71c29a0983d64e65db    |   |   |   |-- cache-id    |   |   |   |-- diff    |   |   |   |-- size    |   |   |   `-- tar-split.json.gz    |   |   |-- bd416bed302bc2f061a2f6848a565483a5f265932d2d4fa287ef511b7d1151c8    |   |   |   |-- cache-id    |   |   |   |-- diff    |   |   |   |-- parent    |   |   |   |-- size    |   |   |   `-- tar-split.json.gz    |   |   `-- dab02287e04c8b8207210b90b4056bd865fcfab91469f39a1654075f550c5592    |   |       |-- cache-id    |   |       |-- diff    |   |       |-- parent    |   |       |-- size    |   |       `-- tar-split.json.gz    |    `-- repositories.json


imagedb&#x2F;content&#x2F;sha256&#x2F;$image_id: content of the image like env, volume, layers used by this image etc
imagedb&#x2F;content&#x2F;metadata&#x2F;sha256&#x2F;$image_id: metadata for this image
layerdb&#x2F;mounts&#x2F;$container_id: mount point for this container
layerdb&#x2F;sha256&#x2F;$chain_id: content of this layer(layer id is stored at $cache_id&#x2F;diff), content is compressed,

The uncompressed data is stored at &#x2F;var&#x2F;lib&#x2F;docker&#x2F;aufs&#x2F;diff&#x2F;sha256&#x2F;$cache_id for this layer, cache_id at $chain_id&#x2F;cache-id
file changes when create a containerwhat changes happens when create a container, please refer to storage driver
]]></content>
      <categories>
        <category>docker</category>
        <category>image</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>docker image</tag>
      </tags>
  </entry>
  <entry>
    <title>encode-toml</title>
    <url>/2021/08/03/encode-toml/</url>
    <content><![CDATA[TomlTOML stands for Tom’s Obvious, Minimal Language. It is a data serialization language designed to be a minimal configuration file format that’s easy to read due to obvious semantics. It is an alternative to YAML and JSON. It aims to be more human friendly than JSON and simpler that YAML.


Basic# Comments in TOML look like this.################# SCALAR TYPES ################## Our root object (which continues for the entire document) will be a map,# which is equivalent to a dictionary, hash or object in other languages.# The key, equals sign, and value must be on the same line# (though some values can be broken over multiple lines).key = &quot;value&quot;string = &quot;hello&quot;number = 42float = 3.14boolean = truedateTime = 1979-05-27T07:32:00-08:00scientificNotation = 1e+12&quot;key can be quoted&quot; = true # Both &quot; and &#x27; are fine&quot;key may contain&quot; = &quot;letters, numbers, underscores, and dashes&quot;# A bare key must be non-empty, but an empty quoted key is allowed&quot;&quot; = &quot;blank&quot;     # VALID but discouraged&#x27;&#x27; = &#x27;blank&#x27;     # VALID but discouraged########### String ############ All strings must contain only valid UTF-8 characters.# We can escape characters and some of them have a compact escape sequence.# For example, \t add a tabulation. Refers to the spec to get all of them.basicString = &quot;are surrounded by quotation marks. \&quot;I&#x27;m quotable\&quot;. Name\tJos&quot;multiLineString = &quot;&quot;&quot;are surrounded by three quotation markson each side and allow newlines.&quot;&quot;&quot;literalString = &#x27;are surrounded by single quotes. Escaping are not allowed.&#x27;multiLineLiteralString = &#x27;&#x27;&#x27;are surrounded by three single quotes on each sideand allow newlines. Still no escaping.The first newline is trimmed in raw strings.   All other whitespace   is preserved. #! are preserved?&#x27;&#x27;&#x27;# For binary data it is recommended that you use Base64, another ASCII or UTF8# encoding. The handling of that encoding will be application specific.############ Integer ############## Integers can start with a +, a - or nothing.## Leading zeros are not allowed. Hex, octal, and binary forms are not allowed.## Values that cannot be expressed as a series of digits are not allowed.int1 = +42int2 = 0int3 = -21integerRange = 64## You can use underscores to enhance readability. Each## underscore must be surrounded by at least one digit.int4 = 5_349_221int5 = 1_2_3_4_5     # VALID but discouraged########## Float ########### Floats are an integer followed by a fractional and/or an exponent part.flt1 = 3.1415flt2 = -5e6flt3 = 6.626E-34############ Boolean ############bool1 = truebool2 = falseboolMustBeLowercase = true############# Datetime #############date1 = 1979-05-27T07:32:00Z # UTC time, following RFC 3339/ISO 8601 specdate2 = 1979-05-26T15:32:00+08:00 # with RFC 3339/ISO 8601 offsetdate3 = 1979-05-27T07:32:00 # without offsetdate4 = 1979-05-27 # without offset or time

array and table##################### COLLECTION TYPES ############################### Array ##########array1 = [ 1, 2, 3 ]array2 = [ &quot;Commas&quot;, &quot;are&quot;, &quot;delimiters&quot; ]array3 = [ &quot;Don&#x27;t mix&quot;, &quot;different&quot;, &quot;types&quot; ]array4 = [ [ 1.2, 2.4 ], [&quot;all&quot;, &#x27;strings&#x27;, &quot;&quot;&quot;are the same&quot;&quot;&quot;, &#x27;&#x27;&#x27;type&#x27;&#x27;&#x27;] ]array5 = [  &quot;Whitespace&quot;, &quot;is&quot;, &quot;ignored&quot;]########## Table ########### Tables (or hash tables or dictionaries) are collections of key/value# pairs. They appear in square brackets on a line by themselves.# Empty tables are allowed and simply have no key/value pairs within them.[table]# Under that, and until the next table or EOF are the key/values of that table.# Key/value pairs within tables are not guaranteed to be in any specific order.[table-1]key1 = &quot;some string&quot;key2 = 123[table-2]key1 = &quot;another string&quot;key2 = 456# Dots are prohibited in bare keys because dots are used to signify nested tables.# Naming rules for each dot separated part are the same as for keys.[dog.&quot;tater.man&quot;]type = &quot;pug&quot;# In JSON land, that would give you the following structure:# &#123; &quot;dog&quot;: &#123; &quot;tater.man&quot;: &#123; &quot;type&quot;: &quot;pug&quot; &#125; &#125; &#125;# Whitespace around dot-separated parts is ignored, however, best practice is to# not use any extraneous whitespace.[a.b.c]            # this is best practice[ d.e.f ]          # same as [d.e.f][ j . &quot;ʞ&quot; . &#x27;l&#x27; ]  # same as [j.&quot;ʞ&quot;.&#x27;l&#x27;]# You don&#x27;t need to specify all the super-tables if you don&#x27;t want to. TOML knows# how to do it for you.# [x] you# [x.y] don&#x27;t# [x.y.z] need these[x.y.z.w] # for this to work# As long as a super-table hasn&#x27;t been directly defined and hasn&#x27;t defined a# specific key, you may still write to it.[a.b]c = 1[a]d = 2# Will generate the following in JSON:# &#123; &quot;a&quot;: &#123;&quot;b&quot;: &#123;&quot;c&quot;: 1&#125;, &quot;d&quot;: 2 &#125; &#125;# You cannot define any key or table more than once. Doing so is invalid.# DO NOT DO THIS[a]b = 1[a]c = 2# DO NOT DO THIS EITHER[a]b = 1[a.b]c = 2# All table names must be non-empty.[]     # INVALID[a.]   # INVALID[a..b] # INVALID[.b]   # INVALID[.]    # INVALID################# Inline table #################inlineTables = &#123; areEnclosedWith = &quot;&#123; and &#125;&quot;, mustBeInline = true &#125;point = &#123; x = 1, y = 2 &#125;#################### Array of Tables ##################### An array of tables can be expressed by using a table name in double brackets.# Each table with the same double bracketed name will be an item in the array.# The tables are inserted in the order encountered.[[products]]name = &quot;array of table&quot;sku = 738594937emptyTableAreAllowed = true[[products]][[products]]name = &quot;Nail&quot;sku = 284758393color = &quot;gray&quot;# Will generate the following in JSON:# &#123;&quot;products&quot;: [&#123;&quot;name&quot;: &quot;array of table&quot;&#125;, &#123;&quot;name&quot;:&quot;Nail&quot;&#125;]&#125;

nested arrays of tables
# You can create nested arrays of tables as well. Each double-bracketed# sub-table will belong to the nearest table element above it.[[fruit]]  name = &quot;apple&quot; # I am a property in fruit table/map  [fruit.geometry]    shape = &quot;round&quot;    note = &quot;I am a property in geometry table/map&quot;  [[fruit.color]]    name = &quot;red&quot;    note = &quot;I am an array item in apple fruit&#x27;s table/map&quot;  [[fruit.color]]    name = &quot;green&quot;    note = &quot;I am in the same array as red&quot;[[fruit]]  name = &quot;banana&quot;  [[fruit.color]]    name = &quot;yellow&quot;    note = &quot;I am an array item in banana fruit&#x27;s table/map&quot;

REF
Toml official website
Online Toml Validator
Learn Toml Minutes

]]></content>
      <categories>
        <category>encode</category>
        <category>toml</category>
      </categories>
      <tags>
        <tag>encode</tag>
        <tag>toml</tag>
      </tags>
  </entry>
  <entry>
    <title>encode_yaml</title>
    <url>/2021/03/26/encode-yaml/</url>
    <content><![CDATA[YamlYAML is a human friendly data serialization standard for all programming languages, it’s superset of JSON.


Basic################# SCALAR TYPES ################## Our root object (which continues for the entire document) will be a map,# which is equivalent to a dictionary, hash or object in other languages.key: valueanother_key: Another value goes here.a_number_value: 100scientific_notation: 1e+12# The number 1 will be interpreted as a number, not a boolean. if you want# it to be interpreted as a boolean, use trueboolean: truenull_value: nullkey with spaces: value#############################################################################   Notice that strings don&#x27;t need to be quoted. However, they can be      #############################################################################however: &#x27;A string, enclosed in quotes.&#x27;&#x27;Keys can be quoted too.&#x27;: &quot;Useful if you want to put a &#x27;:&#x27; in your key.&quot;single quotes: &#x27;have &#x27;&#x27;one&#x27;&#x27; escape pattern&#x27;double quotes: &quot;have many: \&quot;, \0, \t, \u263A, \x0d\x0a == \r\n, and more.&quot;# UTF-8/16/32 characters need to be encodedSuperscript two: \u00B2################################################################################# Multiple-line strings can be written either as a &#x27;literal block&#x27; (using |)   ## or a &#x27;folded block&#x27; (using &#x27;&gt;&#x27;).################################################################################literal_block: |    This entire block of text will be the value of the &#x27;literal_block&#x27; key,    with line breaks being preserved.    The literal continues until de-dented, and the leading indentation is    stripped.        Any lines that are &#x27;more-indented&#x27; keep the rest of their indentation -        these lines will be indented by 4 spaces.folded_style: &gt;    This entire block of text will be the value of &#x27;folded_style&#x27;, but this    time, all newlines will be replaced with a single space.    Blank lines, like above, are converted to a newline character.        &#x27;More-indented&#x27; lines keep their newlines, too -        this text will appear over two lines.##################### COLLECTION TYPES ################################################################################################ Nesting uses indentation. 2 space indent is preferred (but not required!!!)##########################################################################a_nested_map:  key: value  another_key: Another Value  another_nested_map:    hello: hello# Maps don&#x27;t have to have string keys.0.25: a float key# Keys can also be complex, like multi-line objects######################################################################## We use ? followed by a space to indicate the start of a complex key.#######################################################################? |  This is a key  that has multiple lines: and this is its value# YAML also allows mapping between sequences with the complex key syntax# Some language parsers might complain# An example? - Manchester United  - Real Madrid: [2001-01-01, 2002-02-02]# Sequences (equivalent to lists or arrays) look like this# (note that the &#x27;-&#x27; counts as indentation):a_sequence:  - Item 1  - Item 2  - 0.5  # sequences can contain disparate types.  - Item 4  - key: value    another_key: another_value  -    - This is a sequence    - inside another sequence  - - - Nested sequence indicators      - can be collapsed# Since YAML is a superset of JSON, you can also write JSON-style maps and# sequences:json_map: &#123;&quot;key&quot;: &quot;value&quot;&#125;json_seq: [3, 2, 1, &quot;takeoff&quot;]and quotes are optional: &#123;key: [3, 2, 1, takeoff]&#125;

Extra Features######################## EXTRA YAML FEATURES ######################### YAML also has a handy feature called &#x27;anchors&#x27;, which let you easily duplicate# content across your document. Both of these keys will have the same value:anchored_content: &amp;anchor_name This string will appear as the value of two keys.other_anchor: *anchor_name# Anchors can be used to duplicate/inherit propertiesbase: &amp;base  name: Everyone has same name# The regexp &lt;&lt; is called Merge Key Language-Independent Type. It is used to# indicate that all the keys of one or more specified maps should be inserted# into the current map.foo:  &lt;&lt;: *base  age: 10bar:  &lt;&lt;: *base  age: 20# foo and bar would also have name: Everyone has same name# YAML also has tags, which you can use to explicitly declare types.explicit_string: !!str 0.5# Some parsers implement language specific tags, like this one for Python&#x27;s# complex number type.python_complex_number: !!python/complex 1+2j# We can also use yaml complex keys with language specific tags? !!python/tuple [5, 7]: Fifty Seven# Would be &#123;(5, 7): &#x27;Fifty Seven&#x27;&#125; in Python##################### EXTRA YAML TYPES ###################### Strings and numbers aren&#x27;t the only scalars that YAML can understand.# ISO-formatted date and datetime literals are also parsed.datetime: 2001-12-15T02:59:43.1Zdatetime_with_spaces: 2001-12-14 21:59:43.10 -5date: 2002-12-14# The !!binary tag indicates that a string is actually a base64-encoded# representation of a binary blob.gif_file: !!binary |  R0lGODlhDAAMAIQAAP//9/X17unp5WZmZgAAAOfn515eXvPz7Y6OjuDg4J+fn5  OTk6enp56enmlpaWNjY6Ojo4SEhP/++f/++f/++f/++f/++f/++f/++f/++f/+  +f/++f/++f/++f/++f/++SH+Dk1hZGUgd2l0aCBHSU1QACwAAAAADAAMAAAFLC  AgjoEwnuNAFOhpEMTRiggcz4BNJHrv/zCFcLiwMWYNG84BwwEeECcgggoBADs=# YAML also has a set type, which looks like this:set:  ? item1  ? item2  ? item3or: &#123;item1, item2, item3&#125;# Sets are just maps with null values; the above is equivalent to:set2:  item1: null  item2: null  item3: null

REF
YAML official website
Online YAML Validator
Learn YAML Minutes

]]></content>
      <categories>
        <category>encode</category>
        <category>yaml</category>
      </categories>
      <tags>
        <tag>encode</tag>
        <tag>yaml</tag>
      </tags>
  </entry>
  <entry>
    <title>gRPC_Go</title>
    <url>/2021/08/26/gRPC-Go/</url>
    <content><![CDATA[grpcIn gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client has a stub (referred to as just a client in some languages) that provides the same methods as the server.



It is a protocol that is build on top of HTTP&#x2F;2, and it has some handy features. These included the ability to make streaming calls from the client and server side, or Bi-directional streaming. It serializes and deserializes data using Protocol Buffers, and it also provides code generation through the gRPC compiler to currently 11 different languages.
gRPC clients and servers can run and talk to each other in a variety of environments - from servers inside Google to your own desktop - and can be written in any of gRPC’s supported languages. So, for example, you can easily create a gRPC server in Java with clients in Go, Python, or Ruby. In addition, the latest Google APIs will have gRPC versions of their interfaces, letting you easily build Google functionality into your applications
No input rpc parameter definesIf you don’t want any input or output parameters, you can use the well-known proto google.protobuf.Empty. However, this is discouraged as it prevents you from adding parameters to the method in the future. Instead, you would be encouraged to follow the normal practice of having a message for the request, but simply with no contents.
service Greeter &#123;    rpc SayHello (SayHelloRequest) returns (SayHelloResponse) &#123;&#125;&#125;message SayHelloRequest &#123;&#125; // service has no input
implicit stream contextstream also has context as well like unary rpc call which passed context explicitly, while for stream it’s implicitly!
ctx := stream.Context() &lt;-ctx.Done


MetadataMetadata is information about a particular RPC call (such as authentication details) in the form of a list of key-value pairs, where the keys are strings and the values are typically strings, but can be binary data. Metadata is opaque to gRPC itself - it lets the client provide information associated with the call to the server and vice versa.
Access to metadata is language dependent

metadata-go
grpc go basic

http2As grpc based on http2, in order to understand deeply, we need to learn some core concepts of http2.A “stream” is an independent, bidirectional sequence of frames exchanged between the client and server within an HTTP/2 connection.

Stream

A bidirectional flow of bytes within an established connection, which may carry one or more messages. 


A single HTTP&#x2F;2 connection can contain multiple concurrently open streams, with either endpoint interleaving frames from multiple streams.
Streams can be established and used unilaterally or shared by either the client or server.
Streams can be closed by either endpoint.
Streams are identified by an integer. Stream identifiers are assigned to streams by the endpoint initiating the stream.




Message

A complete sequence of frames that map to a logical request or response message. 


Frame

The smallest unit of communication in HTTP&#x2F;2, each containing a frameheader, which at a minimum identifies the stream to which the frame belongs.


All communication is performed over a single TCP connection that can carry any number of bidirectional streams.
Each stream has a unique identifier and optional priority information that is used to carry bidirectional messages.
Each message is a logical HTTP message, such as a request, or response, which consists of one or more frames.
The frame is the smallest unit of communication that carries a specific type of data—e.g., HTTP headers, message payload, and so on. Frames from different streams may be interleaved and then reassembled via the embedded stream identifier in the header of each frame.







Stream is not http2 connection(mostly tcp connection), it’s also bidirectional stuff, close stream does not means close its underlaying connection.

http2 introduction
http2 rfc

Unary RPC callgRPC service methods have exactly one input message and exactly one output message, handler is called only when its get the whole message. Typically, these messages are used as input and output to only one method. This is on purpose, as it allows easily adding new parameters later (to the messages) while maintaining backward compatibility.
In a unary rpc call, the client sends a single request and the server responds with a single message.

pros and cons

easy to use
only effective to small data
if data is huge, it causes delay to response
no interactive support.

writing service in protogreet.proto
syntax = &quot;proto3&quot;;package greet;//used by proto itself(independent with different language)// to prevent naming conflicts between different projects(protos).// import &quot;google/protobuf/timestamp.proto&quot;;option go_package = &quot;github.com/hello/runtime/proto/greet&quot;;// used by protoc when generate go specific code// The greeting service definition.service Greeter &#123;  // Sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;// The request message containing the user&#x27;s name.message HelloRequest &#123;  string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123;  string message = 1;&#125;

buildPrerequisite
$ wget -O ./protoc-3.15.8-linux-x86_64.zip https://github.com/protocolbuffers/protobuf/releases/download/v3.15.8/protoc-3.15.8-linux-x86_64.zip$ unzip protoc-3.15.8-linux-x86_64.zip -d /usr/local# Install the protocol compiler plugins for Go using the following commands# protoc-gen-go: go plugin or gogo/protobuf# proto-gen-go-grpc: go rpc plugin$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.26$ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.1

compile rpcif multiple protos belong to same package, you must provide all of them to protoc command!!!
# put your source at $GOPATH/src/github.com/hello$ cd $GOPATH/src/github.com/hello# If the paths=import flag is specified, the output file is placed in a directory named after the Go package&#x27;s import path For example, an input file pro/hello.proto with a Go import path of github.com/hello/runtime/proto/greet results in an output file at github.com/hello/runtime/proto/greet/hello.pb.go. This is the default output mode if a paths flag is not specified.# If the paths=source_relative flag is specified, the output file is placed in the same relative directory as the input file. For example, an input file pro/hello.proto results in an output file at pro/hello.pb.go# --go_out and --go-grpc_out is based path, all other paths are relative to it!!!$ protoc --go_out=$GOPATH/src/ --go_opt=paths=import --go-grpc_out=$GOPATH/src/ --go-grpc_opt=paths=import proto/greet.proto -I=xxx/other/proto:protocols/# generated code at $GOPATH/src/github.com/hello/runtime/proto/greet$ protoc --go_out=$GOPATH/src/ --go_opt=paths=source_relative --go-grpc_out=$GOPATH/src/ --go-grpc_opt=paths=source_relative proto/greet.proto -I=xxx/other/proto:protocols/# generated code at $GOPATH/src/proto$ protoc --go_out=. --go_opt=paths=source_relative --go-grpc_out=. --go-grpc_opt=paths=source_relative proto/hello.proto# generated code at ./proto

Implement rpcServer  
package mainimport (    &quot;context&quot;    &quot;log&quot;    &quot;net&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/metadata&quot;    pb &quot;github.com/hello/runtime/protocols/greet&quot;    &quot;google.golang.org/grpc/reflection&quot;)const (    port = &quot;:50051&quot;)// server is used to implement greet.GreeterServer.type server struct &#123;    pb.UnimplementedGreeterServer // must be first field    // your staff here&#125;/// return value: HelloReply, error// SayHello implements greet.GreeterServerfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123;    md, ok := metadata.FromIncomingContext(ctx)    if ok &#123;        // metadata is map: var MD map[string][]string        // key is string, while value is string array!!!        log.Printf(&quot;metadata: %v&quot;, md.Get(&quot;k1&quot;))    &#125;    log.Printf(&quot;Received: %v&quot;, in.GetName())    // connection is close when returns!!!    return &amp;pb.HelloReply&#123;Message: &quot;Hi &quot; + in.GetName()&#125;, nil&#125;func main() &#123;    lis, err := net.Listen(&quot;tcp&quot;, port)    if err != nil &#123;        log.Fatalf(&quot;failed to listen: %v&quot;, err)    &#125;    s := grpc.NewServer()    // register services(server&#123;&#125;) with rpc server    pb.RegisterGreeterServer(s, &amp;server&#123;&#125;)        // for debugging    reflection.Register(s)    log.Printf(&quot;server listening at %v&quot;, lis.Addr())    if err := s.Serve(lis); err != nil &#123;        log.Fatalf(&quot;failed to serve: %v&quot;, err)    &#125;&#125;

Client
package mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;)func main() &#123;    fmt.Println(&quot;Client..&quot;)    con, err := grpc.Dial(&quot;localhost:50051&quot;, opts, grpc.WithInsecure(), grpc.WithBlock())    if err != nil &#123;        log.Fatalf(&quot;Error connecting: %v \n&quot;, err)    &#125;    defer con.Close()    //client for specific service    c := pb.NewGreeterClient(con)    req := pb.HelloRequest&#123;Name: &quot;tom&quot;&#125;    res, err := c.SayHello(context.Background(), &amp;req)    if err != nil &#123;        log.Fatalf(&quot;Error on Echo rpc call: %v\n&quot;, err)    &#125; else &#123;        fmt.Printf(&quot;Response: %v\n&quot;, res)    &#125;&#125;

client stream rpc callIn a client streaming rpc call, the client sends a bunch of requests and once it is done streaming, the server will return a single message.
stream here is stream of particular request message, the unit is request but byte!!
Making a client stream request is useful when the client needs to send resources one by one to the server, so they can be processed right away, and once the streaming call is finished the client gets the response.
For client stream, client side needs a stream to send and also server needs a stream to receive messages, both side need to change.
Steps

client send one request, then another

client notify server I finished the stream

server receive request one by one

server see client finish the stream send one reply.


server in protosyntax = &quot;proto3&quot;;package greet;//used by proto itself(independent with different language)// to prevent naming conflicts between different projects(protos).// import &quot;google/protobuf/timestamp.proto&quot;;option go_package = &quot;github.com/hello/runtime/protocols/greet&quot;;// used by protoc when generate go specific codeservice Greeter &#123;  // Sends a greeting  rpc SayHello (stream HelloRequest) returns (HelloReply) &#123;&#125;&#125;// The request message containing the user&#x27;s name.message HelloRequest &#123;  string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123;  string message = 1;&#125;
buildSame as unary rpc
implement rpcserver
package mainimport (    &quot;fmt&quot;    &quot;io&quot;    &quot;log&quot;    &quot;net&quot;    &quot;strings&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/reflection&quot;)const (    port = &quot;:50051&quot;)// server is used to implement greet.GreeterServer.type greetServer struct &#123;    pb.UnimplementedGreeterServer // must be first field    // your staff here&#125;// SayHello implements greet.GreeterServerfunc (s *greetServer) SayHello(stream pb.Greeter_SayHelloServer) error &#123;    data := []string&#123;&#125;    for &#123;        req, err := stream.Recv()        if err == io.EOF &#123;            // response message is returned through stream, not return value for unary rpc call            return stream.SendAndClose(&amp;pb.HelloReply&#123;Message: strings.Join(data, &quot;,&quot;)&#125;)        &#125;        if err != nil &#123;            return fmt.Errorf(&quot;internal error&quot;)        &#125;                data = append(data, req.GetName())    &#125;&#125;func main() &#123;    lis, err := net.Listen(&quot;tcp&quot;, port)    if err != nil &#123;        log.Fatalf(&quot;failed to listen: %v&quot;, err)    &#125;    s := grpc.NewServer()    // register services(server&#123;&#125;) with rpc server    pb.RegisterGreeterServer(s, &amp;greetServer&#123;&#125;)    // for debugging    reflection.Register(s)    log.Printf(&quot;server listening at %v&quot;, lis.Addr())    if err := s.Serve(lis); err != nil &#123;        log.Fatalf(&quot;failed to serve: %v&quot;, err)    &#125;&#125;

client
package mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;log&quot;    &quot;time&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;)func main() &#123;    fmt.Println(&quot;Client..&quot;)    con, err := grpc.Dial(&quot;localhost:50051&quot;, opts, grpc.WithInsecure(), grpc.WithBlock())    if err != nil &#123;        log.Fatalf(&quot;Error connecting: %v \n&quot;, err)    &#125;    defer con.Close()    //client for specific service    c := pb.NewGreeterClient(con)    req1 := pb.HelloRequest&#123;Name: &quot;tom&quot;&#125;    req2 := pb.HelloRequest&#123;Name: &quot;jack&quot;&#125;    // rpc retuns a stream handler    stream, err := c.SayHello(context.Background())    if err != nil &#123;        log.Fatalf(&quot;Error on Echo rpc call: %v\n&quot;, err)    &#125; else &#123;        // send several requests through stream        err = stream.Send(&amp;req1)        if err != nil &#123;            log.Fatalf(&quot;Error on sending: %v\n&quot;, err)        &#125;        fmt.Printf(&quot;sent: %v\n&quot;, req1.GetName())        time.Sleep(10 * time.Second)        err = stream.Send(&amp;req2)        if err != nil &#123;            log.Fatalf(&quot;Error on sending: %v\n&quot;, err)        &#125;        fmt.Printf(&quot;sent: %v\n&quot;, req2.GetName())    &#125;    // this will call CloseSend to notify I sent all, then receive    res, err := stream.CloseAndRecv()    if err != nil &#123;        log.Fatalf(&quot;Error on recv: %v\n&quot;, err)    &#125; else &#123;        fmt.Printf(&quot;Response :%s\n&quot;, res.GetMessage())    &#125;&#125;

server stream rpc call
For server stream, server side needs a stream to send and also client needs a stream to receive response messages, both side need to change.
server in protosyntax = &quot;proto3&quot;;package greet;//used by proto itself(independent with different language)// to prevent naming conflicts between different projects(protos).// import &quot;google/protobuf/timestamp.proto&quot;;option go_package = &quot;github.com/hello/runtime/protocols/greet&quot;;// used by protoc when generate go specific codeservice Greeter &#123;  // Sends a greeting  rpc SayHello (HelloRequest) returns (stream HelloReply) &#123;&#125;&#125;// The request message containing the user&#x27;s name.message HelloRequest &#123;  string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123;  string message = 1;&#125;
buildSame as unary rpc
implement rpcserver
package mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;net&quot;    &quot;time&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/reflection&quot;)const (    port = &quot;:50051&quot;)// server is used to implement greet.GreeterServer.type greetServer struct &#123;    pb.UnimplementedGreeterServer // must be first field    // your staff here&#125;// SayHello implements greet.GreeterServerfunc (s *greetServer) SayHello(req *pb.HelloRequest, stream pb.Greeter_SayHelloServer) error &#123;    res1 := pb.HelloReply&#123;Message: &quot;hello: &quot; + req.GetName()&#125;    err := stream.Send(&amp;res1)    if err != nil &#123;        fmt.Println(&quot;error while sending response&quot;)    &#125;    time.Sleep(5 * time.Second)    res2 := pb.HelloReply&#123;Message: &quot;hi: &quot; + req.GetName()&#125;    err = stream.Send(&amp;res2)    // context = stream.Context() get context if needs    if err != nil &#123;        fmt.Println(&quot;error while sending response&quot;)    &#125;    // stream is close when it returns    return nil&#125;func main() &#123;    lis, err := net.Listen(&quot;tcp&quot;, port)    if err != nil &#123;        log.Fatalf(&quot;failed to listen: %v&quot;, err)    &#125;    s := grpc.NewServer()    // register services(server&#123;&#125;) with rpc server    pb.RegisterGreeterServer(s, &amp;greetServer&#123;&#125;)    reflection.Register(s) // for testing    log.Printf(&quot;server listening at %v&quot;, lis.Addr())    if err := s.Serve(lis); err != nil &#123;        log.Fatalf(&quot;failed to serve: %v&quot;, err)    &#125;&#125;
client
package mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;io&quot;    &quot;log&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;)func main() &#123;    fmt.Println(&quot;Client..&quot;)    opts := grpc.WithInsecure()    // WithBlock() blocks here until, error or connection is setup    con, err := grpc.Dial(&quot;localhost:50051&quot;, opts, grpc.WithInsecure(), grpc.WithBlock())    if err != nil &#123;        log.Fatalf(&quot;Error connecting: %v \n&quot;, err)    &#125;    defer con.Close()    //client for specific service    c := pb.NewGreeterClient(con)    req1 := pb.HelloRequest&#123;Name: &quot;tom&quot;&#125;    stream, err := c.SayHello(context.Background(), &amp;req1)    if err != nil &#123;        log.Fatalf(&quot;Error on sending: %v\n&quot;, err)    &#125; else &#123;        for &#123;            res, err := stream.Recv()            if err == io.EOF &#123;// server close the stream, all is done!!!                break            &#125;            if err != nil &#123;                fmt.Println(err)                break            &#125;            fmt.Printf(&quot;response: %v\n&quot;, res)        &#125;    &#125;&#125;

bi-direction stream rpc callIn a bi-directional streaming rpc call, both the client and the server sends multiple messages to each other. Using this type of rpc call, can be a little bit more complicated, since you have to take care of error handling from the server side and the client side, plus in some cases it can add more latency. And perhaps it could be a better option to use a unary call.

Both the client and server can stop receiving or sending messages at any point in time, either because some errors occurred or because some other business logic happened.

server in protosyntax = &quot;proto3&quot;;package greet;//used by proto itself(independent with different language)// to prevent naming conflicts between different projects(protos).// import &quot;google/protobuf/timestamp.proto&quot;;option go_package = &quot;github.com/hello/runtime/protocols/greet&quot;;// used by protoc when generate go specific codeservice Greeter &#123;  // Sends a greeting  rpc SayHello (stream HelloRequest) returns (stream HelloReply) &#123;&#125;&#125;// The request message containing the user&#x27;s name.message HelloRequest &#123;  string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123;  string message = 1;&#125;
buildSame as unary rpc
implement rpcserver
package mainimport (    &quot;fmt&quot;    &quot;io&quot;    &quot;log&quot;    &quot;net&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;    &quot;google.golang.org/grpc/reflection&quot;)const (    port = &quot;:50051&quot;)// server is used to implement greet.GreeterServer.type greetServer struct &#123;    pb.UnimplementedGreeterServer // must be first field    // your staff here&#125;// SayHello implements greet.GreeterServerfunc (s *greetServer) SayHello(stream pb.Greeter_SayHelloServer) error &#123;    for &#123;        req, err := stream.Recv()        if err == io.EOF &#123;            return nil        &#125;        if err != nil &#123;            return fmt.Errorf(&quot;server recv error&quot;)        &#125; else &#123;            err = stream.Send(&amp;pb.HelloReply&#123;Message: &quot;hello &quot; + req.GetName()&#125;)            if err == io.EOF &#123;                return nil            &#125;            if err != nil &#123;                return fmt.Errorf(&quot;server send error&quot;)            &#125;        &#125;    &#125;&#125;func main() &#123;    lis, err := net.Listen(&quot;tcp&quot;, port)    if err != nil &#123;        log.Fatalf(&quot;failed to listen: %v&quot;, err)    &#125;    s := grpc.NewServer()    // register services(server&#123;&#125;) with rpc server    pb.RegisterGreeterServer(s, &amp;greetServer&#123;&#125;)        // for debugging    reflection.Register(s)    log.Printf(&quot;server listening at %v&quot;, lis.Addr())    if err := s.Serve(lis); err != nil &#123;        log.Fatalf(&quot;failed to serve: %v&quot;, err)    &#125;&#125;
client
package mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;io&quot;    &quot;log&quot;    &quot;sync&quot;    &quot;time&quot;    pb &quot;github.com/hello/proto&quot;    &quot;google.golang.org/grpc&quot;)func main() &#123;    fmt.Println(&quot;Client..&quot;)    // WithBlock() blocks here until, error or connection is setup    con, err := grpc.Dial(&quot;localhost:50051&quot;, opts, grpc.WithInsecure(), grpc.WithBlock())    if err != nil &#123;        log.Fatalf(&quot;Error connecting: %v \n&quot;, err)    &#125;    defer con.Close()    //client for specific service    c := pb.NewGreeterClient(con)    // rpc retuns a stream handler    stream, err := c.SayHello(context.Background())    if err != nil &#123;        log.Fatalf(&quot;Error on Echo rpc call: %v\n&quot;, err)    &#125;    // start a goroutine to send    go func() &#123;        req1 := pb.HelloRequest&#123;Name: &quot;tom&quot;&#125;        stream.Send(&amp;req1)        fmt.Println(&quot;sent: tom&quot;)        time.Sleep(time.Second * 5)        req2 := pb.HelloRequest&#123;Name: &quot;jack&quot;&#125;        stream.Send(&amp;req2)        fmt.Println(&quot;sent: jack&quot;)        // tell sever sending is done, connection is still alive, but stream is close        stream.CloseSend()        fmt.Println(&quot;sending done&quot;)    &#125;()    var wg sync.WaitGroup    wg.Add(1)    go func() &#123;        defer wg.Done()        for &#123;            res, err := stream.Recv()            if err == io.EOF &#123;                return            &#125;            if err != nil &#123;                // continue next                fmt.Println(&quot;error in recv&quot;)                continue            &#125;            fmt.Printf(&quot;Response: %v\n&quot;, res)        &#125;    &#125;()    // block until receive io.EOF    wg.Wait()&#125;

Debug grpcThere are two tools can be used as grpc client to test grpc server, see below

grpcui(like postman), WebUI
grpcurl(like curl), command line

grpcui（web)On way to edit source code to include reflection in your grpc server or without change see below
+ import &quot;google.golang.org/grpc/reflection&quot;  s := grpc.NewServer()  pb.RegisterGreeterService(s, &amp;pb.GreeterService&#123;SayHello: sayHello&#125;)+ // Register reflection service on gRPC server.+ reflection.Register(s)

Run grpcui as below
$ GO111MODULE=on go install github.com/fullstorydev/grpcui/cmd/grpcui@latest# -bind: webserver address# -port: webserver port# -plaintext: connect grpc server without tls# localhost:50051: grpc server# With this command, you must register your rpc with reflection as above$ grpcui -open-browser=false -bind=10.0.2.15 -port=8000 -plaintext localhost:50051# if you can NOT change your source code, there is an another way to run grpcui# xxx.proto who defines rpc call and message.# -proto can be relative path $ grpcui -open-browser=false -proto=./path/to/xxx.proto -bind=10.0.2.15 -port=8000 -plaintext localhost:50051# More advanced, if your xxx.proto import other protos, you need to add -import-path to let grpcui to find them# other protos path relative to --import-path(used for imported protos)$ grpcui -open-browser=false -proto=./path/to/xxx.proto -import-path=/path/to/depen/ -bind=10.0.2.15 -port=8000 -plaintext localhost:50051# then open browser at http://10.0.2.15:8000/

command line toolOn way to edit source code to include reflection in your grpc server or without change see below
+ import &quot;google.golang.org/grpc/reflection&quot;  s := grpc.NewServer()  pb.RegisterGreeterService(s, &amp;pb.GreeterService&#123;SayHello: sayHello&#125;)+ // Register reflection service on gRPC server.+ reflection.Register(s)

Run grpcurl command
$ GO111MODULE=on go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest# list all services$ grpcurl -plaintext localhost:50051 listgrpc.reflection.v1alpha.ServerReflectiongreet.Greeter# list all methods$ grpcurl -plaintext localhost:50051 list greet.Greetergreet.Greeter.SayHello# desribe all methods$ grpcurl -plaintext localhost:50051 describe greet.Greetergreet.Greeter is a service:service Greeter &#123;  rpc SayHello ( .greet.HelloRequest ) returns ( .greet.HelloReply );&#125;# describe message type$ grpcurl -plaintext localhost:50051 describe greet.HelloRequestgreet.HelloRequest is a message:message HelloRequest &#123;  string name = 1;&#125;# call rpc$ grpcurl -plaintext -d &#x27;&#123;&quot;name&quot;: &quot;jason&quot;&#125;&#x27; localhost:50051 greet.Greeter.SayHello&#123;  &quot;message&quot;: &quot;Hi jason&quot;&#125;# if you can&#x27;t register your grpc server with reflection, you can pass -proto and -import-path to grpcurl as grpcui does!!

gogogogoprotobuf is a fork of golang&#x2F;protobuf with extra code generation features.
This code generation is used to achieve:

fast marshalling and unmarshalling
more canonical Go structures
goprotobuf compatibility
less typing by optionally generating extra helper code
peace of mind by optionally generating test and benchmark code
other serialization formats

gogo depends on grpc when generates grpc stub code, that means it justs wrapper grpc only. so for grpc, you must install grpc as well. go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.3.2, make sure install proper version tested by gogo.
Install
Choose one binary and install it, different binaries have different speed and customization. more refer to gogo.
Usage
# can only github.com/golang/protobuf/protoprotoc --gofast_out=. myproto.proto# explicit use gogo github.com/golang/protobuf/protoprotoc -I=. -I=$GOPATH/src -I=$GOPATH/src/github.com/golang/protobuf/proto --&#123;binary&#125;_out=. myproto.proto# use gogo github.com/golang/protobuf/proto by defaultprotoc --gofast_out=plugins=grpc:. my.proto

pros and cons
]]></content>
      <categories>
        <category>go</category>
        <category>rpc</category>
      </categories>
      <tags>
        <tag>go</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title>gdb-commands</title>
    <url>/2019/11/25/gdb-commands/</url>
    <content><![CDATA[IntroductionCore file is generated when program crashes, if it’s not turned off, see limitation for the current user by $ ulimit -a or set it with unlimited by $ ulimit -c unlimited to allow core generation.  
ulimit reads &#x2F;etc&#x2F;security&#x2F;limits.conf that controls the limitation for the system, like core file, max opened file etc  
  
Frequently used command&gt;run a b&gt;info locals&gt;p var&gt;break [file:] line  &gt;break [file:] func  &gt;break [above two] if expr &gt;info break&gt;delete [n] &gt;watch global_var&gt;watch -l local_var if $_caller_is(&quot;caller_function&quot;, 0) &gt;backtrace&gt;info frame&gt;frame [n]&gt;continue&gt;step  &gt;next  &gt;finish&gt;list (show next ten lines of source)  &gt;list - (show previous ten lines)  &gt;list line  &gt;list func &gt;info threads  &gt;thread 2  &gt;ptype struct task_struct

Inside Gdb$ gdb app_binary core_file  

how gdb load debug infoif you compile you application with &#39;-g&#39; option, compiler will write DWARF(debug info) to the objects, actually, it includes two sectionsdebug_info and debug_line sections  

debug_info save function ,variable  
debug_line  map source line to machine code

How does these section save information, it uses DIE (debug info entry), each has lots attributes(key-value) to show the info  
Each DIE has one or more attribute/value pairs  
Each attribute has a name  
– Describes meaning of attribute  
– Value specified for each attribute  
– Data format specified in attribute encoding  
  
● Examples  
– DW_AT_name – Name of object DIE describes  
– DW_AT_location – Source location of object  
– DW_AT_low_pc – Start address of object  
– DW_AT_high_pc – End address of object  
– DW_AT_type – Pointer to DIE describing type  
  
DIE for variable  
  
1c28: DW_TAG_variable  
DW_AT_name : decode  
DW_AT_decl_file : 1  
DW_AT_decl_line : 1782  
DW_AT_type : &lt;0x657&gt;  
DW_AT_location : 0x24ed (location list)  

show all dwarf info  
$ readelf -w  

core file locationBy default, core file is generated at the current directory or &#x2F;var&#x2F;log&#x2F;core, change it if needed following the below rules:  
core file生成的地方是在/proc/sys/kernel/core_pattern文件定义的。  
改动到生成到自己定义的目录的方法是：  
echo &quot;pattern&quot; &gt; /proc/sys/kernel/core_pattern  
并且只有超级用户可以修改这两个文件。  
&quot;pattern&quot;类似我们C语言打印字符串的格式，相关标识如下：  
%%: 相当于%  
%p: 相当于&lt;pid&gt;  
%u: 相当于&lt;uid&gt;  
%g: 相当于&lt;gid&gt;  
%s: 相当于导致dump的信号的数字  
%t: 相当于dump的时间  
%h: 相当于hostname  
%e: 相当于执行文件的名称  
这时用如下命令设置生成的core file到系统/tmp目录下，并记录pid以及执行文件名  
$ echo &quot;/tmp/core-%e-%p&quot; &gt; /proc/sys/kernel/core_pattern  
after such operations. core file should be generated when segment fault happens.  
  
change this pattern live  
  
# echo &#39;/tmp/core_%e.%p.%t&#39; | sudo tee /proc/sys/kernel/core_pattern  
core file with name core.program.processid.timestamp  
  
Or change it forever  
  
edit /etc/sysctl.conf  
kernel.core_pattern = /tmp/core_%e.%p.%t  
$ systemctl restart systemd-sysctl  

core file not generated  

$ ulimit (output must be: unlimited)  
$ cat /proc/sys/kernel/core_pattern (the path must be writable)

CommandsGetting help&gt;help  
&gt;help class  
&gt;help command  

Executing your program# run with args
&gt;run a b
# another way to run with args
&gt;set args a b
&gt;run

# run without args
&gt;run  
&gt;kill

Symbol table&gt;info address s  
(info address var_name; info address fun_name) where it is.  
  
&gt;info func [regex]  
show names,types of defined functions regular pattern search all matched functions  
  
&gt;info var [regex]  
show names types of global, static variables, not value of it!!  

&gt;info var ngx_show_version  
All variables matching regular expression &quot;ngx_show_version&quot;:  
  
&gt;info locals  

&gt;ptype struct task_struct
show definition of task_struct

&gt;ptype /o struct task_struct
show definition of task_struct with offset and total size

Breakpoints and watchpoints&gt;break [file:] line  
&gt;break [file:] func  
&gt;break *addr (set breakpoint at address addr)  
&gt;break [above three] if expr  
# program stops when watched var is written
&gt;watch expr | watch var  
# program stops when watched var is read  
&gt;rwatch expr (read)  
# program stops when watched var is read or write  
&gt;awatch expr (read/or write)  
&gt;info break  
&gt;info watch  
&gt;delete [n]  
&gt;clear [file:]fun  
&gt;clear [file:]line  
&gt;disable [n]  
&gt;enable [n]  
&gt;ignore n count (ignore breakpoint n, count times) [n] can be number for breakpoint or watchpoint  

Program stack&gt;backtrace (print trace of all frames in stack)  
&gt;frame [n] (select frame number n)  
&gt;info frame (describe selected frame)  
&gt;info args (arguments of selected frame)  
&gt;info locals (local variables of selected frame)  
&gt;info reg [rn] (register values [for regs rn] in selected:info reg rax or info reg r10)  

Working files&gt;file [file] (use file for both symbols and executable)  
&gt;core-file [file] ( read file as coredump;or discard)  
&gt;exce-file [file] ( use file as executable only; or discard)  

&gt;symbol-file [file]( use symbol table from file, or discard)  
Loads symbol table information from a specified file, replacing any previously loaded symbols.
Used to set the main symbol file for the debugging session.

&gt;add-symbol-file addr ( read additional symbols from file, dynamically loaded at addr.)
Reads additional symbol table information from a specified file.
Adds new symbol data to the existing symbol table without discarding previously loaded symbols. if existing symbol table already has a symbol, it&#39;s not added.
Typically used for loading symbols from dynamically loaded modules or shared libraries

Execution control&gt;continue (continue running)  
&gt;step  
&gt;stepi (step by machine instructions rather than source lines)  
&gt;next  
&gt;nexti  
&gt;finish  
&gt;return [expr] (pop selected stack frame without executing [setting return value])  
&gt;signal num (resume execution with signal )  
&gt;jump line (resume execution at specified line number)  
&gt;jump *address  
&gt;set var=expr (evaluate expr without displaying it; use for altering program variables)  

Display&gt;print [/f] [expr] (print a;print /x a;print /t a)  
    f can be  
        x    hexadecimal  
        t     binary  
        c    character  
&gt;x [/Nuf] expr (examine memory at address expr; x/1bx 0x123;x/1hx 0x123;x/1wx 0x123)  
    N is the length you want to see!!  
    u can be  
        b individual bytes  
        h halfwords (two bytes)  
        w words (four bytes)  
        g giant words (eight bytes)  
&gt;disassem [addr] (display memory as machine instructions from addr)  
&gt;disassem /s function_name (display machine code as well as source code for this function)  

Automatic display&gt;display [/f] expr (show value of expr each time program stops)  
&gt;undisplay n  
&gt;disable disp n  
&gt;enable disp n  
&gt;info display  

Source files&gt;dir names (add directory names to front of source path as prefix)  
&gt;dir (clear source path)  
&gt;show dir  
&gt;list (show next ten lines of source)  
&gt;list - (show previous ten lines)  
&gt;list line  
&gt;list func  

debug multiple processeswhen parent folks the child, you can use this method to debug the child, switch to child process, by default, gdb does not follow!NOTE:

If you want gdb follow, make sure the followed process can debug and has symbol.
DO NOT follow if run execxx() after fork() as you have nothing to know the command to run, does it have debug symbol or notgdb&gt;show follow-fork-mode
gdb&gt;set follow-fork-mode child  
gdb&gt;set follow-fork-mode parent

(make sure set breakpoint or watch before running)

but if there are more child process want to debug use sleep in that process and attach it to debug first change code add sleep  
#ps -ef | grep xx (to see the child)  
gdb&gt;attach child_process_id  
gdb&gt;b function_name  
gdb&gt;c  

multiple threadsgdb&gt;info threads  
gdb&gt;thread 2  

show mapped shared memorygdb&gt;info proc all  

show offset of a given fieldgdb&gt; p &amp;((struct _IO_proc_file *)0)-&gt;next  

call a function in gdbgdb&gt;call printf(&quot;hello&quot;)  
(output is the return value)  
  
test.c  
  
#include &lt;stdio.h&gt;  
#include &lt;stdlib.h&gt;  
  
struct rule &#123;  
    int start;  
    int end;  
&#125;;  
void f(int a) &#123;  
    printf(&quot;%d\n&quot;, a);  
&#125;  
  
void f2(struct *rule) &#123;  
    printf(&quot;%d %d\n&quot;, rule-&gt;start, rule-&gt;end);  
&#125;  
  
int main() &#123;  
  
    return 0;  
&#125;  
  
$ gcc -d -o test test.c  
$ gdb test  
&gt; b main  
&gt; r  
&gt;set $var=12  
&gt;call f($var)  
  
&gt;call f(12)  
  
&gt;set $ptr=malloc(sizeof(struct rule))  
&gt;p $ptr  
&gt;set ((struct rule*)$ptr)-&gt;start=12  
&gt;set ((struct rule*)$ptr)-&gt;end=10  
&gt; p *((struct rule*)$ptr)  
&gt;call f2($ptr)  
  
&gt;set $ptr=malloc(4)  
&gt;set *((int*)$ptr)=12  
&gt;p *((int*)$ptr)  

gdb debug c++set breakpoint on a member function  
gdb&gt;b class::function  
  
show fields/members of the given class/struct instance  
gdb&gt;ptype class_instance  
  
show attributes of a given instance  
gdb&gt;p *class_pointer  
  
show private attribute of a given class instance  
(you can show all attributes[public, protected, private] by gdb])  
gdb&gt;p class_pointer-&gt;private_attribute  
  
show global variable with namespace  
gdb&gt; p &#39;name_space:log_level&#39;  
  
disassemble c++ function  
gdb&gt;disassem &#39;c++ function signature without return type&#39;  
  
show function signature  
gdb&gt;info functions xxx  

ignore SIGPIPE in gdbSIGPIPE happens when read&#x2F;write a closed socket.  
gdb&gt;handle SIGPIPE nostop  
gdb&gt;handle SIGPIPE nostop noprint pass  

how to fix no such file or directory$cdir compiled path when building binary, get its value  
gdb&gt; info source  
  
$cwd  working path when debugging the binary, get its value by  
gdb&gt; pwd  
  
gdb&gt; list  
No such file or directory  
  
First you need to let gdb store the source path info by &#39;-g&#39;, later on put the source code at proper path.  
  
gdb saves the file path with absolute path, search it when debugging  
$gcc -g -o test /tmp/a.c  
  
gdb saves the file path with relative path, search it by adding compiled path or current working path  
$gcc -g -o test a.c  
  
  
$gdb test  
gdb&gt;list  
if test build with absolute path, if not found, search compiled path, then working path  
if test build with relative path, first check $cdir/file_name, then $cwd/file_name  
if not found &#39;No such file or directory&#39;  

Sometime you only get the binary file, first check  
$ objdump -WL test  
  
#  aa:     file format elf64-x86-64  
#  
#  Decoded dump of debug contents of section .debug_line:  
#  
#  CU: /tmp/a.c:  
#  File name                            Line number    Starting address  
#  a.c                                            5            0x400526  
#  a.c                                            6            0x40052e  
#  a.c                                            8            0x400535  
#  a.c                                            9            0x400549  
#  a.c                                           10            0x40054e  
#  
#  CU(compiled unit) shows gdb saves absolute path in debuginfo  
  
$ objdump -WL test  
# aa:     file format elf64-x86-64  
#  
# Decoded dump of debug contents of section .debug_line:  
#  
# CU: a.c:  
# File name                            Line number    Starting address  
# a.c                                            5            0x400526  
# a.c                                            6            0x40052e  
# a.c                                            8            0x400535  
# a.c                                            9            0x400549  
# a.c                                           10            0x40054e  
#  
# CU(compiled unit) shows gdb saves relative path in debuginfo  
# In this case you also need to know the compiled path  
  
$ objdump -W aa | grep DW_AT_comp_dir  
# &lt;15&gt;   DW_AT_comp_dir    : (indirect string, offset: 0x38): /tmp  
# DW_AT_comp_dir     DW_FORM_strp  
#  
# As you can see, the compiled path is /tmp  
  
  
# If you debug the binary on your compliing machine, and you don&#39;t move you source code  
# every thing is fine!!!  
#  
# BUT if you get the binary from other guys or you move the source code, it will shows  
# &#39;No such file or directory!&#39;  
  
# 1. compiled with absolute path  
$ gdb  
gdb&gt;set substitute-path $from $to  
# let&#39;s say you moved a.c from /tmp to /home/  
$ gdb test  
gdb&gt; set substitute-path /tmp /home  
  
# 2. binary file saves with relative path  
$ gdb  
gdb&gt; dir $new_path  

debug with separate debug files.GDB supports two ways of specifying the separate debug info file:  

The executable contains a debug link that specifies the name of the separate debug info file(.gnu_debuglink)  
The executable contains a build ID, a unique bit string that is also present in the corresponding debug info file(the build is most stored at .note.gnu.build-id)

# check the separate debug info file with command  
$ readelf -n /lib/x86_64-linux-gnu/libc-2.23.so | grep  BUILD_ID -C 2  
Displaying notes found at file offset 0x00000270 with length 0x00000024:  
  Owner                 Data size	Description  
  GNU                  0x00000014	NT_GNU_BUILD_ID (unique build ID bitstring)  
    Build ID: 40572882c66d064f9e4134cc94e4127798aad742  
  
# check the debug link  
$ readelf -x.gnu_debuglink /lib/x86_64-linux-gnu/libc-2.23.so  

Depending on the way the debug info file is specified, GDB uses two different methods of looking for the debug file  

For the “debug link” method, GDB looks up the named file in the directory of the executable file, then in a subdirectory of that directory named &#96;.debug’, and finally under the global debug directory, in a subdirectory whose name is identical to the leading directories of the executable’s absolute file name.  
For the “build ID” method, GDB looks in the .build-id’ subdirectory of the global debug directory for a file named &#96;nn&#x2F;nnnnnnnn.debug‘, where nn are the first 2 hex characters of the build ID bit string, and nnnnnnnn are the rest of the bit string

Example  
suppose you ask GDB to debug &#x2F;usr&#x2F;bin&#x2F;ls, which has a debug link that specifies the file ls.debug, and a build ID whose value in hex is abcdef1234. If the global debug directory is &#x2F;usr&#x2F;lib&#x2F;debug, then GDB will look for the following debug information files, in the indicated order:  
# global debug dir: /usr/lib/debug by default  
/usr/lib/debug/.build-id/ab/cdef1234.debug  
/usr/bin/ls.debug  
/usr/bin/.debug/ls.debug  
/usr/lib/debug/usr/bin/ls.debug  

Example to use build-id not from default path  
&gt;gdb  
&gt;set debug-file-directory /tmp/lib/debug  
&gt;file /usr/bin/nginx  
&gt;core-file core.nginx  
&gt;bt  

detach symbol from binary(compiled with -g)Symbol tables store key-value pairs that map names (like variables and functions) to their memory addresses, they serve basic purposes like verifying variable declarations,implementing type checking, and determining scope resolution, it’s always stored in binary file when you compile application, no need any option to enable it.
Debug information is more comprehensive than just symbol tables, it includes additional context like:

Source code filenames
Line numbers mapping code to source
Variable types and their attributes
Scope information
Class structures (methods and members)

Debug information provides richer context specifically for debugging purposes, Debug symbols extend symbol tables with additional information
# generate binary with debuginfo and symbol tables, even without -g option, symbol tables are still stored in binary file
$ gcc -o hello hello.c -g

# --------------- check debuginfo with several tools ---------------
$ objdump -h hello | grep debug
 27 .debug_aranges 00000030  0000000000000000  0000000000000000  0000303f  2**0
                  CONTENTS, READONLY, DEBUGGING, OCTETS
 28 .debug_info   000000f3  0000000000000000  0000000000000000  0000306f  2**0
                  CONTENTS, READONLY, DEBUGGING, OCTETS
 29 .debug_abbrev 000000b3  0000000000000000  0000000000000000  00003162  2**0
                  CONTENTS, READONLY, DEBUGGING, OCTETS
 30 .debug_line   00000069  0000000000000000  0000000000000000  00003215  2**0
                  CONTENTS, READONLY, DEBUGGING, OCTETS
 31 .debug_str    00000104  0000000000000000  0000000000000000  0000327e  2**0
                  CONTENTS, READONLY, DEBUGGING, OCTETS
 32 .debug_line_str 0000001e  0000000000000000  0000000000000000  00003382  2**0
                  CONTENTS, READONLY, DEBUGGING, OCTETS
$ file hello
hello: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=9211bbda2c800fb9cbad99aea8a3936cf9689491, for GNU/Linux 3.2.0, with debug_info, not stripped

$ readelf -S hello | grep .debug
  [28] .debug_aranges    PROGBITS         0000000000000000  0000303f
  [29] .debug_info       PROGBITS         0000000000000000  0000306f
  [30] .debug_abbrev     PROGBITS         0000000000000000  00003162
  [31] .debug_line       PROGBITS         0000000000000000  00003215
  [32] .debug_str        PROGBITS         0000000000000000  0000327e
  [33] .debug_line_str   PROGBITS         0000000000000000  00003382

# --------------- check debuginfo with several tools ---------------


# --------------- check symbol tables ---------------
$ readelf -s hello

Symbol table &#39;.dynsym&#39; contains 8 entries:
   Num:    Value          Size Type    Bind   Vis      Ndx Name
     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND 
     1: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND _[...]@GLIBC_2.34 (2)
     2: 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND _ITM_deregisterT[...]
     3: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND puts@GLIBC_2.2.5 (3)
     4: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND [...]@GLIBC_2.2.5 (3)
     5: 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND __gmon_start__
     6: 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND _ITM_registerTMC[...]
     7: 0000000000000000     0 FUNC    WEAK   DEFAULT  UND [...]@GLIBC_2.2.5 (3)

Symbol table &#39;.symtab&#39; contains 39 entries:
   Num:    Value          Size Type    Bind   Vis      Ndx Name
     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND 
     1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS Scrt1.o
     2: 000000000000038c    32 OBJECT  LOCAL  DEFAULT    4 __abi_tag
     3: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS crtstuff.c
     4: 00000000000010b0     0 FUNC    LOCAL  DEFAULT   16 deregister_tm_clones
     5: 00000000000010e0     0 FUNC    LOCAL  DEFAULT   16 register_tm_clones
     6: 0000000000001120     0 FUNC    LOCAL  DEFAULT   16 __do_global_dtors_aux
     7: 0000000000004014     1 OBJECT  LOCAL  DEFAULT   26 completed.0
     8: 0000000000003db8     0 OBJECT  LOCAL  DEFAULT   22 __do_global_dtor[...]
     9: 0000000000001160     0 FUNC    LOCAL  DEFAULT   16 frame_dummy
    10: 0000000000003db0     0 OBJECT  LOCAL  DEFAULT   21 __frame_dummy_in[...]
    11: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS h.c
    12: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS crtstuff.c
    13: 0000000000002130     0 OBJECT  LOCAL  DEFAULT   20 __FRAME_END__
    14: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS 
    15: 0000000000003dc0     0 OBJECT  LOCAL  DEFAULT   23 _DYNAMIC
    16: 000000000000202c     0 NOTYPE  LOCAL  DEFAULT   19 __GNU_EH_FRAME_HDR
    17: 0000000000003fb0     0 OBJECT  LOCAL  DEFAULT   24 _GLOBAL_OFFSET_TABLE_
    18: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND __libc_start_mai[...]
    19: 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND _ITM_deregisterT[...]
    20: 0000000000004000     0 NOTYPE  WEAK   DEFAULT   25 data_start
    21: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND puts@GLIBC_2.2.5
    22: 0000000000004014     0 NOTYPE  GLOBAL DEFAULT   25 _edata
    23: 0000000000001169    26 FUNC    GLOBAL DEFAULT   16 hello_function
    24: 00000000000011c8     0 FUNC    GLOBAL HIDDEN    17 _fini
    25: 0000000000000000     0 FUNC    GLOBAL DEFAULT  UND printf@GLIBC_2.2.5
    26: 0000000000004010     4 OBJECT  GLOBAL DEFAULT   25 global_var
    27: 0000000000004000     0 NOTYPE  GLOBAL DEFAULT   25 __data_start
    28: 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND __gmon_start__
    29: 0000000000004008     0 OBJECT  GLOBAL HIDDEN    25 __dso_handle
    30: 0000000000002000     4 OBJECT  GLOBAL DEFAULT   18 _IO_stdin_used
    31: 0000000000004018     0 NOTYPE  GLOBAL DEFAULT   26 _end
    32: 0000000000001080    38 FUNC    GLOBAL DEFAULT   16 _start
    33: 0000000000004014     0 NOTYPE  GLOBAL DEFAULT   26 __bss_start
    34: 0000000000001183    67 FUNC    GLOBAL DEFAULT   16 main
    35: 0000000000004018     0 OBJECT  GLOBAL HIDDEN    25 __TMC_END__
    36: 0000000000000000     0 NOTYPE  WEAK   DEFAULT  UND _ITM_registerTMC[...]
    37: 0000000000000000     0 FUNC    WEAK   DEFAULT  UND __cxa_finalize@G[...]
    38: 0000000000001000     0 FUNC    GLOBAL HIDDEN    12 _init

$ objdump --syms hello

hello:     file format elf64-x86-64

SYMBOL TABLE:
0000000000000000 l    df *ABS*	0000000000000000              Scrt1.o
000000000000038c l     O .note.ABI-tag	0000000000000020              __abi_tag
0000000000000000 l    df *ABS*	0000000000000000              crtstuff.c
00000000000010b0 l     F .text	0000000000000000              deregister_tm_clones
00000000000010e0 l     F .text	0000000000000000              register_tm_clones
0000000000001120 l     F .text	0000000000000000              __do_global_dtors_aux
0000000000004014 l     O .bss	0000000000000001              completed.0
0000000000003db8 l     O .fini_array	0000000000000000              __do_global_dtors_aux_fini_array_entry
0000000000001160 l     F .text	0000000000000000              frame_dummy
0000000000003db0 l     O .init_array	0000000000000000              __frame_dummy_init_array_entry
0000000000000000 l    df *ABS*	0000000000000000              h.c
0000000000000000 l    df *ABS*	0000000000000000              crtstuff.c
0000000000002130 l     O .eh_frame	0000000000000000              __FRAME_END__
0000000000000000 l    df *ABS*	0000000000000000              
0000000000003dc0 l     O .dynamic	0000000000000000              _DYNAMIC
000000000000202c l       .eh_frame_hdr	0000000000000000              __GNU_EH_FRAME_HDR
0000000000003fb0 l     O .got	0000000000000000              _GLOBAL_OFFSET_TABLE_
0000000000000000       F *UND*	0000000000000000              __libc_start_main@GLIBC_2.34
0000000000000000  w      *UND*	0000000000000000              _ITM_deregisterTMCloneTable
0000000000004000  w      .data	0000000000000000              data_start
0000000000000000       F *UND*	0000000000000000              puts@GLIBC_2.2.5
0000000000004014 g       .data	0000000000000000              _edata
0000000000001169 g     F .text	000000000000001a              hello_function
00000000000011c8 g     F .fini	0000000000000000              .hidden _fini
0000000000000000       F *UND*	0000000000000000              printf@GLIBC_2.2.5
0000000000004010 g     O .data	0000000000000004              global_var
0000000000004000 g       .data	0000000000000000              __data_start
0000000000000000  w      *UND*	0000000000000000              __gmon_start__
0000000000004008 g     O .data	0000000000000000              .hidden __dso_handle
0000000000002000 g     O .rodata	0000000000000004              _IO_stdin_used
0000000000004018 g       .bss	0000000000000000              _end
0000000000001080 g     F .text	0000000000000026              _start
0000000000004014 g       .bss	0000000000000000              __bss_start
0000000000001183 g     F .text	0000000000000043              main
0000000000004018 g     O .data	0000000000000000              .hidden __TMC_END__
0000000000000000  w      *UND*	0000000000000000              _ITM_registerTMCloneTable
0000000000000000  w    F *UND*	0000000000000000              __cxa_finalize@GLIBC_2.2.5
0000000000001000 g     F .init	0000000000000000              .hidden _init
# --------------- check symbol tables ---------------

# detach debuginfo and symbol tables to a separate file, foo still has them even after saved to a separate file
$ objcopy --only-keep-debug foo foo.debug  
  
# strip debuginfo from foo, foo has no debuginfo now  
# Remove debugging info only
$ strip -g foo  

# strip all(debuginfo and symbol tables)
$ strip foo  

watchpoints is deleted by gdb automaticallygdb will delete watchpoints if out of scope of variable used by watchpoints, this usually happenswhen you add a watchpoint on a local variable, to prevent gdb delete such watchpoints, add -l&#x2F;-locationto the watchpoint, but with this type of watchpoint, gdb will pick up changes that other functions make to that same address on the stack, so you can add the qualification if $_caller_is(“your_caller_func”, 0)  
gdb&gt; watch -location $local_var  
  
# better way to use  
gdb&gt; watch -location $local_var if $_caller_is(&quot;caller_function&quot;, 0)  

run gdb command automatically when launch gdb# create a file ~/.gdbinit  
# put gdb command in the file each line  
# then start gdb  
$ gdb  

pax on applicationpaxctl  is  a tool that allows PaX flags to be modified on a per-binary  basis(need kernel support!!!)    PaX is part of common  security-enhancing  kernel  patches  and  secure   distributions,   such   as  GrSecurity  and  Hardened  Gentoo,  respectively.  Your system needs to be running a properly  patched  and  configured kernel for this program to have any effect.    -P     enforce paging based non-executable pages (PAGEEXEC)    -p     do not enforce paging based non-executable pages (NOPAGEEXEC)    -E     emulate trampolines (EMUTRAMP)    -e     do not emulate trampolines (NOEMUTRAMP)    -M     enforce secure memory protections (MPROTECT)    -m     do not enforce secure memory protections (NOMPROTECT)    -R     randomize memory regions (RANDMMAP)    -r     do not randomize memory regions (NORANDMMAP)    -X     randomize   base   address   of   normal  (ET_EXEC)  executables         (RANDEXEC)    -x     do not randomize base address of  normal  (ET_EXEC)  executables         (NORANDEXEC)    -S     enforce segmentation based non-executable pages (SEGMEXEC)    -s     do   not   enforce   segmentation   based  non-executable  pages         (NOSEGMEXEC)    -v     view flags    -z     reset all flags (further flags still apply)    -c     create the PT_PAX_FLAGS program header if it does not  exist  by         converting the PT_GNU_STACK program header if it exists    -C     create  the  PT_PAX_FLAGS program header if it does not exist by         adding a new program header, if it is possible    -q     suppress error messages    -Q     report flags in short format    can&#x27;t insert breakpoint even binary compiled with &#x27;-g&#x27; option  gdb&gt;  warning: Cannot call inferior functions, Linux kernel PaX protection forbids return to non-executable pages!  Warning:  Cannot insert breakpoint 1.  Cannot access memory at address 0x4fbbcb    try disable the global setting kernel part as it say &#x27;Linux kernel PaX protection forbids&#x27;      To disable PaX policy globally      sysctl -w kernel.pax.softmode=1        To enable PaX policy globally      sysctl -w kernel.pax.softmode=0    if not working, try on this specific program      #apt-get install paxctl        #paxctl -v binary_file        /*disable security protection for this app */      #paxctl -pemrxs binary        /* if you meet problem like when runs &#x27;paxctl -pemrxs binary&#x27; */      file /xx/nginx does not have a PT_PAX_FLAGS program header, try conversion    If you see  errors like this   gdb&gt; break test_print  gdb&gt; c  Warning:  Cannot insert breakpoint 1.  Cannot access memory at address 0x4fbbc    use hbreak to have a try  gdb&gt; hbreak test_print    
use glibc source code in gdbmost of time, you application link with glibc dynamic!    check the glibc version that you application used.    # ldd xx_binary  $ ldd nginx | grep libc  	libcrypt.so.1 =&gt; /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007fed50270000)  	libcrypto.so.1.0.0 =&gt; /lib/x86_64-linux-gnu/libcrypto.so.1.0.0 (0x00007fed4eeb7000)  	libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fed4e436000)    install glibc source file and debug symbol from official repo    ```bash  # Ubuntu18# source repo  $ grep &#x27;^deb &#x27; /etc/apt/sources.list | sed &#x27;s/^deb /deb-src /g&#x27; | sudo tee /etc/apt/sources.list.d/deb-src.list  $ cat  /etc/apt/sources.list.d/deb-src.list  # debug symbol repo  $ printf &quot;deb http://ddebs.ubuntu.com %s main restricted universe multiverse\n&quot; $(lsb_release -cs)&#123;,-updates,-security,-proposed&#125; | sudo tee -a /etc/apt/sources.list.d/ddebs.list    $ apt update -y    # make sure the libc6 same version with process used!!!!  # otherwise, you can&#x27;t get from ubuntu repo, but from github or somewhere.  # git clone git://sourceware.org/git/glibc.git    # download source  $ apt source libc6    # check the debug symbol pkg  $ apt-cache search libc6-dbg  $ apt-get install -y libc6-dbg  # OR Centos7 ```    check where should I put the source file    Way1:  ...  gdb&gt;set filename-display absolute  gdb&gt;bt  #0  _IO_new_proc_open (fp=fp@entry=0x333bc001110,      command=command@entry=0x333cbfcb620 &quot;PYTHONPATH=/path/to/python /usr/bin/timeout -k 25 25 /xx/bin/cli.py --get --xb-status &quot;, mode=&lt;optimized out&gt;, mode@entry=0x536e720e4a &quot;re&quot;) at /build/glibc-Cl5G7W/glibc-2.23/libio/iopopen.c:213    ...    so you can see you should put source file at /build/glibc-Cl5G7W/glibc-2.23 !!    Way2:  gdb&gt; set filename-display relative  gdb&gt; bt  gdb&gt; dir $root_of_glibc    
gdb with lots of question marks even with symbol table loaded In some case, even with debug binary(symbol table), gdb still show lots of question marks, like this  $ gdb nginx core.nginx.1615779511.31694.134.11    Reading symbols from nginx...done.-------&gt;with symbol    [New LWP 31694]    Core was generated by nginx: worker  process                                                        .  Program terminated with signal SIGSEGV, Segmentation fault.    #0  0x0000184bd5ac9ecd in ?? ()    (gdb) bt    #0  0x0000184bd5ac9ecd in ?? ()    #1  0x0000000000000001 in ?? ()    #2  0x757f796f19023d00 in ?? ()    #3  0x0000184bd7031100 in ?? ()    g#4  0x0000000000000000 in ?? ()      This is because of memory protection enabled when complie binary, so that address can&#x27;t map to the correct value in binary, hence ? shows.      diable memory protection by `paxctl`    $ paxctl -mc binary    $ gdb nginx core.nginx.1615779511.31694.134.11    Reading symbols from nginx...done.      warning: exec file is newer than core file.    [New LWP 31694]    [Thread debugging using libthread_db enabled]    Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;.    Core was generated by nginx: worker   process Program terminated with signal SIGSEGV, Segmentation fault.    #0  0x0000184bd5ac9ecd in ngx_http_run_posted_requests (c=0x7371edd76d10) at edge/nginx/src/http/ngx_http_request.c:2296  2296	        pr = r-&gt;main-&gt;posted_requests;  (gdb) bt  #0  0x0000184bd5ac9ecd in ngx_http_run_posted_requests (c=0x7371edd76d10) at edge/nginx/src/http/ngx_http_request.c:2296  #1  0x0000184bd5b2e31f in ngx_epoll_process_events (cycle=0x184bd7031100, timer=&lt;optimized out&gt;, flags=&lt;optimized out&gt;) at edge/nginx/src/event/modules/ngx_epoll_module.c:937  #2  0x0000184bd5b3022e in ngx_process_events_and_timers (cycle=cycle@entry=0x184bd7031100) at edge/nginx/src/event/ngx_event.c:242  #3  0x0000184bd5b42055 in ngx_worker_process_cycle (cycle=cycle@entry=0x184bd7031100, data=data@entry=0x1) at edge/nginx/src/os/unix/ngx_process_cycle.c:989  #4  0x0000184bd5b40815 in ngx_spawn_process (cycle=cycle@entry=0x184bd7031100, proc=proc@entry=0x184bd5b41f60 &lt;ngx_worker_process_cycle&gt;, data=data@entry=0x1,       name=name@entry=0x184bd628fbd3 &quot;worker process&quot;, respawn=respawn@entry=-4) at edge/nginx/src/os/unix/ngx_process.c:213  #5  0x0000184bd5b41581 in ngx_start_worker_processes (cycle=cycle@entry=0x184bd7031100, n=8, type=type@entry=-4) at edge/nginx/src/os/unix/ngx_process_cycle.c:425  #6  0x0000184bd5b4387a in ngx_master_process_cycle (cycle=0x184bd7031100) at edge/nginx/src/os/unix/ngx_process_cycle.c:275  #7  0x0000184bd59ecf74 in main (argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;) at edge/nginx/src/core/nginx.c:390  ]]></content>
      <categories>
        <category>linux</category>
        <category>coredump</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>coredump</tag>
      </tags>
  </entry>
  <entry>
    <title>git-command</title>
    <url>/2019/12/03/git-command/</url>
    <content><![CDATA[GitAfter clone a repo from git server, git actually setups three areas for you, one is working directory(you alway see), staging(index) area(cache) and repository, git commands mainly work on these three areas, staging area is an intermediate area where commits can be formatted and reviewed before completing the commit.
One thing that sets Git apart from other tools is that it’s possible to quickly stage some of your files and commit them without committing all of the other modified files in your working directory or having to list them on the command line during the commit.



# you edit a.c b.c d.c while you only staged a.c b.c$ git add a.c b.c# only commit a.c b.c to repository$ git commit -m &quot;commit staged a.c b.c&quot;

Add a ‘-a’ to your commit command in order to add all changed files to the staging area.

#  commit add modified a.c b.c d.c$ git commit -am &quot;commit a.c b.c d.c&quot;

File Status
Files in the working directory can be in the following states as shown by git status:

“Untracked”: the file in the working directory is not in the index. It must be added to the index before it can be committed to the repository.

“Changed but not updated”: The file in the working directory is not the same version as in the index. git add is used to add the working directory version to the index. If a commit is done before git add, the version in the index will be put in the repository. The version in the working directory be untouched and will be different than the version in the index and the repository.

“Changes to be committed”: The file in the working directory is the same version as in the index. The file is ready for a commit.

not listed: if a file is not shown in git status then the file in the working directory is the same version as in the index and repository.

“new file”: git add has been done (the file is in the index). The file has not been committed to the repository. It can be removed from the index or could be committed to the repository.

“modified”: the version in the working directory is different than the version in the index. It can be removed from the index or could be committed to the repository.

“deleted”: git rm has removed the file from the working directory and index but has not removed it from the repository. git reset HEAD file_name can be used to restore the file from the repository to the index and working directory. git commit will make the removal permanent.


configIn order to use git, it’s better to config it, here is an example of the config file located at ~/.gitconfig, edit it directly or use cli to do the config
$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;your_email@whatever.com&quot;$ git config --global core.autocrlf input$ git config --global core.safecrlf true

edit file directly
[push]        default = simple[user]        name = Jason        email = xx@yy.com[core]        #提交时转换为LF，检出时转换为CRLF        #git config --global core.autocrlf true        #提交时转换为LF，检出时不转换        #git config --global core.autocrlf input        #提交检出均不转换        #git config --global core.autocrlf false        #拒绝提交包含混合换行符的文件        #git config --global core.safecrlf true        #允许提交包含混合换行符的文件        #git config --global core.safecrlf false        #提交包含混合换行符的文件时给出警告        #git config --global core.safecrlf warn        autocrlf = input        safecrlf = warn        editor = vim[pack]        windowMemory = 1g[http]        postbuffer = 524288000#        sslverify = true#        proxy = http://proxy.company.com:3128#[sendemail]#       smtpencryption = ssl#       smtpserver = stbeehive.xxx.com#       smtpuser = xx@yy.com#       smtpserverpot = 465#       suppresscc = all#       confirm = always#you can suppress particular type emails to auto cc list by#signedoffbycc = no#suppressfrom = yes#cocover = no#cccover = no#another alternate way, simple#suppresscc=all

# check git config$ git config --list

commandsgit review# this will check the .gitreview and push diff to that server$ git review$ cat .gitreview [gerrit]host=git.xxy.comproject=fw_ver/cdbport=12023track=trueusepushurl=true# OR use below if review server and repo uses are same(ugly way)# git push origin HEAD:refs/for/&lt;branch_name&gt;
git branch# show local branches$ git branch# switch to branch$ git checkout $branch# create a branch based on current | tag | based branch$ git branch $branch [tag|base branch]# create then switch to new branch$ git checkout -b $branch [tag | branchname]# create a master(same name as remote branch) branch and set it track$ git checkout --track origin/master$ git branch -d $branch$ git branch -D $branch# as you know git pull pulls changes from remote server to remote branch# origin/$branch, lets say if you create a branch from origin/master called master, if master does not track origin/master, so after git pull origin/master is updated but master is not, better set track for master branch# show local and its remote tracking branch if has$ git branch -vv# set remote tracking branch(origin/master) for local master branch$ git branch --set-upstream-to=origin/master master# to see which local branch contains a particular commit$ git branch --contains da6f215e4a6f379d094acd31c71c7696dae50afc# to see which remote branch contains a particular commit.$ git branch -r --contains da6f215e4a6f379d094acd31c71c7696dae50afc# show remote branches$ git branch -r# push or create a new branch on remote server$ git push origin $branch# delete a branch from remote server$ git push origin :$branch# list branches which contains a given branch, also meaning the given branch merged into them$ git branch --contains $branch -r# list branches which are merged into a given branch, also meaning the given branch contains them$ git branch --merged master -a# list branches which are not merged into a given branch, also meaning the given branch does not contain them$ git branch --no-merged master -a

diff&#x2F;add&#x2F;rm&#x2F;commit&#x2F;reset# compare working dir with staging area and staging area with repo then show status for all files$ git status .$ git status# show modification of head commit or a given commit$ git show$ git show $commit# show file content of a given commit$ git show $commit:path/to/file# show diff between two commits$ git diff --name-only $commitidA..$commitidB# show diff between working dir and staging area$ git diff $file$ git diff .$ git diff# show diff between staging area and repo$ git diff --cached$ git diff --cached $file# add changes to staging area$ git add .$ git add $file$ git add $dir# commit changes$ git commit -m &quot;only commit changes in staging area&quot;$ git commit -a -m &quot;no need git add, but only commit all tracked files&quot;# edit a submitted commit without creating a new commit$ git add $update$ git commit --amend# reset staging area, modification in working dir remains$ git reset --soft HEAD^ # reset to previous commit and keep modification$ git reset --soft $commit# reset staging area and working dir, repo both three part# commit and changes before this commit are lost!!!$ git reset --hard $commit# discard changes in working dir$ git checkout [$commit] -- $file$ git checkout [$commit] -f $file$ git checkout [$commit] -- $dir# delete files from working dir$ git rm *.c$ git rm -r $dir# delete files from index(staging area)$ git rm --cached *.c

pull&#x2F;push&#x2F;merge&#x2F;cherry-pick&#x2F;rebase# fetch latest code from remote server to remote branch (git branch -r)# update local branch(git branch) if they track remote branch.$ git pull# fetch latest code from remote server to remote branch (git branch -r)# but not update local branch even they track remote branch$ git fetch# push commit changes on current branch to remote server(origin/xxx branch tracked)$ git push [--force]# push commit changes on another branch to remote server$ git push origin $branch [--force]# rebase can do: delete commit, edit commit, merge commit etc# rebase the top 3 commits, but they may conflict when rebasing$ git rebase -i HEAD~3$ git rebase -i $commit # rebase from it to current# if conflicts happen when rebasing do below or abort rebasing$ git status . # check conflict files, the solve conflicts.$ git add $fixed_conflict_file$ git rebase --continue# OR$ git rebase --abort# cherry-pick pick a commit or commits to local branch# -x   append a line that says &quot;(cherry picked from commit …​)&quot; to the original commit message# -s   --sign-off# -e   --edit$ git cherry-pick -x -s -e $start^..$end [start-end]$ git cherry-pick -x -s -e $start..$end (start-end]$ git cherry-pick -x -s -e $commit$ git cherry-pick -x -s -e $commit $commit # in order# if conflicts happen when cherry-picking do below or abort$ git status . # check conflict files$ git add $fix_confict_files$ git cherry-pick --continue# OR$ git cherry-pick --abort# revert a commit, will create a new commit for this revert action$ git revert -i $commit$ git revert HEAD$ git revert HEAD^ (revert two commits)# merge dev to current branch, commit one by one at last create a merge commit$ git merge dev

git pull vs git pull --rebasewhen updating local branch(if it tracks remote branch), git pull will pull all new changes from remote branch then create a merge commit(contains all changes) on local branchwhile git pull --rebase will cherry-pick new changes from remote branch to local branchExample:(D--E master is local branch)Suppose you have two commits in local branch:      D---E master     /A---B---C---F origin/masterAfter &quot;git pull&quot;, will be:      D--------E     /          \A---B---C---F----G   masterAfter &quot;git pull --rebase&quot;, there will be no merge point G. Note that (D and E become different commits!)A---B---C---F---D&#x27;---E&#x27;   master

git logcommit date vs author dateIn Git, the author date is when someone first creates a commit with git commit.
# show logs$ git log process.c                            # logs of one file$ git log --oneline .                          # oneline logs of the current dir$ git log  -10 --reverse                       # top 10 logs of repo in reverse order$ git log  $old_commit..$new_commit            # logs(commit) between two commits (old, new]$ git log --oneline $old_commit~..$new_commit            # logs(commit) between two commits [old, new]# show logs of a particular author at a period of time$ git log  --author=jason --since=&quot;2016-02-01&quot; # --before=&quot;2016-02-11&quot;$ git log -p $commit                           # show changes of given commit####################### change author of commit #################################### change current commit author from command line$ git commit --amend --author=&quot;Author Name &lt;email@address.com&gt;&quot; --no-edit# change Committer and Author for current commit$ GIT_COMMITTER_NAME=&quot;Jason&quot; GIT_COMMITTER_EMAIL=&quot;jaluo@163.com&quot; git commit --amend --author=&quot;Jason &lt;jaluo@163.com&gt;&quot; --no-edit# without edit commit msg but update author based on currect ~/.gitconfig$ git config user.name &quot;New User&quot;$ git config user.email &quot;newuser@gmail.com&quot;$ git commit --amend --reset-author --no-edit####################### change date of commit ####################################for each commit, there are two dates, author date(when create the patch) and commit date$ git log                                      # only show author date, but the log is sorted by commit date)$ git log --pretty=fuller                      # show both dates(author date and commit date)# overwrite author date when edit a commit by --date option# NOTE: this will reset commit date as well, equal with author date# &#x27;+0800&#x27; is the timezone$ git commit --amend --date=now$ git commit --amend --date=&quot;Wed Jun 22 22:28:27 2022 +0800&quot;# without edit commit msg  but update author date(also commit date) to now$ git commit --amend --date=now --no-edit# without edit commit msg  but only update commit date to now$ git commit --amend --no-edit# show author date and commit data$ git log --pretty=fuller .# change commit date and author date to specific time$ GIT_COMMITTER_DATE=&quot;Wed Jun 22 22:28:27 2022 +0800&quot; git commit --amend --no-edit --date=&quot;Wed Jun 22 22:28:27 2022 +0800&quot;

git patch# create two patches HEAD~2 and print them out$ git format-patch -2 --stdout# create 2 patch files$ git format-patch -2 -o out # save to out dir# create one patch file with several commits# commits with [cc1dde0dd, 6de6d4b06]$ git format-patch cc1dde0dd^..6de6d4b06 --stdout &gt; foo.patch# each patch in this file is applied separately!!!$ git am foo.patch# apply a git patch$ git apply [-p1] xx.patch# apply with commit msg$ git am xx.patch# revert a applied patch$ git apply --reverse xx.patch# The --reject option will instruct git to not fail if it cannot determine how to apply a patch,# but instead to apply individual hunks it can apply and create reject files (.rej) for hunks it cannot applygit apply --reject --whitespace=fix xx.patch

git tag# show all tags$ git tag -l# create a tag on HEAD or given commit$ git tag $tname$ git tag $tname $commit# update a tag with new commit$ git tag -f $tname $new_commit# delete a tag$ git tag -d $tname# push all local tags to remote server$ git push --tags$ git push -f --tags# delete a tag from remote server$ git push origin :refs/tags/$tname# create tag with commit$ git tag -a -m &quot;comments&quot; $tname [$commitid]# update a tag$ git tag -f -a -m &quot;comments&quot; $tname [$commitid]# show the commit that a tag points to$ git rev-list -1 $tname# show all tags which contain a specific commit$ git tag --contains 95d0a8e

merge commit from another repo# nginx as another repo, want to merge one commint from nginx repo$ git remote add nginx https://github.com/nginx/nginx.git$ git remote shownginxorigin# show nginx remote branch$ git remote show nginx* remote nginx  Fetch URL: https://github.com/nginx/nginx.git  Push  URL: https://github.com/nginx/nginx.git  HEAD branch: master  Remote branches:    branches/default         new (next fetch will store in remotes/nginx)    branches/radix_with_skip new (next fetch will store in remotes/nginx)    branches/stable-0.5      new (next fetch will store in remotes/nginx)    branches/stable-0.6      new (next fetch will store in remotes/nginx)    branches/stable-0.7      new (next fetch will store in remotes/nginx)    branches/stable-0.8      new (next fetch will store in remotes/nginx)    branches/stable-1.0      new (next fetch will store in remotes/nginx)    branches/stable-1.10     new (next fetch will store in remotes/nginx)    branches/stable-1.12     new (next fetch will store in remotes/nginx)    branches/stable-1.14     new (next fetch will store in remotes/nginx)    branches/stable-1.16     new (next fetch will store in remotes/nginx)    branches/stable-1.18     new (next fetch will store in remotes/nginx)    branches/stable-1.2      new (next fetch will store in remotes/nginx)    branches/stable-1.20     new (next fetch will store in remotes/nginx)    branches/stable-1.4      new (next fetch will store in remotes/nginx)    branches/stable-1.6      new (next fetch will store in remotes/nginx)    branches/stable-1.8      new (next fetch will store in remotes/nginx)    master                   new (next fetch will store in remotes/nginx)  Local ref configured for &#x27;git push&#x27;:    master pushes to master (local out of date)######################### Fetch ############################ fetch one branch to local than we can merge from it$ git fetch nginx branches/stable-1.18$ git branch -rnginx/branches/stable-1.18# Or fetch all branches to local$ git remote update######################### Merge ############################ Then cherry pick, or merge, delte remote is optional... cherry pick, merge .....$ git merge  nginx/branches/stable-1.18 # merge to current branch$ git remote remove nginx# OR you can create a local copy or just use  nginx/branches/stable-1.18$ git checkout  nginx/branches/stable-1.18 -b stable-1.18... cherry pick, merge .....$ git branch -D stable-1.18$ git remote remove nginx

git filter-repoRewrite git history tool
$ pip3 install git-filter-repo# examples$ 

FAQcherry-pick part of a commit$ git cherry-pick -x -s -e -n $commit  # -n is the key$ git checkout HEAD -- $not_wanted_files$ git add $wanted_file$ git cherry-pick --continue
clean untracked files and untracked dirs# Cleans the working tree by recursively removing files that are not under version control, starting from the current directory.# Normally, only files unknown to Git are removed, but if the -x option is specified, ignored files are also removed. This can, for example, be useful to remove all build products.# -d#   Remove untracked directories in addition to untracked files. If an untracked directory is managed by a different Git repository, it is not removed by default# -x#   Don’t use the standard ignore rules read from .gitignore (per directory) and $GIT_DIR/info/exclude, but do still use the ignore rules given with -e#   options. This allows removing all untracked files, including build products. This can be used (possibly in conjunction with git reset) to create a pristine#   working directory to test a clean build.# -f, --force#   If the Git configuration variable clean.requireForce is not set to false, git clean will refuse to run unless given -f or -n.## -n, --dry-run#   Don’t actually remove anything, just show what would be done.# -X#   Remove only files ignored by Git. This may be useful to rebuild everything from scratch, but keep manually created files.# NOTE: -x will delete manually created files!!!$ git clean -xdf
see commit for each line of a file$ git annotate $file
check if local branch tracked or not$ git branch -vv# set track$ git checkout master$ git branch --set-upstream-to=origin/master
hide local change, not add to staging area# you have to switch to another branch but you don&#x27;t want to commit your local change and you want to keep the changes$ git stash                   # hide the local change$ git status                  # will show everything is up to date$ git stash list              # show the hidden change$ git stash show -p stask@[2] # show the changed on that stash$ git stash apply stash@[2]   # restore that stash$ git stash pop               # restore the latest stashed local changes$ git stash clear             # drop all stashes, drop save hidden changes

move one repo from one server to another# copy from org$ git clone --bare $org_uri/project.git# create a new repo at new server$ cd project.git$ git push --mirror $new_uri

binary searchfind which commit introduces a bug
# good as the low bad as the high commit()# For example. one good commit you know is goodcmt# one bad commit you know is badcmt$ git bisect reset$ git bisect start$ git bisect good $goodcmt$ git bisect bad $badcmt#after this, the middle commit is checked out, test it# if the middle commit works$ git bisect good# Or not work$ git bisect bad

find which commit fixes the bug
# For example. one good commit you know is goodcmt# one bad commit you know is badcmt$ git bisect reset$ git bisect start$ git bisect good $badcmt$ git bisect bad $goodcmt#after this, the middle commit is checked out, test it# if the middle commit works$ git bisect bad# Or not work$ git bisect good

see history for git commit –amend$ git reflog# reset the latest git commit --amend and keep the change$ git reset --soft @&#123;1&#125;$ git reset HEAD
diff formatdiff --git a/src/core/ngx_conf_file.c b/src/core/ngx_conf_file.cindex 98e9df6..359178c 100644--- a/src/core/ngx_conf_file.c+++ b/src/core/ngx_conf_file.c@@ -1418,6 +1418,7 @@ ngx_conf_set_bitmask_slot(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)     np = (ngx_uint_t *) (p + cmd-&gt;offset);     value = cf-&gt;args-&gt;elts;+    /* supported ignore haders */     mask = cmd-&gt;post;     for (i = 1; i &lt; cf-&gt;args-&gt;nelts; i++) &#123;@@ -1429,11 +1430,13 @@ ngx_conf_set_bitmask_slot(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)                 continue;             &#125;+            /* get supported ignore header */             if (*np &amp; mask[m].mask) &#123;                 ngx_conf_log_error(NGX_LOG_WARN, cf, 0,                                    &quot;duplicate value \&quot;%s\&quot;&quot;, value[i].data);             &#125; else &#123;+                /* set corresponding bit with 1, means ignore such header */                 *np |= mask[m].mask;             &#125;
Hunk 1
-1418,6    -:    old file    1418: old file line    6:    continuous lines+1418,7    +:    new file    1418: new file line    7:    continuous lines6---&gt;7 one line addedtotal means:old file from 1418(total 6 lines) changed as below(diff part)from line 1418(total 7 lines) in new file

Hunk 2
-1429,11    -:    old file    1429: old file line    11:   continuous lines+1430,13    +:    new file    1430: new file line    13:   continuous lines11--&gt;13 means two lines addedtotal means:old file from 1429(total 11 lines) changed as below(diff part)from line 1430(total 13 lines) in new file# old line number counting no Hunk 1 applied# new line counting        with Hunk 1 appliedso you can see new line from 1430, as Hunk1 added 1 line!!!
how many commits# how many commits from HEAD to the beginning$ git rev-list HEAD --count$ git rev-list $commit --count# find the nth commit from beginning$git rev-list HEAD --counttotal$git log --skip=$(total-nth) --max-count=1# find the 3th commit from beginning$git rev-list HEAD --count10# 7 = 10-3$git log --skip=7 --max-count=1

show commits of all users# with log title of each commit$ git shortlogÁlvaro Fernández Rojas (1):      net: dsa: tag_brcm: add support for legacy tags#  without log title for each commit and no order$ git shortlog -s...1 Álvaro Fernández Rojas# order by author(commit count from high to low) for current dir$ git shortlog -n -s .66  Wang Xiao52  lishaipeng 3  YaZenzeng ...# order by author(commit count from high to low) for whole repo$ git shortlog -ns --all --no-merges# count commits submitted by given author$ git shortlog -n -s --author=jason

see latest commit of all branches# order latest commit of each branch by Date$ git branch -r | grep -v HEAD | while read b; do git log -n 1 --color --format=&quot;%ci _%C(magenta)%cr %C(bold cyan)$b%Creset %s %C(bold blue)&lt;%an&gt;%Creset&quot; $b ;done | sort -r# get top 20$ git branch -r | grep -v HEAD | while read b; do git log -n 1 --color --format=&quot;%ci _%C(magenta)%cr %C(bold cyan)$b%Creset %s %C(bold blue)&lt;%an&gt;%Creset&quot; $b ;done | sort -r | head -20

submodulesubmodule cheat-sheet
Ref
Using git

]]></content>
      <categories>
        <category>git</category>
        <category>command</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>hardware-gpu</title>
    <url>/2021/08/05/hardware-gpu/</url>
    <content><![CDATA[IntroductionThe graphics processing unit, or GPU was originally designed to accelerate the rendering of 3D graphics. you can use graphics SDK(DirectX, OpenGL) to use GPU for rendering, but it has become one of the most important types of computing technology now, it is designed for parallel processing, the GPU is used in a wide range of applications, including:

Graphics and video rendering.
Artificial intelligence (AI)
Machining learning
Deep learning
More



GPUGPUs come in two basic types of a computer: integrated and discrete. An integrated GPU does not come on its own separate card at all and is instead embedded alongside the CPU. A discrete GPU is a distinct chip that is mounted on its own circuit board and is typically attached to a PCI Express slot.
Integrated Graphics Processing Unit
The majority of GPUs on the market are actually integrated graphics. So, what are integrated graphics and how does it work in your computer? A CPU that comes with a fully integrated GPU on its motherboard allows for thinner and lighter systems, reduced power consumption, and lower system costs.
Intel® Graphics Technology, which includes Intel® Iris® Plus and Intel® Iris® Xe graphics, is at the forefront of integrated graphics technology. 
Discrete Graphics Processing Unit
Many computing applications can run well with integrated GPUs. However, for more resource-intensive applications with extensive performance demands, a discrete GPU (sometimes called a dedicated graphics card) is better suited to the job.
GPU vs Graphics CardWhile the terms GPU and graphics card (or video card) are often used interchangeably, there is a subtle distinction between these terms. Much like a motherboard contains a CPU, a graphics card refers to an add-in board that incorporates the GPU. This board also includes the raft of components required to both allow the GPU to function and connect to the rest of the system.
GPU vs CPU


CPU
GPU



Central Processing Unit
Graphics Processing Unit


Several cores
Many cores


Low latency
High throughput


Good for serial processing
Good for parallel processing


Can do a handful of operations at once
Can do thousands of operations at once



Architecturally, the CPU is composed of just a few cores with lots of cache memory that can handle a few software threads at a time. In contrast, a GPU is composed of hundreds of cores that can handle thousands of threads simultaneously.
GPUThe GPU is a processor that is made up of many smaller and more specialized cores(special, worker, student). By working together, the cores deliver massive performance when a processing task can be divided up and processed across many cores.  
CPUCPU is commonly referred to as the brain of the computer(generic, smart, professor). It is essential to all modern computing systems as it executes the commands and processes needed for your computer and operating system.
DifferenceThe CPU is suited to a wide variety of workloads, especially those for which latency or per-core performance are important. A powerful execution engine, the CPU focuses its smaller number of cores on individual tasks and on getting things done quickly. This makes it uniquely well equipped for jobs ranging from serial computing to running databases.
GPUs began as specialized ASICs developed to accelerate specific 3D rendering tasks. Over time, these fixed-function engines became more programmable and more flexible. While graphics and the increasingly lifelike visuals of today’s top games remain their principal function, GPUs have evolved to become more general-purpose parallel processors as well, handling a growing range of applications.
if you run a small task, CPU is faster than GPU, but if you run a complex task, GPU is faster as GPUs break complex problems into thousands or millions of separate tasks and work them out at once
CPU vs GPU intel
MarketIntel has the big market due to its Integrated Graphics Processing Unit, while for Dedicated GPU, Nvidia and AMD are the two main players.

Program GPU based applicationEarlier years, There are only two SDK OpenGL(Apple lead this(open source): linux, mac, windows) and DirectX(Microsoft) to program GPD based application, applications can use them to render 2D and 3D computer graphics which is the main task of GPU, but today, except graphic rending, GPU is also designed for parallel processing, so the SDK should add proper APIS to support this, hence openGL ---&gt; OpenCL, DirectX with new version supports this, beside OpenCL and new version DirectX which support different vendors, Nvidia provides its own SDK Cuda which is more powerful and easy to use if you are using Nvidia GPU which has Cuda Arch.
OpenGL vs DirectX









Basis
OpenGL
DirectX


Definition
It is an open-source cross-platform application of interface programming which works on rendering of vector graphics of 2d and 3d graphics.
DirectX is a collection APIs which re using for many different types of multimedia platform as well as game programming.


Developer
It was developed by the Khronos group but originally Silicon Graphics Inc. started its development in 1991 and released on June 30, 1992, for public use.
Microsoft was its developer and launched it on September 30, 1995.


Operating systems
OpenGL can be installed on Linux, Mac OS, and Microsoft Windows operating systems.
DirectX can run on Microsoft Windows, Dreamcast, Xbox, Xbox One, Xbox 360, Xbox series, and Series S because it is compatible with these operating systems.


Latest version
The latest version of OpenGL was released 3 years ago means on July 31, 2017, and named 4.6.
12 Ultimate API was released on November 10, 2020 as its latest collection.


Availability
For working with OpenGL you can visit on www.opengl.org and have it on your personal computer.
Visit www.microsoft.com for DirectX and you can get any of the members of its collection to start working with that.


Supported file format
It supports .obj, .3ds as 3d model file formats for working on them.
You can have 3d model of this software in c4d, max, obj, fbx, ma, 3ds, blend, 3dm, and many more file formats.


Written in Language
OpenGL is written in C or C++ computer language.
As C and C++ are game designer’s pet languages so that is way DirectX is also written in C and C++ computer languages.


OpenCL vs Cuda

Note: Most GPUs are designed for a specific usage, real-time 3D graphics or other mass calculations:  

Gaming
GeForce GTX, RTX
Nvidia Titan
Radeon HD, R5, R7, R9, RX, Vega and Navi series
Radeon VII


Cloud Gaming
Nvidia GRID
Radeon Sky


Workstation
Nvidia Quadro
AMD FirePro
AMD Radeon Pro


Cloud Workstation
Nvidia Tesla
AMD FireStream


Artificial Intelligence training and Cloud
Nvidia Tesla
AMD Radeon Instinct


Automated&#x2F;Driverless car
Nvidia Drive PX



Nvidia Family: mobile GPU (Tegra), discrete GPU for a laptop (GeForce GT), desktop (GeForce GTX) and server (Quadro and Tesla)
Suggestion

GPU from Nvidia, Use Cuda
NOT from Nvidia, Use OpenCL, or DirectX for Windows

NvidiaTo be more precise, Cuda is not a language or an API. Cuda is a platform for parallel computing and at the same time, it’s a programming model to utilize GPU to speed up general purpose computing. The developer still can write software at C or C++, and incorporate some extensions in the form of a few basic keywords.
Cuda C program and Cuda simple example
]]></content>
      <categories>
        <category>hardware</category>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>cuda</tag>
        <tag>opengl</tag>
      </tags>
  </entry>
  <entry>
    <title>go-advanced</title>
    <url>/2022/06/30/go-advanced/</url>
    <content><![CDATA[Advancedstructtagspackage mainimport (    &quot;fmt&quot;    &quot;reflect&quot;)type Person struct &#123;    Name string `k1:&quot;v1&quot; k2:&quot;v2&quot;`&#125;func main() &#123;    var p Person    tp := reflect.TypeOf(p)    for i := 0; i &lt; tp.NumField(); i++ &#123;        f := tp.Field(i)        fmt.Println(f.Tag.Get(&quot;k1&quot;))        fmt.Println(f.Tag.Get(&quot;k2&quot;))    &#125;&#125;main()v1v2

Rules for writing tag



whitespace, double quote &quot;, colon : are special in tags.

Tag keys must not contain space (Unicode value 32), quote &quot;(Unicode value 34) and : colon (Unicode value 58) characters.

To form a valid key-value pair, no space characters are allowed to follow the colon in the supposed key-value pair. So

optional: &quot;yes&quot; doesn’t form key-value pairs.


different key-value pairs are separated by whitespace 

space characters in tag values are important. So

json:&quot;author, omitempty&quot;,json:&quot; author,omitempty&quot; andjson:&quot;author,omitempty&quot; are different from each other.


each struct field tag should present as a single line to be wholly meaningful.


Exported structIf a struct type starts with a capital letter, then it is an exported type and it can be accessed from other packages. Similar the fields of a struct start with caps, they can be accessed from other packages.
Structs EqualityStructs are value types and are comparable if each of their fields are comparable. Two struct variables are considered equal if their corresponding fields(field with same name) are equal.
anonymous fieldsIt is possible to create structs with fields that contain only a type without the field name. These kinds of fields are called anonymous fields. Even though anonymous fields do not have an explicit name, by default the name of an anonymous field is the name of its type.
type Person struct &#123;    string    int&#125;// same as type Person struct &#123;    string string    int int&#125;
Person struct has 2 fields with name string and int!!!
package mainimport (    &quot;fmt&quot;)type Person struct &#123;    string    int&#125;func main() &#123;    p1 := Person&#123;        string: &quot;naveen&quot;,        int:    50,    &#125;    fmt.Println(p1.string)    fmt.Println(p1.int)&#125;

Promoted fieldsFields that belong to an anonymous field which is also a struct are called promoted fields since they can be accessed as if they belong to the struct which holds the anonymous struct. promoted fields can be functions as well!!!
package mainimport (    &quot;fmt&quot;)type Address struct &#123;    city  string    state string&#125;func (a *Address) City() string &#123;    return a.city&#125;type Person struct &#123;    name    string    age     int    Address //anonymous struct field&#125;type Personx struct &#123;    name     string    age      int    *Address //anonymous struct field *Address is new type.&#125;func main() &#123;    p := Person&#123;        name: &quot;Naveen&quot;,        age:  50,        Address: Address&#123;            city:  &quot;Chicago&quot;,            state: &quot;Illinois&quot;,        &#125;,    &#125;    px := Personx&#123;        name: &quot;Naveen&quot;,        age:  50,        Address: &amp;Address&#123; // different ways for assigning            city:  &quot;Chicago&quot;,            state: &quot;Illinois&quot;,        &#125;,    &#125;    fmt.Println(&quot;Name:&quot;, p.name)    fmt.Println(&quot;Age:&quot;, p.age)    fmt.Println(&quot;City:&quot;, p.Address.city)  // can access this way as well    fmt.Println(&quot;City:&quot;, p.Address.state) // can access this way as well    fmt.Println(&quot;City:&quot;, p.city)          //city is promoted field    fmt.Println(&quot;City:&quot;, p.City())        //City() function is promoted field    fmt.Println(&quot;State:&quot;, p.state)        //state is promoted field    // same way as above for accessing pointer    fmt.Println(&quot;Name:&quot;, px.name)    fmt.Println(&quot;Age:&quot;, px.age)    fmt.Println(&quot;City:&quot;, px.Address.city)  // can access this way as well    fmt.Println(&quot;City:&quot;, px.Address.state) // can access this way as well    fmt.Println(&quot;City:&quot;, px.city)          //city is promoted field    fmt.Println(&quot;City:&quot;, px.City())        //City() is promoted field    fmt.Println(&quot;State:&quot;, px.state)        //state is promoted field&#125;
classThere is no class in Go, but you can bind functions with struct, hence it behaves like a class.func (p Person) speak() string &#123;&#125; The receiver appears in its own argument list between the func keyword and the method name.
You can only declare a method with a receiver whose type is defined in the same package as the method. You cannot declare a method with a receiver whose type is defined in another package

Value receiver makes a copy of the type and pass it to the function. The function stack now holds an equal object but at a different location on memory. That means any changes done on the passed object will remain local to the method. The original object will remain unchanged.

Pointer receiver passes the address of a type to the function. The function stack has a reference to the original object. So any modifications on the passed object will modify the original object.


If you want to change the state of the receiver in a method, manipulating the value of it, use a pointer receiver. It’s not possible with a value receiver, which copies by value. Any modification to a value receiver is local to that copy. If you don’t need to manipulate the receiver value, use a value receiver(pointer receiver can be used as well).
The Pointer receiver avoids copying the value on each method call. This can be more efficient if the receiver is a large struct.
Value receivers are concurrency safe, while pointer receivers are not concurrency safe. Hence a programmer needs to take care of it.
RULES for receivers

Try to use same receiver type for all your methods as much as possible, not use both.
If state modification needed, use pointer receiver if not use value receiver(but can use pointer receive as well, good case for large struct).

Specific class(not interface) method supports

call pointer receiver on non-pointer object(which is converted to pointer automatically)
call value receiver on pointer object(which is converted to value object automatically)

Note: Above supports only work for class method, NOT normal function, as normal function with a pointer argument must take a pointer, normal function with a value argument must take a value object not a pointer
type Person struct &#123;    name string&#125;// type T and type *T are different types but *T contains method of T!!!func (p *Person) speak() string &#123;    return &quot;Speak() called by &quot; + p.name&#125;func (p Person) say() string &#123;    return &quot;say() called by &quot; + p.name&#125;func test() &#123;    p1 := Person&#123;&quot;Jack&quot;&#125;    p1.speak() // (&amp;p).speak() automatically done by Go, but speak() is not a method of Person!!        p2 := &amp;Person&#123;&quot;jason&quot;&#125;    p2.say() //(*p2).say() automatically done by Go, but say() is a method of Person!!!&#125;

If you were to call p.speak(), the compiler would automatically change that to (&amp;p).speak(). A similar conversion happens in the other direction if you have a method with a non-pointer receiver and you call it on a pointer, easier to use.
Call method on struct nil pointer, no exception in Go, nil return
import &quot;fmt&quot;type Vertex struct &#123;  x, y int  // this is no method delcare here!!!&#125;// Bad way if want to update, but it&#x27;s ok if not updating caller objectfunc (v Vertex) Update() &#123; // value receiver  v.x = 10  v.y = 20&#125;// Good way use (v *Vertex) to bind with struct Vertex// bound with Vertex structfunc (v *Vertex) Scale(f int) &#123; // pointer receiver  v.x = v.x * f  v.y = v.x * f&#125;func classDemo() &#123;    // call pointer receiver on non-pointer object    v1 := Vertex&#123;1, 2&#125;        v1.Update() // copy of v1 is passed!!!    fmt.Println(v1) // v1 is not changed at all        v1.Scale(2) // pointer receiver (&amp;v1).Scale() is called automatcially by Go    fmt.Println(v1)            // call value receiver on pointer object    v2 := &amp;Vertex&#123;1, 2&#125;        v2.Update() // value receiver (*v2).Update() is called automatcially by Go    fmt.Println(*v2) // v2 is not changed at all        v2.Scale(2)    fmt.Println(*v2)&#125;classDemo()

&#123;1 2&#125;
&#123;2 4&#125;
&#123;1 2&#125;
&#123;2 4&#125;

InterfaceTypes implicitly satisfy an interface if they implement all required methods defined by that interface.
type T and type *T are different types but *T contains all methods of T, but the other side is not true, even not true you still can call *T method from T object like above(actually implicit conversion happened)  
The method set of any other type T consists of all methods declared with receiver type T. The method set of the corresponding pointer type *T is the set of all methods declared with receiver *T or T (that is, it also contains the method set of T). 
That means:

If a type T implements all methods of an interface using value receiver, then both value T and pointer of that type *T can be used while assigning to that interface variable or while passing to a function which accept an argument as that interface.  

If a type T implements all methods of an interface using pointer receiver, then the only pointer of that type *T can be used while assigning to that interface variable or while passing to a function that accepts an argument as that interface.


type Humaner interface &#123;    speak() string // no func keyword at the beginning&#125;// two common used interface varvar hi Humaner = struct_objectvar hi Humaner = &amp;struct_objectvar hi *Humaner = &amp;struct_object // Never see such way used!!!


interface &#123;&#125; is a special type which has no method, hence all types can be converted to it, it looks like void* in C but it’s not an pointer in Go, an string, int, object, &amp;object can assign to it as well
type Person struct &#123;    Name string&#125;func test(a interface&#123;&#125;) &#123;    var b interface &#123;&#125;&#125;test(12)test(&quot;hello&quot;)// both are ok!!test(Person&#123;Name: &quot;Jason&quot;&#125;)test(&amp;Person&#123;&quot;Jason&quot;&#125;)

Call method on interface nil pointer, runtime error!!!
More details, refer to inside interface
import (    &quot;fmt&quot;)type Person struct &#123;    name string&#125;type Humaner interface &#123;    speak() string    say() string&#125;func (p Person) say() string &#123;    return &quot;say() called by &quot; + p.name&#125;func (p *Person) speak() string &#123;    return &quot;speak() called by &quot; + p.name&#125;func speakSomething(h Humaner) &#123;    fmt.Println(&quot;area&quot;, h.speak())&#125;func saySomething(h Humaner) &#123;    fmt.Println(&quot;area&quot;, h.say())&#125;func demoInterface() &#123;    p := Person&#123;&quot;harsh&quot;&#125;        //works    speakSomething(&amp;p)    // works because *person has method implemented by person as well.     // both do NOT work(compiler error) because type person does not implment speak() method    // hence can&#x27;t convert type person to human interface    // speakSomething(p) compiler error    // saySomething(p)   compiler error.        //works, as for specific class p.speak() converted to (&amp;p).speak() by Go automatically    fmt.Println(p.speak())        fmt.Println(p.say())&#125;demoInterface()

area speak() called by harsh
speak() called by harsh
say() called by harsh

embedding and composing structEmbedding old way used like C  
type User struct &#123;  Name string&#125;type Admin struct &#123;  u User  permissions map[string]string&#125;

Embedding go supported new way  
type User struct &#123;  Name string&#125;type Admin struct &#123;  User  //anonymous fields  /* all its methods are “promoted” to the Admin as well.    * That means one can reference the name of the user via admin.Name no intermediate call to the u needed, like Admin.u.Name   // short way: Admin.Name   // another way: Admin.User.Name    */   permissions map[string]string&#125;

Composing typeswhich consists of embedding various types to create other types&#x2F;interfaces
type ReadWriter interface &#123;    Reader //anonymous fields    Writer //anonymous fields&#125;
What I can see from the above definition is that a ReadWriter is an interface which must contain all the functions defined on both Reader and Writer, which are defined elsewhere.
NOTE

There is no function signature in struct type like we did for C
If you embed a Interface in struct, that means you declare a Interface variable of the struct!!!

empty structInstance of empty struct struct&#123;&#125; in doesn’t occupy any memory. It is of zero byte. it’s used mostly in two cases:

Empty struct is a very good use case in a channel when you only want to use a channel for notification and not for actually passing in any data. but some one uses bool channel, which is accepted, but empty struct is better choice!!!

Implementation of Set data structure. A set is a data structure that holds elements without any particular order. An element only appears once in a set. We use map[keyType]struct&#123;&#125; for set. struct{} is only just to let us know if an element exists in the set or not.


package mainimport &quot;fmt&quot;type Set map[string]struct&#123;&#125;func (s Set) Add(key string) &#123;    // struct &#123;&#125; is type, while struct&#123;&#125;&#123;&#125; is an instance of this type.    s[key] = struct&#123;&#125;&#123;&#125;&#125;func (s Set) Delete(key string) &#123;    delete(s, key)&#125;func (s Set) Has(key string) bool &#123;    _, ok := s[key]    return ok&#125;func main() &#123;    s := Set&#123;&#125;    s.Add(&quot;a&quot;)        // save slot for same key    s.Add(&quot;a&quot;)    s.Add(&quot;b&quot;)    s.Delete(&quot;b&quot;)        // no error even for no exist!!!    s.Delete(&quot;c&quot;)    fmt.Printf(&quot;set %v has &#x27;a&#x27;: %v\n&quot;, s, s.Has(&quot;a&quot;))    fmt.Printf(&quot;set %v has &#x27;b&#x27;: %v\n&quot;, s, s.Has(&quot;b&quot;))&#125;main()

set map[a:&#123;&#125;] has &#39;a&#39;: true
set map[a:&#123;&#125;] has &#39;b&#39;: false

package mainimport &quot;fmt&quot;import &quot;time&quot;func worker(ch chan struct&#123;&#125;) &#123;    // send    time.Sleep(time.Second * 1)    fmt.Println(&quot;after 1s, sent notification, data is ready&quot;)    ch &lt;- struct&#123;&#125;&#123;&#125;&#125;func main() &#123;    // channel buffer is zero    // some one use ch := make(chan bool) but empty struct is better!!!    ch := make(chan struct&#123;&#125;)    go worker(ch)    fmt.Println(&quot;blocked due to no data&quot;)    // read from channel, if no data block here    &lt;-ch    fmt.Println(&quot;wake up after 1s as data is ready&quot;)    close(ch)&#125;main()

blocked due to no data
after 1s, sent notification, data is ready
wake up after 1s as data is ready

packageA package is a collection of source files in the same directory that are compiled together. Functions, types, variables, and constants defined in one source file are visible to all other source files within the same package. 
one package per directory, you can NOT have multiple packages in same directory
create a runnable programA standalone executable Go program must have package main declaration. If a program is part of the main package, then go build(go install) will create a binary file; which upon execution calls main function of the program, binary file is created only for man package  
create a libraryIf a program is part of a package other than main, then a package archive file is created with go build(go install) command
Package declaration(package xxx at beginning of xx.go) which should be first line of code, file name can be different than package name. When you import a package, package declaration is used to create package reference variable.
Export name(var or method from a package)A name is exported if it begins with a capital letter, exported means it can be accessed from other package.
package scopeA package scope is a region within a package where a declared variable(even it’s not exported) is accessible from within a package (across all the files in the package).
package init()func init()&#123;&#125; is called by Go when a package is initialized. It does not take any arguments and does not return any value, hence func init()&#123;&#125; is a special function of xx.go file should be only one for a package.
Package aliasWhen you import a package, Go creates a variable using the package declaration of the package. If you are importing multiple packages with the same name, this will lead to a conflict, use alias to avoid conflict if happens.
import (    log &quot;fmt&quot; // log is alias for fmt package.)

Publish your packagePublish it on GitHub and you are good to go. If your package is executable, people can use it as a command-line tool else they can import it in a program and use it as a utility module.
inside import statementimport github.com/example/hello// github.com/example is a path!!!// hello is also a path under example/// as you know package name can be different with dir who contains it, but most of time it&#x27;s same.

Above statement essentially means that import package present at directory hello. It doesn’t mean import package hello, it import package under hello/, that also means package name can be different with its directory.
Note

Go does NOT allow multiple packages at same directory
import is not recursive, if you have packages under subdirectory, you should import that subdirectory as well.
dot import: If an explicit period (.) appears instead of a name, all the package’s exported identifiers declared in that package’s package block will be declared in the importing source file’s file block and must be accessed without a qualifier. import . &quot;fmt&quot;, then use Println(&quot;hello&quot;). but it’s not good way.

searching packagePath of searching packages depends on GO111MODULE is enabled or not but both way check standard library firstly
$GOROOT=/usr/local/go for standard library like fmt, path, cmd, buffio etc.$GOPATH=/home/go for third-party library.
GO111MODULE&#x3D;on you must have go.mod in your project to build and run!!!  

$CURRENT_DIR/vendor is NOT checked anymore!!!
$GOROOT/pkg/&#123;arch&#125;/xxx.a  precompiled
$GOROOT/src standard library for source code
$GOPATH/pkg/mod/xxx workspace for source code

GO111MODULE&#x3D;off  

$CURRENT_DIR/vendor NOT $CURRENT_DIR/vendor/src!!!
$GOROOT/pkg/&#123;arch&#125;/xxx.a precompiled
$GOROOT/src standard library for source code
$GOPATH/pkg/&#123;arch&#125;/xxx.a precompiled
$GOPATH/src/xxx workspace for source code 
Must put your project at $GOPATH&#x2F;src to make it build

when GO111MODULE&#x3D;off, go get would fetch all the sources by using their import paths and store them in $GOPATH&#x2F;src. There was no versioning storing a single git checkout of every package and the ‘master’ branch would represent a stable version of the package.
Go Modules (GO111MODULE&#x3D;on) were introduced with Go 1.11, Go Modules stores tagged versions with go.mod keeping track of each package’s version

manually run  GO111MODULE=on go get would fetch all the sources with tagged versions and saved it at $GOPATH/pkg/mod/
automatically run go get when you run go build or go install based on tagged version from go.mod of each module,you must have go.mod of each module

import packageimport statement imports the package under that path, as one package per directory, hence only one package is imported for the path, most of time ,for easy to use, path and package name are same, but the path and package name can be different, if they are different, you need to know both package path and package name, while if they are same, you just need to know one, details refer to package and folder name
go env# check all env of go$ go envGOENV=&quot;/home/ubuntu/.config/go/env&quot;GO111MODULE=&quot;on&quot;GOBIN=&quot;&quot;GOMODCACHE=&quot;/home/ubuntu/go/pkg/mod&quot;GOPATH=&quot;/home/ubuntu/go&quot;GOROOT=&quot;/usr/local/go&quot;GOPROXY=&quot;https://goproxy.io,direct&quot;...# set env$ vi /home/ubuntu/.config/go/envGOBIN=&quot;&quot;$ go env -w GOBIN=&quot;&quot;

frequently used env

$GOBIN: bin dir of workspace which stores binary for application after go install, default $GOPATH/bin
$GOMODCACHE: mod(module) cache source code(xx.go) if mod is not standard library when GO111MODULE is on(mod with version), default $GOPATH/pkg/mod.
$GOPATH: working path, has bin&#x2F;, src&#x2F;, mod&#x2F;, src is used to store download non-standard library(without version)
$GOROOT: Standard library of Go

workspaceA workspace is Go’s way to facilitate project management. A workspace, in a nutshell, is a directory on your system where Go looks for source code files, manages dependency packages and build distribution binary files
A workspace can have multiple applications, if different apps refer to same package, they share the same package files at this workspace.
You can have as many workspaces as you want, as long as you keep GOPATH environment variable pointed to the current working workspace directory.
A Go workspace directory must have three sub-directories src, pkg and bin, $GOPATH points to active workspace.

pkg:

The pkg directory contains Go package objects(get by go get). They are the compiled versions of the original package source code or source code at pkg&#x2F;mod for GO111MODULE enabled.


bin:

The bin directory contains the binary executable files. These files are created by go install commands. go install command runs go build command internally and then outputs these files to the bin directory


src:  

The src directory contains Go packages. A package in nutshell is a project directory containing Go source code (.go files). Any packages installed using GO111MODULE=off go get command will reside here as well (and its dependency packages).



moduleGo code is grouped into packages, and packages are grouped into modules, a module can have several related packages but not at same directory, a module is logical groups to track dependencies of all packages in go.mod file, a module only needs one go.mod file at root directory, subdirectory does not need it all. 
Go must provide all of their dependencies via either Go modules with a go.mod file, or a vendor directory, go.mod is created with go mod init example.com/greetngs and updated when run go mod tidy, go.mod only tracks the deps(write a record in it), the downloaded modules is saved at $GOMODCACHE.
In GO111MODULE=off, if a package or a parent folder(parent&#39;s parent ...) of a package contains folder named vendor it will be searched for dependencies using the vendor folder as an import path root. While vendor folders can be nested, in most cases it is not advised or needed. when GO111MODULE&#x3D;off Any package in the vendor folder will be found before the standard library.
go.moduseful command used within a module  
# initialize new module in current directory# use example.com/greetings. # If you publish a module, this must be a path from which your module can be downloaded by Go tools. Mostly it&#x27;s your code&#x27;s repository.$ go mod init example.com/greetings# edit go.mod from command line$ go mod edit# edit source code, import packages etc, then run go mod tidy# go mod tidy ensures that the go.mod file matches the source code in the module. It adds any missing module requirements necessary to build the current module&#x27;s packages and dependencies, and it removes requirements on modules that don&#x27;t provide any relevant packages. It also adds any missing entries to go.sum and removes unnecessary entries.# add missing and remove unused modules, download package if not found locally$ go mod tidy# show all import information$ go list -m -json all

project layout 
|-- prj|   |-- greetings|   |   |-- go.mod|   |   `-- greetings.go|   `-- hello|       |-- go.mod|       `-- hello.go


creating a module(library) used by others$ mkdir greetings$ cd greetings$ go mod init example.com/greetingsgo: creating new go.mod: module example.com/greetings# go.mod file to track your code&#x27;s dependencies, it&#x27;s updated automatically.# vi greatings.gopackage greetingsimport &quot;fmt&quot;// Hello returns a greeting for the named person.func Hello(name string) string &#123;    // Return a greeting that embeds the name in a message.    message := fmt.Sprintf(&quot;Hi, %v. Welcome!&quot;, name)    return message&#125;# optional$ go tool compile -pack greetings.gogreetings.a$ go tool compile greetings.gogreetings.o

creating a module(runnable application)$ mkdir hello$ cd hello$ go mod init example.com/hellogo: creating new go.mod: module example.com/hello# vi hello.gopackage mainimport (    &quot;fmt&quot;    &quot;example.com/greetings&quot; //By convention, the package name is the same as the last element of the import path)func main() &#123;    // Get a greeting message and print it.    message := greetings.Hello(&quot;Gladys&quot;)    fmt.Println(message)&#125;# For production use, you’d publish the example.com/greetings module from its repository, go tools will download it from there# For now, because you haven&#x27;t published the module yet, you need to adapt the example.com/hello module so it can find the example.com/greetings code on your local file system.# use the go mod edit command to edit the example.com/hello module to redirect Go tools from its module path (where the module isn&#x27;t) to the local directory (where it is)$ go mod edit -replace=example.com/greetings=../greetings# add dependency in go.mod$ go mod tidy$ go build &amp; ./hello  // build run, binary is written to disk$ go run .           // build, run without writing binary to disk$ go install &amp; hello // build, install, and run

Importing packages from your module(base) [root@centos go]# tree.|-- go.mod|-- greet|   `-- greet.go`-- hello.go1 directory, 3 files(base) [root@centos go]# cat go.mod module example.com/hellogo 1.15# module path: example.com/hello

package mainimport (    &quot;example.com/hello/greet&quot; // import this way: module_path/greet)func main() &#123;        greet.Say(&quot;hello&quot;)&#125;

vendorIt is a folder found in a module that stores a copy of all the code the module depends on. The code is used to compile the final executable when the go build command is run with GO111MODULE=off.
By default, there is no vendor folder at all, but you can create it with go mod vendor or govendor govendor tool, after this all deps are copied to vendor fold, that means you can build your project without downloading deps if you switch to another machine or the deps are deleted from Internet, you have a total copy of it.
It’s old way, should not use it anymore, refer to migrate to go mod to update your project.
exceptionNo exception in Go like python or C++, library should return a value and err if want caller check error.
In Go, there is a built-in error type which defines like this
type error interface &#123;    Error() string&#125;
So that any type who satisfies this interface implements Error() method can be used as error.

fmt.Errorf(&quot;error %d&quot;, 10) returns struct instance which implements such method
The fmt package formats an error value by calling its Errorf() method.

func great(a, b int)(int, error) &#123;                                                  if a &gt; b &#123;                                                                          return a, nil                                                               &#125;                                                                               return b, fmt.Errorf(\&quot;%v is not great than %v\&quot;, a, b)                     &#125;                                                                               

recover from panicdefer function is called even panic happens.
When panic is called, including implicitly for run-time errors such as indexing a slice out of bounds or failing a type assertion, it immediately stops execution of the current function and begins unwinding the stack of the goroutine, running any deferred functions along the way. If that unwinding reaches the top of the goroutine&#39;s stack, the program dies. 
A call to recover() stops the unwinding and returns the argument passed to panic. Because the only code that runs while unwinding is inside deferred functions, recover is only useful inside deferred functions.
One application of recover is to shut down a failing goroutine inside a server without killing the other executing goroutines.
import &quot;fmt&quot;func server() &#123;    works := [10]int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125;        for i := range works &#123;        go safelyDo(i)    &#125;&#125;func do(i int) &#123;    if i % 2 == 0 &#123;        panic(&quot;panic when processing work&quot;)    &#125; else &#123;        fmt.Println(&quot;work is done&quot;)    &#125;&#125;func safelyDo(i int) &#123;    defer func() &#123;        if err := recover(); err != nil &#123;            fmt.Println(&quot;work failed:&quot;, err)        &#125;    &#125;()    do(i)&#125;server()


package numberimport &quot;fmt&quot;func add(a, b int) (int, error) &#123;    if a &lt; 0 || b &lt; 0 &#123;        return 0, fmt.Errorf(&quot;not support negative adding&quot;)    &#125;        return a + b, nil&#125;func test() &#123;    v, err := add(1, 2)    if err != nil &#123;        fmt.Println(err)    &#125; else &#123;        fmt.Println(&quot;1+2=&quot;, v)    &#125;        v, err = add(-1, 2)    if err != nil &#123;        fmt.Println(err)    &#125; else &#123;       fmt.Println(&quot;-1+2=&quot;, v)    &#125;&#125;test()

1+2= 3
not support negative adding

cgo(call C in Go)Cgo lets Go packages call C code. In order to use C code in Go, you first need to import a “pseudo-package”, “C” a special name interpreted by cgo as a reference to C’s name space, and include headers needed by C code when compiling with fixed format.
If the import of &quot;C&quot; is immediately preceded by a comment, that comment, called the preamble, is used as a header when compiling the C parts of the package by gcc!, The preamble may contain any C code, including function and variable declarations and definitions. These may then be referred to from Go code as though they were defined in the package “C”. All names declared in the preamble may be used, even if they start with a lower-case letter.
NOTECFLAGS, CPPFLAGS, CXXFLAGS, FFLAGS and LDFLAGS may be defined with pseudo #cgo directives within these comments to tweak the behavior of the C, C++ or Fortran compiler.
Note: No space line between import “C” and its header comment
/*#include &lt;stdio.h&gt;#include &lt;xxx.h&gt;#include &lt;xxx.c&gt; */import &quot;C&quot;// OR if needs to link other library or set FLAG// #cgo CFLAGS: -DPNG_DEBUG=1// #cgo linux CFLAGS: -DLINUX=1// #cgo LDFLAGS: -lpng// #include &lt;stdio.h&gt;// #include &lt;png.h&gt;import &quot;C&quot;

type mapping between C and goThe standard C numeric types are available under the names

C.char, C.schar (signed char), C.uchar (unsigned char)
C.short, C.ushort (unsigned short)
C.int, C.uint (unsigned int)
C.long, C.ulong (unsigned long), C.longlong (long long), C.ulonglong (unsigned long long)
C.float, C.double
The C type void is represented by Go’s unsafe.Pointer.*
The C types __int128_t and __uint128_t are represented by [16]byte.
C.struct_person{} refer to struct person&#123;&#125; defined in C
C.sizeof_struct_person get the len of struct person&#123;&#125; defined in C

Access struct

To access a struct, union, or enum type directly, prefix it with struct_, union_, or enum_, as in C.struct_stat.

sizeof

The size of any C type T is available as C.sizeof_T, like C.sizeof_struct_stat == sizeof(struct stat)

pass Go array to C function

n, err :&#x3D; C.f(&amp;array[0]) &#x2F;&#x2F; pass address of the first element.

Memory allocations made by C code are unknown to Go’s memory manager.   

When you create a C string with C.CString (or any C memory allocation) you must remember to free the memory when you’re done with it by calling C.free.

write C within go filepackage main// #include &lt;stdio.h&gt;//// void hello() &#123;//     printf(&quot;hello\n&quot;);// &#125;import &quot;C&quot;func main() &#123;    C.hello()&#125;
write C out of go, compile as library, go import the librarysum.c
#include &lt;stdio.h&gt;int sum(int a, int b, char* msg) &#123;    printf(&quot;say: &#x27;%s&#x27; adding\n&quot;, msg);    return a + b;&#125;
sum.h
int sum(int a, int b, char* msg);

Then create shared library
$ gcc -fPIC -c sum.c$ gcc -shared -o libsum.so sum.o$ lslibsum.so  sum.c  sum.h  sum.o  test.go

test.go
package main// #cgo CFLAGS: -I./// #cgo LDFLAGS: -L./ -lsum// #include &quot;sum.h&quot;// #include &lt;stdlib.h&gt;import &quot;C&quot;import (    &quot;fmt&quot;    &quot;unsafe&quot;)func main() &#123;    C.hello()    a := C.int(1)    b := C.int(2)    // create memory without go track    msg := C.CString(&quot;starting&quot;)    c := C.sum(a, b, msg)    // free it.    C.free(unsafe.Pointer(msg)) //defined in stdlib.h    fmt.Printf(&quot;sum(1,2)=%v\n&quot;, int(c))&#125;

$ go run test.gosay: &#x27;&#x27; addingsum(1,2)=3

cgo guideline
Runtimefunc NumCPU() int

NumCPU returns the number of logical CPUs usable by the current process.The set of available CPUs is checked by querying the operating system at process startup. Changes to operating system CPU allocation after process startup are not reflected. 

func GOMAXPROCS(n int) int

GOMAXPROCS sets the maximum number of CPUs that can be executing simultaneously and returns the previous setting. It defaults to the value of runtime.NumCPU. If n &lt; 1, it does not change the current setting. This call will go away when the scheduler improves. 

func Gosched()

Gosched yields the processor, allowing other goroutines to run. It does not suspend the current goroutine, so execution resumes automatically.runtime library

concurrency(multiple threads)The channel introduces many use cases in which channels are used to do data synchronizations among goroutines. In fact, channels are not the only synchronization techniques provided in Go. There are some other synchronization techniques supported by Go. For some specified circumstances, using the synchronization techniques other than channel are more efficient and readable than using channels.
ways to use channel, think chan bool as type
- var m chan bool // bool channel- var m map[string] chan bool // each element of map is a bool channel.- var m chan int // int channel- var m []chan int // each element of the array is an int channel- func gen(nums []int) &lt;-chan int &#123;&#125; // reading channel as return value- func gen(c chan int) int &#123;&#125; // channel as parameter- var ch2 chan&lt;- int// channel only for writing- var ch3 &lt;-chan int // channel only for reading- ch4 := make(chan int)- ch5 := &lt;-chan int(ch4) // convert to unary channel for reading- ch6 := chan&lt;- int(ch4) // convert to unary channel for writing// return a channel for readingfunc gen(nums []int) &lt;-chan int &#123;&#125;type Request struct &#123;    args        []int    f           func([]int) int    resultChan  chan int // channel var, need to initialize by make(chan int) later on&#125;

 

A goroutine is a lightweight &#39;thread&#39; managed by the Go runtime. by default GO creates a pool of linux threads, the number of this pool equals to number of processor, goroutine runs on these threads.
go f(x, y, z)
The evaluation of f, x, y, and z happens in the current goroutine and the execution of f happens in the new goroutine.
goroutines run in the same address space, so access to shared memory must be synchronized. The sync package provides useful primitives.
communication between goroutines
Channels are a typed conduit through which you can send and receive values with the channel operator, &lt;-
ch = make(chan int) // create a channel with buffer size 0, no buffer, which only access int as its message// &lt;- is an operator, NO operator -&gt;!!!// the left side is receiver, the right side is senderch &lt;- v        // Send v to channel ch. blocked until data is read!!!v := &lt;- ch     // Receive from ch, and assign value to v blocked until data is sent!!!v, ok := &lt;- ch // check if channel is close, when closed, v is nil, ok is false, otherwise, v is value, ok is true&lt;- ch          // discard result

By default, send and receive block until the other side is ready. This allows goroutines to synchronize without explicit locks or condition variables.
Receivers always block until there is data to receive. If the channel is unbuffered(buffer size 0), the sender blocks until the receiver has received the value. If the channel has a buffer, the sender blocks only until if the buffer is full, this means waiting until some receiver has retrieved a value.
ch = make(chan int, 10) // buffer is 10, if no one receives, the 11th sending blocks!!!
Sends to a buffered channel block only when the buffer is full. Receives block when the buffer is empty
A sender can close a channel to indicate that no more values will be sent. Receivers can test whether a channel has been closed by assigning a second parameter to the receive expression: afterv, ok := &lt;- ch  
ok is false if there are no more values to receive and the channel is closed, ok is true if more values to receive, even it’s closed!!!
package mainimport &quot;fmt&quot;func main() &#123;    c := make(chan int, 5)    c &lt;- 3    c &lt;- 4    c &lt;- 5    d, ok := &lt;-c    fmt.Println(d, ok)    close(c)    d, ok = &lt;-c    fmt.Println(d, ok)&#125;........3 true4 true

The loop for i :&#x3D; range c receives values from the channel repeatedly until it is closed.
Note:

Only the sender should close a channel, never the receiver
Sending on a closed channel will cause a panic. 
Reading on closed channel, error happens, but not panic!!
For zero buffer channel, send and receive block until the other side is ready

package mainimport (    &quot;fmt&quot;)// chan int, type of channelfunc fibonacci(n int, c chan int) &#123;    x, y := 0, 1    for i := 0; i &lt; n; i++ &#123;        c &lt;- x // send x to channel        x, y = y, x+y    &#125;    close(c) // close the channel&#125;func main() &#123;    c := make(chan int, 10)    // pass channel to another routine    go fibonacci(cap(c), c)    for i := range c &#123;        // receives from channel until it&#x27;s closed, blocked if no more data in the channel        fmt.Println(i)    &#125;&#125;main()

0
1
1
2
3
5
8
13
21
34

multiple channelsThe select statement lets a goroutine wait on multiple communication operations.
A select blocks until one of its cases can run, then it executes that case. the default case in a select runs if no other case is ready(no event on channel), select only works for channel, not socket fd, if you want to monitor multiple fds for high performance use gnet

if no default case, select quits until one channel is ready!!! otherwise block for ever, so it&#39;s one time execution, if you need to select more time, put it in for loop!!!

It chooses one at random if multiple channels are ready
package mainimport &quot;fmt&quot;func fibonacci(c chan int, quit chan struct&#123;&#125;) &#123;    x, y := 0, 1    for &#123; // infinite loop        select &#123;        // x is the result written to channel        case c &lt;- x:            // go down when write to channel is done, wake from block.            // go down only when x is read by reader as it&#x27;s unbuffered channel            x, y = y, x+y            case _, ok:= &lt;-quit: // run this case only when channel is ready successfully!!!            fmt.Println(ok)            return        &#125;    &#125;&#125;func main() &#123;    c := make(chan int)    quit := make(chan struct&#123;&#125;)    go func() &#123;        for i := 0; i &lt; 10; i++ &#123;            v := &lt;-c // full format v, ok: = &lt;-c            fmt.Println(v)        &#125;        quit &lt;- struct&#123;&#125;&#123;&#125; //write empty to signal channel quit    &#125;() // here call the unnamed function    fibonacci(c, quit)&#125;main()

0
1
1
2
3
5
8
13
21
34
true

MutexWhat we just want to make sure only one goroutine can access a shared variable at a time to avoid conflicts?
This concept is called mutual exclusion, and the conventional name for the data structure that provides it is mutex.
Go’s standard library provides mutual exclusion with sync.Mutex and its two methods:
import &quot;sync&quot;var mu sync.Mutexmu.Lock()...mu.UnLock()var mu sync.RWMutexmu.RLock() // reader...mu.RUnLock()mu.Lock() // writer...mu.UnLock()

We can define a block of code to be executed in mutual exclusion by surrounding it with a call to Lock and Unlock.
We can also use defer to ensure the mutex will be unlocked as in the Value method.
mutex is not associated with particular goroutine, it’s global and can be accessed by all goroutines, locked when the mutex lock bit is set.
NOTE

nested is not supportedm := sync.Mutex&#123;&#125;m.Lock()m.Lock()// block for ever here!!!

package mainimport (    &quot;fmt&quot;    &quot;sync&quot;    &quot;time&quot;)func main() &#123;    var m sync.Mutex    m.Lock()    go func() &#123;        time.Sleep(time.Second)        fmt.Println(&quot;Hi&quot;)        m.Unlock() // make a notification    &#125;()    // if m.Lock runs before m.Unlock in gorountine, it blocks as it&#x27;s alrealdy locked above    // as lock on unlocked mutext will block!!!    m.Lock() // wait to be notified    fmt.Println(&quot;Bye&quot;) // Byes is always after Hi printed!!!&#125;main()

Hi
Bye

WaitGroupEach sync.WaitGroup value maintains a counter internally. The initial value of the counter is zero.
The *WaitGroup type has three methods: Add(delta int), Done() and Wait().

we can call the wg.Add(delta) method to change the counter value maintained by wg.
the method call wg.Done() is totally equivalent to the method call wg.Add(-1).
if a call wg.Add(delta) (or wg.Done()) modifies the counter maintained by wg to negative, panic will happen.
when a goroutine calls wg.Wait(),
if the counter maintained by wg is already zero, then the call wg.Wait() can be viewed as a no-op.
otherwise (the counter is positive), the goroutine will enter blocking state. It will enter running state again (a.k.a., the call wg.Wait() returns) when another goroutine modifies the counter to zero, generally by calling wg.Done().



Generally, a WaitGroup value is used for the scenario that one goroutine waits until all of several other goroutines finish their respective jobs.
Note

The Wait() method can be called in multiple goroutines. When the counter becomes zero, all of them will be notified, in a broadcast way. 

A WaitGroup value can be reused after one call to its Wait method returns. But please note that each Add method call with a positive delta that occurs when the counter is zero must happen before any Wait call starts, otherwise, data races may happen.

Must call Add() in main goroutine not the one runs the job.


package mainimport (    &quot;fmt&quot;    &quot;math/rand&quot;    &quot;sync&quot;    &quot;time&quot;)func main() &#123;    // set seed    rand.Seed(time.Now().UnixNano())    const N = 5    var values [N]int32    var wg sync.WaitGroup        wg.Add(N)    for i := 0; i &lt; N; i++ &#123;        i := i        go func() &#123;            // Int31n returns, as an int32, a non-negative pseudo-random number in the half-open interval [0,n)             // from the default Source. It panics if n &lt;= 0.             values[i] = 50 + rand.Int31n(50) //            fmt.Printf(&quot;Done:%v\n&quot;, i)            wg.Done() // &lt;=&gt; wg.Add(-1)        &#125;()    &#125;    wg.Wait()    // All elements are guaranteed to be    // initialized now.    fmt.Printf(&quot;values:%v\n&quot;, values)&#125;main()

Done:4
Done:2
Done:1
Done:3
Done:0
values:[63 93 53 59 60]

OnceA *sync.Once value has a Do(f func()) method, which takes a solo parameter with type func().
The code in the invoked argument function(doSomething()) is guaranteed to be executed before any once.Do() method call returns.Generally, a Once value is used to ensure that a piece of code will be executed exactly once in concurrent programming.
package mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;sync&quot;)func main() &#123;    x := 0    doSomething := func() &#123;        // only runs once!!!        x++        fmt.Println(&quot;Hello in once&quot;)    &#125;    var wg sync.WaitGroup    var once sync.Once    for i := 0; i &lt; 5; i++ &#123;        wg.Add(1)        go func() &#123;            defer wg.Done()            //Hello is guaranteed to be printed before all five world!!!            once.Do(doSomething)                        // all below code runs after soSomething return            fmt.Println(&quot;world!&quot;)        &#125;()    &#125;    wg.Wait()    fmt.Printf(&quot;x = %v\n&quot;, x) // x = 1&#125;main()

Hello in once
world!
world!
world!
world!
world!
x = 1

CondThe sync.Cond type provides an efficient way to do notifications among goroutines.
Each sync.Cond value holds a sync.Locker field with name L. The field value is often a value of type *sync.Mutex or *sync.RWMutex. So, in order to use Cron, you must have a mutex as well!!!

c.Wait() must be called when c.L is locked, otherwise, a c.Wait() will cause panic. A c.Wait() call will first push the current caller goroutine into the waiting goroutine queue maintained by c, then call c.L.Unlock() to unlock&#x2F;unhold the lock c.L. then make the current caller goroutine enter blocking state. Once the caller goroutine is unblocked and enters running state again, c.L.Lock() will be called (in the resumed c.Wait() call) to try to lock and hold the lock c.L again, The c.Wait() call will exit after the c.L.Lock() call returns.

a c.Signal() call will unblock the first goroutine in (and remove it from) the waiting goroutine queue maintained by c, if the queue is not empty.

a c.Broadcast() call will unblock all the goroutines in (and remove them from) the waiting goroutine queue maintained by c, if the queue is not empty.


cond.Broadcast() and cond.Signal() are not required to be called when cond.L is locked. you can also call them after unlock.
package mainimport (    &quot;fmt&quot;    &quot;math/rand&quot;    &quot;sync&quot;    &quot;time&quot;)func main() &#123;    rand.Seed(time.Now().UnixNano())    const N = 10    var values [N]string    // cond needs a mutex inside    cond := sync.NewCond(&amp;sync.Mutex&#123;&#125;)    for i := 0; i &lt; N; i++ &#123;        d := time.Second * time.Duration(rand.Intn(5)) / 10        go func(i int) &#123;            time.Sleep(d) // simulate a workload            // Changes must be made when            // cond.L is locked.            cond.L.Lock()            values[i] = string(&#x27;a&#x27; + i)// &#x27;a&#x27; is bytes(uint8!!!)            //cond.Broadcast()            cond.L.Unlock()            // &quot;cond.Broadcast()&quot; can be put here            // when cond.L lock is unlocked. so that the waked one can get lock right now.            cond.Broadcast()        &#125;(i)    &#125;    // This function must be called when    // cond.L is locked.    checkCondition := func() bool &#123;        fmt.Println(values)        for i := 0; i &lt; N; i++ &#123;            // we have the lock here.            if values[i] == &quot;&quot; &#123;                // not all values are set.                return false            &#125;        &#125;        return true    &#125;    cond.L.Lock()    defer cond.L.Unlock()    // not for loop.    for !checkCondition() &#123;        // Must be called when cond.L is locked.        cond.Wait() // when blockes, mutex is unlocked!!! when waked up, it gets lock again!!!    &#125;&#125;main()

[         ]
[    e     ]
[ b   e     ]
[a b   e     ]
[a b c  e     ]
[a b c  e   h  ]
[a b c  e f  h  ]
[a b c d e f  h  ]
[a b c d e f  h  j]
[a b c d e f g h  j]
[a b c d e f g h i j]

atomicgo provides atomic operations Add、CompareAndSwap、Load、Store、Swap from &quot;sync/atomic&quot;
func AddInt32(addr *int32, delta int32) (new int32)func AddUint32(addr *uint32, delta uint32) (new uint32)func AddInt64(addr *int64, delta int64) (new int64)func AddUint64(addr *uint64, delta uint64) (new uint64)func AddUintptr(addr *uintptr, delta uintptr) (new uintptr)func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool)func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool)func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool)func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool)func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool)func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool)func LoadInt32(addr *int32) (val int32)func LoadInt64(addr *int64) (val int64)func LoadUint32(addr *uint32) (val uint32)func LoadUint64(addr *uint64) (val uint64)func LoadUintptr(addr *uintptr) (val uintptr)func LoadPointer(addr *unsafe.Pointer) (val unsafe.Pointer)func StoreInt32(addr *int32, val int32)func StoreInt64(addr *int64, val int64)func StoreUint32(addr *uint32, val uint32)func StoreUint64(addr *uint64, val uint64)func StoreUintptr(addr *uintptr, val uintptr)func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer)func SwapInt32(addr *int32, new int32) (old int32)func SwapInt64(addr *int64, new int64) (old int64)func SwapUint32(addr *uint32, new uint32) (old uint32)func SwapUint64(addr *uint64, new uint64) (old uint64)func SwapUintptr(addr *uintptr, new uintptr) (old uintptr)func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer)

reflectionfrom specific type to generic typeyou can convert any type to interface &#123;&#125; just for parameter passing.
var a int = 10var b *int = &amp;avar c string = &quot;hello&quot;var i interface&#123;&#125;i = ai = bi = c// all are ok!!!

from interface to specific typeAs any type can be converted to interface&#123;&#125; type, but how can we convert it back to particular type.
A type assertion provides access to an interface value’s underlying concrete value.  
t := i.(T)  If i does not hold a T, the statement will trigger a panic.  
This statement asserts that the interface value i holds the concrete type T and assigns the underlying T value to the variable t.  
To test whether an interface value holds a specific type, a type assertion can return two values: the underlying value and a boolean value that reports whether the assertion succeeded.  
t, ok := i.(T)  
If i holds a T(i is an instance of T), then t will be the underlying value and ok will be true.If not, ok will be false and t will be the zero value of type T, and no panic occurs.   
 T is generic type, it can be int or *int(*int is also a type)
generic value, v, ok :&#x3D; i.(T) &#x2F;&#x2F; i is var of interface{}
if v, ok := i.(int); ok &#123;    fmt.Printf(&quot;Twice %d is %d\n&quot;, v, v*2)&#125; else if v, ok := i.(*int); ok&#123;    fmt.Printf(&quot;Twice %d is %d\n&quot;, *v, *v*2)&#125; else if v, ok = i.(string); ok &#123;    fmt.Printf(&quot;%s is %d bytes long\n&quot;, v, len(v))&#125; else &#123;   fmt.Printf(&quot;I don&#x27;t know about type %T!\n&quot;, v)&#125;

type switch(only valid for switch)A type switch is like a regular switch statement, but the cases in a type switch specify types (not values), and those values are compared against the type of the value held by the given interface value.
func do(i interface&#123;&#125;) &#123;    switch v := i.(type) &#123; // i.(type), type is keyword here!!!	case int:		fmt.Printf(&quot;Twice %v is %v\n&quot;, v, v*2) // v is type int	case string:		fmt.Printf(&quot;%q is %v bytes long\n&quot;, v, len(v)) // v is type string	default:		fmt.Printf(&quot;I don&#x27;t know about type %T!\n&quot;, v)	&#125;&#125;func test() &#123;	do(21)	do(&quot;hello&quot;)	do(true)&#125;

more about reflection

Advanced feature between interface and specfic typeyou can convert any type to interface &#123;&#125; just for parameter passing
but as interface &#123;&#125; has no method, hence you can NOT get more information about the type&#x2F;value it behinds, so refection package gives advanced feature.
reflect implements run-time reflection, allowing a program to manipulate objects with arbitrary types. The typical use is to take a value with static type interface{} and extract its dynamic type information by calling TypeOf(), which returns a Type(an interface defined in reflect package).
A call to ValueOf() returns a Value(an interface defined in reflect package) representing the run-time data. Zero takes a Type and returns a Value representing a zero value for that type.

reflect.Type(type interface &#123;&#125;) returned by reflect.Typeof(any instance)

rType implements generic methods defined by reflect.Type
mapType, ptrType, structType(which embed rType) etc implement specific methods defined reflect.Type which are valid for its type, so specific type implements all methods defined by reflect.Type


reflect.Value(type Value struct &#123;&#125;) returned by reflect.Valueof(any instance)

Value has generic methods called by all values
Value has specific methods which are valid for specific type



Method of specific type



Kind
Methods applicable



Int*
Bits


Uint*
Bits


Float*
Bits


Complex*
Bits


Array
Elem, Len


Chan
ChanDir, Elem


Func
In, NumIn, Out, NumOut, IsVariadic


Map
Key, Elem


Ptr
Elem


Slice
Elem


Struct
Field, FieldByIndex, FieldByName,FieldByNameFunc, NumField



Type.Elem() returns a type’s element type.

Type.PtrTo() returns the pointer type with element t.

For example, if t represents type Foo, PtrTo(t) represents *Foo.


v.Indirect() returns the value that v points to(v is pointer Value)

v := reflect.Valueof(&amp;Person&#123;Name: &quot;tom&quot;&#125;), vv := v.Indirect()


v.Addr() returns the pointer’s Value (v is object Value)

v := reflect.Valueof(Person&#123;Name: &quot;tom&quot;&#125;), vp: = v.Addr()




Ref  

Reflection examples
Laws of Reflection

package mainimport (    &quot;fmt&quot;    &quot;reflect&quot;    &quot;strconv&quot;)type Person struct &#123;    Name string `k1:&quot;v1&quot; k2:&quot;v2&quot;`    id   int&#125;func (p *Person) Ident() string &#123;    return p.Name + &quot; &quot; + strconv.Itoa(p.id)&#125;func (p Person) Say(msg string) string &#123;    return msg + p.Name&#125;func main() &#123;    p := Person&#123;Name: &quot;tom&quot;, id: 10&#125;            /*********************************************************************/    // typeof from interface    rp := reflect.TypeOf(p)    // rp.Elem() returns a type&#x27;s element type.    fmt.Printf(`p := Person&#123;&#125;type :%vsize(byte): %vnumField: %vnumMethod: %valign: %vfieldalign: %v`, rp.String(), rp.Size(), rp.NumField(), rp.NumMethod(), rp.Align(), rp.FieldAlign())    // NOTE: Ident is not a method of Person    // Go compiler auto converted p.Ident() to (&amp;p).Ident()    fmt.Println(p.Ident(), (&amp;p).Ident()) //    m, ok := rp.MethodByName(&quot;Say&quot;)    if ok &#123;        fmt.Printf(&quot;method name: %v, type: %v, index: %v\n&quot;, m.Name, m.Type, m.Index)    &#125; else &#123;        fmt.Printf(&quot;Say method not found\n&quot;)    &#125;    f, _ := rp.FieldByName(&quot;Name&quot;)    fmt.Printf(&quot;field name: %v, type: %v, Tag: %v\n&quot;, f.Name, f.Type, f.Tag)    // Type to Value: nv := reflect.New(rp)    // if rp(map):    nv := reflect.MakeMap(rp)            /*********************************************************************/    // ValueOf from interface    v := reflect.ValueOf(p)    // Value to Type: rp := v.Type()    rp = v.Type()    fmt.Printf(`p := Person&#123;&#125;type :%vsize(byte): %vnumField: %vnumMethod: %valign: %vfieldalign: %v`, rp.String(), rp.Size(), rp.NumField(), rp.NumMethod(), rp.Align(), rp.FieldAlign())    vName := v.FieldByName(&quot;Name&quot;)    name := vName.String()    // we know it&#x27;s string has String() method,     // otherwise name := vName.Interface().(*NType)        fmt.Println(&quot;name=&quot;, name)    vSay := v.MethodByName(&quot;Say&quot;)    if vSay.IsValid() &#123;        Say := vSay.Interface().(func(string) string)        fmt.Println(Say(&quot;hello &quot;))    &#125; else &#123;        fmt.Println(&quot;method Say not found&quot;)    &#125;        /*******************************************************************************/    // back to Person struct    p = v.Interface().(Person)    // both print Person, but v.Interface() can&#x27; not access field as it&#x27;s interface!!!    fmt.Println(p, v.Interface())&#125;main()

p := Person&#123;&#125;
type :struct &#123; Name string &quot;k1:\&quot;v1\&quot; k2:\&quot;v2\&quot;&quot;; 𒀸id int &#125;
size(byte): 24
numField: 2
numMethod: 0
align: 8
fieldalign: 8
tom 10 tom 10
Say method not found
field name: Name, type: string, Tag: k1:&quot;v1&quot; k2:&quot;v2&quot;
p := Person&#123;&#125;
type :struct &#123; Name string &quot;k1:\&quot;v1\&quot; k2:\&quot;v2\&quot;&quot;; 𒀸id int &#125;
size(byte): 24
numField: 2
numMethod: 0
align: 8
fieldalign: 8
name= tom
method Say not found
&#123;tom 10&#125; &#123;tom 10&#125;

import cycleCyclic dependency is fundamentally a bad design and is a compile-time error in Golang(error: import cycle not allowed). we should change our design to solve this in either way.
Solve import cycle

put them in the same package
use interface to solve the import cycle issue

dep|-- a|   `-- a.go|-- b|   `-- b.go|-- c|   `-- c.go`-- main.go


package aimport (  &quot;fmt&quot;  &quot;github.com/mantishK/dep/b&quot;)type A struct &#123;&#125;func (a A) PrintA() &#123;  fmt.Println(a)&#125;func NewA() *A &#123;  a := new(A)  return a&#125;func RequireB() &#123;  o := b.NewB()  o.PrintB()&#125;

package bimport (  &quot;fmt&quot;  &quot;github.com/mantishK/dep/a&quot;)type B struct &#123;&#125;func (b B) PrintB() &#123;  fmt.Println(b)&#125;func NewB() *B &#123;  b := new(B)  return b&#125;func RequireA() &#123;  o := a.NewA()  o.PrintA()// B wants to PrintA from A&#125;
package mainimport (    &quot;github.com/mantishK/dep/a&quot;)func main() &#123;    o := a.NewA()    o.PrintA()&#125;


Soloutionwe must introduce an interface in a new package say c. This interface will have all the methods that are in struct A and are accessed by struct B.
package ctype C interface &#123;    PrintA()&#125;

package aimport (    &quot;fmt&quot;    &quot;github.com/mantishK/dep/b&quot;)type A struct &#123;&#125;func (a A) PrintA() &#123;    fmt.Println(&quot;hello a&quot;)&#125;func NewA() *A &#123;    a := new(A)    return a&#125;func (a A) PrintBFromA() &#123;    o := b.NewB(a)    o.PrintB()&#125;

package bimport (    &quot;fmt&quot;    &quot;github.com/mantishK/dep/c&quot;)type B struct &#123;    c c.C&#125;func (b B) PrintB() &#123;    fmt.Println(&quot;hello b&quot;)&#125;func NewB(c c.C) *B &#123;    // pass interface(A) to instance b    b := new(B)    b.c = c    return b&#125;func (b B) PrintAFromB() &#123;    // access PrintA    b.c.PrintA()&#125;


package mainimport (    &quot;github.com/mantishK/dep/a&quot;    &quot;github.com/mantishK/dep/b&quot;)func main() &#123;    o := a.NewA()    o.PrintA()    o.PrintBFromA()    b := b.NewB(o)    b.PrintAFromB()&#125;

Deep copyThere is no deep copy built-in function provided by Go, if you want to deep copy, you have to copy it by your self, go only does shadow copy.

For channel, slice, dict, interface, pointer, assigned var points to same memory.
For struct, non-point, assigned var has its own memory, shadow copied.
Deep copy for map two ways.
Marshal –&gt; then Unmarshal
iterate each item, then do copy, if item is a map, recursive deep is needed!!!


copy for slice, copy(dst, src), not recursive, not deep.

Go does the same thing as C language
a := []int&#123;1,2,3&#125;b := a // b is copy of a, they points to same memory, same thing for map as well.c := make([]int, len(a))copy(c, a) // copy a to c, not support map, for map, you have to copy each by your self.type Person struct &#123;    Name string&#125;p1 := Person &#123;Name: &quot;Jack&quot;&#125;p2 := p1 // p2 is copy of p1, they points to different memory.ps := []Person&#123;    &#123;        Name:&quot;Jack&quot;,    &#125;,&#125;for _, p := range ps &#123;    p.Name = &quot;cool&quot; // p is copy of array item.&#125;// ps is not changed at all

TipsAnonymous struct typeMost of time, if we frequently use a struct type, we should define a new type, use it for short like this
type Student struct &#123;    name string    id int&#125;var s1 Student = Student&#123;&quot;jason&quot;, 1&#125;

But we still can use a unnamed struct if only use it for several times, like this
import &quot;fmt&quot;func test() &#123;    s1 := struct &#123;        name string        id int    &#125;&#123;&quot;jason&quot;, 1&#125;        test1(s1)    test2(&amp;s1)&#125;func test1(s struct &#123;name string; id int&#125;) &#123;     fmt.Println(s.name, s.id)&#125;func test2(s *struct &#123;name string; id int&#125;) &#123;     fmt.Println(s.name, s.id)&#125;

Anonymous functionAnonymous function is a function without name, it’s mostly used in two cases

used as goruntine entry point
saved in function object, then call it later.
used as return value.
defer function.

func test &#123;    go func() &#123;        fmt.Println(&quot;hello go&quot;)    &#125;()            f := func() &#123;        fmt.Println(&quot;hello go&quot;)    &#125;    f()&#125;

Reader and WriterThe io package specifies the io.Reader interface, which represents the read end of a stream of data.
The Go standard library contains many implementations of this interface, including files, network connections, compressors, ciphers, and others.
The io.Reader interface has a Read method:  
func (T) Read(b []byte) (n int, err error)
Read populates the given byte slice(should be create first with make()) with data and returns the number of bytes populated and an error value. It returns an io.EOF error when the stream ends.
package mainimport (    &quot;fmt&quot;    &quot;io&quot;    &quot;strings&quot;)func main() &#123;    // return Reader interface    r := strings.NewReader(&quot;Hello, Reader!&quot;)        // buffer to store reading    b := make([]byte, 8)    for &#123;        n, err := r.Read(b) // must check err when reading        fmt.Printf(&quot;n = %v err = %v b = %v\n&quot;, n, err, b)        fmt.Printf(&quot;b[:n] = %q\n&quot;, b[:n])        if err == io.EOF &#123;            break        &#125;    &#125;&#125;main()

n = 8 err = &lt;nil&gt; b = [72 101 108 108 111 44 32 82]
b[:n] = &quot;Hello, R&quot;
n = 6 err = &lt;nil&gt; b = [101 97 100 101 114 33 32 82]
b[:n] = &quot;eader!&quot;
n = 0 err = EOF b = [101 97 100 101 114 33 32 82]
b[:n] = &quot;&quot;

never use break for each case in switch&#x2F;selectAs go break each matched case automatically, there is no need to use break for each case explicitly in case you want to break in the middle of the case.
tick := time.NewTicker(2 * time.Second)                                         var a int                                                                       for &#123;                                                                               select &#123;                                                                        case &lt;-tick.C:                                                                      a++                                                                             if a &gt; 2 &#123;                                                                          fmt.Println(&quot;break out&quot;)                                                        break  // case is break, hence go next loop of for, for is not break!!!                                                     &#125;                  fmt.Println(&quot;reach me&quot;)    &#125;                                                                                                                       fmt.Println(&quot;running&quot;)                                                 &#125;        

Handle OS differenceMost of time, go code can shared by different OS like linux and windows etc, but in some case, we may need to handle special cases that depends on OS, hence we need to know the OS in Go code $GOOS gives the way for you to deal that.
package mainimport (    &quot;fmt&quot;    &quot;runtime&quot;)func main() &#123;    switch &#123;    case runtime.GOOS == &quot;windows&quot;:        fmt.Println(&quot;You are running on windows&quot;)    case runtime.GOOS == &quot;linux&quot;:        fmt.Println(&quot;You are running on linux&quot;)    default:        fmt.Println(&quot;You are running on an OS that we do not support&quot;)    &#125;&#125;

HTTP over Unix// A quick and dirty demo of talking HTTP over Unix domain socketspackage mainimport (    &quot;fmt&quot;    &quot;io&quot;    &quot;net&quot;    &quot;net/http&quot;    &quot;os&quot;    &quot;os/exec&quot;    &quot;time&quot;    &quot;github.com/codegangsta/martini&quot;)// overriding ne.Dialer.Dial to force unix socket connectionvar addr = &quot;http.sock&quot;func fakeDial(_, _ string) (net.Conn, error) &#123;    return net.DialTimeout(&quot;unix&quot;, addr, time.Second*32)&#125;var transport http.RoundTripper = &amp;http.Transport&#123;    Dial: fakeDial,&#125;func main() &#123;    m := martini.Classic()    m.Get(&quot;/demo&quot;, handler)    // normal unix listener    listener, err := net.Listen(&quot;unix&quot;, addr)    if err != nil &#123;        fmt.Println(err.Error())        return    &#125;    defer func() &#123;        cmd := exec.Command(&quot;/bin/rm&quot;, &quot;-rf&quot;, &quot;http.sock&quot;)        cmd.Run()    &#125;()    // as you can see server has no special setting for unix listener    // serve http request on any listener(here unix listener)    go http.Serve(listener, m) // http.Serve takes any net.Listener implementation    time.Sleep(1 * time.Second)    c := http.Client&#123;&#125;    // client usage is different for tcp and unix transport!!!    c.Transport = transport // use the unix dialer based on unix to send http request(no tcp as tcp+http)    // as you can see the address is sock path not ip:port like tcp    resp, err := c.Get(&quot;http://http.sock/demo&quot;)    if err != nil &#123;        fmt.Println(err.Error())        return    &#125;        // io.copy from resp.Body to os.StdOut(Writer)    io.Copy(os.Stdout, resp.Body)&#125;func handler() (code int, body string) &#123;    code = 200    body = &quot;ok\n&quot;    return&#125;

WaitGroup vs ChannelActually, there are designed for different scenarios, If you are dispatching one-off jobs to be run in parallel without needing to know the results of each job, then you can use a WaitGroup. But if you need to collect the results from the goroutines then you should use a channel. channel is designed to pass data while WaitGrop for wait jobs(which runs in goroutine) to finish.
waitgroup
package mainimport (    &quot;fmt&quot;    &quot;sync&quot;)func work() &#123;    fmt.Println(&quot;job done&quot;)&#125;func main() &#123;    var wg sync.WaitGroup    wg.Add(1)    go func() &#123;        defer wg.Done()        work()    &#125;()    wg.Wait()    fmt.Println(&quot;got one job done&quot;)&#125;

notify channel
package mainimport (    &quot;fmt&quot;)func work(ch chan struct&#123;&#125;) &#123;    fmt.Println(&quot;job done&quot;)    ch &lt;- struct&#123;&#125;&#123;&#125;&#125;func main() &#123;    ch := make(chan struct&#123;&#125;)    go func() &#123;        work(ch)    &#125;()    &lt;-ch    fmt.Println(&quot;got one job done&quot;)&#125;

if you have several jobs to run, waitgroup is best choice to use!!
package mainimport (    &quot;fmt&quot;    &quot;sync&quot;)func work() &#123;    fmt.Println(&quot;job done&quot;)&#125;func main() &#123;    var wg sync.WaitGroup    wg.Add(2)    go func() &#123;        defer wg.Done()        work()    &#125;()    go func() &#123;        defer wg.Done()        work()    &#125;()    wg.Wait()    fmt.Println(&quot;got two jobs done&quot;)&#125;main()

job done
job done
got two jobs done

do we need to close(channel) explicitlyNo, It’s OK to leave a Go channel open forever and never close it. When the channel is no longer used(no read and write), it will be garbage collected.
It is only necessary to close a channel explicitly if the receiver is looking for a close. Closing the channel is a control signal on the channel indicating that no more data follows, or to notify goroutine to quit otherwise it may blocking for ever
Danger

closing a closed channel will panic, so it is dangerous to close a channel if the closers don’t know whether or not the channel is closed.
sending values to a closed channel will panic, so it is dangerous to send values to a channel if the senders don’t know whether or not the channel is closed.

Rule to close if required

don’t close a channel from the receiver side.
don’t close a channel if the channel has multiple concurrent senders.

In other words, we should only close a channel in a sender goroutine if the sender is the only sender of the channel. more solution to close a channel, refer to channel closing
package mainimport (    &quot;fmt&quot;    &quot;time&quot;)var ch = make(chan struct&#123;&#125;)func test() &#123;    go func() &#123;        fmt.Println(&quot;goroutine blocks&quot;)        &lt;-ch        fmt.Println(&quot;goroutine quit&quot;)    &#125;()    fmt.Println(&quot;test return, but goroutine is blocking&quot;)&#125;func main() &#123;    test()        // if main is a daemon process, gorountine waits for signal while sender is quit    // when sender quits, it should notify receive to quit as well, otherwise, it&#x27;s blocks for ever        time.Sleep(1 * time.Second)    close(ch)    time.Sleep(1 * time.Second)&#125;main()

test return, but goroutine is blocking
goroutine blocks
goroutine quit

print value of struct pointer embeded in another structBy default, struct pointer printed with its content if it’s not embeded, but if struct pointer as field of another struct, the pointer address is printed,  As when call print, the String() of that type is called, but by default golang does not provide String() for struct pointer, but struct only, that’s why pointer address is printed, In order to print pinter content(not address), there are several ways we can use

iterate each pointer in struct, print it with fmt.Println(&quot;%s&quot;, *p)
with json.Marshel(), but only exported field are printed
For each pointer, implement its func (s *XX)String()string&#123;&#125; method

ackage main                                                                                                                                                    import (                                                                            &quot;fmt&quot;                                                                       )                                                                                                                                                               type Class struct &#123;                                                                 Id int                                                                      &#125;                                                                                                                                                               // comment out to see what happens                                              func (c *Class) String() string &#123;                                                   return fmt.Sprintf(&quot;%v&quot;, c.Id)                                              &#125;                                                                                                                                                               // comment out to see what happens                                              func (p *Person) String() string &#123;                                                  return fmt.Sprintf(&quot;%v %v&quot;, p.Name, p.Cls)                                  &#125;                                                                                                                                                               type Person struct &#123;                                                                Name string // string has default String()                                      Cls  *Class // no default String() for *Class                               &#125;                                                                                                                                                               func main() &#123;                                                                       p1 := Person&#123;Name: &quot;tom&quot;, Cls: &amp;Class&#123;Id: 1&#125;&#125;                                   p2 := Person&#123;Name: &quot;jack&quot;, Cls: &amp;Class&#123;Id: 2&#125;&#125;                                  sp := []Person&#123;p1, p2&#125;                                                          fmt.Printf(&quot;%v\n&quot;, sp)                                                          fmt.Printf(&quot;%+v\n&quot;, sp)                                                                                                                                         spp := []*Person&#123;&amp;p1, &amp;p2&#125; // Person has no default String()                    fmt.Printf(&quot;%v\n&quot;, spp)                                                         fmt.Printf(&quot;%+v\n&quot;, spp)                                                    &#125;                      

[&#123;tom 1&#125; &#123;jack 2&#125;][&#123;Name:tom Cls:1&#125; &#123;Name:jack Cls:2&#125;][tom 1 jack 2][tom 1 jack 2]


package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;)type Class struct &#123;    Id int&#125;type Person struct &#123;    Name string    Cls  *Class&#125;func main() &#123;    p1 := Person&#123;Name: &quot;tom&quot;, Cls: &amp;Class&#123;Id: 1&#125;&#125;    p2 := Person&#123;Name: &quot;jack&quot;, Cls: &amp;Class&#123;Id: 2&#125;&#125;    spp := []*Person&#123;&amp;p1, &amp;p2&#125;    data, _ := json.Marshal(spp)    fmt.Println(string(data))&#125;main()

[&#123;&quot;Name&quot;:&quot;tom&quot;,&quot;Cls&quot;:&#123;&quot;Id&quot;:1&#125;&#125;,&#123;&quot;Name&quot;:&quot;jack&quot;,&quot;Cls&quot;:&#123;&quot;Id&quot;:2&#125;&#125;]

block forever in gofunc blockForever() &#123;    select&#123; &#125;&#125;func blockForever() &#123;    m := sync.Mutex&#123;&#125;    m.Lock()    m.Lock()&#125;func blockForever() &#123;    c := make(chan struct&#123;&#125;)    &lt;-c&#125;

block forever
Channel with timeoutActually, there is no timeout parameter of API when reading&#x2F;writing channel, so we use other way to do this.
Way1
func main() &#123;    ch := make(chan struct&#123;&#125;, 1)    go func() &#123;        fmt.Println(&quot;do something...&quot;)        time.Sleep(4*time.Second)        ch&lt;- struct&#123;&#125;&#123;&#125;    &#125;()        select &#123;    case &lt;-ch:        fmt.Println(&quot;done&quot;)    case &lt;-time.After(3*time.Second):        fmt.Println(&quot;timeout&quot;)    &#125;&#125;
Way2
ch := make(chan string)ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)defer cancel()go func() &#123;    time.Sleep(time.Second * 4)    ch &lt;- &quot;done&quot;&#125;()select &#123;case res := &lt;-ch:    fmt.Println(res)case &lt;-ctx.Done():    fmt.Println(&quot;timout&quot;, ctx.Err())&#125;
Good reason to use contextAnother advantage of using context is that it can take advantage of its natural transmission characteristics in multiple goroutines, so that all goroutines that pass the context can receive cancellation notifications at the same time. we can call cancel() once, while quits all gorotines who listen on it..
Ref
Go Specification
Concurrency Synchronization Techniques

]]></content>
      <categories>
        <category>go</category>
        <category>program</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>hardware-fpga-smartnic</title>
    <url>/2021/08/03/hardware-fpga-smartnic/</url>
    <content><![CDATA[FPGASince the inception of FPGA(field programmable gate array) technology there were actually only two FPGA companies in the market: Xilinx and Altera. Along the years, both companies did a terrific job growing the market and protecting their market share. When Intel acquired Altera, Xilinx was left as the only major FPGA company in the market. Xilinx had ~50% market share while Altera (Intel) has ~37% and Lattice Semiconductor has a 10% market share. In October 2020, AMD has acquired Xilinx. This concludes that the 2 largest CPU vendors have acquired the 2 largest FPGA companies.



FPGA Companies

Xilinx (AMD)
Altera (Intel)
Lattice Semiconductor
QuickLogic
…

What’s FPGAField Programmable Gate Arrays (FPGAs) are semiconductor devices that are based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. FPGAs can be reprogrammed to desired application or functionality requirements after manufacturing. This feature distinguishes FPGAs from Application Specific Integrated Circuits (ASICs), which are custom manufactured for specific design tasks.
FPGA ApplicationFPGA is a technology or semi-manufactures, not an end product, but due to their programmable nature, FPGAs are an ideal fit for many different markets. it can provide comprehensive solutions consisting of FPGA devices, advanced software, and configurable, ready-to-use IP cores for markets and applications
SmartNICSmartNICs provide several significant benefits to data-center networks including:

Accelerating network, storage, and compute tasks by executing them directly on the network interface card, eliminating the need to run these workloads on servers and freeing up CPU cycles, thus dramatically improving server performance and reducing overall power consumption resulting in lower system-level total cost of ownership.
Offloading increasingly complex networking tasks including overlay tunneling protocols such as VxLAN and complex virtual switching from server processors, freeing the server processors to perform actual, revenue-generating tasks.
Improving effective network bandwidth and throughput by executing offloaded functions in fast hardware instead of slower software.
Providing additional, flexible functionality that easily accommodates new and constantly changing networking and storage protocols.


SmartNIC designs

Multicore SmartNICs, based on ASICs containing multiple CPU cores
FPGA-based SmartNICs
FPGA-augmented SmartNICs, which combine hardware-programmable FPGAs with ASIC network controllers

Multicore SmartNICA multicore SmartNIC design likely includes an ASIC that incorporates many software-programmable microprocessor cores. These cores are typically higher performance Arm processors, which process packets and offload from the main (and expensive) server CPUs. Multicore SmartNIC ASICs may also incorporate fixed function hardware engines that can offload well-defined tasks such as standardized security and storage protocols.
However, multicore SmartNICs suffer from at least two limitations:

They are based on software-programmable processors which are slower when used for network processing due to a lack of processor parallelism.
The fixed-function hardware engines in these multicore ASICs lack the data-plane programmability and flexibility that is increasingly required for SmartNIC offloading.

FPGA-based SmartNICAn FPGA-based SmartNIC employs the expanded hardware programmability of FPGAs to build any data-plane functions required by the tasks offloaded to the SmartNIC. Because FPGAs are reprogrammable, the data-plane functions implemented by the FPGA can be torn down and reconfigured at will and in real time. All such offloaded functions operate at hardware – not software – speeds.
FPGA-based SmartNIC designs can accelerate network functions by several orders of magnitude over implementations based purely on software. Use of FPGAs in SmartNIC designs delivers the wire-speed performance and power efficiency of custom hardware with the ability to create deep packet&#x2F;network processing pipelines that support complex offload tasks and improve single-flow network performance. Replicating these pipelines by exploiting the large amount of hardware parallelism inherent in FPGAs improves megaflow performance enough to meet the high-performance, high-bandwidth, high-throughput needs of next-generation data-center architectures based on speedier Ethernet networks.
Price is very high, expensive!!!
FPGA-Augmented SmartNIC(also called system-on chip (SoC))It adds FPGA capabilities to the NIC. The on-board FPGA in a FPGA-augmented SmartNIC design enables significantly higher performance and feature expansion.
More refer to SmartNIC design and Mellanox SmartNIC
BlueField is the great one in market.
SmartNIC MarketSmartNIC vendor









SmartNIC Vendor
SmartNIC Model
Chip Technology


AWS
Nitro Gen 3
SoC


Azure
Catapult v3
Intel FPGA


Alibaba Cloud
X-Dragon III
SoC


GCP
(generic Intel-based)
SoC


Mellanox
BlueField 2
SoC


Mellanox
Innova-2 Flex
Xilinx FPGA


Silicom
(many)
Intel and Xilinx FPGA


Pensando
Naples Gen 1
SoC



Mellanox company(Acquired by Nvidia)
ConnectX(multicore SmartNIC)
Innova SmartNIC(based on Xilinx Fpga)
BlueField2(FPGA-Augmented)
Bluefield-2 actually belongs to the SoC branch, meaning it is less performant but much more programmable. It is “so programmable” that it can actually be thought of as a Server in your Server. It has an 8-core ARM64 CPU, 8–16GB DRAM, up to 2x100G Ethernet interface + 1Gb OOB (out-of-band) management port and&#x2F;or UART (universal asynchronous receiver-transmitter) to do the same. It can also be connected to an IPMI network to monitor it just like a server. Bluefield-2 DPU runs its own Linux distribution on top and has a Mellanox ConnectX-6 Ethernet module. It also supports DPDK and even P4 via the DOCA SDK specifically tailored to program the Bluefield via “native C” libraries and API





]]></content>
      <categories>
        <category>hardware</category>
        <category>smartnic</category>
      </categories>
      <tags>
        <tag>fpga</tag>
        <tag>smartnic</tag>
      </tags>
  </entry>
  <entry>
    <title>hardware-usb</title>
    <url>/2022/04/12/hardware-usb/</url>
    <content><![CDATA[Usb ControllerA Universal Serial Bus (USB) host controller is an interface that allows an enabled piece of hardware to interact and communicate with a particular piece of software. The USB connection is an input and output port that comes standard with most computers and a variety of other digital equipment that allows data to be transmitted through a cable or any other form of direct connection. A USB host controller manages the communication between peripheral devices and the computer system. Most modern computers have hi-speed host controllers and many older computers can have a host controller easily installed in an open slot on its motherboard
There are several types of USB host controller interface that support different types of USB ports. The open host controller interface (OHCI) is the standard for most and even supports USB 1.1. The universal host controller interface (UHCI) from Intel® supports USB 1 in both fast and slow speeds. Other types of host controller interfaces include the enhanced host controller interface (EHCI)USB2.0, which is rated super-fast by publicly specified standards, and the newest host controller standard, called the extensible host controller interface (xHCI) USB3.0. The xHCI has been designed for improved speed, power, and efficiency than its predecessors.



Usb Device(itself) layoutA Universal Serial Bus (USB) device defines its capabilities and features through configurations, interfaces(function), alternate settings, and endpoints.
A USB configuration defines the capabilities and features of a device, mainly its power capabilities and interfaces. The device can have multiple configurations, but only one is active at a time. The active configuration isn’t chosen by the USB driver stack, but might be initiated by an application, a driver, the device driver. The device driver selects an active configuration.
A configuration can have one or more USB interfaces that define the functionality of the device. Typically, there is a one-to-one correlation between a function and an interface. However, certain devices expose multiple interfaces related to one function. In that case, the device can have an interface association descriptor (IAD). An IAD groups together interfaces that belong to a particular function.
Each interface contains one or more endpoints, which are used to transfer data to and from the device. In addition, the interface contains alternate settings that define the bandwidth requirements of the function associated with the interface.
To sum up, a group of endpoints form an interface, and a set of interfaces constitutes a configuration in the device.

For multifunction devices, the device has multiple interfaces. To use a particular function or an interface, the client driver selects the interface and an associated alternate setting. Consider a multi-function USB device such as a webcam. The device has two functions, video-capture (camera) and audio input (microphone). The device defines an endpoint in a video interface that streams video.


Interface 0 has three alternate settings. Only one of the alternate settings is active at any given time. Notice that Alternate Setting 0 doesn’t use an endpoint, whereas Alternate Settings 1 and 2 use Endpoint 1. Interface 1 has two alternate settings. Similar to Interface 0, Alternate Setting 0 doesn’t use an endpoint. Alternate Setting 1 is defined to use Endpoint 1.

Endpoints can’t be shared between two interfaces within a configuration. The device uses the endpoint address to determine the target endpoint for a data transfer or endpoint operation, such as pipe reset. All those operations are initiated by the host.
use device layout
EndPoint and pipesAn endpoint is a buffer on a USB device. Endpoint is a term that relates to the hardware itself, independent of the host operating system. The host can send and receive data to or from that buffer. Endpoints can be categorized into control and data endpoints.
Every USB device must provide at least one control endpoint at address 0 called the default endpoint or Endpoint0. This endpoint is bidirectional. that is, the host can send data to the endpoint and receive data from it within one transfer. The purpose of a control transfer is to enable the host to obtain device information, configure the device, or perform control operations that are unique to the device.
Data endpoints are optional and used for transferring data. They are unidirectional, has a type (control, interrupt, bulk, isochronous) and other properties. All those properties are described in an endpoint descriptor (see Standard USB descriptors).
Most Usb Devices have at least three endpoints, Control, Data In, Data OUT endpoints, but can be MORE
Use endpoints and pipes
**PipeData is transferred between a USB device and the USB host through an abstraction called a pipe. Pipes is purely a software term. A pipe talks to an endpoint on a device, and that endpoint has an address. The other end of a pipe is always the host controller.
An unconfigured endpoint is called an endpoint while a configured endpoint is called a pipe.

FAQHow many usb devices does a system support?controller endpoints limit
Usb hub vs Usb Switchhub vs swtich
]]></content>
      <categories>
        <category>hardware</category>
        <category>usb</category>
      </categories>
      <tags>
        <tag>usb</tag>
        <tag>HCI</tag>
      </tags>
  </entry>
  <entry>
    <title>go-lib</title>
    <url>/2022/07/01/go-lib/</url>
    <content><![CDATA[stdstringsPackage strings implements simple functions to manipulate UTF-8 encoded strings. 
- func Contains(s, substr string) bool- func ContainsAny(s, chars string) bool any Unicode code points in chars are within s. - func Count(s, substr string) int- func Fields(s string) []string   splited by whitespace- func HasPrefix(s, prefix string) bool whether the string s begins with prefix.- func HasSuffix(s, suffix string) bool- func Index(s, substr string) first instance of substr in s, or -1 if substr is not present in s. - func IndexAny(s, chars string) int- func LastIndex(s, substr string) int- func LastIndexAny(s, chars string) int- func Join(elems []string, sep string) string- func Split(s, sep string) []string any separator!!!- func Map(mapping func(rune) rune, s string) string //Map returns a copy of the string s with all its characters modified according to the mapping function. If mapping returns a negative value, the character is dropped from the string with no replacement.**  - func Replace(s, old, new string, n int) string- func ReplaceAll(s, old, new string) string- func ToLower(s string) string- func ToUpper(s string) string- func Trim(s, cutset string) string// // Trim returns a slice of the string s with all leading and trailing Unicode code points contained in cutset removed- func TrimLeft(s, cutset string) string//returns a slice of the string s with all leading Unicode code(check one by one) points contained in cutset removed.**- func TrimRight(s, cutset string) string- func TrimSpace(s string) string- func TrimSuffix(s, suffix string) string- func TrimPrefix(s, prefix string) string//prefix as a whole to check- func TrimSpace(s string) string//TrimSpace returns a slice of the string s, with all leading and trailing white space removed, as defined by Unicode.

Note

Trim, TrimLeft, TrimRight, check ‘each character’ in the cutset
TrimSuffix, TrimPrefix check ‘suffix and prefix’ as a whole not a set”



package mainimport (    &quot;fmt&quot;    &quot;strings&quot;)func main() &#123;    s := &quot;no C K dno&quot;    fmt.Printf(&quot;string: %s\n&quot;, s)    fmt.Printf(&quot;hasprefix: %v, hassuffix: %v\n&quot;, strings.HasPrefix(s, &quot;no&quot;), strings.HasSuffix(s, &quot;dno&quot;))    fmt.Printf(&quot;split by `o`: %v, fields(split by space): %v\n&quot;, strings.Split(s, &quot;o&quot;), strings.Fields(s))    fmt.Printf(&quot;lower: %s, upper: %s, title: %s\n&quot;, strings.ToLower(s), strings.ToUpper(s), strings.Title(s))    fmt.Printf(&quot;contains?: %v\n&quot;, strings.ContainsAny(s, &quot;CK&quot;))    fmt.Printf(&quot;trim: %v, left: %v, right: %v, prefix: %v\n&quot;, strings.Trim(s, &quot;no&quot;), strings.TrimLeft(s, &quot;no&quot;), strings.TrimRight(s, &quot;no&quot;), strings.TrimPrefix(s, &quot;no&quot;))&#125;main()

string: no C K dno
hasprefix: true, hassuffix: true
split by `o`: [n  C K dn ], fields(split by space): [no C K dno]
lower: no c k dno, upper: NO C K DNO, title: No C K Dno
contains?: true
trim:  C K d, left:  C K dno, right: no C K d, prefix:  C K dno

bytes(manipulation of byte slices)Package bytes implements functions for the manipulation of byte slices. It is similar to the facilities of the strings package.
Package strings implements simple functions to manipulate UTF-8 encoded strings. 
- func Contains(b, subslice []byte) bool- func ContainsAny(b []byte, chars string) bool- func Count(s, sep []byte) int- func Equal(a, b []byte) bool- func HasPrefix(s, prefix []byte) bool- func HasSuffix(s, suffix []byte) bool- func Index(s, sep []byte) int //sep as a whole- func IndexAny(s []byte, chars string) int- func LastIndex(s, sep []byte) int- func LastIndexAny(s []byte, chars string) int- func Join(s [][]byte, sep []byte) []byte- func Split(s, sep []byte) [][]byte- func Map(mapping func(r rune) rune, s []byte) []byte- func Replace(s, old, new []byte, n int) []byte // old and new as a whole- func ReplaceAll(s, old, new []byte) []byte- func ToLower(s []byte) []byte- func ToUpper(s []byte) []byte- func TrimLeft(s, cutset []byte) []byte- func TrimRight(s, cutset []byte) []byte- func TrimSpace(s []byte) []byte- func TrimSuffix(s, suffix []byte) []byte // suffix as a whole- func TrimPrefix(s, prefix []byte) []byte

container(list)Package list implements a doubly linked list. 
import (    &quot;container/list&quot;    &quot;fmt&quot;)type Person struct &#123;    name string&#125;func listDemo() &#123;    // Create a new list and put some numbers in it.    l := list.New()    e4 := l.PushBack(Person&#123;&quot;d&quot;&#125;)    e1 := l.PushFront(Person&#123;&quot;a&quot;&#125;)    l.InsertBefore(Person&#123;&quot;c&quot;&#125;, e4)    l.InsertAfter(Person&#123;&quot;b&quot;&#125;, e1)    // Iterate through list and print its contents.    for e := l.Front(); e != nil; e = e.Next() &#123;        fmt.Println(e.Value)    &#125;            // Create a new list and put some numbers in it.    l = l.Init()    e4 = l.PushBack(4)    e1 = l.PushFront(1)    l.InsertBefore(3, e4)    l.InsertAfter(2, e1)    // Iterate through list and print its contents.    for e := l.Front(); e != nil; e = e.Next() &#123;        fmt.Println(e.Value)    &#125;&#125;listDemo()

&#123;a&#125;
&#123;b&#125;
&#123;c&#125;
&#123;d&#125;
1
2
3
4

regexPackage regexp implements regular expression search. 
There are 16 methods of Regexp that match a regular expression and identify the matched text. Their names are matched by this regular expression:Find(All)?(String)?(Submatch)?(Index)?
If ‘All’ is present, the routine matches successive non-overlapping matches of the entire expression. Empty matches abutting a preceding match are ignored. The return value is a slice containing the successive return values of the corresponding non-&#39;All&#39; routine. These routines take an extra integer argument, n. If n &gt;&#x3D; 0, the function returns at most n matches/submatches; otherwise, it returns all of them.
If ‘String’ is present, the argument is a string; otherwise it is a slice of bytes; return values are adjusted as appropriate.
If ‘Submatch’ is present, the return value is a slice identifying the successive submatches of the expression. Submatches are matches of parenthesized subexpressions (also known as capturing groups) within the regular expression, numbered from left to right in order of opening parenthesis. Submatch 0 is the match of the entire expression, submatch 1 the match of the first parenthesized subexpression, and so on.
If ‘Index’ is present, matches and submatches are identified by byte index pairs within the input string: result[2n:2n+1] identifies the indexes of the nth submatch. The pair for n&#x3D;&#x3D;0 identifies the match of the entire expression. If ‘Index’ is not present, the match is identified by the text of the match&#x2F;submatch. If an index is negative or text is nil, it means that subexpression did not match any string in the input. For ‘String’ versions an empty string means either no match or an empty match. 
func (re *Regexp) FindAll(b []byte, n int) [][]bytefunc (re *Regexp) FindAllIndex(b []byte, n int) [][]intfunc (re *Regexp) FindAllString(s string, n int) []stringfunc (re *Regexp) FindAllStringIndex(s string, n int) [][]intfunc (re *Regexp) FindAllStringSubmatch(s string, n int) [][]stringfunc (re *Regexp) FindAllStringSubmatchIndex(s string, n int) [][]intfunc (re *Regexp) FindAllSubmatch(b []byte, n int) [][][]bytefunc (re *Regexp) FindAllSubmatchIndex(b []byte, n int) [][]intfunc (re *Regexp) Find(b []byte) []bytefunc (re *Regexp) FindIndex(b []byte) (loc []int)func (re *Regexp) FindString(s string) stringfunc (re *Regexp) FindStringIndex(s string) (loc []int)func (re *Regexp) FindStringSubmatch(s string) []stringfunc (re *Regexp) FindStringSubmatchIndex(s string) []intfunc (re *Regexp) FindSubmatch(b []byte) [][]bytefunc (re *Regexp) FindSubmatchIndex(b []byte) []intfunc (re *Regexp) Match(b []byte) boolfunc (re *Regexp) MatchString(s string) boolfunc (re *Regexp) ReplaceAll(src, repl []byte) []bytefunc (re *Regexp) ReplaceAllLiteral(src, repl []byte) []bytefunc (re *Regexp) ReplaceAllLiteralString(src, repl string) stringfunc (re *Regexp) ReplaceAllString(src, repl string) string

Frequently used  

*func (re Regexp) FindString(s string) string

FindString returns a string holding the text of the leftmost match in s of the regular expression. If there is no match, the return value is an empty string, but it will also be empty if the regular expression successfully matches an empty string. Use FindStringIndex or FindStringSubmatch if it is necessary to distinguish these cases.


*func (re Regexp) FindStringIndex(s string) (loc []int)

FindStringIndex returns a two-element slice of integers defining the location of the leftmost match in s of the regular expression. The match itself is at s[loc[0]:loc[1]]. A return value of nil indicates no match.


*func (re Regexp) MatchString(s string) bool

MatchString reports whether the string s contains any match of the regular expression re.


*func (re Regexp) ReplaceAllLiteralString(src, repl string) string

ReplaceAllLiteralString returns a copy of src, replacing matches of the Regexp with the replacement string repl. The replacement repl is substituted directly, without using Expand


*func (re Regexp) ReplaceAllString(src, repl string) string

ReplaceAllString returns a copy of src, replacing matches of the Regexp with the replacement string repl. Inside repl, $ signs are interpreted as in Expand, so for instance $1 represents the text of the first submatch.


*func (re Regexp) FindStringSubmatch(s string) []string

FindStringSubmatch returns a two-element slice of strings holding the text of the leftmost match of the regular expression in s and the matches, if any, of its subexpressions, as defined by the ‘Submatch’ description in the package comment. A return value of nil indicates no match.


*func (re Regexp) FindStringSubmatchIndex(s string) []int

FindStringSubmatchIndex returns a two-element slice holding the index pairs identifying the leftmost match of the regular expression in s and the matches, if any, of its subexpressions, as defined by the ‘Submatch’ and ‘Index’ descriptions in the package comment. A return value of nil indicates no match.



package demo_regeximport (    &quot;fmt&quot;    &quot;regexp&quot;)func regexDemo() &#123;    re := regexp.MustCompile(`foo.?`) // pattern must be in `` not &#x27;&#x27;!!!    fmt.Printf(&quot;%q\n&quot;, re.FindString(&quot;seafood fool&quot;))    fmt.Printf(&quot;%q\n&quot;, re.FindString(&quot;meat&quot;))        fmt.Printf(&quot;%v\n&quot;, re.FindStringIndex(&quot;seafood fool&quot;))    fmt.Printf(&quot;%v\n&quot;, re.FindStringIndex(&quot;meat&quot;))        fmt.Printf(&quot;%v\n&quot;, re.MatchString(&quot;seafood fool&quot;))    fmt.Printf(&quot;%v\n&quot;, re.MatchString(&quot;meat&quot;))        re = regexp.MustCompile(`a(x+)b`)        fmt.Println(re.ReplaceAllLiteralString(&quot;-ab-axxb-&quot;, &quot;T&quot;))    fmt.Println(re.ReplaceAllLiteralString(&quot;-ab-axxb-&quot;, &quot;$&#123;1&#125;&quot;))        fmt.Println(re.ReplaceAllString(&quot;-ab-axxb-&quot;, &quot;T&quot;))    fmt.Println(re.ReplaceAllString(&quot;-ab-axxb-&quot;, &quot;$&#123;1&#125;&quot;)) // $&#123;1&#125; submatch (the first group)        fmt.Println(re.FindStringSubmatch(&quot;-ab-axxb-&quot;))    fmt.Println(re.FindStringSubmatchIndex(&quot;-ab-axxb-&quot;))&#125;regexDemo()

&quot;food&quot;
&quot;&quot;
[3 7]
[]
true
false
-ab-T-
-ab-$&#123;1&#125;-
-ab-T-
-ab-xx-
[axxb xx]
[4 8 5 7]

exec(run shell from go)Package exec runs external commands. It wraps os.StartProcess to make it easier to remap stdin and stdout, connect I&#x2F;O with pipes, and do other adjustments.
Unlike the “system” library call from C and other languages, the os/exec package intentionally does not invoke the system shell and does not expand any glob patterns or handle other expansions, pipelines, or redirections typically done by shells. The package behaves more like C’s “exec” family of functions. To expand glob patterns, either call the shell directly, taking care to escape any dangerous input, or use the path&#x2F;filepath package’s Glob function. To expand environment variables, use package os’s ExpandEnv. 
sync method, goroutine blocks

cmd.Run(): output is saved to cmd.Stdout and cmd.Stderr, by default discard, you can set them with buffer to get the output
cmd.Output(): only return stdout
cmd.CombinedOutput(): return stdout and stderr

async method

func (c *Cmd) Start() error
Start starts a separate process to run  the specified command but does not wait for it to complete. If Start returns successfully, the c.Process field will be set. A child process to run command




The Wait method will return the exit code and release associated resources once the command exits. 


func (c *Cmd) Wait() error
Wait waits for the command to exit and waits for any copying to stdin or copying from stdout or stderr to complete.The command must have been started by Start.



package mainimport (	&quot;log&quot;	&quot;os/exec&quot;)func main() &#123;	cmd := exec.Command(&quot;sleep&quot;, &quot;5&quot;)	err := cmd.Start() // a child process is created to run command if no error happens	if err != nil &#123;		log.Fatal(err)	&#125;	log.Printf(&quot;Waiting for command to finish...&quot;)	err = cmd.Wait()	log.Printf(&quot;Command finished with error: %v&quot;, err)&#125;

discard outputpackage mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;os/exec&quot;    &quot;runtime&quot;)func main() &#123;    // check binary exist before run    path, err := exec.LookPath(&quot;ls&quot;)    if err != nil &#123;        fmt.Printf(&quot;didn&#x27;t find &#x27;ls&#x27; executable\n&quot;)    &#125; else &#123;        fmt.Printf(&quot;&#x27;ls&#x27; executable is in &#x27;%s&#x27;\n&quot;, path)    &#125;        cmd := exec.Command(&quot;ls&quot;, &quot;-lah&quot;)    if runtime.GOOS == &quot;windows&quot; &#123;        cmd = exec.Command(&quot;tasklist&quot;)    &#125;        err := cmd.Run()    if err != nil &#123;      log.Fatalf(&quot;cmd.Run() failed with %s\n&quot;, err)    &#125;&#125;main()

&#39;ls&#39; executable is in &#39;/usr/bin/ls&#39;

print output to consolepackage mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;os&quot;    &quot;os/exec&quot;    &quot;runtime&quot;)func main() &#123;    cmd := exec.Command(&quot;ls&quot;, &quot;-lah&quot;)    if runtime.GOOS == &quot;windows&quot; &#123;        cmd = exec.Command(&quot;tasklist&quot;)    &#125;    // print output to console    cmd.Stdout = os.Stdout    cmd.Stderr = os.Stderr    err := cmd.Run() // only return error    if err != nil &#123;      log.Fatalf(&quot;cmd.Run() failed with %s\n&quot;, err)    &#125;&#125;main()

total 476K
drwxr-xr-x  4 root root  165 Jul  1 17:23 .
drwxr-xr-x 16 root root  242 May 30 16:48 ..
drwxr-xr-x  3 root root   54 Jul  1 15:07 debug
-rw-r--r--  1 root root 102K Jul  1 09:18 go_advanced.ipynb
-rw-r--r--  1 root root 105K Jul  1 14:42 Go_basics.ipynb
-rw-r--r--  1 root root  13K Jun 30 11:27 go-context.ipynb
-rw-r--r--  1 root root 185K Jul  1 17:23 Go_lib.ipynb
-rw-r--r--  1 root root  57K Jun 30 10:23 Go_network_rpc.ipynb
drwxr-xr-x  2 root root  181 Jun 30 11:08 .ipynb_checkpoints

capture output(stdout&#x2F;stderr) with one bufferpackage mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;os/exec&quot;    &quot;runtime&quot;)func main() &#123;    cmd := exec.Command(&quot;ls&quot;, &quot;-l&quot;, &quot;-a&quot;, &quot;-h&quot;)    if runtime.GOOS == &quot;windows&quot; &#123;        cmd = exec.Command(&quot;tasklist&quot;)    &#125;        // out, err := cmd.CombinedOutput() //combine(stderr and stdout)    out, err := cmd.Output() // only stdout    if err != nil &#123;        log.Fatalf(&quot;cmd.Run() failed with %s\n&quot;, err)    &#125;    fmt.Printf(&quot;out:\n%s\n&quot;, string(out))&#125;main()

out:
total 476K
drwxr-xr-x  4 root root  165 Jul  1 17:23 .
drwxr-xr-x 16 root root  242 May 30 16:48 ..
drwxr-xr-x  3 root root   54 Jul  1 15:07 debug
-rw-r--r--  1 root root 102K Jul  1 09:18 go_advanced.ipynb
-rw-r--r--  1 root root 105K Jul  1 14:42 Go_basics.ipynb
-rw-r--r--  1 root root  13K Jun 30 11:27 go-context.ipynb
-rw-r--r--  1 root root 185K Jul  1 17:23 Go_lib.ipynb
-rw-r--r--  1 root root  57K Jun 30 10:23 Go_network_rpc.ipynb
drwxr-xr-x  2 root root  181 Jun 30 11:08 .ipynb_checkpoints

capture output(stdout and stderr) with two bufferpackage mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;os/exec&quot;    &quot;runtime&quot;    &quot;bytes&quot;)func main() &#123;    cmd := exec.Command(&quot;ls&quot;, &quot;-lah&quot;)    var stdout, stderr bytes.Buffer    cmd.Stdout = &amp;stdout    cmd.Stderr = &amp;stderr        err := cmd.Run()    if err != nil &#123;        log.Fatalf(&quot;cmd.Run() failed with %s\n&quot;, err)    &#125;    outStr, errStr := stdout.String(), stderr.String()    fmt.Printf(&quot;out:\n%s\nerr:\n%s\n&quot;, outStr, errStr)&#125;main()

out:
total 476K
drwxr-xr-x  4 root root  165 Jul  1 17:23 .
drwxr-xr-x 16 root root  242 May 30 16:48 ..
drwxr-xr-x  3 root root   54 Jul  1 15:07 debug
-rw-r--r--  1 root root 102K Jul  1 09:18 go_advanced.ipynb
-rw-r--r--  1 root root 105K Jul  1 14:42 Go_basics.ipynb
-rw-r--r--  1 root root  13K Jun 30 11:27 go-context.ipynb
-rw-r--r--  1 root root 185K Jul  1 17:23 Go_lib.ipynb
-rw-r--r--  1 root root  57K Jun 30 10:23 Go_network_rpc.ipynb
drwxr-xr-x  2 root root  181 Jun 30 11:08 .ipynb_checkpoints

err:

capture output and with time outUse exec.CommandContext(), the os&#x2F;exec package handles checking the timeout and killing the process if it expires, user no need to kill the process by himself.
timeout example
package mainimport (    &quot;context&quot;    &quot;fmt&quot;    &quot;os/exec&quot;    &quot;time&quot;)func main() &#123;    // Create a new context and add a timeout to it    ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second)    defer cancel() // The cancel should be deferred so resources are cleaned up    // Create the command with our context    cmd := exec.CommandContext(ctx, &quot;ping&quot;, &quot;-c 4&quot;, &quot;-i 1&quot;, &quot;8.8.8.8&quot;)    // This time we can simply use Output() to get the result.    out, err := cmd.Output()    // We want to check the context error to see if the timeout was executed.    // The error returned by cmd.Output() will be OS specific based on what    // happens when a process is killed.    if ctx.Err() == context.DeadlineExceeded &#123;        fmt.Println(&quot;Command timed out&quot;)        return    &#125;    // If there&#x27;s no context error, we know the command completed (or errored).    fmt.Println(&quot;Output:&quot;, string(out))    if err != nil &#123;        fmt.Println(&quot;Non-zero exit code:&quot;, err)    &#125;&#125;main()

Command timed out

run command as daemonThat means after c.Process starts, we should release it from parent, hence init takes it as its parent, command runs daemon.
import &quot;os/exec&quot;import &quot;time&quot;func Daemon() &#123;    cmd := exec.Command(&quot;sleep&quot;, &quot;100&quot;)    cmd.Stdin = nil    cmd.Stdout = nil    cmd.Stderr = nil    cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123;Setsid: true&#125;    err := cmd.Start() // runs in another process(child)    // I&#x27;m here in parent    if err == nil &#123; // no error happens, a child process is created        fmt.Println(&quot;trying to release child process after 30s&quot;)        time.Sleep(30 * time.Second)        fmt.Println(&quot;child process released, command runs in daemon&quot;)        cmd.Process.Release() // release it to make it orphan, init process takes it        // parent exits but sleep is still running        fmt.Println(&quot;parent exited, sleep cmd is still running in separate process&quot;)        os.Exit(0)    &#125;&#125;func main() &#123;    Daemon()&#125;


repl.go:9:24: undefined &quot;syscall&quot; in syscall.SysProcAttr &lt;*ast.SelectorExpr&gt;

timePackage time provides functionality for measuring and displaying time. 
import &quot;time&quot;import &quot;fmt&quot;func handle(int)&#123;&#125;func statusUpdate() string &#123; return &quot;&quot; &#125;func timeDemo1() &#123;    // 10 minute 10 * Time.Minute    // 10s       10 * time.Second    // 100ms     100 * time.Millisecond        time.Sleep(1 * time.Second)    fmt.Println(time.Now())    fmt.Println(time.Weekday())        start := time.Now()    time.Sleep(10 * time.Millisecond)    end := time.Now()    // check whether end is before or after start        fmt.Println(end.After(start))    fmt.Println(end.Before(start))    fmt.Println(end.Unix())     // seconds since 1970            // peridical timeout, write time to channel every 5s    // can shutdown ticker at any time        ticker := time.NewTicker(time.Second) // signal tick every 1s    defer ticker.Stop()    done := make(chan struct&#123;&#125;)    go func() &#123;        //send true after 3 s        time.Sleep(3 * time.Second)        done &lt;- struct&#123;&#125;&#123;&#125;    &#125;()        for &#123;        select &#123;        case &lt;-done:            fmt.Println(&quot;Done!&quot;)            return            case t := &lt;-ticker.C: //ticker.C is a channel: chan time            fmt.Println(&quot;Current time: &quot;, t)        &#125;    &#125;&#125;timeDemo1()//timeDemo2()

2022-07-01 17:23:20.113770275 +0800 CST m=+6926.821548718
Sunday
true
false
1656667400
Current time:  2022-07-01 17:23:21.124828279 +0800 CST m=+6927.832606722
Current time:  2022-07-01 17:23:22.124729486 +0800 CST m=+6928.832507925
Current time:  2022-07-01 17:23:23.124739482 +0800 CST m=+6929.832517925
Done!

Periodically timeoutfunc NewTicker(d Duration) *Ticker                                                                                          

NewTicker returns a new Ticker containing a channel that will send the time on the channel after each tick. The period of the ticks is specified by the duration argument. The ticker will adjust the time interval or drop ticks to make up for slow receivers. The duration d must be greater than zero; if not, NewTicker will panic. Stop the ticker to release associated resources.

func Tick(d Duration) &lt;-chan Time                                                                                                 

Tick is a convenience wrapper for NewTicker providing access to the ticking channel only. While Tick is useful for clients that have no need to shut down the Ticker, be aware that without a way to shut it down the underlying Ticker cannot be recovered by the garbage collector; it &quot;leaks&quot;. Unlike NewTicker, Tick will return nil if d &lt;&#x3D; 0.

import &quot;time&quot;import &quot;fmt&quot;func timeDemo() &#123;    ticker := time.NewTicker(time.Second) // signal tick every 1s    defer ticker.Stop()        done := make(chan bool)    go func() &#123;        time.Sleep(1 * time.Second)        done &lt;- true    &#125;()    var ok bool    for &#123;        if ok &#123;            break        &#125;        select &#123;            case ok =&lt;-done:            fmt.Println(&quot;done&quot;)            case t := &lt;-ticker.C: //ticker.C is a channel: chan time            fmt.Println(&quot;current time: &quot;, t)        &#125;    &#125;        // peridical timeout, write time to channel every 1s    // you can&#x27;t shutdown the ticker as no way to know the Ticker instance!!!    t := time.Tick(1 * time.Second)    i := 0    for next := range t &#123;        fmt.Printf(&quot;%v\n&quot;, next)        if i++; i == 2 &#123;            break        &#125;    &#125;&#125;func main() &#123;    timeDemo()&#125;main()

current time:  2022-07-01 17:23:24.126393711 +0800 CST m=+6930.834172154
done
2022-07-01 17:23:25.12738211 +0800 CST m=+6931.835160502
2022-07-01 17:23:26.127205432 +0800 CST m=+6932.834983825

Once timeoutimport &quot;time&quot;import &quot;fmt&quot;func handle(n int) &#123;&#125;func timeDemo() &#123;    //one time timeout    var c chan int    select &#123;    case m := &lt;-c:        handle(m)    case &lt;-time.After(1 * time.Second):        fmt.Println(&quot;timed out&quot;)    &#125;&#125;func main() &#123;    timeDemo()&#125;main()

timed out

timestamppackage mainimport (    &quot;fmt&quot;    &quot;time&quot;)func main() &#123;    fmt.Println(time.Now())    timeUnix := time.Now().Unix()         //second    timeUnixNano := time.Now().UnixNano() //nano second    fmt.Println(timeUnix)    fmt.Println(timeUnixNano)    // convert timestamp to human readable    fmt.Println(time.Unix(timeUnix, 0))    // convert timestamp to human readable with format specified, must this value 2006-01-02 15:04:05!!!    fmt.Println(time.Unix(timeUnix, 0).Format(&quot;2006-01-02 15:04:05&quot;))&#125;main()

2022-07-01 17:23:27.129936571 +0800 CST m=+6933.837715027
1656667407
1656667407129992154
2022-07-01 17:23:27 +0800 CST
2022-07-01 17:23:27

sorting with GoPackage sort provides primitives for sorting slices and user-defined collections, you can provides less function for custom types.
sort for int, float, stringimport (    &quot;fmt&quot;    &quot;sort&quot;)func sortDemo() &#123;    s := []int&#123;5, 2, 6, 3, 1, 4&#125; // unsorted    sort.Ints(s) // sorted in increasing order    fmt.Println(s, sort.IntsAreSorted(s))        //Sort string in increasing order    s := []string&#123;&quot;Bravo&quot;, &quot;Gopher&quot;, &quot;Alpha&quot;, &quot;Grin&quot;, &quot;Delta&quot;&#125;    sort.Strings(s)    fmt.Println(s)        // sort slice of any type!!!&#125;sortDemo()

[1 2 3 4 5 6] true
[Alpha Bravo Delta Gopher Grin]

sorting sliceAny type of slice, you must provide less functionfunc SliceStable(x interface&#123;&#125;, less func(i, j int) bool)func Slice(x interface&#123;&#125;, less func(i, j int) bool)
Slice sorts the slice x given the provided less function. It panics if x is not a slice.
The sort is not guaranteed to be stable: equal elements may be reversed from their original order. For a stable sort, use SliceStable.
The less function must satisfy the same requirements as the Interface type’s Less method.Less reports whether the element with index i must sort before the element with index j, less returns true means i must be sorted firstly
package mainimport (    &quot;fmt&quot;    &quot;sort&quot;)func main() &#123;    people := []struct &#123;        Name string        Age  int    &#125;&#123;        &#123;&quot;Gopher&quot;, 7&#125;,        &#123;&quot;Alice&quot;, 55&#125;,        &#123;&quot;Vera&quot;, 24&#125;,        &#123;&quot;Bob&quot;, 75&#125;,    &#125;    sort.Slice(people, func(i, j int) bool &#123; return people[i].Name &lt; people[j].Name &#125;)    fmt.Println(&quot;By name:&quot;, people)    sort.Slice(people, func(i, j int) bool &#123; return people[i].Age &lt; people[j].Age &#125;)    fmt.Println(&quot;By age:&quot;, people)&#125;main()

By name: [&#123;Alice 55&#125; &#123;Bob 75&#125; &#123;Gopher 7&#125; &#123;Vera 24&#125;]
By age: [&#123;Gopher 7&#125; &#123;Vera 24&#125; &#123;Alice 55&#125; &#123;Bob 75&#125;]

encodingPackage encoding defines interfaces shared by other packages that convert data to and from byte-level and textual representations. like encoding/xml  encoding/base64 encoding/hex etcbase64

func (enc *Encoding) Decode(dst, src []byte) (n int, err error)

func (enc *Encoding) DecodeString(s string) ([]byte, error)

func (enc *Encoding) Encode(dst, src []byte)

func (enc *Encoding) EncodeToString(src []byte) string


hex

func Decode(dst, src []byte) (int, error)

func DecodeString(s string) ([]byte, error)

func Encode(dst, src []byte) int

func EncodeToString(src []byte) string


import (    &quot;encoding/base64&quot;    &quot;encoding/hex&quot;    &quot;fmt&quot;)func base64Demo() &#123;    sc := [] byte(&quot;hello&quot;)    encode := base64.StdEncoding.EncodeToString(sc)    //base64.URLEncoding    fmt.Println(encode)    sc, _ := base64.StdEncoding.DecodeString(encode)    fmt.Println(string(sc))&#125;func hexDemo() &#123;    sc := [] byte(&quot;hello&quot;)    encode := hex.EncodeToString(sc)        fmt.Println(encode)    sc, _ := hex.DecodeString(encode)    fmt.Println(string(sc))&#125;base64Demo()hexDemo()

aGVsbG8=
hello
68656c6c6f
hello

jsonGo supports encode&#x2F;decode any type to&#x2F;from json, but most of time, we use map or struct for json encoding and decoding.
Here are frequently used APIs provided by encoding/json package.  
func Marshal(v interface&#123;&#125;) ([]byte, error)

Marshal traverses the value v recursively.

Encoding Note  

The encoding of each struct field can be customized by the format string stored under the “json” key in the struct field’s tag. The format string gives the name of the field, possibly followed by a comma-separated list of options. The name may be empty in order to specify options without overriding the default field name.  


&#x2F;&#x2F; Field appears in JSON as key “myName”.Field int json:&quot;myName&quot;  


&#x2F;&#x2F; Field appears in JSON as key “myName” and&#x2F;&#x2F; the field is omitted from the object if its value is empty,&#x2F;&#x2F; empty in GO means default value of each type&#x2F;&#x2F; integer: 0&#x2F;&#x2F; string: “”&#x2F;&#x2F; pointer: nil&#x2F;&#x2F; slice: has zero elements&#x2F;&#x2F; omitempty: means if it’s not set or set with default value, these fileds are not marshed or unmarshed!!!Field int json:&quot;myName,omitempty&quot;  


&#x2F;&#x2F; Field is ignored by this package.Field int json:&quot;-&quot;


The map’s key type must either be a string, an integer typemap[string]interface{} to store arbitrary JSON objects, []interface{} to store arbitrary JSON arrays.


Pointer values encode as the value pointed to. A nil pointer encodes as the null JSON value.

func MarshalIndent(v interface&#123;&#125;, prefix, indent string) ([]byte, error)

MarshalIndent is like Marshal but applies Indent to format the output. Each JSON element in the output will begin on a new line beginning with prefix followed by one or more copies of indent according to the indentation nesting.

func Unmarshal(data []byte, v interface&#123;&#125;) error

Unmarshal parses the JSON-encoded data and stores the result in the value pointed to by vUnmarshal uses the inverse of the encodings that Marshal uses, allocating maps, slices, and pointers as necessary, If v is nil or not a pointer, Unmarshal returns an InvalidUnmarshalError.

To unmarshal JSON into a struct, Unmarshal matches incoming object keys to the keys used by Marshal (either the struct field name or its tag), preferring an exact match but also accepting a case-insensitive match. By default, object keys which don’t have a corresponding struct field are ignored.
map json key to struct field

For a given JSON key Foo, Unmarshal will attempt to match the struct fields in this order:
an exported (public) field with a struct tag json:”Foo”,
an exported field named Foo, or
an exported field named FOO, FoO, or some other case-insensitive match.



Only fields that are found in the destination type will be decoded, any unexported fields in the destination struct will be unaffected.
encode—&gt; Marshal()   —&gt; to json stringdecode—&gt; Unmarshal() —&gt; to struct or map
What to use (Structs vs maps)As a general rule of thumb, if you can use structs to represent your JSON data, you should use them. The only good reason to use maps would be if it were not possible to use structs due to the uncertain nature of the keys or values in the data.
If we use maps, we will either need each of the keys to have the same data type, or use a generic type and convert it later.
NOTE  

Only exported field are encode and marshaled

json.Marshal(obj), json.Marshal(&amp;obj) both are ok 

 json.Unmarshal(&amp;obj) must use pointer as parameter, the pointer must not be nil!!!

json.NewEncoder(os.Stdout).Encode(obj), json.NewEncoder(os.Stdout).Encode(&amp;obj) both are ok, write encoded data from obj to os.Stdout.

json.Decoder(os.Stdin).Decode(&amp;obj) must use pointer as parameter, read decoded data from os.Stdin to obj

Marshal is used to encode struct/map to string while Encode() is used to encode struct/map and write to io.Writer, similar thing happens for Unmarshal and Decode().

You can Marshal from one struct, but Unmarshal to another struct, but only the ‘matched’ field are assigned, others use default, the ‘matched’ field must have same type!!! but the tag&#x2F;name obeys above rules.


Sugestion for json.Decoder() vs json.Unmarshal()

use json.Decoder if your data is coming from an io.Reader stream, or you need to decode multiple values from a stream of data

Use json.Unmarshal if you already have the JSON string in memory.


json.Decoder(io.Reader).Decode(&amp;obj) blocks if no data available, returns if EOF, error or data available. if io.Reader is a file, it reads them all to buffer, then UnMarshal, if io.Reader is a connection, receives data then Unmarshal()
Marshal a string, just quoted them, “” is added
s := &quot;hello&quot;buf1 := []byte(s) // buf1[0] == &#x27;h&#x27; len(buf1) == 5buf2, _:= json.Marshal(s)// buf[0] == &#x27;&quot;&#x27;, len(buf2)==7


package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;)type Person struct &#123;    Name string    Msg *string&#125;type Student struct &#123;    Name string    Id   int&#125;func main() &#123;    s := Student&#123;        Name: &quot;tom&quot;,        Id:   100,    &#125;    bytes, _ := json.Marshal(&amp;s)    fmt.Println(&quot;json string: &quot;,string(bytes))        // \r, \n is ignore when Unmarshal!!!    bytes = append(bytes, &#x27;\r&#x27;)    bytes = append(bytes, &#x27;\n&#x27;)    p := &amp;Person&#123;&#125;    err := json.Unmarshal(bytes, p)    if err != nil &#123;        fmt.Println(err)    &#125; else &#123;        fmt.Println(p.Name)    &#125;&#125;main()

json string:  &#123;&quot;Name&quot;:&quot;tom&quot;,&quot;Id&quot;:100&#125;
tom

import (    &quot;encoding/json&quot;    &quot;fmt&quot;    &quot;time&quot;)func json_struct_demo() &#123;        type FruitBasket struct &#123;        Name    string        Fruit   []string  // an array        Id      int64  `json:&quot;ref&quot;` // struct tag used as json key when encoding        private string   // An unexported field is not encoded, if you run with main()!!!        Created time.Time    &#125;        basket := FruitBasket&#123;        Name:    &quot;Standard&quot;,        Fruit:   []string&#123;&quot;Apple&quot;, &quot;Banana&quot;, &quot;Orange&quot;&#125;,        Id:      999,        private: &quot;Second-rate&quot;,        Created: time.Now(),    &#125;        var jsonData []byte    //jsonData, err := json.Marshal(basket) //encode return a slice of byte    jsonData, err := json.MarshalIndent(basket, &quot;&quot;, &quot;    &quot;)    if err != nil &#123;        fmt.Println(err)    &#125;    fmt.Println(string(jsonData)) //convert slice to a string        jd := []byte(`    &#123;        &quot;Name&quot;: &quot;Standard&quot;,        &quot;Fruit&quot;: [            &quot;Apple&quot;,            &quot;Banana&quot;,            &quot;Orange&quot;        ],        &quot;ref&quot;: 999,        &quot;Created&quot;: &quot;2018-04-09T23:00:00Z&quot;    &#125;`)            // when passed with struct, you must know the field of the json string    // so that it can be mapped to the proper filed of the struct!!!        var bt FruitBasket    err := json.Unmarshal(jd, &amp;bt) // must pass pointer!!!    if err != nil &#123;        fmt.Println(err)    &#125;    fmt.Println(bt.Name, bt.Fruit, bt.Id)    fmt.Println(bt.Created)        fmt.Printf(&quot;%+v\n&quot;, bt)&#125;json_struct_demo()

&#123;
    &quot;Name&quot;: &quot;Standard&quot;,
    &quot;Fruit&quot;: [
        &quot;Apple&quot;,
        &quot;Banana&quot;,
        &quot;Orange&quot;
    ],
    &quot;ref&quot;: 999,
    &quot;饞€竝rivate&quot;: &quot;Second-rate&quot;,
    &quot;Created&quot;: &quot;2022-07-01T17:23:27.137247781+08:00&quot;
&#125;
Standard [Apple Banana Orange] 999
2018-04-09 23:00:00 +0000 UTC
&#123;Name:Standard Fruit:[Apple Banana Orange] Id:999 饞€竝rivate: Created:2018-04-09 23:00:00 +0000 UTC&#125;

import (    &quot;encoding/json&quot;    &quot;fmt&quot;)func json_map_demo() &#123;       m := map[string]interface&#123;&#125;&#123;        &quot;Name&quot;: &quot;Eve&quot;,        &quot;Age&quot;:  6.0,        &quot;Parents&quot;: []interface&#123;&#125;&#123;            &quot;Alice&quot;,            &quot;Bob&quot;,            12,        &#125;,    &#125;    var jsonData []byte    //jsonData, err := json.Marshal(basket) //encode return a slice of byte    jsonData, err := json.MarshalIndent(m, &quot;&quot;, &quot;    &quot;)    if err != nil &#123;        fmt.Println(err)    &#125;    fmt.Println(string(jsonData)) //convert slice to a string                js := `&#123;&quot;name&quot;: &quot;battery sensor&quot;, &quot;capacity&quot;: 40&#125;`    var reading map[string]interface&#123;&#125; // any type of value, nil map here!        err := json.Unmarshal([]byte(js), &amp;reading)    if err == nil &#123;        fmt.Printf(&quot;%+v %v %v %T\n&quot;,                   reading, reading[&quot;name&quot;],                   reading[&quot;capacity&quot;],                   reading[&quot;capacity&quot;]) // type of number in json after parsed, it&#x27;s float64!!!    &#125;&#125;json_map_demo()

&#123;
    &quot;Age&quot;: 6,
    &quot;Name&quot;: &quot;Eve&quot;,
    &quot;Parents&quot;: [
        &quot;Alice&quot;,
        &quot;Bob&quot;,
        12
    ]
&#125;
map[capacity:40 name:battery sensor] battery sensor 40 float64

package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;    &quot;os&quot;)func main() &#123;    f, err := os.OpenFile(&quot;/tmp/json&quot;, os.O_CREATE|os.O_RDWR, 0644)    if err != nil &#123;        fmt.Println(&quot;failed to open file /tmp/json&quot;)        return    &#125;    defer f.Close()    ne := json.NewEncoder(f)    ev := struct &#123;        Name  string `json:&quot;name&quot;`        Score int    `json:&quot;score&quot;`    &#125;&#123;&quot;hi&quot;, 100&#125;    // offset moved to end of file    err = ne.Encode(ev)    if err != nil &#123;        fmt.Println(&quot;encode json failed&quot;)    &#125;    type Person struct &#123;        YourName  string `json:&quot;name&quot;`        YourScore int    `json:&quot;score&quot;`    &#125;    var dv Person    // move offset back to beginning of the file    _, err = f.Seek(0, 0)    if err != nil &#123;        fmt.Println(&quot;Seek back beginning failed&quot;)    &#125;    de := json.NewDecoder(f)    err = de.Decode(&amp;dv)    if err != nil &#123;        fmt.Println(&quot;decode from json failed&quot;)    &#125;    fmt.Printf(&quot;%+v\n&quot;, dv)&#125;main()

&#123;YourName:hi YourScore:100&#125;

omitemptyIf field is set to its default value, the key itself will be omitted from the JSON object. omitempty only works for Marshal() to string!!!
zero value(default) for each type  

0 for numeric types,
false for the boolean type
“” (the empty string) for string.
nil for pointer
len(map) &#x3D;&#x3D; 0
len(slice) &#x3D;&#x3D; 0

No default value for struct object, so omitempy has no effect for embeded object!!!
In cases where an empty value does not exist, omitempty is of no use. An embedded struct, for example, does not have an empty value:
package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;)type dimension struct &#123;    Height int    Width  int `json:&quot;,omitempty&quot;` // effect here&#125;type Dog struct &#123;    Breed    string    WeightKg int    Size     dimension `json:&quot;,omitempty&quot;` // no effect here!!!&#125;func main() &#123;    d := Dog&#123;        Breed: &quot;pug&quot;,        // dimension is not set but not omitted!!!    &#125;    b, _ := json.Marshal(d)    fmt.Println(string(b))&#125;

&#123;&quot;Breed&quot;:&quot;pug&quot;,&quot;WeightKg&quot;:0,&quot;Size&quot;:&#123;&quot;Height&quot;:0&#125;&#125;
In this case, even though we never set the value of the Size attribute, and set its omitempty tag, it still appears in the output. This is because structs do not have any empty value in Go. To solve this, use a struct pointer instead :
package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;)type Dog struct &#123;    Breed    string    WeightKg int&#125;type Cat struct &#123;    Breed    string    WeightKg int `json:&quot;weight,omitempty&quot;`    PWeightKg *int `json:&quot;pweight,omitempty&quot;`&#125;func main() &#123;    d := Dog&#123;        Breed: &quot;hal&quot;,    &#125;        nw := 0    c0 := Cat&#123;        Breed: &quot;cal&quot;,        WeightKg: 0, // omitted as its value is default!!! warning!!!        PWeightKg: &amp;nw, // not omitted as for pointer default value is nil    &#125;    c1 := Cat&#123;        Breed: &quot;cal&quot;,        WeightKg: 1,    &#125;        b, _ := json.Marshal(d)    fmt.Println(string(b))    b, _ := json.Marshal(c0)    fmt.Println(string(b))    b, _ := json.Marshal(c1)    fmt.Println(string(b))&#125;main()

&#123;&quot;Breed&quot;:&quot;hal&quot;,&quot;WeightKg&quot;:0&#125;
&#123;&quot;Breed&quot;:&quot;cal&quot;,&quot;pweight&quot;:0&#125;
&#123;&quot;Breed&quot;:&quot;cal&quot;,&quot;weight&quot;:1&#125;

json.marshal for built-in typeAs you can json.Marshal() can be used for built-in type, the result is several bytes for that value, then you can Unmarshal to that type!
json.Marshal() for nil will result ‘null’ string
package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;)func main() &#123;    vb := true    data, _ := json.Marshal(vb)    fmt.Println(data)    fmt.Println(string(data))    fmt.Println(&quot;============&quot;)    vi := 12    data, _ = json.Marshal(vi)    fmt.Println(data)    fmt.Println(string(data))    fmt.Println(&quot;============&quot;)    vs := &quot;ab&quot;    data, _ = json.Marshal(vs)    fmt.Println(data)    fmt.Println(string(data))    fmt.Println(&quot;============&quot;)    vsc := []string&#123;&quot;a&quot;, &quot;b&quot;&#125;    data, _ = json.Marshal(vsc)    fmt.Println(data)    fmt.Println(string(data))    fmt.Println(&quot;============&quot;)    var vn interface&#123;&#125; = nil    data, _ = json.Marshal(vn)// nul string for nil !!!    fmt.Println(data)    fmt.Println(string(data))    fmt.Println(&quot;============&quot;)&#125;main()

[116 114 117 101]
true
============
[49 50]
12
============
[34 97 98 34]
&quot;ab&quot;
============
[91 34 97 34 44 34 98 34 93]
[&quot;a&quot;,&quot;b&quot;]
============
[110 117 108 108]
null
============

xmlMarshalfunc Marshal(v any) ([]byte, error)Marshal returns the XML encoding of v.
Marshal handles an array or slice by marshaling each of the elements. Marshal handles a pointer by marshaling the value it points at or, if the pointer is nil, by writing nothing. Marshal handles an interface value by marshaling the value it contains or, if the interface value is nil, by writing nothing. Marshal handles all other data by writing one or more XML elements containing the data.
The name for the XML elements is taken from, in order of preference:

the tag on the XMLName field, if the data is a struct
the value of the XMLName field of type Name: the only one use value as name of element.!!!
the tag of the struct field used to obtain the data
the name of the struct field used to obtain the data
the name of the marshaled type.

The XML element for a struct contains marshaled elements for each of the exported fields of the struct, with these exceptions:

the XMLName field, described above, is omitted.
a field with tag “-“ is omitted.
a field with tag “name,attr” becomes an attribute with the given name in the XML element.
a field with tag “,attr” becomes an attribute with the field name in the XML element.
a field with tag “,chardata” is written as character data, not as an XML element.(field value used for struct, as no &lt;$tag&gt;&lt;/$tag&gt; generated)
a field with tag “,cdata” is written as character data wrapped in one or more &lt;![CDATA[...]]&gt; tags, not as an XML element.
a field with tag “,innerxml” is written verbatim, not subject to the usual marshaling procedure.
a field with tag “,comment” is written as an XML comment, not subject to the usual marshaling procedure. It must not contain the “–” string within it.
a field with a tag including the “omitempty” option is omitted if the field value is empty. The empty values are false, 0, any nil pointer or interface value, and any array, slice, map, or string of length zero.
an anonymous struct field is handled as if the fields of its value were part of the outer struct.
a field implementing Marshaler is written by calling its MarshalXML method.
a field implementing encoding.TextMarshaler is written by encoding the result of its MarshalText method as text.

NOTE

If a field uses a tag “a&gt;b&gt;c”, then the element c will be nested inside parent elements a and b. Fields that appear next to each other that name the same parent will be enclosed in one XML element.

If the XML name for a struct field is defined by both the field tag and the struct’s XMLName field, the names must match.


 

Unmarshalfunc Unmarshal(data []byte, v any) errorUnmarshal parses the XML-encoded data and stores the result in the value pointed to by v, which must be an arbitrary struct, slice, or string. Well-formed data that does not fit into v is discarded.
Because Unmarshal uses the reflect package, it can only assign to exported (upper case) fields. Unmarshal uses a case-sensitive comparison to match XML element names to tag values and struct field names.
Unmarshal maps an XML element to a struct using the following rules. In the rules, the tag of a field refers to the value associated with the key ‘xml’ in the struct field’s tag

If the struct has a field of type []byte or string with tag “,innerxml”, Unmarshal accumulates the raw XML nested inside the element in that field. The rest of the rules still apply.

If the struct has a field named XMLName of type Name, Unmarshal records the element name in that field.

If the XMLName field has an associated tag of the form “name” or “namespace-URL name”, the XML element must have the given name (and, optionally, name space) or else Unmarshal returns an error.

If the XML element has an attribute whose name matches a struct field name with an associated tag containing “,attr” or the explicit name in a struct field tag of the form “name,attr”, Unmarshal records the attribute value in that field.

If the XML element has an attribute not handled by the previous rule and the struct has a field with an associated tag containing “,any,attr”, Unmarshal records the attribute value in the first such field.

If the XML element contains character data, that data is accumulated in the first struct field that has tag “,chardata”. The struct field may have type []byte or string. If there is no such field, the character data is discarded.

If the XML element contains comments, they are accumulated in the first struct field that has tag “,comment”.  The struct field may have type []byte or string. If there is no such field, the comments are discarded.

If the XML element contains a sub-element whose name matches the prefix of a tag formatted as “a” or “a&gt;b&gt;c”, unmarshal will descend into the XML structure looking for elements with the given names, and will map the innermost elements to that struct field. A tag starting with “&gt;” is equivalent to one starting with the field name followed by “&gt;”.

If the XML element contains a sub-element whose name matches a struct field’s XMLName tag and the struct field has no explicit name tag as per the previous rule, unmarshal maps the sub-element to that struct field.

If the XML element contains a sub-element whose name matches a field without any mode flags (“,attr”, “,chardata”, etc), Unmarshal maps the sub-element to that struct field.

If the XML element contains a sub-element that hasn’t matched any of the above rules and the struct has a field with tag “,any”, unmarshal maps the sub-element to that struct field.

An anonymous struct field is handled as if the fields of its value were part of the outer struct.

A struct field with tag “-“ is never unmarshaled into.


NOTE

If Unmarshal encounters a field type that implements the Unmarshaler interface, Unmarshal calls its UnmarshalXML method to produce the value from the XML element. Otherwise, if the value implements encoding.TextUnmarshaler, Unmarshal calls that value’s UnmarshalText method.

package mainimport (    &quot;encoding/xml&quot;    &quot;fmt&quot;)// when Plant used as field of another field.// we explitly declare it&#x27;s tag name here, if user set tag, tag name must be same as defined here!!!type Plant struct &#123;    // xml.Name the root tag of struct, no need to set value for it!!    // if no tag for it, you have to set value of XMLName as it&#x27;s value used for xml    XMLName xml.Name `xml:&quot;plant&quot;`    // the attr of the root tag with id as its name    Id int `xml:&quot;id,attr&quot;`    // nested tag of the root with &#x27;name&#x27; as its name    Name string `xml:&quot;name&quot;`    // Even without explicitly name a tag, it&#x27;s exported if it starts with capital letter!!!    // nested tag of the root with &#x27;Origin&#x27; as its name as it&#x27;s not named explicitly    // several such tags as it&#x27;s an array    // by default empty value is not saved to xml    Origin []string&#125;func (p Plant) String() string &#123;    return fmt.Sprintf(&quot;Plant id=%v, name=%v, origin=%v&quot;,        p.Id, p.Name, p.Origin)&#125;func main() &#123;    coffee := &amp;Plant&#123;Id: 27, Name: &quot;Coffee&quot;&#125;    //coffee.Origin = []string&#123;&quot;Ethiopia&quot;, &quot;Brazil&quot;&#125;    out, _ := xml.MarshalIndent(coffee, &quot; &quot;, &quot;  &quot;)    fmt.Println(string(out))    fmt.Println(&quot;==================================&quot;)    fmt.Println(xml.Header + string(out))    var p Plant    if err := xml.Unmarshal(out, &amp;p); err != nil &#123;        panic(err)    &#125;    fmt.Println(&quot;==================================&quot;)    fmt.Println(p)    fmt.Println(&quot;==================================&quot;)    tomato := &amp;Plant&#123;Id: 81, Name: &quot;Tomato&quot;&#125;    tomato.Origin = []string&#123;&quot;Mexico&quot;, &quot;California&quot;&#125;    type Nesting struct &#123;        XMLName xml.Name `xml:&quot;nesting&quot;`        // a&gt;b&gt;c: c(newplant) is used as name for Plant if Plant has no xml.Name specified explicitly        // but no error if we set here newplant for Plant event it&#x27;s not used at all        Plants []*Plant `xml:&quot;parent&gt;child&gt;newplant&quot;`        // ERROR: can NOT declare newplant as its name, as Plant has already declare it!!!        // Plant *Plant `xml:&quot;newplant,omitempty&quot;`    &#125;    nesting := &amp;Nesting&#123;&#125;    nesting.Plants = []*Plant&#123;coffee, tomato&#125;    out, err := xml.MarshalIndent(nesting, &quot; &quot;, &quot;  &quot;)    if err != nil &#123;        fmt.Println(err)    &#125; else &#123;        fmt.Println(string(out))    &#125;&#125;main()

 &lt;plant id=&quot;27&quot;&gt;
   &lt;name&gt;Coffee&lt;/name&gt;
 &lt;/plant&gt;
==================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
 &lt;plant id=&quot;27&quot;&gt;
   &lt;name&gt;Coffee&lt;/name&gt;
 &lt;/plant&gt;
==================================
&#123;&#123; plant&#125; 27 Coffee []&#125;
    ==================================
     
       
         
           
             Coffee
           
           
             Tomato
             Mexico
             California
           
         
       
     


#### chardata
In xml, mostly only the leaf node has value, leaf nodes can be defined by a field of struct, `the field type can be built-in type like int, string, leaf node without attr can use built-in type!!!`, like this:
type Garden struct &#123;                                                                Cool     string `xml:&quot;cool,omitempty&quot;`  &#125;gd = Garden&#123;&#125;gd.Cool = &quot;jas&quot;
The generate xml:
&lt;Garden&gt;  &lt;cool&gt;jas&lt;/cool&gt;&lt;/Garden&gt;

What about if we want to add attr to cool, there is no way to do as, cool is built-in type, you can not add attr to it, you need to declare a struct, like this, as Value is chardata, not XML element, that means **`chardata is needed for leaf node that has attrs and its value`**
type CoolT &#123;    Name string `xml:&quot;name,attr&quot;`    Value string `xml:&quot;,chardata&quot;`&#125;type Garden struct &#123;                                                                Cool     string `xml:&quot;cool,omitempty&quot;`  &#125;gd = Garden&#123;&#125;gd.Cool = &quot;jas&quot;

#### custom UnmarshalXML and MarshalXML
For user type, user can implement its own UnmarshalXML and marshalXML of that type, which is called by xml.Marshal() and xml.UnmarshalXML(), if you do not want to use the default logic of this process.

### gob
`Gob is like pickle in python and Protobuf`, they are used to serialize object, the serialized object is a stream which is self-describing(`have type, value, name et`c). `Each data item in the stream is preceded by a specification of its type, expressed in terms of a small set of predefined types`. Pointers are not transmitted, but the things they point to are transmitted; that is, the values are flattened. Nil pointers are not permitted, as they have no value. Recursive types work fine, but recursive values (data with cycles) are problematic. This may change.

To use gobs, create an Encoder and present it with a series of data items as values or addresses that can be dereferenced to values. The Encoder makes sure all type information is sent before it is needed. At the receive side, a Decoder retrieves values from the encoded stream and unpacks them into local variables.

The source and destination values/types `need not correspond exactly`. For structs, fields (identified by name) that are in the source but absent from the receiving variable will be ignored. Fields that are in the receiving variable but missing from the transmitted type or value will be ignored in the destination. **If a field with the same name is present in both, their types must be compatible**. Both the receiver and transmitter will do all necessary indirection and dereferencing to convert between gobs and actual Go values.

struct &#123; A, B int &#125;
can be sent from or received into any of these Go types below:

struct &#123; A, B int &#125;	// the same*struct &#123; A, B int &#125;	// extra indirection of the structstruct &#123; *A, **B int &#125;	// extra indirection of the fieldsstruct &#123; A, B int64 &#125;	// different concrete value type; see below


package mainimport (    &quot;bytes&quot;    &quot;encoding/gob&quot;    &quot;fmt&quot;    &quot;log&quot;)type P struct &#123;    X, Y, Z int    Name    string&#125;type Q struct &#123;    X, Y *int32 // when decode from P, Z is ignored, int converted to int32    Name string&#125;// This example shows the basic usage of the package: Create an encoder,// transmit some values, receive them with a decoder.func main() &#123;    // Initialize the encoder and decoder. Normally enc and dec would be    // bound to network connections and the encoder and decoder would    // run in different processes but here is just a demo to show how it works !!!    var network bytes.Buffer        // Stand-in for a network connection    enc := gob.NewEncoder(&amp;network) // Will write to network.    dec := gob.NewDecoder(&amp;network) // Will read from network.    // Encode (send) some values.    err := enc.Encode(P&#123;3, 4, 5, &quot;Pythagoras&quot;&#125;) // instance of P struct serialized to buffer.    if err != nil &#123;        log.Fatal(&quot;encode error:&quot;, err)    &#125;    err = enc.Encode(P&#123;1782, 1841, 1922, &quot;Treehouse&quot;&#125;) // another instance of P struct serialized to buffer.    if err != nil &#123;        log.Fatal(&quot;encode error:&quot;, err)    &#125;    // Decode (receive) and print the values. as it&#x27;s stream first in, first out!!!    var q Q    err = dec.Decode(&amp;q) //after decode, first instance is removed from stream.    if err != nil &#123;        log.Fatal(&quot;decode error 1:&quot;, err)    &#125;    fmt.Printf(&quot;%q: &#123;%d, %d&#125;\n&quot;, q.Name, *q.X, *q.Y)        err = dec.Decode(&amp;q)    if err != nil &#123;        log.Fatal(&quot;decode error 2:&quot;, err)    &#125;    fmt.Printf(&quot;%q: &#123;%d, %d&#125;\n&quot;, q.Name, *q.X, *q.Y)&#125;main()

    "Pythagoras": &#123;3, 4&#125;
    "Treehouse": &#123;1782, 1841&#125;


## hash(get hash value)
Package crc32 implements the 32-bit cyclic redundancy check, or CRC-32, checksum mostly `used for data transfer over network`.

`Package maphash` provides `hash functions on byte sequences`. These hash functions are intended to be used to implement hash tables or other data structures that need to `map arbitrary strings or byte sequences to a uniform distribution on unsigned 64-bit integers`. Each different instance of a hash table or data structure should use its own Seed.
**maphash API**
- func (h *Hash) Sum64() uint64
- func (h *Hash) Write(b []byte) (int, error)
- func (h *Hash) WriteString(s string) (int, error)

import &quot;hash/crc32&quot;import &quot;hash/crc64&quot;import &quot;hash/maphash&quot;import &quot;fmt&quot;func maphashDemo() &#123;    var h maphash.Hash    // Add a string to the hash, and print the current hash value.    h.WriteString(&quot;hello, &quot;)    fmt.Printf(&quot;%v\n&quot;, h.Sum64())        h.Reset() // seed unchanged, so same value for same string    h.WriteString(&quot;hello, &quot;)    fmt.Printf(&quot;%v\n&quot;, h.Sum64())&#125;func crcDemo() &#123;    &#125;maphashDemo()crcDemo()

## math
Package rand implements pseudo-random number generators.   
**random API**  
- func (r *Rand) Float32() float32 [0, 1)- func (r *Rand) Int() int- func (r *Rand) Int31() int32- func (r *Rand) Int63() int64                                    - func (r *Rand) Intn(n int) int        range [0, n)- func (r *Rand) Int31n(n int32) int32        [0, n)- func (r *Rand) Int63n(n int64) int64         \[0, n)                                    - func (r *Rand) Uint32() uint32- func (r *Rand) Uint64() uint64


import &quot;math/rand&quot;import &quot;fmt&quot;func randDemo() &#123;    // set seed    r := rand.New(rand.NewSource(99))    fmt.Println(r.Float32(), r.Int(), r.Intn(100), r.Uint32())&#125;randDemo()

    0.2635776 5864379541603903257 90 2699839765


## crypto
Package, crypto, aes, des, rsa, sha1, sha256, hmac, md5

## file/IO ops
There are many ways to read and write IO in golang. At present, I know `IO library, OS library, ioutil library, bufio library, bytes library and so on`, which one to choose?  which library should be used in what scenario?

- The OS library mainly deals with the operating system, so file operations are basically linked with the OS library, such as creating a file, deleting a file, change file mode, creating dir etc. This library is often used with ioutil library, bufio library, etc
    - *os.File is a type that implements io.Reader, and io.Writer (among others) which streams bytes to or from a file on disk
      It is useful if you don't want to read the whole file into memory, It has the downside of being a lower level construct,  meaning data must often be processed in loops (with error checks on each iteration), and that it must be manually managed (via Close())


- io library belongs to the bottom interface definition library. 
    - its function is to `define some basic interfaces and some basic constants,` and to explain the functions of these interfaces. The common interfaces are reader, writer, etc. Generally, `this library is only used to call its constants, such as io.EOF`.


- The ioutil library is included in the IO directory. It is mainly used as a toolkit. There are some practical functions, such as readall (read data from a source), readfile (read file content), WriteFile (write data to a file), readdir (get directory), it's easy to use, like readfile, `it does some operation automatically which we should do by ourself if use os.File`
    - It automatically allocates a byte slice of the correct size (no need to Read + append in a loop)
    - It automatically closes the file
    - It returns the first error that prevented it from working (so you only need a single error check)

- bufio provides wrapper types for io.Reader and io.Writer that buffer the input / output to improve efficiency.
    - If you are reading a file in one or a few large steps, you probably don't need it either
    - buffered input and output add some extra concerns
    - bufio.Scanner is a nice utility type to efficiently read independent lines of text from an io.Reader
        
        
- bytes provides helper functions and types for interacting with byte slices ([]byte)
    - `bytes.Reader turns a []byte into a io.Reader (as well as an io.Seeker to rewind)`
    - `bytes.Buffer uses []bytes to implement a reader/writer, it is useful when you want to use code that takes an io.Writer, and store the results in memory for use later`
       
       
- bufio vs bytes.Buffer: both of them provide a layer of caching function. The main difference between them is that `bufio is for file to memory caching, wrap other Reader/Writer`, while `bytes.Buffer is for memory to memory caching`.

 

**Suggestion**

- **file operation, create/delete/chmod use` os library`**
- **small file, load at once, use `ioutil`**
- **large file, `bufio` support read line by line, or read word by word etc**
- **control read size of file by yourself, read many times, `os.File`**
- **In memory reader/writer `bytes.Buffer`**

When Read encounters an error or end-of-file condition after successfully reading n > 0 bytes, it returns the number of bytes read. It may return the (non-nil) error from the same call or return the error (and n == 0) from a subsequent call. An instance of this general case is that **a Reader returning a non-zero number of bytes at the end of the input stream may return either err == EOF or err == nil. The next Read should return 0, EOF**.

Callers should always process the n > 0 bytes returned before considering the error err. Doing so correctly handles I/O errors that happen after reading some bytes and also both of the allowed EOF behaviors. 

[IO-Cookbook](https://jesseduffield.com/Golang-IO-Cookbook/)

 

src: - bufio  - bufio.go - bytes  - buffer.go  - reader.go - io  - ioutil   - ioutil.go  - io.go - os  - file.go - strings   - reader.go


io/ioutil was a poorly defined collection of helpers, you should use os.xxx if avaiable.

### os


standard package `os` provides basic operations file system while File provides read/write operations.

- func Chdir(dir string) error
- func Chmod(name string, mode FileMode) error
- func Chown(name string, uid, gid int) error

- func Stat(name string) (FileInfo, error)
- func IsExist(err error) bool
- func IsNotExist(err error) bool

- func ReadDir(name string) ([]DirEntry, error)
- func Mkdir(name string, perm FileMode) error
- func MkdirAll(path string, perm FileMode) error
- func MkdirTemp(dir, pattern string) (string, error)

- func Remove(name string) error
- func RemoveAll(path string) error
- func Rename(oldpath, newpath string) error

- func ReadFile(name string) ([]byte, error) // read all at once

- func Create(name string) (*File, error)
- func Open(name string) (*File, error)
- func OpenFile(name string, flag int, perm FileMode) (*File, error)


- func (f *File) Close() error
- func (f *File) Read(b []byte) (n int, err error) // read size of slice from file
- func (f *File) ReadAt(b []byte, off int64) (n int, err error)
- func (f *File) Write(b []byte) (n int, err error)
- func (f *File) WriteAt(b []byte, off int64) (n int, err error)
- func (f *File) WriteString(s string) (n int, err error)
- func (f *File) ReadDir(n int) ([]DirEntry, error)
- func (f *File) Sync() error


package mainimport (    &quot;fmt&quot;    &quot;os&quot;)func FileMakePath(path string) error &#123;                                               if stat, err := os.Stat(path); os.IsExist(err) &#123;                                     if stat.IsDir() &#123;                                                                    return nil                                                                   &#125;                                                                                return fmt.Errorf(&quot;[util] %s exists, but not dir&quot;, path)                     &#125;                                                                                                                                                                 // create if not exists                                                          if err := os.MkdirAll(path, 0777); err != nil &#123;                                      return err                                                                   &#125;                                                                                                                                                                return nil                                                                   &#125;      func main() &#123;    info, err := os.Stat(&quot;/tmp/test&quot;)    if err != nil &#123;        if os.IsNotExist(err) &#123;            fmt.Println(&quot;/tmp/test not found, creating it&quot;)            _, err := os.Create(&quot;/tmp/test&quot;)            if err != nil &#123;                fmt.Printf(&quot;create failed: %v\n&quot;, err)            &#125; else &#123;                fmt.Println(&quot;created&quot;)                os.Chmod(&quot;/tmp/test&quot;, 0777)            &#125;        &#125; else &#123;            fmt.Println(err)        &#125;    &#125; else &#123;        fmt.Printf(&quot;stats of /tmp/test: %v\n&quot;, info)    &#125;    // If the file doesn&#x27;t exist, create it, or append to the file    file, err := os.OpenFile(&quot;/tmp/test&quot;, os.O_CREATE|os.O_WRONLY, 0644)    if err != nil &#123;        fmt.Println(err)        return    &#125;    // write data to file    file.Write([]byte(&quot;ab\n&quot;))    file.WriteString(&quot;hello&quot;)    file.Close()    data := make([]byte, 100)    file, err = os.Open(&quot;/tmp/test&quot;)    // read data from file at most 100 bytes!!    _, err = file.Read(data)    if err != nil &#123;        fmt.Println(&quot;read error &quot;, err)        file.Close()        return    &#125;    fmt.Printf(&quot;file content: \n%s\n&quot;, string(data))    file.Close()    if os.Remove(&quot;/tmp/test&quot;) != nil &#123;        fmt.Println(&quot;failed to remove file /tmp/test&quot;)    &#125; else &#123;        fmt.Println(&quot;removed file /tmp/test&quot;)    &#125;&#125;main()

    /tmp/test not found, creating it
    created
    file content: 
    ab
    hello                                                                                            
    removed file /tmp/test


### ioutil
#### read whole file once


package mainimport (    &quot;fmt&quot;    &quot;strings&quot;    &quot;io/ioutil&quot;)func main() &#123;    fmt.Println(&quot;write hello to file&quot;)    // easy to write and read file !!!        err := ioutil.WriteFile(&quot;/tmp/test.txt&quot;, []byte(&quot;hello \nworld&quot;), 0644)    if err != nil &#123;        fmt.Println(err)        return    &#125;    // read whole file at once, if file is large not good way    file, err := ioutil.ReadFile(&quot;/tmp/test.txt&quot;)    if err != nil &#123;        fmt.Println(err)        return    &#125;    fmt.Printf(&quot;read content: %s&quot;, file)        // readall from io.Reader    r := strings.NewReader(&quot;hello\nboy\n&quot;)    buf, _ := ioutil.ReadAll(r)    fmt.Printf(string(buf))&#125;main()

    write hello to file
    read content: hello 
    worldhello
    boy


### bufio
In some case, **file is large, we want to read line by line, or read until meet delimiter or read word by word**, this what bufio supports, it also provides read any data size but with buffer.

**bufio is a wrapper of another io.Reader/io.Writer, then gives more advanced feature, hence in order to use it, you need an io.Reader firstly, as it wraps io.Reader/io.Writer, hence you should NOT close it with bufio interface, but with io.Reader/io.Writer interface, use interfaces provided by bufio for reading/writing only**


**API**
- `func (b *Reader) Read(p []byte) (n int, err error)`
>Read reads data into p. It returns the number of bytes read into p. The bytes are taken from at most one Read on the underlying Reader, hence n may be less than len(p).
- `func (b *Reader) ReadBytes(delim byte) ([]byte, error)`
>ReadBytes reads until the first occurrence of delim in the input, returning a slice **containing the data up to and including the delimiter**. If ReadBytes encounters an error before finding a delimiter, it returns the data read before the error and the error itself (often io.EOF). ReadBytes returns err != nil if and only if the returned data does not end in delim. For simple uses, a Scanner may be more convenient.
- `func (b *Reader) ReadString(delim byte) (string, error)`
>Same as ReadBytes as it just calls ReadBytes directly, then convert bytes to string 

- `func (s *Scanner) Scan() bool`
>returns until the first occurrence of delim in the input(or error, EOF), by default delim  is `\n`, or `\r\n`

**NOTE: `\r\n, \n` is returned by `ReadBytes() and ReadString()` which is not true for Scanner.Text(),Scanner.Bytes() which striped**

#### read file line by line


package mainimport (    &quot;bufio&quot;    &quot;fmt&quot;    &quot;io&quot;    &quot;strings&quot;)func main() &#123;    r := strings.NewReader(&quot;hello boy\ngirl\n&quot;)    // Reader    br := bufio.NewReader(r)    line, err := br.ReadString(&#x27;\n&#x27;)    if err != nil &#123;        fmt.Printf(&quot;%v&quot;, err)    &#125; else &#123;        // line with \n!!!, its count is 10!!!        fmt.Printf(&quot;%v: %d\n&quot;, line, len([]rune(line)))    &#125;    line, err = br.ReadString(&#x27;\n&#x27;)    if err != nil &#123;        if err == io.EOF &#123;            fmt.Printf(&quot;%v&quot;, line)        &#125; else &#123;            fmt.Printf(&quot;%v&quot;, err)        &#125;    &#125; else &#123;        fmt.Printf(&quot;%v: %d\n&quot;, line, len([]rune(line)))    &#125;    fmt.Println(&quot;use scanner&quot;)    // Scanner to read line by line    // return to start position    r.Seek(0, 0)    scanner := bufio.NewScanner(r)    // read word by word: scanner.Split(bufio.ScanWords)    for scanner.Scan() &#123;// returns until \n!!!        //scaner.Text() has stripped line, no \n        fmt.Println(scanner.Text(), len([]rune(scanner.Text())))        fmt.Println(string(scanner.Bytes()), len(scanner.Bytes()))    &#125;    if err := scanner.Err(); err != nil &#123;        fmt.Println(err)    &#125;&#125;main()

    hello boy
    : 10
    girl
    : 5
    use scanner
    hello boy 9
    hello boy 9
    girl 4
    girl 4


### io.Copy and io.Pipe and bytes.Buffer and string.NewReader()

Package io provides basic interfaces to I/O primitives. Its primary job is to wrap existing implementations of such primitives, such as those in package os, into `shared public interfaces that abstract the functionality, plus some other related primitives`.

In most cases, `we did NOT use io library directly, but it provides two convenient ways io.Copy and io.Pipe which is used in most application.`

- `func Copy(dst Writer, src Reader) (written int64, err error)`
>Copy copies from src to dst until either EOF is reached on src or an error occurs. It returns the number of bytes copied and the first error encountered while copying, if any. A successful Copy returns err == nil, not err == EOF. Because Copy is defined to read from src until EOF, it does not treat an EOF from Read as an error to be reported.
If src implements the WriterTo interface, the copy is implemented by calling src.WriteTo(dst). Otherwise, if dst implements the ReaderFrom interface, the copy is implemented by calling dst.ReadFrom(src). 

- `func Pipe() (*PipeReader, *PipeWriter)`
>Pipe creates a `synchronous in-memory pipe`. It can be used to connect code expecting an io.Reader with code expecting an io.Writer. Reads and Writes on the pipe are matched one to one except when multiple Reads are needed to consume a single Write. That is, **each Write to the PipeWriter blocks until it has satisfied one or more Reads from the PipeReader that fully consume the written data**. The data is copied directly from the Write to the corresponding Read (or Reads); there is no internal buffering.

**Note**
- pipe.Write() returns only when **error happens or all data is received by reader, otherwise it blocks**
- pipe.Read() returns data received and tell Writer how many it receives, so that writer can write immediately after get the notification

**Cases**
- io.Copy: Get data(bytes) from reader, without any change, Write to writer **(no user involved)**, this what io.Copy mostly used, reader and writer can anyone like file, string, in memory etc!!

package mainimport (    &quot;bytes&quot;    &quot;io&quot;    &quot;os&quot;    &quot;strings&quot;)func main() &#123;    r := strings.NewReader(&quot;hello boy\ngirl\n&quot;)    // copy from reader to writer    // support any reader and any writer    io.Copy(os.Stdout, r)    var buffer *bytes.Buffer    buffer = bytes.NewBufferString(&quot;hello boy\ngirl\n&quot;)    io.Copy(os.Stdout, buffer)&#125;

- io.Pipe() same as io.Copy, but more limitation and more efficient, as reader and writer are bound from beginning and they are in memory, **need user call write() and read() API**.


package mainimport (    &quot;fmt&quot;    &quot;io&quot;    &quot;strings&quot;    &quot;time&quot;)func main() &#123;    preader, pwriter := io.Pipe()    go func() &#123;        reader := strings.NewReader(&quot;ab&quot;)        fmt.Println(&quot;writer writes data and blocks as reader is not ready&quot;)        io.Copy(pwriter, reader)        fmt.Println(&quot;writer waked up after data is read&quot;)    &#125;()    // Read blocks until writer write data or writer close/error    buf := make([]byte, 60)    fmt.Println(&quot;reader sleep 1 seconds before reading&quot;)    time.Sleep(time.Second * 1)    c, err := preader.Read(buf)    if err != nil &#123;        fmt.Println(&quot;error reading&quot;)    &#125; else &#123;        buf[2] = &#x27;c&#x27;        //fmt.Println(string(buf))        fmt.Println(string(buf[:c]))    &#125;&#125;main()

    reader sleep 1 seconds before reading
    writer writes data and blocks as reader is not ready
    writer waked up after data is read
    ab


package main// Importing fmt, io, and bytesimport (    &quot;fmt&quot;    &quot;io&quot;)// Calling mainfunc main() &#123;    // Calling Pipe method    pipeReader, pipeWriter := io.Pipe()    // Using Fprint in go function to write    // data to the file    go func() &#123;        // block here until reader read it all or read call Close()        n, err := fmt.Fprint(pipeWriter, &quot;Geeks\n&quot;)        // the print may be printed before or after last rcv!!!        fmt.Printf(&quot;pipeWriter returns %v bytes is written err: %v\n&quot;, n, err)        // Using Close method to close write        pipeWriter.Close()    &#125;()    buf := make([]byte, 2)    for &#123;        n, err := io.ReadFull(pipeReader, buf)        if err == io.EOF &#123;            break        &#125; else &#123;            fmt.Printf(&quot;rcv: %v bytes: %q\n&quot;, n, string(buf))        &#125;    &#125;&#125;

**Possible output**
rcv: 2 bytes: &quot;Ge&quot;rcv: 2 bytes: &quot;ek&quot;rcv: 2 bytes: &quot;s\n&quot;pipeWriter returns 6 bytes is written err: &lt;nil&gt;rcv: 2 bytes: &quot;Ge&quot;rcv: 2 bytes: &quot;ek&quot;pipeWriter returns 6 bytes is written err: &lt;nil&gt;rcv: 2 bytes: &quot;s\n&quot;


import (    &quot;fmt&quot;    &quot;io&quot;    &quot;time&quot;)// Calling mainfunc main() &#123;    // Calling Pipe method    pipeReader, pipeWriter := io.Pipe()    // Using Fprint in go function to write    // data to the file    go func() &#123;        // block here until reader read it all or read call Close()        n, err := fmt.Fprint(pipeWriter, &quot;Geeks\n&quot;)        fmt.Printf(&quot;pipeWriter returns: %v bytes is written err: %v\n&quot;, n, err)        // Using Close method to close write        pipeWriter.Close()    &#125;()    // data into buffer    fmt.Println(&quot;starts to read&quot;)    buf := make([]byte, 2)    for &#123;        n, err := io.ReadFull(pipeReader, buf)        if err == io.EOF &#123;            break        &#125; else &#123;            fmt.Printf(&quot;rcv: %v bytes: %q\n&quot;, n, string(buf))            pipeReader.Close()            // sleep a while for writer to quit first            time.Sleep(time.Second * 2)            break        &#125;    &#125;&#125;main()

    starts to read
    rcv: 2 bytes: "Ge"
    pipeWriter returns: 2 bytes is written err: io: read/write on closed pipe



package mainimport (    &quot;bytes&quot;    &quot;fmt&quot;    &quot;io&quot;    &quot;log&quot;    &quot;os&quot;    &quot;strings&quot;)func main() &#123;    r := strings.NewReader(&quot;some io.Reader stream to be read\n&quot;)    if _, err := io.Copy(os.Stdout, r); err != nil &#123;        // copy from reder to os.Stdout(console)        log.Fatal(err)    &#125;    // an new memory buffer.    buf := make([]byte, 60)    buffer := bytes.NewBuffer(buf)    // after Copy, reader reach to EOF, we we should reset it to read data again    r.Seek(0, 0)    // read from reader to buffer!!!    buffer.ReadFrom(r)    fmt.Printf(&quot;%s&quot;, buffer.String())    fmt.Printf(&quot;%q&quot;, buffer.String())&#125;main()

    some io.Reader stream to be read
                                                                some io.Reader stream to be read
    "\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00some io.Reader stream to be read\n"

### list all files(file,dir) under a dir
- os.ReadDir (return all)
- os.File.ReadDir(can limit return entries)
- ioutil.ReadDir(less efficient than os.ReadDir
- filepath.Glob(support patterns)
- filepath.Walk(support recursive sub-dirs, above does not support this)

## signal

**[signal wiki page](https://en.wikipedia.org/wiki/Signal_(IPC))**

package mainimport (    &quot;fmt&quot;    &quot;os&quot;    &quot;os/signal&quot;    &quot;syscall&quot;)func main() &#123;        c := make(chan os.Signal, 1)    // os.Interrupt&lt;----&gt;syscall.SIGINT (Ctrl+C)    // os.Kill     &lt;----&gt;syscall.SIGKILL    signal.Notify(c, os.Interrupt)    signal.Notify(c, os.Kill)    signal.Notify(c, syscall.SIGQUIT)    signal.Notify(c, syscall.SIGTERM)    select &#123;    case s := &lt;-c:        fmt.Println(&quot;sig: &quot;, s)    &#125;&#125;

## random
Random numbers are generated by a Source. Top-level functions, such as and Int, use a default shared Source that produces a deterministic sequence of values each time a program is run. Use the Seed function to initialize the default Source if different behavior is required for each run. The default Source is safe for concurrent use by multiple goroutines, but Sources created by NewSource are not.

This package's outputs might be easily predictable regardless of how it's seeded. For random numbers suitable for security-sensitive work, see the `crypto/rand` package.

- Int/Int31/Int63: generate random number without range
- Intn/Int31n/Int63n: generate random number from [0, n)


package mainimport (    &quot;fmt&quot;    &quot;math/rand&quot;    &quot;time&quot;)func main() &#123;    rand.Seed(time.Now().UnixNano())    fmt.Println(rand.Int(), rand.Intn(10))    fmt.Println(rand.Int31(), rand.Int31n(10))    fmt.Println(rand.Int63(), rand.Int63n(10))        // generate bunch of numbers    buf := make([]byte, 6)    rand.Read(buf)    fmt.Println(buf)&#125;main()

    5656737410240953005 4
    670732630 8
    846728672197744499 9
    [40 95 241 197 58 113]


## template
There are two packages operating with templates `text/template and html/template`. Both provide the same interface, however the html/template package is used to generate HTML output safe against code injection.

Templates are executed by applying them to a `data structure`. Annotations in the template refer to elements of the data structure (typically a field of a struct or a key in a map) to control execution and derive values to be displayed. `Execution of the template walks the structure and sets the cursor, represented by a period '.' and called "dot", to the value at the current location in the structure as execution proceeds.`, **data structure is represented by a period '.'**

To obtain data from a struct, you can use the `&#123;&#123; .FieldName &#125;&#125;` action, which will replace it with FieldName value of given struct, on parse time.

There is also the &#123;&#123; . &#125;&#125; action that you can use to refer to a value of non-struct types.
Templates are provided to the appropriate functions either as string or as “raw string”. Actions represents the data evaluations, functions or control loops. They are delimited by &#123;&#123; &#125;&#125;. Other, non delimited parts are left untouched.
Here is the list of actions. “Arguments” and “pipelines” are evaluations of data, defined in detail in the corresponding sections that follow.
&#123;&#123;/* a comment */&#125;&#125;&#123;&#123;- /* a comment with white space trimmed from preceding and following text */ -&#125;&#125;	A comment; discarded. May contain newlines.	Comments do not nest and must start and end at the	delimiters, as shown here.&#123;&#123;pipeline&#125;&#125;	The default textual representation (the same as would be	printed by fmt.Print) of the value of the pipeline is copied	to the output.&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;end&#125;&#125;	If the value of the pipeline is empty, no output is generated;	otherwise, T1 is executed. The empty values are false, 0, any	nil pointer or interface value, and any array, slice, map, or	string of length zero.	Dot is unaffected.&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;else&#125;&#125; T0 &#123;&#123;end&#125;&#125;	If the value of the pipeline is empty, T0 is executed;	otherwise, T1 is executed. Dot is unaffected.&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;else if pipeline&#125;&#125; T0 &#123;&#123;end&#125;&#125;	To simplify the appearance of if-else chains, the else action	of an if may include another if directly; the effect is exactly	the same as writing		&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;else&#125;&#125;&#123;&#123;if pipeline&#125;&#125; T0 &#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;range pipeline&#125;&#125; T1 &#123;&#123;end&#125;&#125;	The value of the pipeline must be an array, slice, map, or channel.	If the value of the pipeline has length zero, nothing is output;	otherwise, dot is set to the successive elements of the array,	slice, or map and T1 is executed. If the value is a map and the	keys are of basic type with a defined order, the elements will be	visited in sorted key order.    Dot is changed if pipeline is not zero&#123;&#123;range pipeline&#125;&#125; T1 &#123;&#123;else&#125;&#125; T0 &#123;&#123;end&#125;&#125;	The value of the pipeline must be an array, slice, map, or channel.	If the value of the pipeline has length zero, dot is unaffected and	T0 is executed; otherwise, dot is set to the successive elements	of the array, slice, or map and T1 is executed.    Dot is change if pipeline is not zero    &#123;&#123;template &quot;name&quot;&#125;&#125;	The template with the specified name is executed with nil data.&#123;&#123;template &quot;name&quot; pipeline&#125;&#125;	The template with the specified name is executed with dot set	to the value of the pipeline.    New temaplte&#x27;s dot is set with pipeline&#123;&#123;block &quot;name&quot; pipeline&#125;&#125; T1 &#123;&#123;end&#125;&#125;	A block is shorthand for defining a template		&#123;&#123;define &quot;name&quot;&#125;&#125; T1 &#123;&#123;end&#125;&#125;	and then executing it in place		&#123;&#123;template &quot;name&quot; pipeline&#125;&#125;	The typical use is to define a set of root templates that are	then customized by redefining the block templates within.&#123;&#123;with pipeline&#125;&#125; T1 &#123;&#123;end&#125;&#125;	If the value of the pipeline is empty, no output is generated;	otherwise, dot is set to the value of the pipeline and T1 is	executed.    Dot is change if pipeline is not zero    &#123;&#123;with pipeline&#125;&#125; T1 &#123;&#123;else&#125;&#125; T0 &#123;&#123;end&#125;&#125;	If the value of the pipeline is empty, dot is unaffected and T0	is executed; otherwise, dot is set to the value of the pipeline	and T1 is executed.    Dot is change if pipeline is not zero    . is change during scope, while $ is unchanged, it&#x27;s top .pipeline is empty means.0-     intergernil-   pointer, interfacelen=0  array, slice, map, string

 

Basics
// template from a filet1, err := template.ParseFiles(&quot;test.html&quot;)// template from stringt1, err := template.New(&quot;test&quot;).Parse(`&#123;&#123;&quot;raw string&quot; | printf&#125;&#125;`)// output to stdoutt1.Execute(os.Stdout, &quot;hello world&quot;)// output to any Writert1.Execute(Writer, &quot;hello world&quot;)

Rules to write template, refer to template format and built-in functions, template examples, extra functions provided by sprig
package mainimport (    &quot;html/template&quot;    &quot;os&quot;)type Inventory struct &#123;    Name string    Id   uint    Cool string&#125;func main() &#123;    sweaters := Inventory&#123;&quot;wool&quot;, 17, &quot;&quot;&#125;    tmpl, err := template.New(&quot;test&quot;).Parse(`&#123;&#123;&quot;raw string&quot; | printf&#125;&#125;&#123;&#123;.Name&#125;&#125;&#123;&#123; with $Id :=  .Id&#125;&#125;&#123;&#123;- $Id -&#125;&#125;&#123;&#123;end -&#125;&#125;&#123;&#123;if .Cool&#125;&#125; &#123;&#123;/*must has such field!!!*/&#125;&#125;&#123;&#123;.Cool&#125;&#125;&#123;&#123;else&#125;&#125;NO Cool in field&#123;&#123;end -&#125;&#125;&#123;&#123;if (gt .Id 10)&#125;&#125;&#123;&#123;- .Id -&#125;&#125;&#123;&#123;end&#125;&#125;`)    if err != nil &#123;        panic(err)    &#125;    // render with data from struct!!!    err = tmpl.Execute(os.Stdout, sweaters)    if err != nil &#123;        panic(err)    &#125;&#125;main()

raw string
wool
17
NO Cool in field
17

non-standarduuiduuid.UUID is new type(alias) [16]byte
package mainimport (    &quot;fmt&quot;    &quot;github.com/google/uuid&quot;)func main() &#123;    id := uuid.New()    fmt.Println(id)&#125;


string to uuid bytes
uid, _:= uuid.Parse(&quot;b1762c24-48ff-4f67-a66c-e3aeb66051f0&quot;)bytes := uid[:] // convert UUID to [16]byte
bytes to uuid string
uid, _:= uuid.FromBytes(bytes)ustr := uid.String()  // convert UUID to string

logBesides the standard log package, there are lots of log library for Go, here are some popular ones.
package mainimport (    &quot;log&quot;    &quot;os&quot;)func main() &#123;    // These two line can be omitted if print log to console    outfile, _ := os.OpenFile(&quot;/tmp/my.log&quot;, os.O_APPEND|os.O_WRONLY|os.O_CREATE, 0600)    logger := log.New(outfile, &quot;log_prefix &quot;, log.Llongfile | log.Ldate|log.Ltime|log.Lmicroseconds)    logger.Println(&quot;hello, log!!!&quot;)    //print to console directly    log.Println(&quot;hello console&quot;)&#125;// cat /tmp/my.log// log_prefix 2021/10/09 11:40:58.169531 /home/data/tmp/lg.go:13: hello, log!!!


logrus–&gt;structured logger, completely API compatible with the standard library logger
zap–&gt;structured, leveled logging in Go, high performance
glog–&gt;simple, stable, standard lib provided by Go
oklog—-&gt;no updates years, old
seelog–&gt;no updates years, old
zerolog –&gt;json output

Suggestion:  

For simple usage, use glog
Structured, use logrus
Need high performance, zap is best choice.

glogformat when writing logs &lt;header&gt;] &lt;message&gt;, header has below format.Lmmdd hh:mm:ss.uuuuuu threadid file:line

L: log level, I(INFO), W(WARNING), E(ERROR), F(FATAL))
mmdd hh:mm:ss.uuuuuu time of log
threadid: thread id
file:line: file trigger the log

Log format ExampleI0715 13:28:17.923715   31920 main.go:14] hello message
API  

glog.Flush()
glog.Info()&#x2F;glog.Infof()&#x2F;glog.Infoln()
glog.Warning()&#x2F;glog.Warningf()&#x2F;glog.Warningln()
glog.Error()&#x2F;glog.Errorf()&#x2F;glog.Errorln()
glog.Fatal()&#x2F;glog.Fatalf()&#x2F;glog.Fatalln(), note os.Exit() when Fatalx() called.

NOTE

glog.Infof(), glog.Errorf(), glog.Warningf() will auto added \n at last if user not set

glog saves different log level to different files, but low log file contains high log as well, say INFO log files has ERROR log as well!!!

glog.Info&#x2F;Warning&#x2F;Error&#x2F;Fatal always writes log to file, no switch to turn it on&#x2F;off

use glog.V() and pass -v flag to control whether or not to write log to Info file, as V only provides Infof(), that means, it’s INFO log!!!, so do NOT use it for error, warning, trace logs!!!

By default, glog saves logs to &#x2F;tmp, change it to your path with -log_dir

By default, glog does not print log to std, change it to stdio instead of file with -logtostderr

Need both std and file, use -alsologtostderr as well as -log_dir

must put it at main(): flag.Parse()


# log to /var/log dir, make sure it exists$ go run main.go  -log_dir=/var/log# you can you see, for each start(as time pid is different), glog will create 4 logs files!!!# 20210727-062649.9900--&gt;date: 2021 year 07.27  time:06.26.49  pid: 9900$ ls /var/logg.centos.root.log.INFO.20210727-062649.9900 (named: &lt;program name&gt;.&lt;hostname&gt;.&lt;user name&gt;.log.&lt;severity level&gt;.&lt;date&gt;.&lt;time&gt;.&lt;pid&gt;)$ cat g.centos.root.log.INFO.20210727-062649.9900I1115 09:52:34.216494   29618 g.go:36] hellodate  time              pid   file:line] $msg# log to stderr$ go run main.go -logtostderr


Sample code
package mainimport (    &quot;flag&quot;    &quot;github.com/golang/glog&quot;)func main() &#123;    flag.Parse()    defer glog.Flush()    glog.Info(&quot;This is info message&quot;)    glog.Infof(&quot;This is info message: %v&quot;, 12345)    glog.Infoln(&quot;This is info message:&quot;, 12345)&#125;


V style
package mainimport (    &quot;flag&quot;    &quot;github.com/golang/glog&quot;)func main() &#123;    flag.Parse()    defer glog.Flush()    // log only print when set -v equal or larger than 2    // $ go run g.log -v 2, the number of level can be range from 0-int32!    // but most of time we use 0--5    if glog.V(2) &#123;        glog.Info(&quot;Starting transaction...&quot;)    &#125;    glog.V(2).Infof(&quot;Processed elements %d&quot;, 10)&#125;

logruslog Format$level[0000] message $field=$value $field=$valueINFO[0000] A walrus appears                              animal=walrus
API  

log.Trace(“Something very low level.”)
log.Debug(“Useful debugging information.”)
log.Info(“Something noteworthy happened!”)
log.Warn(“You should probably take a look at this.”)
log.Error(“Something failed but I’m not quitting.”)
log.Fatal(“Bye.”) &#x2F;&#x2F; Calls os.Exit(1) after logging
log.Panic(“I’m bailing.”) &#x2F;&#x2F; Calls panic() after logging

Sample code
package main                                                                                                                                                    import (                                                                            &quot;os&quot;                                                                                                                                                            log &quot;github.com/sirupsen/logrus&quot;                                            )                                                                                                                                                               func main() &#123;                                                                       // Log as JSON instead of the default ASCII formatter.                          //log.SetFormatter(&amp;log.JSONFormatter&#123;&#125;)                                                                                                                        // Output to stdout instead of the default stderr                               // Can be any io.Writer, see below for File example                             // log.SetOutput(os.Stdout)                                                                                                                                     // You could set this to any `io.Writer` such as a file                         file, err := os.OpenFile(&quot;logrus.log&quot;, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)    if err == nil &#123;                                                                     log.SetOutput(file)                                                         &#125; else &#123;                                                                            log.Info(&quot;Failed to log to file, using default stderr&quot;)                     &#125;                                                                                                                                                               log.WithFields(log.Fields&#123;                                                          &quot;animal&quot;: &quot;walrus&quot;,                                                         &#125;).Info(&quot;A walrus appears&quot;)                                                 &#125;                                                                               

gopsutilIn most monitor system, we need to get some info about the system, like CPU, memory, disk, net, process etc, gopsutil provides better API to support these, you do NOT need to deal with OS related files or API, gopsutil handles this for you, its supports get info about below aspects.

cpu
disk
docker
host
load
mem
net
process
winservices

package mainimport (    &quot;fmt&quot;    &quot;github.com/shirou/gopsutil/v3/mem&quot;    // &quot;github.com/shirou/gopsutil/mem&quot;  // to use v2)func main() &#123;    v, _ := mem.VirtualMemory()    // almost every return value is a struct    fmt.Printf(&quot;Total: %v, Free:%v, UsedPercent:%f%%\n&quot;, v.Total, v.Free, v.UsedPercent)    // convert to JSON. String() is also implemented    fmt.Println(v)&#125;
More details, please refer to gopsutil
file opsAs os package provides basic function to operation file system, it still have more work for user to do,that’s why afero comes in, it gives a lot of very powerful possibilities.
aferoAfero is a filesystem framework providing a simple, uniform and universal API interacting with any filesystem, as an abstraction layer providing interfaces, types and methods, it provides significant improvements over using the os package alone, most notably the ability to create mock and testing filesystems without relying on the disk.
It is suitable for use in any situation where you would consider using the OS package as it provides an additional abstraction that makes it easy to use a memory backed file system during testing. It also adds support for the http filesystem for full interoperability.
File System Methods Available:
Chmod(name string, mode os.FileMode) : errorChown(name string, uid, gid int) : errorChtimes(name string, atime time.Time, mtime time.Time) : errorCreate(name string) : File, errorMkdir(name string, perm os.FileMode) : errorMkdirAll(path string, perm os.FileMode) : errorName() : stringOpen(name string) : File, errorOpenFile(name string, flag int, perm os.FileMode) : File, errorRemove(name string) : errorRemoveAll(path string) : errorRename(oldname, newname string) : errorStat(name string) : os.FileInfo, error

File Interfaces and Methods Available:
io.Closerio.Readerio.ReaderAtio.Seekerio.Writerio.WriterAtName() : stringReaddir(count int) : []os.FileInfo, errorReaddirnames(n int) : []string, errorStat() : os.FileInfo, errorSync() : errorTruncate(size int64) : errorWriteString(s string) : ret int, err error

Utilities
DirExists(path string) (bool, error)Exists(path string) (bool, error)FileContainsBytes(filename string, subslice []byte) (bool, error)GetTempDir(subPath string) stringIsDir(path string) (bool, error)IsEmpty(path string) (bool, error)ReadDir(dirname string) ([]os.FileInfo, error)ReadFile(filename string) ([]byte, error)SafeWriteReader(path string, r io.Reader) (err error)TempDir(dir, prefix string) (name string, err error)TempFile(dir, prefix string) (f File, err error)Walk(root string, walkFn filepath.WalkFunc) errorWriteFile(filename string, data []byte, perm os.FileMode) errorWriteReader(path string, r io.Reader) (err error)

Declare a backendThe backend can be In memory, native OS system, readonly etc
import &quot;github.com/spf13/afero&quot;var AppFs = afero.NewMemMapFs()//ORvar AppFs = afero.NewOsFs()//ORvar AppFs = afero.NewReadOnlyFs(afero.NewOsFs())

CLIThere are lots of pkgs to develop CLI, here only list some of them.

flag: simple, standard lib, not support subcommand
pflag: implementing POSIX&#x2F;GNU-style –flags
go-flags: This library provides similar functionality to the builtin flag library of go, but provides much more functionality and nicer formatting.
urfave&#x2F;cli: popular one
spf13&#x2F;cobra: more powerful, used by k8s, docker etc

Suggestion  

very simple use, no subcommand needed, less options, use flag
simple use, lots of options, use go-flags
complex command line cobra

flagpackage mainimport (    &quot;flag&quot;    &quot;fmt&quot;)// flag.int returns the var which is used to access parsed valuevar cliName = flag.String(&quot;yourname&quot;, &quot;nick&quot;, &quot;Input Your Name&quot;)var cliAge = flag.Int(&quot;yourage&quot;, 28, &quot;Input Your Age&quot;)var cliGender = flag.String(&quot;yourgender&quot;, &quot;male&quot;, &quot;Input Your Gender&quot;)var cliFlag intfunc init() &#123;    // IntVar uses an existing var    flag.IntVar(&amp;cliFlag, &quot;flagname&quot;, 1234, &quot;Just for demo&quot;)&#125;func main() &#123;    // start parsing from os.Args[1:], os.Args[0] is the app name    flag.Parse()    // print parsed value    fmt.Println(&quot;name=&quot;, *cliName)    fmt.Println(&quot;age=&quot;, *cliAge)    fmt.Println(&quot;gender=&quot;, *cliGender)    fmt.Println(&quot;flagname=&quot;, cliFlag)&#125;

go-flagsPackage flags provides an extensive command line option parser. The flags package is similar in functionality to the go builtin flag package but provides more options and uses reflection to provide a convenient and succinct way of specifying command line options.
Supported features:

Options with short names (-v)                                                 
Options with long names (–verbose)                                           
Options with and without arguments (bool v.s. other type)                     
Options with optional arguments and default values                            
Multiple option groups each containing a set of options                       
Generate and print well-formatted help message                                
Passing remaining command line arguments after – (optional)                  
Ignoring unknown command line options (optional)                              
Supports -I&#x2F;usr&#x2F;include -I&#x3D;&#x2F;usr&#x2F;include -I &#x2F;usr&#x2F;include option argument specification
Supports multiple short options -aux                                          
Supports all primitive go types (string, int{8..64}, uint{8..64}, float)      
Supports same option multiple times (can store in slice or last option counts)
Supports maps                                                                 
Supports function callbacks                                                   
Supports namespaces for (nested) option groups

package mainimport (    &quot;fmt&quot;    &quot;os&quot;    &quot;os/exec&quot;    &quot;strings&quot;    gflags &quot;github.com/jessevdk/go-flags&quot;)func main() &#123;    var opts struct &#123;        // Slice of bool will append &#x27;true&#x27; each time the option        // is encountered (can be set multiple times, like -vvv)        Verbose []bool `short:&quot;v&quot; long:&quot;verbose&quot; description:&quot;Show verbose debug information&quot;`        // Example of automatic marshalling to desired type (uint)        Offset uint `long:&quot;offset&quot; description:&quot;Offset&quot;`        // Example of a callback, called each time the option is found.        Call func(string) `short:&quot;c&quot; description:&quot;Call phone number&quot;`        // Example of a required flag        Name string `short:&quot;n&quot; long:&quot;name&quot; description:&quot;A name&quot; required:&quot;true&quot;`        // Example of a flag restricted to a pre-defined set of strings        Animal string `long:&quot;animal&quot; choice:&quot;cat&quot; choice:&quot;dog&quot;`        // Example of a value name        File string `short:&quot;f&quot; long:&quot;file&quot; description:&quot;A file&quot; value-name:&quot;FILE&quot;`        // Example of a slice of strings        StringSlice []string `short:&quot;s&quot; description:&quot;A slice of strings&quot;`        // Example of a map        IntMap map[string]int `long:&quot;intmap&quot; description:&quot;A map from string to int&quot;`    &#125;    // Callback which will invoke callto:&lt;argument&gt; to call a number.    // Note that this works just on OS X (and probably only with    // Skype) but it shows the idea.    opts.Call = func(num string) &#123;        cmd := exec.Command(&quot;open&quot;, &quot;callto:&quot;+num)        cmd.Start()        cmd.Process.Release()    &#125;    parser := gflags.NewParser(&amp;opts, gflags.Default|gflags.IgnoreUnknown|gflags.PassAfterNonOption)    parser.Usage = &quot;start \n\n start a pod&quot;    args, err := parser.Parse()    if err != nil &#123;        os.Exit(-1)    &#125;    fmt.Printf(&quot;Verbosity: %v\n&quot;, opts.Verbose)    fmt.Printf(&quot;Offset: %d\n&quot;, opts.Offset)    fmt.Printf(&quot;Name: %s\n&quot;, opts.Name)    fmt.Printf(&quot;Animal: %s\n&quot;, opts.Animal)    fmt.Printf(&quot;StringSlice: %v\n&quot;, opts.StringSlice)    fmt.Printf(&quot;IntMap: [a:%v b:%v]\n&quot;, opts.IntMap[&quot;a&quot;], opts.IntMap[&quot;b&quot;])    // after parse, args is changed, parased ones are removed    fmt.Printf(&quot;Remaining args: %s\n&quot;, strings.Join(args, &quot; &quot;))&#125;

spf13&#x2F;cobraCobra is a library providing a simple interface to create powerful modern CLI interfaces similar to git &amp; go tools.
Cobra provides:

Easy subcommand-based CLIs: app server, app fetch, etc.
Fully POSIX-compliant flags (including short &amp; long versions)
Nested subcommands
Global, local and cascading flags
Easy generation of applications &amp; commands with cobra init appname &amp; cobra add cmdname
Intelligent suggestions (app srver… did you mean app server?)
Automatic help generation for commands and flags
Automatic help flag recognition of -h, –help, etc.
Automatically generated shell autocomplete for your application (bash, zsh, fish, powershell)
Automatically generated man pages for your application

Cobra is built on a structure of commands, arguments &amp; flags.Commands represent actions, Args are things and Flags are modifiers for those actions.like this git clone URL --bare
How to write CLI by cobra

user guide
auto generate cmd
example using cobra

Go cron schedulinggo cron is a Golang job scheduling package which lets you run Go functions periodically at pre-determined interval using a simple, human-friendly syntax.


Scheduler: The scheduler tracks all the jobs assigned to it and makes sure they are passed to the executor when ready to be run. The scheduler is able to manage overall aspects of job behavior like limiting how many jobs are running at one time.
Job: The job is simply aware of the task (go function) it’s provided and is therefore only able to perform actions related to that task like preventing itself from overruning a previous task that is taking a long time.
Executor: The executor, as it’s name suggests, is simply responsible for calling the task (go function) that the job hands to it when sent by the scheduler.

TagsJobs may have arbitrary tags added which can be useful when tracking many jobs. The scheduler supports both enforcing tags to be unique and when not unique, running all jobs with a given tag.
Interval supports

milliseconds
seconds
minutes
hours
days
weeks
months

s := gocron.NewScheduler(time.UTC)s.TagsUnique()_, _ = s.Every(1).Week().Tag(&quot;foo&quot;).Do(task)_, err := s.Every(1).Week().Tag(&quot;foo&quot;).Do(task)// error!!!s := gocron.NewScheduler(time.UTC)s.Every(2).Day().Tag(&quot;tag&quot;).At(&quot;10:00&quot;).Do(task)s.Every(1).Minute().Tag(&quot;tag&quot;).Do(task)s.RunByTag(&quot;tag&quot;)// both jobs will run

Example
s := gocron.NewScheduler(time.UTC)s.Every(5).Seconds().Do(func()&#123; ... &#125;)// strings parse to durations.Every(&quot;5m&quot;).Do(func()&#123; ... &#125;)s.Every(5).Days().Do(func()&#123; ... &#125;)task = func()&#123; ... &#125;// cron expressions supporteds.Cron(&quot;*/1 * * * *&quot;).Do(task) // every minute// you can start running the scheduler in two different ways:// starts the scheduler asynchronouslys.StartAsync()//OR//starts the scheduler and blocks current execution path s.StartBlocking()

More examples, refer to cron example
Note:Q: I’ve removed my job from the scheduler, but how can I stop a long-running job that has already been triggered?A: We recommend using a means of canceling your job, e.g. a context.WithCancel().
HTTP ServerRestful API serverHere we only list pure restful api server package, which is small, fast.

pat
gorilla&#x2F;mux
httrouter
go-chi&#x2F;chi

Let’s focus on mux which is powerful and used in many production
gorilla&#x2F;muxPaths can have variables. They are defined using the format {name} or {name:pattern}. If a regular expression pattern is not defined, the matched variable will be anything until the next slash.
r := mux.NewRouter()r.HandleFunc(&quot;/products/&#123;key&#125;&quot;, ProductHandler)r.HandleFunc(&quot;/articles/&#123;category&#125;/&quot;, ArticlesCategoryHandler)r.HandleFunc(&quot;/articles/&#123;category&#125;/&#123;id:[0-9]+&#125;&quot;, ArticleHandler)func ArticlesCategoryHandler(w http.ResponseWriter, r *http.Request) &#123;    vars := mux.Vars(r)    w.WriteHeader(http.StatusOK)    fmt.Fprintf(w, &quot;Category: %v\n&quot;, vars[&quot;category&quot;])&#125;

 

Routes can also be restricted
r := mux.NewRouter()// restrict a particular router.HandleFunc(&quot;/products&quot;, ProductsHandler).Host(&quot;www.example.com&quot;).Methods(&quot;GET&quot;).Schemes(&quot;http&quot;)
 

Group several routes that share the same requirements
r := mux.NewRouter()s := r.PathPrefix(&quot;/products&quot;).Subrouter()// &quot;/products/&quot;s.HandleFunc(&quot;/&quot;, ProductsHandler)// &quot;/products/&#123;key&#125;/&quot;s.HandleFunc(&quot;/&#123;key&#125;/&quot;, ProductHandler)// &quot;/products/&#123;key&#125;/details&quot;s.HandleFunc(&quot;/&#123;key&#125;/details&quot;, ProductDetailsHandler)
 

Graceful shutdown
package mainimport (    &quot;context&quot;    &quot;flag&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;os&quot;    &quot;os/signal&quot;    &quot;time&quot;    &quot;github.com/gorilla/mux&quot;)func main() &#123;    var wait time.Duration    flag.DurationVar(&amp;wait, &quot;graceful-timeout&quot;, time.Second*15, &quot;the duration for which the server gracefully wait for existing connections to finish - e.g. 15s or 1m&quot;)    flag.Parse()    r := mux.NewRouter()    // Add your routes as needed    srv := &amp;http.Server&#123;        Addr: &quot;0.0.0.0:8000&quot;,        // Good practice to set timeouts to avoid Slowloris attacks.        WriteTimeout: time.Second * 15,        ReadTimeout:  time.Second * 15,        IdleTimeout:  time.Second * 60,        Handler:      r, // Pass our instance of gorilla/mux in.    &#125;    // Run our server in a goroutine so that it doesn&#x27;t block.    go func() &#123;        if err := srv.ListenAndServe(); err != nil &#123;            log.Println(err)        &#125;    &#125;()    c := make(chan os.Signal, 1)    // We&#x27;ll accept graceful shutdowns when quit via SIGINT (Ctrl+C)    // SIGKILL, SIGQUIT or SIGTERM (Ctrl+/) will not be caught.    signal.Notify(c, os.Interrupt)    // Block until we receive our signal.    &lt;-c    // Create a deadline to wait for.    ctx, cancel := context.WithTimeout(context.Background(), wait)    defer cancel()    // Doesn&#x27;t block if no connections, but will otherwise wait    // until the timeout deadline.    srv.Shutdown(ctx)    // Optionally, you could run srv.Shutdown in a goroutine and block on    // &lt;-ctx.Done() if your application should wait for other services    // to finalize based on context cancellation.    log.Println(&quot;shutting down&quot;)    os.Exit(0)&#125;
show all routes call walk of the router
r.Walk(func(route *mux.Route, router *mux.Router, ancestors []*mux.Route) error &#123;     pathTemplate, err := route.GetPathTemplate()     if err == nil &#123;         fmt.Println(&quot;ROUTE:&quot;, pathTemplate)     &#125;     queriesTemplates, err := route.GetQueriesTemplates()     if err == nil &amp;&amp; len(queriesTemplates) != 0 &#123;         fmt.Println(&quot;Queries templates:&quot;, strings.Join(queriesTemplates, &quot;,&quot;))     &#125;     methods, err := route.GetMethods()     if err == nil &#123;         fmt.Println(&quot;Methods:&quot;, strings.Join(methods, &quot;,&quot;))     &#125; else &#123;         fmt.Println(&quot;Methods: Any&quot;)     &#125;     fmt.Println()     return nil &#125;)
NOTE

Routes are tested in the order they were added to the router. If two routes match, the first one wins
each handler must declare like this func HandlerXXX(w http.ResponseWriter, r *http.Request) &#123;&#125;

package mainimport (    &quot;fmt&quot;    &quot;net/http&quot;    &quot;log&quot;    &quot;github.com/gorilla/mux&quot;)func YourHandler(w http.ResponseWriter, r *http.Request) &#123;    // no content-type, default text/plain    w.WriteHeader(http.StatusOK)    fmt.Printf(&quot;Gorialla\n&quot;)        //  w.Header().Set(&quot;Content-type&quot;, &quot;application/json&quot;)    //  p := struct &#123;    //     Name string    //  &#125;&#123;    //      Name: &quot;gorilla&quot;,    //  &#125;    //  w.WriteHeader(http.StatusOK)    //  json.NewEncoder(w).Encode(&amp;p)&#125;func main() &#123;    r := mux.NewRouter()    // Routes consist of a path and a handler function.    r.HandleFunc(&quot;/&quot;, YourHandler)        srv := &amp;http.Server&#123;        Addr: &quot;0.0.0.0:8000&quot;,        // Good practice to set timeouts to avoid Slowloris attacks.        WriteTimeout: time.Second * 15,        ReadTimeout:  time.Second * 15,        IdleTimeout:  time.Second * 60,        Handler:      r, // Pass our instance of gorilla/mux in.    &#125;        if err := srv.ListenAndServe(); err != nil &#123;        log.Println(err)    &#125;&#125;

yaml vs json vs tomlTOML shares traits with other file formats used for application configuration and data serialization, such as YAML and JSON. TOML and JSON both are simple and use ubiquitous data types, making them easy to code for or parse with machines. TOML and YAML both emphasize human readability features, like comments that make it easier to understand the purpose of a given line. TOML differs in combining these, allowing comments (unlike JSON) but preserving simplicity (unlike YAML).
spec

yaml wiki
yaml spec
yaml example
json spec
toml spec

Golang support

Yaml
BurntSushi&#x2F;toml
This Go package provides a reflection interface similar to Go’s standard library json and xml packages. This package also supports the encoding.TextUnmarshaler and encoding.TextMarshaler interfaces so that you can define custom data representations, it supports tag as well like json   type b struct{Port string toml:&quot;port,omitempty&quot;&#125;


Viper
Find, load, and unmarshal a configuration file in JSON, TOML, YAML, HCL, INI, envfile or Java properties formats.
Provide a mechanism to set default values for your different configuration options.
Provide a mechanism to set override values for options specified through command line flags.
Provide an alias system to easily rename parameters without breaking existing code.
Make it easy to tell the difference between when a user has provided a command line or config file which is the same as the default.



toml decodevar config tomlConfig// one line of code to parse toml file!!!if _, err := toml.DecodeFile(&quot;example.toml&quot;, &amp;config); err != nil &#123;    fmt.Println(err)    return&#125;

# This is a TOML document. Boom.title = &quot;TOML Example&quot;[owner]name = &quot;Tom Preston-Werner&quot;organization = &quot;GitHub&quot;bio = &quot;GitHub Cofounder &amp; CEO\nLikes tater tots and beer.&quot;dob = 1979-05-27T07:32:00Z # First class dates? Why not?[database]server = &quot;192.168.1.1&quot;ports = [ 8001, 8001, 8002 ]connection_max = 5000enabled = true[servers]  # You can indent as you please. Tabs or spaces. TOML don&#x27;t care.  [servers.alpha]  ip = &quot;10.0.0.1&quot;  dc = &quot;eqdc10&quot;  [servers.beta]  ip = &quot;10.0.0.2&quot;  dc = &quot;eqdc10&quot;[clients]data = [ [&quot;gamma&quot;, &quot;delta&quot;], [1, 2] ] # just an update to make sure parsers support it# Line breaks are OK when inside arrayshosts = [  &quot;alpha&quot;,  &quot;omega&quot;]


package mainimport (    &quot;fmt&quot;    &quot;time&quot;    &quot;github.com/BurntSushi/toml&quot;)type tomlConfig struct &#123;    Title   string    Owner   ownerInfo    DB      database `toml:&quot;database&quot;` // key in toml    Servers map[string]server    Clients clients&#125;type ownerInfo struct &#123;    Name string    Org  string `toml:&quot;organization&quot;`    Bio  string    DOB  time.Time&#125;type database struct &#123;    Server  string    Ports   []int    ConnMax int `toml:&quot;connection_max&quot;`    Enabled bool&#125;type server struct &#123;    IP string    DC string&#125;type clients struct &#123;    Data  [][]interface&#123;&#125;    Hosts []string&#125;func main() &#123;    var config tomlConfig    if _, err := toml.DecodeFile(&quot;example.toml&quot;, &amp;config); err != nil &#123;        fmt.Println(err)        return    &#125;    fmt.Printf(&quot;Title: %s\n&quot;, config.Title)    fmt.Printf(&quot;Owner: %s (%s, %s), Born: %s\n&quot;,        config.Owner.Name, config.Owner.Org, config.Owner.Bio,        config.Owner.DOB)    fmt.Printf(&quot;Database: %s %v (Max conn. %d), Enabled? %v\n&quot;,        config.DB.Server, config.DB.Ports, config.DB.ConnMax,        config.DB.Enabled)    for serverName, server := range config.Servers &#123;        fmt.Printf(&quot;Server: %s (%s, %s)\n&quot;, serverName, server.IP, server.DC)    &#125;    fmt.Printf(&quot;Client data: %v\n&quot;, config.Clients.Data)    fmt.Printf(&quot;Client hosts: %v\n&quot;, config.Clients.Hosts)&#125;


package mainimport (    &quot;fmt&quot;    &quot;github.com/BurntSushi/toml&quot;)func main() &#123;    type Config struct &#123;        Age int    &#125;    var outputs Config    outputs.Age = 10 // default value if user not set            if _, err := toml.Decode(&quot;&quot;, &amp;outputs); err != nil &#123;        fmt.Println(err)    &#125; else &#123;        fmt.Printf(&quot;decode from encoded buffer(toml to struct): %+v\n&quot;, outputs)    &#125;&#125;main()

decode from encoded buffer(toml to struct): &#123;Age:10&#125;

toml encode// dst can be bytes.Buffer or File who implements io.Writere := toml.NewEncoder(dst)// inputs are struct which will be encode as toml as output to dsterr := e.Encode(inputs)


package mainimport (    &quot;bytes&quot;    &quot;log&quot;    &quot;net&quot;    &quot;time&quot;    &quot;github.com/BurntSushi/toml&quot;)func main() &#123;    type Config struct &#123;        Age        int        Cats       []string        Pi         float64        Perfection []int        DOB        time.Time        Ipaddress  net.IP    &#125;    var inputs = Config&#123;        13,        []string&#123;&quot;one&quot;, &quot;two&quot;, &quot;three&quot;&#125;,        3.145,        []int&#123;11, 2, 3, 4&#125;,        time.Now(),        net.ParseIP(&quot;192.168.59.254&quot;),    &#125;        // log to console!!!    log.SetOutput(os.Stdout)        var firstBuffer bytes.Buffer    // Encoder can be file or bytes.Buffer who implements io.Writer    e := toml.NewEncoder(&amp;firstBuffer)    err := e.Encode(inputs)    if err != nil &#123;        log.Fatalln(err)    &#125; else &#123;        // encode inputs to firstBuffer        log.Println(&quot;encoded first buffer toml format:&quot;, firstBuffer.String())    &#125;    var outputs Config    // decode from firstBuffer to outputs    if _, err := toml.Decode(firstBuffer.String(), &amp;outputs); err != nil &#123;        log.Printf(&quot;Could not decode:\n-----\n%s\n-----\n&quot;,            firstBuffer.String())        log.Fatalln(err)    &#125; else &#123;        log.Printf(&quot;decode from encoded buffer(toml to struct): %+v&quot;, outputs)    &#125;    // could test each value individually, but I&#x27;m lazy    var secondBuffer bytes.Buffer    e2 := toml.NewEncoder(&amp;secondBuffer)    // encode outputs to secondBuffer    err = e2.Encode(outputs)    if err != nil &#123;        log.Fatalln(err)    &#125; else &#123;        log.Println(&quot;encoded second buffer toml format:&quot;, firstBuffer.String())    &#125;    if firstBuffer.String() != secondBuffer.String() &#123;        log.Printf(            firstBuffer.String(),            &quot;\n\n is not identical to\n\n&quot;,            secondBuffer.String())    &#125;&#125;main()

2022/07/01 17:23:30 encoded first buffer toml format: Age = 13
Cats = [&quot;one&quot;, &quot;two&quot;, &quot;three&quot;]
Pi = 3.145
Perfection = [11, 2, 3, 4]
DOB = 2022-07-01T17:23:30.172328016+08:00
Ipaddress = &quot;192.168.59.254&quot;

2022/07/01 17:23:30 decode from encoded buffer(toml to struct): &#123;Age:13 Cats:[one two three] Pi:3.145 Perfection:[11 2 3 4] DOB:2022-07-01 17:23:30.172328016 +0800 CST Ipaddress:192.168.59.254&#125;
2022/07/01 17:23:30 encoded second buffer toml format: Age = 13
Cats = [&quot;one&quot;, &quot;two&quot;, &quot;three&quot;]
Pi = 3.145
Perfection = [11, 2, 3, 4]
DOB = 2022-07-01T17:23:30.172328016+08:00
Ipaddress = &quot;192.168.59.254&quot;

bitOnly support 64 bits [0, 63].
package mainimport (    &quot;fmt&quot;    &quot;k8s.io/kubernetes/pkg/kubelet/cm/topologymanager/bitmask&quot;)func main() &#123;    bm1 := bitmask.NewEmptyBitMask()    bm1.Add(0)    bm1.Add(1, 2)    if bm1.IsEmpty() &#123;        fmt.Println(&quot;bm1 is empty&quot;)    &#125; else &#123;        fmt.Println(&quot;bm1(0 as the first bit):&quot;, bm1)    &#125;    bm2 := bitmask.NewEmptyBitMask()    bm2.Add(3)    if bm1.IsEqual(bm2) &#123;        fmt.Println(&quot;bm1 and bm2 are equal&quot;)    &#125; else &#123;        fmt.Println(&quot;bm1 and bm2 are not equal&quot;)        fmt.Println(&quot;bm1:&quot;, bm1)        fmt.Println(&quot;bm2:&quot;, bm2)    &#125;    bm1.Or(bm2)    fmt.Println(&quot;bm1=bm1|bm2:&quot;, bm1)    bm1.Remove(3)    fmt.Println(&quot;bm1.Remove(3):&quot;, bm1)    fmt.Println(&quot;bm1.GetBits:&quot;, bm1.GetBits())    // IterateBitMasks iterates all possible masks from a list of bits, issuing a callback on each mask.    &quot;    // [0, 1, 2], 瀵规瘡涓彲鑳界殑缁勫悎锛?璋冪敤func!!!\    &quot;    // 1, 10, 100   璁剧疆涓€浣?    &quot;    // 11, 101, 110 璁剧疆涓や綅    &quot;    // 111          璁剧疆涓変綅    bitmask.IterateBitMasks(bm1.GetBits(), func(mask bitmask.BitMask) &#123;        fmt.Println(mask)    &#125;)&#125;

bm1(0 as the first bit): 0111bm1 and bm2 are not equalbm1: 0111bm2: 1000bm1=bm1|bm2: 1111bm1.Remove(3): 0111bm1.GetBits: [0 1 2]011010011010101100111

encoding(protobuffer)A very simple “address book” application that can read and write people’s contact details to and from a file. Each person in the address book has a name, an ID, an email address, and a contact phone number.
How do you serialize and retrieve structured data like this? There are a few ways to solve this problem:

Use gobs to serialize Go data structures. This is a good solution in a Go-specific environment, but it doesn’t work well if you need to share data with applications written for other platforms.
You can invent an ad-hoc way to encode the data items into a single string such as encoding 4 ints as “12:3:-23:67”. This is a simple and flexible approach, although it does require writing one-off encoding and parsing code, and the parsing imposes a small run-time cost. This works best for encoding very simple data.
Serialize the data to XML(or json). This approach can be very attractive since XML is (sort of) human readable and there are binding libraries for lots of languages. This can be a good choice if you want to share data with other applications&#x2F;projects. However, XML is notoriously space intensive, and encoding&#x2F;decoding it can impose a huge performance penalty on applications. Also, navigating an XML DOM tree is considerably more complicated than navigating simple fields in a class normally would be.

Protocol buffers are the flexible, efficient, automated solution to solve exactly this problem. With protocol buffers, you write a .proto description of the data structure you wish to store. From that, the protocol buffer compiler creates a class that implements automatic encoding and parsing of the protocol buffer data with an efficient binary format. The generated class provides getters and setters for the fields that make up a protocol buffer and takes care of the details of reading and writing the protocol buffer as a unit. Importantly, the protocol buffer format supports the idea of extending the format over time in such a way that the code can still read data encoded with the old format.
Rules for compatibilitySooner or later after you release the code that uses your protocol buffer, you will undoubtedly want to “improve” the protocol buffer’s definition. If you want your new buffers to be backwards-compatible, and your old buffers to be forward-compatible and you almost certainly do want this then there are some rules you need to follow. In the new version of the protocol buffer:

you must not change the tag numbers of any existing fields.
you may delete fields.
you may add new fields but you must use fresh tag numbers (i.e. tag numbers that were never used in this protocol buffer, not even by deleted fields).

If you follow these rules, old code will happily read new messages and simply ignore any new fields. To the old code, singular fields that were deleted will simply have their default value, and deleted repeated fields will be empty. New code will also transparently read old messages.
However, keep in mind that new fields will not be present in old messages, so you will need to do something reasonable with the default value. A type-specific default value is used: for strings, the default value is the empty string. For booleans, the default value is false. For numeric types, the default value is zero.
stylestyle guide gives suggestion how to write xx.proto file when define message and rpc, here is a summary from that.
All files should be ordered in the following manner:

License header (if applicable)
File overview
Syntax(proto2 or proto3)
Package(used by proto to import message defined in other package)
Imports (sorted)(imports other protos)
File options(like option go_package &#x3D; “github.com&#x2F;xvrzhao&#x2F;pb-demo&#x2F;proto&#x2F;article”;)
Everything elsesyntax=&quot;proto3&quot;;package hello;import &quot;store/name.proto&quot;option go_package = &quot;github.com/jason/hello&quot;

Message and field namesUse CamelCase (with an initial capital) for message names for example, SongServerRequest. Use underscore_separated_names for field names (including oneof field and extension names) for example, song_name
message SongServerRequest &#123;  optional string song_name = 1; // for go, generated code is SongName!!!&#125;

Repeated fieldsUse pluralized names for repeated fields.
repeated string keys = 1;

EnumsUse CamelCase (with an initial capital) for enum type names and CAPITALS_WITH_UNDERSCORES for value names:
enum FooBar &#123;  FOO_BAR_UNSPECIFIED = 0;  FOO_BAR_FIRST_VALUE = 1;  FOO_BAR_SECOND_VALUE = 2;&#125;

ServicesUse CamelCase (with an initial capital) for both the service name and any RPC method names:
service FooService &#123;  rpc GetSomething(GetSomethingRequest) returns (GetSomethingResponse);  rpc ListSomething(ListSomethingRequest) returns (ListSomethingResponse);&#125;
proto3Default values:

For strings, the default value is the empty string.
For bytes, the default value is empty bytes.
For bools, the default value is false.
For numeric types, the default value is zero.
For enums, the default value is the first defined enum value, which must be 0.
For message fields, the field is not set. Its exact value is language-dependent. See the generated code guide for details.

go references  

proto3 go tutorial
proto3 go reference
go API

build proto3 of goIn order to generate go code, you must have protoc and go plugins installed, here are steps to install them all.
Prerequisite
$ wget -O ./protoc-3.15.8-linux-x86_64.zip https://github.com/protocolbuffers/protobuf/releases/download/v3.15.8/protoc-3.15.8-linux-x86_64.zip$ unzip protoc-3.15.8-linux-x86_64.zip -d /usr/local# Install the protocol compiler plugins for Go using the following commands# protoc-gen-go: go plugin or gogo/protobuf# proto-gen-go-grpc: go rpc plugin as protoc-gen-go does not provide it, hence needs this plugin$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.26$ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.1 # this is not needed if no service!!!

gogo&#x2F;protobufgogo&#x2F;protobuf is a fork of golang&#x2F;protobuf with extra code generation features, it provides several plugins for go code generate, pick the right one for use

protoc-gen-gofast(speed than protoc-gen-go)
protoc-gen-gogofast (same as gofast, but imports gogoprotobuf)
protoc-gen-gogofaster (same as gogofast, without XXX_unrecognized, less pointer fields)
protoc-gen-gogoslick (same as gogofaster, but with generated string, gostring and equal methods)
protoc-gen-gogo (Most Speed and most customization)

# basic way$ go get github.com/gogo/protobuf/protoc-gen-gofast# others install protoc-gen-gogofast, protoc-gen-gogofaster, protoc-gen-gogoslick, protoc-gen-gogo$ go get github.com/gogo/protobuf/proto$ go get github.com/gogo/protobuf/jsonpb$ go get github.com/gogo/protobuf/&#123;binary&#125;$ go get github.com/gogo/protobuf/gogoproto# just use one of them is enough!!!$ protoc --gofast_out=. --gofast_opt=paths=source_relative myproto.proto$ protoc --gogofast_out=. --gogofast_opt=paths=source_relative myproto.proto$ protoc --gogofaster_out=. --gogofaster_opt=paths=source_relative myproto.proto$ protoc --gogoslick_out=. --gogoslick_opt=paths=source_relative myproto.proto# the best one to use!!!$ protoc --gogo_out=. --gogo_opt=paths=source_relative myproto.proto

In order to generate Go code, the Go package’s import path must be provided for every .proto file). There are two ways to specify the Go import path:

by declaring it within the .proto file  option go_package &#x3D; “example.com&#x2F;project&#x2F;protos&#x2F;fizz”;’

by declaring it on the command line when invoking protoc, by passing one or more M$&#123;PROTO_FILE&#125;=$&#123;GO_IMPORT_PATH&#125;  $protoc –go_opt&#x3D;Mprotos&#x2F;buzz.proto&#x3D;example.com&#x2F;project&#x2F;protos&#x2F;fizz protos&#x2F;buzz.proto&#96;


compile protouse cases for code generation of Go
# --go_out=. generate go code under this directory# --go_opt=paths=source_relative generate go code same layout as its proto path under --go_out# --go_opt=paths=import generate go code with go_package under --go_out$ pwd/home/data/hello$ lscool/ protocols/# generate code in the same directory as its proto$ protoc --go_out=. --go_opt=paths=source_relative protocols/greet.proto$ protoc --go_out=. --go_opt=paths=source_relative protocols/*.proto$ ls protocols/greet.pb.go  greet.proto# generate code with same layout as its proto but in different go_out$ protoc --go_out=cool --go_opt=paths=source_relative protocols/greet.proto$ ls cool/protocols/ protocols/cool/protocols/:greet.pb.goprotocols/:greet.proto# generate code with package layout defined in proto in different go_out$ protoc --go_out=cool --go_opt=paths=import protocols/greet.proto$ tree.|-- cool|   `-- github.com|       `-- hello|           `-- runtime|               `-- protocols|                   `-- greet|                       `-- greet.pb.go`-- protocols    `-- greet.proto$ protoc --go_out=$GOPATH/src --go_opt=paths=import protocols/greet.proto# as service will create a separate pb file that&#x27;s different with message# use these option for rpc generation: --go-grpc_out=. --go-grpc_opt=paths=source_relative# if proto imports other protos, add searching path: -I=other/proto/:.

Most used one: put your source at $GOPATH&#x2F;src&#x2F;github.com $ protoc --go_out=$GOPATH/src/ --go_opt=paths=import --go-grpc_out=$GOPATH/src/ --go-grpc_opt=paths=import protocols/greet.proto$ protoc --gogo_out=. --gogo_opt=paths=source_relative --go-grpc_out=$GOPATH/src/ --go-grpc_opt=paths=import protocols/greet.proto
Most used one for testing$ protoc --go_out=. --go_opt=paths=source_relative protocols/greet.proto$ protoc --gogo_out=. --gogo_opt=paths=source_relative protocols/greet.proto
(base) [root@localhost hello]# pwd/home/go/src/github.com/hello(base) [root@localhost hello]# tree.|-- protocols|   `-- greet.proto`-- runtime    `-- protocols        `-- greet            `-- greet.pb.go



generated go codegreet.proto
syntax = &quot;proto3&quot;;package greet;//used by proto itself(independent with different language)// to prevent naming conflicts between different projects(protos).// import &quot;google/protobuf/timestamp.proto&quot;;option go_package = &quot;github.com/hello/runtime/protocols/greet&quot;;// used by protoc when generate go specific codemessage Person &#123;  string name = 1;  int32 id = 2;  // Unique ID number for this person.  string email = 3;  enum PhoneType &#123;    MOBILE = 0;    HOME = 1;    WORK = 2;  &#125;  message PhoneNumber &#123;    string number = 1;    PhoneType type = 2;  &#125;  repeated PhoneNumber phones = 4;  google.protobuf.Timestamp last_updated = 5;&#125;// Our address book file is just one of these.message AddressBook &#123;  repeated Person people = 1;&#125;
For nested message, generated go code with its parent as prefix like type Person_PhoneNumber struct
If a field value isn&#39;t set, a default value is used: zero for numeric types, the empty string for strings, false for bools. For embedded messages, the default value is always the “default instance” or “prototype” of the message, which has none of its fields set. Calling the accessor to get the value of a field which has not been explicitly set always returns that field’s default value.
If a field is repeated, the field may be repeated any number of times (including zero). The order of the repeated values will be preserved in the protocol buffer. Think of repeated fields as dynamically sized arrays. 
There is no correlation between the Go import path and the package specifier in the .proto file. The latter is only relevant to the protobuf namespace, while the former is only relevant to the Go namespace. Also, there is no correlation between the Go import path and the .proto import path. 
The generated Go field names always use camel-case naming, even if the field name in the .proto file uses lower-case with underscores (as it should). Thus, the proto field foo_bar_baz becomes FooBarBaz in Go.
types mapping  



.proto Type
Notes
Go Type



double

float64


float

float32


int32
Uses variable-length encoding. Inefficient for encoding negative numbers if your field is likely to have negative values, use sint32 instead.
int32


int64
Uses variable-length encoding. Inefficient for encoding negative numbers if your field is likely to have negative values, use sint64 instead.
int64


uint32
Uses variable-length encoding.
uint32


uint64
Uses variable-length encoding.
uint64


sint32
Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s.
int32


sint64
Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s.
int64


fixed32
Always four bytes. More efficient than uint32 if values are often greater than 2^28.
uint32


fixed64
Always eight bytes. More efficient than uint64 if values are often greater than 2^56.
uint64


sfixed32
Always four bytes.
int32


sfixed64
Always eight bytes.
int64


bool

bool


string
A string must always contain UTF-8 encoded or 7-bit ASCII text, and cannot be longer than 2^32.
string


bytes
May contain any arbitrary sequence of bytes no longer than 2^32.
[]byte


singular message field
message Baz &#123;  Bar foo = 1;&#125;
&#x3D;&gt;&gt;
type Baz struct &#123;        Foo *Bar // it&#x27;s pointer&#125;

Repeated Fields
message Baz &#123;  repeated Bar foo = 1;&#125;
&#x3D;&gt;
type Baz struct &#123;        Foo  []*Bar&#125;baz := &amp;Baz&#123;  Foo: []*Bar&#123;    &#123;&#125;, // First element. not &amp;?    &#123;&#125;, // Second element.  &#125;,&#125;foo := baz.GetFoo() // foo type is []*Bar.b1 := foo[0] // b1 type is *Bar, the first element in foo.

Map Fields
message Bar &#123;&#125;message Baz &#123;  map&lt;string, Bar&gt; foo = 1;&#125;
&#x3D;&gt;
type Baz struct &#123;        Foo map[string]*Bar&#125;

Enum
message SearchRequest &#123;  enum Corpus &#123;    UNIVERSAL = 0;    WEB = 1;    IMAGES = 2;    LOCAL = 3;    NEWS = 4;    PRODUCTS = 5;    VIDEO = 6;  &#125;  Corpus corpus = 1;  ...&#125;
&#x3D;&gt;
type SearchRequest_Corpus int32const (        SearchRequest_UNIVERSAL SearchRequest_Corpus = 0        SearchRequest_WEB       SearchRequest_Corpus = 1        SearchRequest_IMAGES    SearchRequest_Corpus = 2        SearchRequest_LOCAL     SearchRequest_Corpus = 3        SearchRequest_NEWS      SearchRequest_Corpus = 4        SearchRequest_PRODUCTS  SearchRequest_Corpus = 5        SearchRequest_VIDEO     SearchRequest_Corpus = 6)

use generated codep := pb.Person&#123;        Id:    1234,        Name:  &quot;John Doe&quot;,        Email: &quot;jdoe@example.com&quot;,        Phones: []*pb.Person_PhoneNumber&#123;                &#123;Number: &quot;555-4321&quot;, Type: pb.Person_HOME&#125;,        &#125;,&#125;// writing message to file(serialized)book := &amp;pb.AddressBook&#123;&#125;// ...// Write the new address book back to disk.out, err := proto.Marshal(book)if err != nil &#123;        log.Fatalln(&quot;Failed to encode address book:&quot;, err)&#125;if err := ioutil.WriteFile(fname, out, 0644); err != nil &#123;        log.Fatalln(&quot;Failed to write address book:&quot;, err)&#125;// reading message back to structin, err := ioutil.ReadFile(fname)if err != nil &#123;        log.Fatalln(&quot;Error reading file:&quot;, err)&#125;book := &amp;pb.AddressBook&#123;&#125;if err := proto.Unmarshal(in, book); err != nil &#123;        log.Fatalln(&quot;Failed to parse address book:&quot;, err)&#125;

ptyBy default, Unix-style tty (i.e. console) drivers will take input in &quot;cooked mode&quot;. In this mode, it provides a certain amount of command-line editing. The user can type in a line of input, possibly deleting and retyping some of it (but that doesn’t always work) and the program won&#39;t see it until the user hits enter.
In contrast, raw mode sets up the TTY driver to pass every character to the program as it is typed. Programs are started in cooked mode by default and need to enable raw mode. that means in raw mode, ctrl+c, ctrl+\ passed in as well, signal hanler is not called!!!, but in cooked mode, it’s handler is called, hence program exits!!!
Here is example how to enable raw mode in C terminal in C which calls ioctl to do this.
go packagePty is a Go package for using unix pseudo-terminals.go get github.com/creack/pty
shellrun bash in pty to create a new terminal
package mainimport (    &quot;io&quot;    &quot;log&quot;    &quot;os&quot;    &quot;os/exec&quot;    &quot;os/signal&quot;    &quot;syscall&quot;    &quot;github.com/creack/pty&quot;    &quot;golang.org/x/term&quot;)func test() error &#123;    // Create arbitrary command.    c := exec.Command(&quot;bash&quot;)    // Start the command with a pty.    // run the command async(and cmd&#x27;s std set with pty file /dev/pts/x)    ptmx, err := pty.Start(c)    if err != nil &#123;        return err    &#125;    // Make sure to close the pty at the end.    defer func() &#123; _ = ptmx.Close() &#125;() // Best effort.    // Handle pty size.    ch := make(chan os.Signal, 1)    // window size change signal    signal.Notify(ch, syscall.SIGWINCH)    go func() &#123;        for range ch &#123;            // window size change, change ptmx to os.Stdin size            if err := pty.InheritSize(os.Stdin, ptmx); err != nil &#123;                log.Printf(&quot;error resizing pty: %s&quot;, err)            &#125;        &#125;    &#125;()    ch &lt;- syscall.SIGWINCH                        // Initial resize.    defer func() &#123; signal.Stop(ch); close(ch) &#125;() // Cleanup signals when done.    // Set stdin in raw mode, key types is sent to pty directly without cookied    oldState, err := term.MakeRaw(int(os.Stdin.Fd()))    if err != nil &#123;        panic(err)    &#125;    defer func() &#123; _ = term.Restore(int(os.Stdin.Fd()), oldState) &#125;() // Best effort.    // Copy stdin to the pty and the pty to stdout.    // NOTE: The goroutine will keep reading until the next keystroke before returning.    go func() &#123;        // io.Copy blocks if no input        // when run $ ls        // 1. for each char, l and s Copy from StdIn, then Copy to Stdout        // 2. if enter is pressed, ptmx gets the full input as cmd.Stdin set with ptmx, so cmd gets the input &#x27;ls&#x27;, runs it and writes output to cmd.Stdout which is ptmx        // 2. Copy the ls output from ptmx to os.Stdout, so we can see it        _, _ = io.Copy(ptmx, os.Stdin)        //ptmx implement Reader/Writer interface    &#125;()    // blocks if no output from ptmx    _, _ = io.Copy(os.Stdout, ptmx)    return nil&#125;func main() &#123;    if err := test(); err != nil &#123;        log.Fatal(err)    &#125;&#125;


package mainimport (    &quot;io&quot;    &quot;os&quot;    &quot;os/exec&quot;    &quot;github.com/creack/pty&quot;)func main() &#123;    // create a command, not run here    c := exec.Command(&quot;grep&quot;, &quot;--color=auto&quot;, &quot;bar&quot;)    // Start assigns a pseudo-terminal tty os.File to c.Stdin, c.Stdout,    // and c.Stderr, calls c.Start, and returns the File of pty(/dev/pts/X)    // set Stdxx of command: c.Stdin = c.Stdout = c.Stderr = os.File(/dev/pts/X)    // then run command with c.Start()(async), when NO input, grep hangs for input    f, err := pty.Start(c)    if err != nil &#123;        panic(err)    &#125;    go func() &#123;        // File(/dev/pts/X) of corresponding pty        // write to /dev/pts/X, like input from user, input to command grep        f.Write([]byte(&quot;foo\n&quot;))        f.Write([]byte(&quot;bar\n&quot;))        f.Write([]byte(&quot;baz\n&quot;))        f.Write([]byte&#123;4&#125;) // EOT    &#125;()    // as cmd&#x27;s Stdout set to f, hence grep writes output to file /dev/ptx/X    // copy output from f(read from /dev/ptx/X) to os.Stdout!!!    io.Copy(os.Stdout, f)&#125;main()

foo
bar
baz

FAQuse io.pipe or channel?It depends on the data that will be transmitted, if data is stream, use io.Pipe, if data is fixed type, use channel, as it’s like datagram!!
Ref
standard library
pkg repo

]]></content>
      <categories>
        <category>go program</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>hardware-vgpu</title>
    <url>/2021/08/09/hardware-vgpu/</url>
    <content><![CDATA[IntroductionVirtual GPU (vGPU) enables multiple virtual machines (VMs) to have simultaneous, direct access to a single physical GPU, using the same graphics drivers that are deployed on non-virtualized operating systems. By doing this, vGPU provides VMs with unparalleled graphics performance, compute performance, and application compatibility, together with the cost-effectiveness and scalability brought about by sharing a GPU among multiple workloads. 


Nvidia VGPUNVIDIA vGPU software supports GPU instances on GPUs that support the Multi-Instance GPU (MIG) feature in NVIDIA vGPU and GPU pass through deployments. MIG enables a physical GPU to be securely partitioned into multiple separate GPU instances, providing multiple users with separate GPU resources to accelerate their applications. With MIG, A GPU that can be split into several GPU instances of different sizes, with each instance mapped to one vGPU. MIG needs GPU with Ampere GPU architecture, but not all GPU card supports it.
VGPU typeThe number of physical GPUs that a board has depends on the board. Each physical GPU can support several different types of virtual GPU (vGPU). vGPU types have a fixed amount of frame buffer, number of supported display heads, and maximum resolutions. They are grouped into different series according to the different classes of workload for which they are optimized. Each series is identified by the last letter of the vGPU type name.



Series
Optimal Workload



Q-series
Virtual workstations for creative and technical professionals who require the performance and features of Quadro technology, 3D rendering


C-series
Compute-intensive server workloads, such as artificial intelligence (AI), deep learning, or high-performance computing (HPC)


B-series
Virtual desktops for business professionals and knowledge workers


A-series
App streaming or session-based solutions for virtual applications users


vGPU types determines

frame buffer
display heads(virtual display outputs)
maximum resolution
number of VGPU

ExampleM60-2Q is allocated 2048 Mbytes of frame buffer on a Tesla M60 board. 
NVIDIA vGPU is a licensed product on all supported GPU boards.

Q-series vGPU types require a vWS license.
C-series vGPU types require an NVIDIA Virtual Compute Server (vCS) license but can also be used with a vWS license.
B-series vGPU types require a vPC license but can also be used with a vWS license.
A-series vGPU types require a vApps license.

ARCHHigh-level architecture of NVIDIA vGPU, Under the control of the NVIDIA Virtual GPU Manager running under the hypervisor, NVIDIA physical GPUs are capable of supporting multiple virtual GPU devices (vGPUs) that can be assigned directly to guest VMs.

Time-Sliced NVIDIA vGPU Internal ArchitectureA time-sliced vGPU is a vGPU that resides on a physical GPU that is not partitioned into multiple GPU instances. All time-sliced vGPUs resident on a GPU share access to the GPU’s engines including the graphics (3D), video decode, and video encode engines

This is VGPU Arch for traditional GPU ARCH that most of GPU card support it.
In a time-sliced vGPU, processes that run on the vGPU are scheduled to run in series. Each vGPU waits while other processes run on other vGPUs. While processes are running on a vGPU, the vGPU has exclusive use of the GPU’s engine.
For time-Sliced VGPU, the vgpu type must be same for a single GPU.
MIG-Backed NVIDIA vGPU Internal ArchitectureA MIG-backed vGPU is a vGPU that resides on a GPU instance in a MIG-capable physical GPU. Each MIG-backed vGPU resident on a GPU has exclusive access to the GPU instance’s engines, including the graphics (3D), and video decode engines.
In a MIG-backed vGPU, processes that run on the vGPU run in parallel with processes running on other vGPUs on the GPU. Process run on all vGPUs resident on a physical GPU simultaneously. 

For MIG-Backed vgpu, the vgpu type can be different for a single GPU.
GPU ProductTwo many GPU series

Quadro - This is the workstation version. Higher priced. This is meant for corporate customers, so it is better tested, more memory, etc. This is the highest quality chips. And since they are higher priced, NVIDIA offers better support, easier exchanges etc.
Tesla - This is the range that is focused on HPC. some may not have video output. This is intended for people using CUDA.

Note

Some products are only for graphics while others are only for compute
Tesla M60 and M6 GPUs support compute mode and graphics mode, can switch between them

DebugGPU
# Centos# install vgpu driver$ rpm -iv NVIDIA-vGPU-rhel-7.5-460.73.02.x86_64.rpm$ reboot# verify vgpu driver is loaded correctly$ lsmod | grep vfionvidia_vgpu_vfio       27099  0nvidia              12316924  1 nvidia_vgpu_vfiovfio_mdev              12841  0mdev                   20414  2 vfio_mdev,nvidia_vgpu_vfiovfio_iommu_type1       22342  0vfio                   32331  3 vfio_mdev,nvidia_vgpu_vfio,vfio_iommu_type1# show GPU$ nvidia-smiMon Aug  9 11:24:19 2021       +-----------------------------------------------------------------------------+| NVIDIA-SMI 430.46       Driver Version: 430.46       CUDA Version: N/A      ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 || N/A   27C    P8    19W / 250W |     41MiB / 23039MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla P40           On   | 00000000:04:00.0 Off |                    0 || N/A   27C    P8    19W / 250W |     41MiB / 23039MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla P40           On   | 00000000:84:00.0 Off |                    0 || N/A   26C    P8    19W / 250W |     50MiB / 23039MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla P40           On   | 00000000:85:00.0 Off |                    0 || N/A   31C    P8    18W / 250W |     41MiB / 23039MiB |      0%      Default |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================||  No running processes found                                                 |+-----------------------------------------------------------------------------+# Compute M. means Compute mode enabled# Disp.A: GPU uses for display# Processes: show processes running on each GPU# For GPU supported MIG, there is label MIG M. along with Compute M.# check BDF(bus, domain, function) of each GPU$ lspci | grep NVIDIA03:00.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1)04:00.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1)84:00.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1)85:00.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1)

Create VPUG
# when VGPU is enabled for a GPU, there is a link created at /sys/class/mdev_bus/ pointing to GPU device(PCI bus number)# ls /sys/class/mdev_bus/0000:03:00.0  0000:04:00.0  0000:84:00.0  0000:85:00.0# checkout all supported vgpu type of the given GPU# nvidia-156 is mdev identifier, the vgpu type is at mdev_supported_types/nvidia-156/name $ cd /sys/class/mdev_bus/0000:03:00.0$ ls mdev_supported_types/nvidia-156  nvidia-241  nvidia-284  nvidia-286  nvidia-46  nvidia-48  nvidia-50  nvidia-52  nvidia-54  nvidia-56  nvidia-58  nvidia-60  nvidia-62nvidia-215  nvidia-283  nvidia-285  nvidia-287  nvidia-47  nvidia-49  nvidia-51  nvidia-53  nvidia-55  nvidia-57  nvidia-59  nvidia-61# P40-2B， P40 is GPU type, while 2B is vgpu-type$ cat mdev_supported_types/nvidia-156/name GRID P40-2B# check how many VGPU(depends on type) can be created for a given GPU$ cat mdev_supported_types/nvidia-156/available_instances 12# create a VGPU$ uuidgen2794ee88-7932-4c37-9927-97ef3a5e76c4$ echo &quot;2794ee88-7932-4c37-9927-97ef3a5e76c4&quot;&gt; mdev_supported_types/nvidia-156/create# after this a mdev device(VGPU device) is created$ ls /sys/bus/mdev/devices/2794ee88-7932-4c37-9927-97ef3a5e76c4driver  iommu_group  mdev_type  nvidia  power  remove  subsystem  uevent# assign vgpu to VM # VM must# 1. The VM to which you want to add the vGPUs is shut down. $ virsh edit $vm-name# uuid is the vgpu uuid or use bdf is also ok&lt;device&gt;...  &lt;hostdev mode=&#x27;subsystem&#x27; type=&#x27;mdev&#x27; model=&#x27;vfio-pci&#x27;&gt;    &lt;source&gt;      &lt;address uuid=&#x27;2794ee88-7932-4c37-9927-97ef3a5e76c4&#x27;/&gt;    &lt;/source&gt;  &lt;/hostdev&gt;&lt;/device&gt;# check vm who is using this vgpu$ cat /sys/bus/mdev/devices/2794ee88-7932-4c37-9927-97ef3a5e76c4/nvidia/vm_name# remove a vgpu must know the uuid of vgpu# VM must# 1. The VM to which the vGPU is assigned is shut down$ echo &quot;1&quot;&gt; /sys/bus/mdev/devices/2794ee88-7932-4c37-9927-97ef3a5e76c4/remove


Monitoring GPU performance
# get GPU INFO$ nvidia-smi# get VGPU INFO$ nvidia-smi vgpuMon Aug  9 12:14:14 2021       +-----------------------------------------------------------------------------+| NVIDIA-SMI 430.46                 Driver Version: 430.46                    ||---------------------------------+------------------------------+------------+| GPU  Name                       | Bus-Id                       | GPU-Util   ||      vGPU ID     Name           | VM ID     VM Name            | vGPU-Util  ||=================================+==============================+============||   0  Tesla P40                  | 00000000:03:00.0             |   0%       |+---------------------------------+------------------------------+------------+|   1  Tesla P40                  | 00000000:04:00.0             |   0%       ||      3251639763  GRID P40-6Q    | 9750...  i-y7i5cxa4x4        |      0%    |+---------------------------------+------------------------------+------------+|   2  Tesla P40                  | 00000000:84:00.0             |   0%       |+---------------------------------+------------------------------+------------+|   3  Tesla P40                  | 00000000:85:00.0             |   0%       |+---------------------------------+------------------------------+------------+# get VGPU Details$ nvidia-smi vgpu -qGPU 00000000:03:00.0    Active vGPUs                    : 0GPU 00000000:04:00.0    Active vGPUs                    : 1    vGPU ID                         : 3251639763        VM UUID                     : 97502756-1798-4014-9979-8595f909eeb3        VM Name                     : i-y7i5cxa4x4        vGPU Name                   : GRID P40-6Q        vGPU Type                   : 50        vGPU UUID                   : 0c4b9906-7a2e-2e4e-133d-ca368087e5ab        Guest Driver Version        : Not Available        License Status              : Unlicensed        Accounting Mode             : Disabled        ECC Mode                    : Disabled        Accounting Buffer Size      : 4000        Frame Rate Limit            : N/A        FB Memory Usage            Total                   : 6144 MiB            Used                    : 0 MiB            Free                    : 6144 MiB        Utilization            Gpu                     : 0 %            Memory                  : 0 %            Encoder                 : 0 %            Decoder                 : 0 %        Encoder Stats            Active Sessions         : 0            Average FPS             : 0            Average Latency         : 0        FBC Stats            Active Sessions         : 0            Average FPS             : 0            Average Latency         : 0GPU 00000000:84:00.0    Active vGPUs                    : 0GPU 00000000:85:00.0    Active vGPUs                    : 0# To monitor vGPU engine usage across multiple vGPUs$ nvidia-smi vgpu -u# GPU       vGPU    sm   mem   enc   dec# Idx         Id     %     %     %     %    0          -     -     -     -     -    1 3251639763     0     0     0     0    2          -     -     -     -     -    3          -     -     -     -     -    0          -     -     -     -     -# To monitor vGPU engine usage by applications across multiple vGPUs$ nvidia-smi vgpu -p# GPU       vGPU    process    sm   mem   enc   dec   process         # Idx         Id         Id     %     %     %     %   name                0          -          -     -     -     -     -   -                   1          -          -     -     -     -     -   -                   2          -          -     -     -     -     -   -                   3          -          -     -     -     -     -   -               # If MIG mode is not enabled for the GPU, or if the GPU does not support MIG, this property reflects the number and type of vGPUs that are already running on the GPU.# 1. If no vGPUs are running on the GPU, all vGPU types that the GPU supports are listed.# 2. If one or more vGPUs are running on the GPU, but the GPU is not fully loaded, only the type of the vGPUs that are already running is listed.# 3. If the GPU is fully loaded, no vGPU types are listed.$ nvidia-smi vgpu -cGPU 00000000:03:00.0    GRID P40-6Q    GPU 00000000:04:00.0    GRID P40-6Q    GPU 00000000:84:00.0    GRID P40-6Q    GPU 00000000:85:00.0    GRID P40-6Q    

sysfs for NVIDIA GPUThe sysfs directory for each physical GPU is at

&#x2F;sys&#x2F;bus&#x2F;pci&#x2F;devices&#x2F;
&#x2F;sys&#x2F;class&#x2F;mdev_bus&#x2F;

/sys/class/mdev_bus/           |-parent-physical-device             |-mdev_supported_types               |-nvidia-vgputype-id                 |-available_instances                 |-create                 |-description                 |-device_api                 |-devices                 |-name

The mdev device file that you create to represent the vGPU does not persist when the host is rebooted and must be re-created after the host is rebooted
REF
nvidia vgpu user guide

]]></content>
      <categories>
        <category>hardware</category>
        <category>vgpu</category>
      </categories>
      <tags>
        <tag>vgpu</tag>
      </tags>
  </entry>
  <entry>
    <title>hardware-console-terminal-tty-pty</title>
    <url>/2021/10/15/hardware-console-terminal-tty-pty/</url>
    <content><![CDATA[TerminalGenerally speaking a terminal is a relatively dumb electromechanical device with an input interface (like a keyboard) and an output interface (like a display or sheet of paper).
The computer has an UART driver to read for the hardware device. The sequence of characters is passed to TTY driver which applies the line discipline. The line discipline is in charge of converting special characters (like end of line, backspaces), and echoing what has been received back to the teletype, so that the user can see what it has been typed (line disciplines will be discussed in the next post of the series).
The words terminal and TTY device are basically interchangeable as they mean the same thing.


Serial TerminalA serial port terminal (Serial Port Terminal) is a terminal device connected using a serial port of a computer. The computer treats each serial port as a character device. For a while, these serial port devices were often called terminal devices, because at that time its greatest use was to connect terminals. The device names corresponding to these serial ports are &#x2F;dev&#x2F;ttyS0 &#x2F;dev&#x2F;ttyS1 etc.
terminal emulator(TTY driver in kernel)Let’s move to more recent times. Computers started becoming smaller and smaller, with everything packed in one single box.For the first time the terminal was not a physical device connected via UART to the computer. The terminal became a computer program in the kernel which would send characters directly to the TTY driver, read from it and print to the screen.
it listens for events coming from the keyboard and sends it down to the driver. The difference is that there is no physical device or cable which is connected to the TTY driver.
If you run a Linux OS on your machine press Ctrl+Alt+F1. You’ll get a TTY emulated by the kernel! You can get other TTYs by pressing Ctrl+Alt with the function keys from (F2 to F6), you can login on different TTYs, after that run bash shell inside that TTY. different login different TTY
But there is only one console(&#x2F;dev&#x2F;console) for a computer, if you write message to console(&#x2F;dev&#x2F;console), displayer will show it to you.
$ echo &quot;hello&quot; &gt;/dev/console

In Linux systems, computer monitors are often called console terminals (Console). It emulates a terminal of type Linux (TERM &#x3D; Linux), and there are some device special files associated with it: tty0, tty1, tty2, etc. When you log in on the console, tty1 is used. When using Alt + [F1-F6] key combination, we can switch to tty2, tty3, etc. tty1–tty6, etc. are called virtual terminals, and tty0 is an alias of the currently used virtual terminal, and the information generated by the system will be sent to the terminal (also called console terminal at this time). Therefore, no matter which virtual terminal is currently being used, system information will be sent to the console terminal. You can log in to different virtual terminals, thus allowing the system to have several different sessions at the same time.
ShellShells are user space applications that use the kernel API in just the same way as it is used by other application programs. A shell manages the user–system interaction by prompting users for input, interpreting their input, and then handling an output from the underlying operating system (much like a read–eval–print loop, REPL).
Bash, Zsh, Fish and sh are all different flavors of shells.
PTY(user land program)If we remotely telnet to the host or use xterm, do we also need a terminal interaction? Yes, this is the virtual terminal pty (pseudo-tty) which runs in user land. while pts (pseudo-terminal slave) is the implementation method of pty, and ptmx (pseudo-terminal master ) Used in conjunction with pty. 
Ref
termianl-tty-pty

]]></content>
      <categories>
        <category>linux</category>
        <category>tty</category>
      </categories>
      <tags>
        <tag>console</tag>
        <tag>tty</tag>
        <tag>pty</tag>
      </tags>
  </entry>
  <entry>
    <title>go-basic</title>
    <url>/2021/07/29/go-basic/</url>
    <content><![CDATA[overviewGo is a statically typed, compiled programming language designed at Google， Go is syntactically similar to C, but with memory safety, garbage collection, structural typing, concurrency.  
 Go is influenced by C, but with an emphasis on greater simplicity and safety.

A syntax and environment adopting patterns more common in dynamic languages:

Optional concise variable declaration and initialization through type inference (x :&#x3D; 0 instead of int x &#x3D; 0; or var x &#x3D; 0;).
Fast compilation.
Remote package management (go get) and online package documentation.


Distinctive approaches to particular problems:

Built-in concurrency primitives: light-weight processes (goroutines), channels, and the select statement.
An interface system in place of virtual inheritance, and type embedding instead of non-virtual inheritance.
A toolchain that, by default, produces statically linked native binaries without external dependencies.






SyntaxGo’s syntax includes changes from C aimed at keeping code concise and readable. A combined declaration/initialization operator was introduced that allows the programmer to write i :&#x3D; 3 or s :&#x3D; “Hello, world!”, without specifying the types of variables used. This contrasts with C&#39;s int i = 3; and const char *s = &quot;Hello, world!&quot;; Semicolons(;) still terminate statements,but are implicit when the end of a line occurs. Methods may return multiple values, and returning a result, err pair is the conventional way a method indicates an error to its caller in Go. Go adds literal syntaxes for initializing struct parameters by name and for initializing maps and slices. As an alternative to C’s three-statement for loop, Go’s range expressions allow concise iteration over arrays, slices, strings, maps, and channels.

Built-in Types 

bool
string
int  int8  int16  int32  int64
uint uint8 uint16 uint32 uint64 uintptr
byte &#x2F;&#x2F; alias for uint8, used as char like c，&#39;a&#39; + 1 is valid it&#39;s &#39;b&#39;
rune &#x2F;&#x2F; alias for int32 represents a Unicode code point
float32 float64
complex64 complex128
pointer

The int, uint types are usually 32 bits wide on 32-bit systems and 64 bits wide on 64-bit systems, it depends on arch, different like C, int, uint are 32 bits(4 bytes) even on 64-bit machine

Custom type
// status and bool are two different types.type status bool/* Some type alias declarations */// boolean and bool denote the same type.type boolean = bool


 NOT Supported 

inheritance
assertions
pointer arithmetic
implicit type conversions
NO ~x but ^x in Go for integer.


 ToolsThe main Go distribution includes tools for building, testing, and analyzing code:

go build, which builds Go binaries using only information in the source files themselves, no separate makefiles
go test, for unit testing and microbenchmarks
go fmt, for formatting code
go get, for retrieving and installing remote packages
go vet, a static analyzer looking for potential errors in code
go run, a shortcut for building and executing code, but not save binary to disk.
godoc, for displaying documentation or serving it via HTTP

An ecosystem of third-party tools adds to the standard distribution, such as gocode, which enables code autocompletion in many text editors, goimports, which automatically adds&#x2F;removes package imports as needed, anderrcheck, which detects code that might unintentionally ignore errors. 

single quote vs double quote They are different
To declare either byte or rune we use single quote. While declaring byte we have to specify the type. If we do not specify the type, then the default type is meant as a rune for &#39;a&#39;. A single quote will allow only one character.
func test() &#123;    var ch byte = &#x27;a&#x27;// must declare with type byte, otherwise it&#x27;s rune !!!    ch := byte(&#x27;a&#x27;)        rc := &#x27;a&#x27; /* default it&#x27;s a rune */    rs := &quot;a&quot; /* it&#x27;s string */&#125;

NO -&gt; for pointer type like what we did in C, but works as C like &amp;a, *p, **p, *p=
var a int = 12var p *int = &amp;a // pointer type and get object address*p = 10type Student struct &#123;    x int&#125;var p *Student = &amp;Student&#123;x: 1&#125;p.x = 12 // Go automatically convert it to (*p).x = 12 !!!

string, slice, map behave like pointer, but when assigning and passing as parameter, array, struct are different.
import &quot;fmt&quot;func test() &#123;    var s1 = &quot;hello&quot;    s2 := s1 // both s2 and s1 points to same memory        sc1 := []int&#123;1, 2, 3, 4&#125;    sc2 := sc1 // both sc1 and sc2 points to same underlaying array        m1 := map[string]int&#123;        &quot;a&quot;: 1,        &quot;b&quot;: 2,    &#125;        m2 := m1 // both m1 and m2 points to same memory            /*-----------------------------------------*/    var a1 = [5]int&#123;1, 2, 3, 4, 5&#125;    a2 := a1 // a2 is a copy of a1, different memory!!!        var st1 = struct &#123;        x, y int    &#125;&#123;1, 2&#125;        st2 := st1 //st2 is a copy of st1&#125;


for/range when loop array, slice, map, it’s copy of element, hence if change on that element, make sure use s[i] if element is not pointer.
type Person struct &#123;    name string&#125;func main() &#123;    // slice of Person    ps := []Person&#123;        &#123;name: &quot;a&quot;&#125;,    &#125;    for _, p := range ps &#123;        p.name = &quot;b&quot; // p is copy of each element    &#125;    // nothing changed    fmt.Println(ps)    for i := 0; i &lt; len(ps); i++ &#123;        ps[i].name = &quot;b&quot;    &#125;    fmt.Println(ps)&#125;
Go Wiki Overview
naming conventionGo is to use MixedCaps or mixedCaps rather than underscores to write multiword names. 
Files  

Go follows a convention where source files are all lower case with underscore separating multiple words, client_log.go
Compound file names are separated with _
Files with the suffix _test.go are only compiled and run by the go test tool.

Functions and Methods  

Use camel case, exported functions should start with uppercase
If a name consists of multiple words, each word after the first should be capitalized like this: empName, EmpAddress, etc.
function names are case-sensitive (car, Car and CAR are three different variables).

Constants  

Constant should be capitalized(camel case like Exported named).  WorldStdEncoding

Variables  

shouldn’t include the name of your type in the name of your variable’s name, tetMap
Generally, use relatively simple (short) name(lower case), camel case(NOT _ underscore for multiple worlds) long var.
user to u
userID to uid
serverListener
lpcfg


If variable type is bool, its name should start with Has, Is, Can or Allow, etc.
Single letter represents index: i, j, k

struct interface  

Name of struct or interface should be capitalized and camel case type BJSchool struct{}
method of interface and struct should be capitalized(exported) and camel case type School interface{ Name() string }
field of struct should be low letter starts camel case if not exported, otherwise uppercase type School struct { regStudent int }

import package

package name should be lowercase, no camel case. like import xxx&#x2F;testhello

Non-exported struct fields can be accessed only in the same package, can not be accessed by other package.
printingFMT Print cheat-sheet  
Package fmt implements formatted I&#x2F;O with functions analogous to C’s printf and scanf. The format ‘verbs’ are derived from C’s but are simpler. 
In Golang we can use Printf with a special format code. This determines how a string or integer is formatted. Println does not require a format string.

Printf: Must provide the format and support explicit argument indexes, no auto newline.
Println: No special format support, auto newline for each output, auto space between arguments, just use it’s default.
Print: Print does not insert a newline after each call and no auto space between arguments, it just writes the data to the console with no trailing newline, except this, same as Println.

Above three prints to console while Sxx returns the formated result.

Sprintf: Must provide the format and support explicit argument indexes, No auto newline.
Sprintln: No special format support, auto newline, just use it’s default.
Sprint: Print does not insert a newline after each call, it just writes the data to the console with no trailing newline, except this, same as Sprintln.

Above three prints returns the formated result, while Fxx writes data to io.Writer

Fprintf: Must provide the format and support explicit argument indexes, No auto newline.
Fprintln: No special format support, auto newline, just use it’s default.
Fprint: Print does not insert a newline after each call, it just writes the data to the file with no trailing newline, except this, same as Fprintln.


explicit argument index
a := 10b := 20fmt.Printf(&quot;%v %v\n&quot;, a, b) // 10 20fmt.Printf(&quot;%[2]v %[1]v\n&quot;, a, b) // 20 10 argument index


default format of each type
%c    print for byte and rune only%q    a single-quoted character literal safely escaped with Go syntax, used for byte,rune and string%v    the value in a default format      [when printing struct], the plus flag (%+v) adds field names---&gt;suggested way.      [when printing struct], the plus flag (%#v) adds field names and struct definition%T    a Go-syntax representation of the [type of the value] The default format for %v is:bool:                    %tint, int8, rune, byte:   %duint, uint8 etc.:        %dfloat32, complex64, etc: %gstring:                  %schan:                    %ppointer:                 %p


print in multiple lines
s := `helloworld` // auto enter!!! without \n neededs := &quot;hello\n&quot; +     &quot;world&quot;


%q vs %c vs %s
package mainimport (    &quot;fmt&quot;)func main() &#123;    c := byte(&#x27;a&#x27;)    fmt.Printf(&quot;%q\n&quot;, c)    fmt.Printf(&quot;%c\n&quot;, c)    s := &quot;hello&quot;    fmt.Printf(&quot;%q\n&quot;, s)    fmt.Printf(&quot;%s\n&quot;, s)&#125;&#x27;a&#x27;a&quot;hello&quot;hello
More details refer to fmt package, like C format, but more simpler to use.
import &quot;fmt&quot;func print_demo() &#123;    a := 10    b := 20    s := &quot;hello&quot;        fmt.Print(a, &quot;\n&quot;)    fmt.Print(&quot;Print: no auto space added between arg &quot;, a, s) // no space bettween each argument        fmt.Println()    fmt.Println(a)    fmt.Println(&quot;Println: each arg is separated by space automatically&quot;, a, s)//each argument is separated by space    fmt.Println(&quot;Println: not support special format:&quot;, a, &quot;not cool&quot;) // auto new line        fmt.Printf(&quot;Printf: support special format: %[2]v %[1]v\n&quot;, a, s) //argument must be at most right part, mannually new line            result := fmt.Sprintf(&quot;Sprintf: support special format: %[2]v %[1]v&quot;, a, b) // use argument index    fmt.Println(result)        /* print array, slice, struct */    slice := []int&#123;999, 99, 9&#125;    array := [3]string&#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;    fmt.Println(slice)    fmt.Printf(&quot;%v\n&quot;, array)        type Person struct &#123;        name string        id int    &#125;        var p1 Person = Person&#123;&quot;jason&quot;, 1&#125;    fmt.Println(p1)    fmt.Printf(&quot;%#v\n&quot;, p1)    fmt.Printf(&quot;%+v\n&quot;, p1)    fmt.Printf(&quot;%v\n&quot;, p1)        // s = hello    fmt.Printf(&quot;%v %q\n&quot;, s, s)        var s byte = 65    fmt.Printf(&quot;%v %q\n&quot;, s, s)&#125;print_demo()

10
Print: no auto space added between arg 10hello
10
Println: each arg is separated by space automatically 10 hello
Println: not support special format: 10 not cool
Printf: support special format: hello 10
Sprintf: support special format: 20 10
[999 99 9]
[a b c]
&#123;jason 1&#125;
struct &#123; 𒀸name string; 𒀸id int &#125;&#123;𒀸name:&quot;jason&quot;, 𒀸id:1&#125;
&#123;𒀸name:jason 𒀸id:1&#125;
&#123;jason 1&#125;
hello &quot;hello&quot;
65 &#39;A&#39;

builtin APIfunc append(slice []Type, elems ...Type) []Type

The append built-in function appends elements to the end of a slice. If it has sufficient capacity, the destination is resliced to accommodate the new elements. If it does not sufficient, a new underlying array will be allocated

func cap(v Type) int


Array: the number of elements in v (same as len(v)).2.Pointer to array: the number of elements in *v (same as len(v)).3.Slice: the maximum length the slice can reach when resliced, may different with len(slice)4.if v is nil, cap(v) is zero.5.Channel: the channel buffer capacity, in units of elements;


func close(c chan&lt;- Type)

The close built-in function closes a channel not a file, which must be either bidirectional or send-only. It should be executed only by the sender, never the receiver, and has the effect of shutting down the channel after the last sent value is received. After the last value has been received from a closed channel c, any receive from c will succeed without blocking, returning the zero value for the channel element

func copy(dst, src []Type) int shadow copy(only the top level is copied)

The copy built-in function copies elements from a source slice into a destination slice. (As a special case, it also will copy bytes from a string to a slice of bytes.) The source and destination may overlap. Copy returns the number of elements copied, which will be the minimum of len(src) and len(dst). 

func delete(m map[Type]Type1, key Type)  

delete element specified by key from a map

func len(v Type) int

1.Array: the number of elements in v.2.Pointer to array: the number of elements in *v (even if v is nil).3.Slice, or map: the number of elements in v; if v is nil, len(v) is zero.4.String: the number of bytes in v.5.Channel: the number of elements queued (unread) in the channel buffer;6.if v is nil, len(v) is zero.

func make(t Type, size ...IntegerType) Type

Can be used only for Slice, Map, Channel  

Slice: The size specifies the length. The capacity of the slice is
equal to its length. A second integer argument may be provided to
specify a different capacity; it must be no smaller than the
length. For example, make([]int, 0, 10) allocates an underlying array
of size 10 and returns a slice of length 0 and capacity 10 that is
backed by this underlying array.

Map: An empty map is allocated with enough space to hold the
specified number of elements. The size may be omitted, in which case
a small starting size is allocated.  

Channel: The channel&#39;s buffer is initialized with the specified
buffer capacity. If zero, or the size is omitted, the channel is
unbuffered. 

# for slice, can pass two parameters
s1 = make([]int, 4)   // len=4 and cap = 4
s1 = make([]int, 0, 4)// len=0 and cap = 4

# for map, no one needed
m1 = make(map[string]int)

# for channel, can pass one parameter
c1 = make(chan int)     // buffer size 0(unbuffered)
c2 = make(chan int, 10) // buffer size 10
                                                                                      

func new(Type) *Type

The new built-in function allocates memory. The first argument is a type, not a value, and the value returned is a pointer to a newly allocated zero value of that type. most of time, we does not use it at all.

import &quot;fmt&quot;func builtin_demo() &#123;    s1 := []int&#123;1, 2, 3&#125;    s2 := make([]int, 4)    fmt.Println(s1, s2)        copy(s2, s1)    fmt.Println(s1, s2)        p := new(int)    *p = 12    fmt.Println(p, *p)&#125;builtin_demo()

[1 2 3] [0 0 0 0]
[1 2 3] [1 2 3 0]
0xc0009190d0 12

constantsuntyped value has default typeAn untyped value means the type of the value has not been confirmed yet. For most untyped values, each of them has one default type, All literal constants (unnamed constants) are untyped values. most untyped values are literal constants and named constants.

The default type of a literal constant is determined by its literal form.

The default type of a string literal is string.
The default type of a boolean literal is bool.
The default type of an integer literal is int.
The default type of a rune literal is rune (a.k.a., int32).
The default type of a floating-point literal is float64.
If a literal contains an imaginary part, then its default type is complex128

constant typeunnamed constant(literal constant)  
12&quot;hello&quot;
named constant with untyped value  
const MAX = 12

named constant with typed value  
const MAX int8 = 12

type deduction(type inference)Go supports type deduction. In other words, in many circumstances, programmers don’t need to explicitly specify the types of some values in code. Go compilers will deduce the types for these values by context.
In Go code, if a place needs a value of a certain type and an untyped value (often a constant) is representable as a value of the certain type, then the untyped value can be used in the place. Go compilers will view the untyped value as a typed value of the certain type. it can be viewed as implicit conversions.
constant declaration way= not := for constant declaration
untyped named constant
package main// Declare two individual constants. Yes,// non-ASCII letters can be used in identifiers.const π = 3.1416const Pi = π // &lt;=&gt; const Pi = 3.1416// Declare multiple constants in a group called constant specification can be different types.const (	No         = !Yes	Yes        = true	MaxDegrees = 360	Unit       = &quot;radian&quot;)func main() &#123;	// Declare multiple constants in one line. can be different types!!!	const TwoPi, HalfPi, Unit2 = π * 2, π * 0.5, &quot;degree&quot;&#125;

typed named constant
const X float32 = 3.14const (	A, B int64   = -3, 5 // A and B are same type	Y    float32 = 2.718)// If a basic value literal is bound to a typed constant, // the basic value literal must be representable as a value of the type of the constant.// error: 256 overflows uint8const a uint8 = 256const MaxUint uint = (1 &lt;&lt; 64) - 1 // error on 32-bit as (1 &lt;&lt; 64) - 1 is not representable as 32-bit values
Autocomplete in constant declarationsIn a group-style constant declaration, except the first constant specification, other constant specifications can be incomplete. An incomplete constant specification doesn’t contain the &#x3D; symbol. Compilers will autocomplete the incomplete lines for us by copying the missing part from the first preceding complete constant specification.
const (	X float32 = 3.14	Y           // here must be one identifier, Y has same like X.	A, B = &quot;Go&quot;, &quot;language&quot;	C, _	// In the above line, the blank identifier is required to be present.!!!)


iota is a special value controlled by compiler, its value is reset to 0 for each const keyword the first constant line of group, and increased by 1 for each appearance before next const keyword.
package mainfunc main() &#123;	const (		k = 3 // now, iota == 0		m float32 = iota + .5 // m float32 = 1 + .5		n                     // n float32 = 2 + .5		p = 9             // now, iota == 3		q = iota * 2      // q = 4 * 2		_                 // _ = 5 * 2		r                 // r = 6 * 2		s, t = iota, iota // s, t = 7, 7   iota has the same vale on the same line		u, v              // u, v = 8, 8		_, w              // _, w = 9, 9	)	const x = iota // x = 0	const (		y = iota // y = 0		z        // z = 1	)&#125;


const (    a = 1 &lt;&lt; iota // a == 1    b = 1 &lt;&lt; iota // b == 2    c = 1 &lt;&lt; iota // c == 4)// Same as aboveconst (    a = 1 &lt;&lt; iota // a == 1    b    c)const (	Failed = iota - 1 // == -1	Unknown           // == 0	Succeeded         // == 1)


NOTE  

Constants are declared like variables, but with the const keyword.
Constants can be character, string, boolean, or numeric values.
Constants can NOT be declared using the :&#x3D; syntax.
This is no enum in GO, use const instead
Constants can be declared both at package level and function bodies.

import &quot;fmt&quot;// like enumconst (    Sunday = iota // Sunday == 0    Monday        // Monday == 1    Tuesday    Wednesday    Thursday    Friday    Saturday    numberOfDays)const Pi = 3.14func constanDemo() &#123;    fmt.Printf(&quot;day %v\n&quot;, Sunday)    const World = &quot;world&quot;    fmt.Println(&quot;Hello&quot;, World)    fmt.Println(&quot;Happy&quot;, Pi, &quot;Day&quot;)    const Truth = true    fmt.Println(&quot;Go rules?&quot;, Truth)    const Ch byte = &#x27;a&#x27;    fmt.Println(&quot;Char&quot;, Ch)    fmt.Printf(&quot;Char %c&quot;, Ch)&#125;constanDemo()

day 0
Hello world
Happy 3.14 Day
Go rules? true
Char 97
Char a

variableAll variables are typed values. When declaring a variable, there must be sufficient information provided for compilers to deduce the type of the variable
There are two basic variable declaration forms, the standard one and the short one. The short form can only be used to declare local variables

NOTE  

var is not needed for declaration like in struct, function parameter, function return value
var is a must when declare global variable, optional for local variable.
All variables are addressable and all constants are unaddressable
Go doesn’t support assignment chain, like this a &#x3D; b &#x3D; 123.


Suggestion

constant, use const statement not var statement
global variable, use var statement
local variable, but no need explicit initialization(default value), use var statement
local variable, needs initialization, use := statement
with assignment at declaration, always use short way.

As Go is compiled language, hence we must know the type of each variable at declaration either by explicit or implicit(assigned value), the type of variable is determined at declaration, can’t be change during running!!!
When declaring a variable without specifying an explicit type (either by using the :&#x3D; syntax or var &#x3D; expression syntax), the variable’s type is inferred from the value on the right hand side
standard wayvar lang, website string = &quot;Go&quot;, &quot;https://golang.org&quot;var compiled, dynamic bool = true, falsevar announceYear int = 2009// var of annymous struct type// g is a variable of struct type who has one field.var g struct &#123;    name string&#125;// var of function typevar g func() stringvar g intvar c, d = 1, &quot;hello&quot;var next = 12var (	lang, bornYear, compiled     = &quot;Go&quot;, 2007, true	announceAt, releaseAt    int = 2009, 2012	createdBy, website       string)var g // error as no way to know the type at declarationvar a, b inta = b = 123 // syntax error, Go doesn&#x27;t support assignment chain
short wayShort variable declarations can only be used to declare local variables.
There are several differences between short and standard variable declarations.

In the short declaration form, the var keyword and variable types must be omitted.
The assignment sign must be :&#x3D; instead of &#x3D;.
In the short variable declaration, old variables and new variables can mix at the left of :=. But there must be at least one new variable at the left.

package mainfunc main() &#123;	// Both lang and year are newly declared.	lang, year := &quot;Go language&quot;, 2007	// Only createdBy is a new declared variable.	// The year variable has already been	// declared before, so here its value is just	// modified, or we can say it is redeclared.	year, createdBy := 2009, &quot;Google Research&quot;        if true &#123;        // both month, year are new within this scope        month, year:=12, 2006    &#125;        fmt.Println(year) // it&#x27;s still 2009!!!    	// This is a pure assignment.	lang, year = &quot;Go&quot;, 2012        nextYear := year // same type as year&#125;


NOTE :&#x3D; declare all new variables left, not part of it
package mainimport &quot;fmt&quot;func main() &#123;    a := []string&#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;    i := 0    // for scope    for i, e := range a &#123;        // i is a new var with scope for!!!        fmt.Println(i, e)    &#125;    fmt.Println(&quot;=================&quot;)    fmt.Println(i)&#125;# go run  test.go0 a1 b2 c=================0

default value of each type(without explicit initialization)Variables declared without an explicit initial value are given their zero value, you can access var with zero directly, one except is for map, you can NOT modify nil map!!
var m map[string]int // nil mapm[&quot;a&quot;] = 12 // panic!!!m := map[string]int&#123;&#125; // empty mapm[&quot;a&quot;] = 12 // it&#x27;s okvar s []int // nil slices = append(s, 1)// it&#x27;s ok!!!s := []int&#123;&#125; // empty slices = append(s, 1) // ok as well

zero value for each type  

0 for numeric types,
false for the boolean type
“” (the empty string) for string.
nil for pointer
nil for function type
nil, but len(map) &#x3D;&#x3D; 0
nil, but len(slice) &#x3D;&#x3D; 0
zero value for all fields for struct instance

variable initialization orderWhen a variable depends on another variable b, b should be defined beforehand, else program won’t compile. Go follows this rule inside functions. but it’s not true for global variable.
package mainvar a int = b // it&#x27;s okvar b int = 12func main() &#123;    var c int = d // error    var d int = 12&#125;

pointerThe type *T is a pointer to a T type.
var p *int//The &amp; operator generates a pointer to its operand.i := 42p = &amp;ifmt.Println(*p) // read i through the pointer p*p = 21         // set i through the pointer p

scopeA variable or a named constant declared in an inner code block will shadow the variables and constants declared with the same name in outer code blocks.
package mainconst y = 789var x int = 123func main() &#123;	// The x variable shadows the above declared	// package-level variable x.	var x = true	// A nested code block.	&#123;		// Here, the left x and y are both		// new declared variable. The right		// ones are declared in outer blocks.		x, y := x, y		// In this code block, the just new		// declared x and y shadow the outer		// declared same-name identifiers.		x, z := !x, y/10 // only z is new declared		y /= 100		println(x, y, z) // false 7 78	&#125;	println(x) // true	println(z) // error: z is undefined.&#125;


import &quot;fmt&quot;var ga, gb = 1, &quot;global b&quot; // implicit typevar gc, gd int            // must explicit type as no assignmentfunc var_demo()&#123;    gc = 10    gd = 20        la, lb := &quot;local a&quot;, 30  // implicit type        // lb = &quot;hi&quot; error as lb has type int!!!        var lc string            // must explicit type as no assignment    lc = &quot;local c&quot;    fmt.Println(ga, gb, gc, gd, la, lb, lc)&#125;var_demo()

1 global b 10 20 local a 30 local c

typesGet Max value of integer, use math lib which provides Max of Int8, Int16, Int32, Int64, Int and unsigned version as well.
//do it by yourselfconst MaxUint = ^uint(0)

conversionIdentical types, no need for conversion
Two types are identical if their underlying type literals are structurally equivalent; that is, they have the same literal structure and corresponding components have identical types. In detail:

Two array types are identical if they have identical element types and the same array length.

Two slice types are identical if they have identical element types.

Two struct types are identical if they have the same sequence of fields, and if corresponding fields have the same names, and identical types, and identical tags. Non-exported field names from different packages are always different.
  type A_ID inttype A struct &#123;    id A_ID&#125;type B_ID inttype B struct &#123;    id B_ID&#125;// A and B are different type!!

  type A struct &#123;    name string&#125;type B struct &#123;    name string&#125;// A and B are same type

Two pointer types are identical if they have identical base types.

Two function types are identical if they have the same number of parameters and result values, corresponding parameter and result types are identical, and either both functions are variadic or neither is. Parameter and result names are not required to match.

Two interface types are identical if they have the same set of methods with the same names and identical function types. Non-exported method names from different packages are always different. The order of the methods is irrelevant.

Two map types are identical if they have identical key and element types.

Two channel types are identical if they have identical element types and the same direction.


Different typesUnlike in C, in Go assignment between different types(if possible) requires an explicit conversion, there are two ways to use explicit type conversion, other different types can NOT be converted.


number: int() uint()
number&lt;—&gt;string: strconv.Atoi(&quot;12&quot;), strconv.Itoa(12) Or fmt.Sprintf(&quot;%v&quot;,12)

you can Convert int to string in this way, the result may be not what you want  j := string(97), j is &quot;a&quot; not &quot;97&quot;
import &quot;fmt&quot;import &quot;strconv&quot;func testp(x *int) &#123;    //x is a pointer    *x = 20&#125;func test(x int) &#123;    // x is copy, value passed    x = 10&#125;func typeDemo() &#123;    var a int = 12    // convert int to uint    var b uint = uint(a)        s := strconv.Itoa(10)    c, _ := strconv.Atoi(&quot;20&quot;)        fmt.Println(a, b, s, c)            testp(&amp;a)    fmt.Println(a)    test(a)    fmt.Println(a)&#125;typeDemo()

12 12 10 20
20
20

package mainimport &quot;fmt&quot;type A struct &#123;    name string&#125;type B struct &#123;    name string&#125;func main() &#123;    a := A&#123;name: &quot;jack&quot;&#125;    // B and A are same type, can convert to each other!!!    var b B = B(a)    fmt.Println(b.name)&#125;main()

jack

type A_ID inttype A struct &#123;    id A_ID&#125;type B_ID inttype B struct &#123;    id B_ID&#125;func main() &#123;    a := A&#123;id: 100&#125;    var b B = B(a)    fmt.Println(b.id)&#125;main()

100

stringA string is a struct that has a length and a pointer to a byte array. When you pass a string to another function, it copies the length and the pointer. As a consequence, the new copied string points to the same underlying data.
each element of string is a byte like s[0], string is immutable, you can NOT modify it in place
 

Create a string

var s string
var s &#x3D; “hello”
s :&#x3D; “hello”
var s &#x3D; strconv.Itoa(12): Int to string: “12”
string(97): 97 is “a”, so “a” is printed

 

Ops

s[0], s[0] is byte type!!!
last element s[len(s)-1], s[-1] NOT supported
s[0:3]
s +&#x3D; “extend it”
string([]byte{56,57}) &#x2F;&#x2F; convert byte slice to string, new memory is created!!!
string(slice)
support s1 &#x3D;&#x3D; s2
for _, c:= range s &#123;&#125; c is rune type!!!

 


string itself does not have method like Find, Match while  strings library provides ops for it
Can NOT convert array to string but slice is allowed.  ar :=[2]byte&#123;56,57&#125;fmt.Println(string(ar)) //errorfmt.Println(string(ar[:])) // copy array to slice

import &quot;fmt&quot;func testp(s *string) &#123; // string pointers points to argument    fmt.Println(s, *s)&#125;func test(s string) &#123; // no copy just like pointer, points to same memory    fmt.Println(s)&#125;func stringDemo()&#123;    s := &quot;hello&quot;    b := s  // b and s points to same memory, but can NOT modified.    fmt.Printf(&quot;%c %s %v %v\n&quot;, s[0], s[0:3], &amp;s, &amp;b)    testp(&amp;b)    test(b)        sc := []byte(&quot;boy&quot;) // convert string to byte slice        s = string(sc) // convert byte slice to a string    fmt.Println(s)        s = `helloworld`    // multiple lines    fmt.Println(s)        s1 := &quot;hello&quot;    s2 := &quot;hello&quot;        if s1 == s2 &#123; // compare two strings        fmt.Println(&quot;string equal&quot;)    &#125;        // default format for rune is %d!!!    fmt.Printf(&quot;%T, %c, %v\n&quot;,s1[1], s1[1], s1[1])    fmt.Println(len(s1))&#125;stringDemo()

h hel 0xc0004df3e0 0xc0004df400
0xc0004df400 hello
hello
boy
hello
world
string equal
uint8, e, 101
5

array and sliceLike C an array has an unique type, initialize with {}, arrays cannot be resized, size is fixed at initialization, index from 0 like C.  

Arrays are values. Assigning one array to another copies all its elements.
In particular, if you pass an array to a function, it will receive a copy of the array, not a pointer to it.
The size of an array is part of its type. The types [10]int and [20]int are distinct type.

 

slice and array conversion
//---------------array to slice(no need to define slice firstly)--------------------                               var arr1 = [3]int&#123;1, 2, 3&#125;                                                           // ss is slice  ss := arr1[:] // points to same memory!!!  ss[0] = 4     // arr1 changed as well.  // copy element from array to slice(new memory)  s := make([]int, 2)                                                               copy(s[:], arr1[:]) // s and arr1 points to different memory                      //copy(s[:], arr1[:1]), s does not extend even if arr1 is larger                  fmt.Println(s)                                                                                                                                                    //---------------slice to array(need to define array firstly)--------------------                                                                                                                 sc := []int&#123;1, 2, 3&#125;                                                              var arr2 [3]int                                                                   copy(arr2[:], sc[:]) // copy all        //copy(arr2[:], sc[:2]) // copy part                                              fmt.Println(arr2)                       

 

Create an array

var arr [3]int: all zero
arr :&#x3D; [3]int{1, 2, 3}
arr :&#x3D; [3]int{}: all zero
var arr [3]interface{} &#x2F;&#x2F; array of any type
arr :&#x3D; [3]interface{}{}
arr :&#x3D; [3]interface{}{“a”, 2, 1} array of any type

 

array of maps
// each element of the array is a map.var ts = [2]map[string]int&#123;&#123;&#125;, &#123;&#125;&#125;ts[0][&quot;a&quot;] = 1//ORvar ts [2]map[string]int// must create empty mapts[0] = map[string]int&#123;&#125;ts[1] = map[string]int&#123;&#125;ts[0][&quot;a&quot;] = 1
 

Ops

arr[0]
last element: arr[len(arr) - 1]
arr[0:3] not include arr[3]
for i, v :&#x3D; range arr {fmt.Println(v)}
arr &#x3D; append(arr, ‘a’, ‘b’) &#x2F;&#x2F;arr may point to new memory!!!
arr &#x3D; append(arr, another_arr…) &#x2F;&#x2F; link two arrays

 

sliceAn array has a fixed size must be provided at declaration. A slice, on the other hand, is a dynamically-sized, flexible view into the elements of an array. In practice, slices are much more common than arrays.
A slice is formed by specifying two indexes, a low and high bound(not included), separated by a colon:a[low:high]
you may omit the high or low bounds to use their defaults instead. The default is zero for the low bound and the length of the slice for the high bound.
These slice expressions are equivalent:  
a[0:10] == a[:10]a[0:] == [:]

A slice does not store any data, it just describes a section of an underlying array, Slices are like references to arrays
Changing the elements of a slice modifies the corresponding elements of its underlying array. 
The underlying array is dynamic and can be enlarged(or reduced to smaller one) to a new larger array(may larger than real elements) if append to a slice, hence a slice has both a length and a capacity.
The length of a slice is the number of elements it contains.
The capacity of a slice is the number of elements in the underlying array, counting from the first element in the slice.
The length and capacity of a slice s can be obtained using the expressions len(s) and cap(s). 
 

Create a slice

var sc []int
sc :&#x3D; []int{1, 2}, nsc &#x3D; sc[:] &#x2F;&#x2F; nsc and sc point to same underlaying memory
sc :&#x3D; []byte(“hello”) &#x2F;&#x2F; byte slice from string
sc :&#x3D; make([]int, 0, 5)
arr :&#x3D; [10]int{}; sc :&#x3D; arr[1:5]; sc :&#x3D; arr[:], sc and arr points to same memory
sc :&#x3D; []interface{}{}
sc :&#x3D; []interface{}{“a”, 1} slice of any type.
func test(sp *[] int) pointer to slice!!!

 

Create a slice of map, each slice element is a map
// slice with 0 elementvar ts = []map[string]int&#123;&#125;//ORts := make([]map[string]int, 0)// slice with 1 mapts = append(ts, map[string]int&#123;&#125;)ts[0][&quot;a&quot;] = 1// slice with 2 mapsts = append(ts, map[string]int&#123;&#125;)ts[1][&quot;b&quot;] = 1

 

Ops

sc[0]
last element: sc[len(sc) - 1]
sc[0:3]
for i, v :&#x3D; range sc {fmt.Println(v)}
sc &#x3D; append(sc, 12) &#x2F;&#x2F; sc may point to new memory!!!
sc &#x3D; append(sc, 12, 13) &#x2F;&#x2F; sc may point to new memory!!!
sc &#x3D; append(sc, another_sc…) &#x2F;&#x2F; sc may point to new memory!!!
inset element at index sc = append(sc[:index+1], orig[sc:]...) orig[index] = value
remove element at index sc = append(sc[:index], sc[index+1:]...)

 

Note

For append(), If the backing array of s is too small to fit all the given values a bigger array will be allocated. The returned slice will point to the newly allocated array.
New element is put at the end of len, may overwrite underlaying array if it’s part of it
empty slice is nil with len &#x3D;&#x3D; 0 but len(s)&#x3D;&#x3D;0, s may be not nils2 := make([]int, 0, 4)len(s2) == 0 // but s2 is not nil!!

 

func test() &#123;    s := []int&#123;1, 2, 3, 4, 5&#125;    b := s[0:3] // here b and s shared the same underlaying    b = append(b, 6) // append at b[3] as len of b is 3!!!	fmt.Println(s)	fmt.Println(b) // b and s still shares the same underlaying array, hence 4 is replaced with 6    	b = append(b, 7)	fmt.Println(s)  // b and s still shares the same underlaying array, hence 5 is replaced with 7	fmt.Println(b)		b = append(b, 8) // a new larger underlay array is created and returned	fmt.Println(s)   // s still points to old underlaying array, b points to newly allocated array	fmt.Println(b)       	b = append(b, 9)    fmt.Println(s)	fmt.Println(b)&#125;[1 2 3 6 5][1 2 3 6][1 2 3 6 7][1 2 3 6 7][1 2 3 6 7][1 2 3 6 7 8][1 2 3 6 7][1 2 3 6 7 8 9]

how slice cap changeevery slice has an underlying array, an array may be shared among several slices. If the new slice’s length will exceed the array’s capacity, a new array will be created for the new slice. Usually new capacity will be two times old capacity
cap(s), count elements from the beginning of slice to the end of underlay memory.

make([]byte, 5)
s = s[2:4], cap(s) == 3
Go only supports move start of underlaying array, but the end, the first two is dropped, memory is recycled!!!

s = s[:3], cap(s) == 5!!! 
package main                                                                                                                                                      import &quot;fmt&quot;                                                                                                                                                      func main() &#123;                                                                       b := make([]byte, 10)                                                           fmt.Println(cap(b)) //10                                                        s1 := b[:5]                                                                     fmt.Println(cap(s1)) //10                                                       s2 := s1[1:]                                                                    fmt.Println(cap(s2)) //9                                                        s3 := b[5:]                                                                     fmt.Println(cap(s3)) //5                                                    &#125;               


NOTE

It’s ok to loop a nil slice, same thing for map as wellvar n []int                                                                 for _, i := range n &#123;                                                           fmt.Printf(&quot;%d\n&quot;, i)// nothing print as n is nil                                              &#125;         

import &quot;fmt&quot;func test(sc [] int) &#123;    // slice is refernece to array,     // sc passed with its header(passed value)     // which has a pointer to array memory and length field, append here will not be seen by caller as length is value passed.    sc[0] = 100&#125;func arraySliceDemo() &#123;    var arr = [5]int&#123;1,2,3,4,5&#125;    //arr := [5]int&#123;2, 2, 3, 4, 5&#125;    fmt.Println(arr, arr[len(arr)-1])        // slice of array    var s []int = arr[0:4]    fmt.Println(s)    s[0] =10        b := s // b and s shared the same underlaying array        // the underlaying array is changed as well    fmt.Println(arr, s, cap(s), len(s))        s = []int&#123;10, 20, 30, 40, 50&#125;    fmt.Println(s, cap(s), len(s))        // append supports add more value to slice    s = append(s, 60, 70) // an new array is allocated and slice points to that    fmt.Println(s, cap(s), len(s))        s = make([]int, 5)    // len(a)=5, cap(a) = 5    fmt.Println(s, cap(s), len(s))        s  = make([]int, 0, 5)    // len(a)=0, cap(a)=5    fmt.Println(s, cap(s), len(s))        // anonymous struct here.    s := []struct &#123;        a int        b int    &#125;&#123;        &#123;1, 2&#125;,        &#123;3, 4&#125;,    &#125;    fmt.Println(s)        type Ver struct &#123;        a int        b int    &#125;    s := []Ver&#123;&#123;1,2&#125;, &#123;3,4&#125;&#125;    fmt.Println(s)        // loop each element of a slice or an array    for i, v := range s &#123;        fmt.Println(i, v) // v is copy of each element(copy of pointer of object)!!!    &#125;        for _, v := range s &#123;        // just the value, discard the index        fmt.Println(v)    &#125;        // another way to loop slice or array    for i :=0; i &lt; len(s); i++ &#123;        fmt.Println(s[i])    &#125;        // array    var arr = [5]int&#123;1, 2, 3, 4, 5&#125;    narr := arr // not like slice, narr is a copy of arr!!!    arr[0] = 6  // narr is unchanged, different with C language.        fmt.Println(arr, narr)        // slice    var arr = []int&#123;1,2&#125;    test(arr)    fmt.Println(arr)        // an array of any type    an := [2]interface&#123;&#125;&#123;1, &quot;hi&quot;&#125;        var ab [2]interface&#123;&#125; // interface&#123;&#125; is type!!!    ab[0] = 2    ab[1] = &quot;two&quot;    fmt.Println(an, ab)        sc := []int&#123;1, 2&#125;    nsc := append(sc[:], []int&#123;3, 4&#125;...)// 3,4 append to new slice sc[:]!!!    fmt.Println(sc, nsc)// sc is not changed!!!&#125;arraySliceDemo()

[1 2 3 4 5] 5
[1 2 3 4]
[10 2 3 4 5] [10 2 3 4] 5 4
[10 20 30 40 50] 5 5
[10 20 30 40 50 60 70] 10 7
[0 0 0 0 0] 5 5
[] 5 0
[&#123;1 2&#125; &#123;3 4&#125;]
[&#123;1 2&#125; &#123;3 4&#125;]
0 &#123;1 2&#125;
1 &#123;3 4&#125;
&#123;1 2&#125;
&#123;3 4&#125;
&#123;1 2&#125;
&#123;3 4&#125;
[6 2 3 4 5] [1 2 3 4 5]
[100 2]
[1 hi] [2 two]
[1 2] [1 2 3 4]

import &quot;fmt&quot;func slice_demo1() &#123;    s := []int&#123;2, 3, 5, 7, 11, 13&#125;    printSlice(s)    // Slice the slice to give it zero length.    s = s[:0]    printSlice(s)    // Extend its length. why it&#x27;s extend!!???    s = s[:4]    printSlice(s)&#125;func slice_demo2() &#123;    s := []int&#123;2, 3, 5, 7, 11, 13&#125;    s = s[1:4]    printSlice(s)    s = s[:2]    printSlice(s)    s = s[1:]    printSlice(s)&#125;func printSlice(s []int) &#123;    fmt.Printf(&quot;len=%d cap=%d %v\n&quot;, len(s), cap(s), s)&#125;slice_demo1()fmt.Println(&quot;demo&quot;)slice_demo2()

len=6 cap=6 [2 3 5 7 11 13]
len=0 cap=6 []
len=4 cap=6 [2 3 5 7]
demo
len=3 cap=5 [3 5 7]
len=2 cap=5 [3 5]
len=1 cap=4 [5]

map(dict)Key of map can be of any type for which the equality operator is defined, such as integers, floating point and complex numbers, strings, pointers, interfaces (as long as the dynamic type supports equality), structs and arrays. Slices cannot be used as map keys, because equality is not defined on them, value can by any type like int, string, slice, function etc
 

Create a map

var m map[string]int: map[string]int sits at right side when assigning values
m :&#x3D; map[string]int{}  &#x2F;&#x2F; empty map
m :&#x3D; map[string]int{“a”: 1, “b”: 2}
m :&#x3D; make(map[string]int)
m :&#x3D; map[string]interface{}{}
m :&#x3D; map[string]interface{}{“a”: 1, “b”: “b”} key must be quoted when it’s string literal
m :&#x3D; map[string]func(i string){} map of function object.

 

create map whose value is a slice
var m = map[string][]int&#123;&#125; // empty map//ORvar m = make(map[string][]int)m[&quot;a&quot;] = []int&#123;&#125; // create a new slicem[&quot;a&quot;] = append(m[&quot;a&quot;], 1)m[&quot;a&quot;] = append(m[&quot;a&quot;], 2)m[&quot;b&quot;] = []int&#123;3, 4&#125;&#125;

 

create map whose value is func object
package mainimport &quot;fmt&quot;func hello(m string) &#123;    fmt.Println(m)&#125;func main() &#123;    m := map[string]func(i string)&#123;&#125;    m[&quot;a&quot;] = hello    m[&quot;a&quot;](&quot;hello&quot;)&#125;

 

Ops

m[“c”]&#x3D; 3
elem, ok &#x3D; m[key]
delete(m, “c”): It’s safe to do even if the key is absent from the map

 

you CAN NOT assign value for nil map, you must create it first!!!
var mt map[string]int // nil mapmt[&quot;cool&quot;] = 12 ERROR!!!mt := map[string]int&#123;&#125; // map is createdmt := make(map[string]int) // map is created

But it’s ok to loop a nil map
var n map[string]int                                                             for _, i := range n &#123;                                                           fmt.Printf(&quot;%d\n&quot;, i)// nothing print as n is nil                                              &#125;         

 

Note

Access map by map[key] NOT map.key
The key of map must be same type, but the value can be any type when use interface&#123;&#125; as value type.

import &quot;fmt&quot;func test(m map[string]int) &#123;    m[&quot;f&quot;] = 100    // m[12] = &quot;hi&quot; // ERROR!!!&#125;func mapDemo() &#123;    // literal maps    // &#123;&#125; is initializer        var m1 = map[string]int &#123; // &#123;&quot;a&quot;:1, &quot;b&quot;:2 &#125; is initializer same line, , can be omitted        &quot;a&quot;: 1,        &quot;b&quot;: 2, // each element must end with ,!!!    &#125;        //m3 and m1 points to same memory, like pointer!!!    m3 := m1        // change map    test(m3)        m1[&quot;e&quot;] = 15        m2 := map[string]int &#123;        &quot;c&quot;: 3,        &quot;d&quot;: 4, // each element must end with even the last one if at different line!!!    &#125;        m2 := map[string]int &#123;&quot;c&quot;: 3, &quot;d&quot;: 4&#125; // the last , can be omitted if at same line with &#125;    m2[&quot;e&quot;] = 13      delete(m2, &quot;e&quot;)    fmt.Println(m1, m3, m2)        //Mutating Maps        // var mt map[string]int    // mt[&quot;cool&quot;] = 12 ERROR!!!        m := make(map[string]int) // map without element    m[&quot;Answer&quot;] = 42    fmt.Println(&quot;The value:&quot;, m[&quot;Answer&quot;])    m[&quot;Answer&quot;] = 48    fmt.Println(&quot;The value:&quot;, m[&quot;Answer&quot;])    delete(m, &quot;Answer&quot;)    fmt.Println(&quot;The value:&quot;, m[&quot;Answer&quot;])    v, ok := m[&quot;Answer&quot;]    fmt.Println(&quot;The value:&quot;, v, &quot;Present?&quot;, ok)    m[&quot;a&quot;] = 1    m[&quot;b&quot;] = 2    fmt.Println(m)    // loop a map    for k, v := range m &#123;        fmt.Println(k, v)    &#125;        var mn = map[string]interface&#123;&#125;&#123;&#125;    mn[&quot;a&quot;] = 12    mn[&#x27;b&#x27;] = &quot;hi&quot;    fmt.Println(mn, mn[&quot;a&quot;], mn[&#x27;b&#x27;])&#125;mapDemo()

map[a:1 b:2 e:15 f:100] map[a:1 b:2 e:15 f:100] map[c:3 d:4]
The value: 42
The value: 48
The value: 0
The value: 0 Present? false
map[a:1 b:2]
a 1
b 2
map[a:12 b:hi] 12 hi

structStruct fields can be accessed by struct instance or through a struct pointer which uses . NOT -&gt; like what did in C
type Vertex struct &#123;    x, y int //NOT var x int!!!      //as it&#x27;s lowercase, non-exported field!!!&#125;
 

Create a struct instance

var st Vertex
st :&#x3D; Vertex{1, 2} &#x2F;&#x2F; unamed assignment, must provide all values!!!
st :&#x3D; &amp;Vertex{x: 1} &#x2F;&#x2F; named assignment, can provide part of values!!!
st :&#x3D; Vertex{x: 1, y: 2} NOT “x” or “y” when use named index!!!
st :&#x3D; Vertex{}

 

Ops

st.x &#x3D; 10
p :&#x3D; &amp;st
p.x &#x3D; 10 not p-&gt;x

 

Note

Access field of struct by st.field_name not st[“field_name”] like what we do for map
pointer still uses p.field_name to access filed which is converted to (*p).field by Go automatically

import &quot;fmt&quot;type Vertex struct &#123;    x, y, z int //no var     //as it&#x27;s lowercase, non-exported field!!!&#125;var (    v1 = Vertex&#123;1, 2, 3&#125;  // has type Vertex    v2 = Vertex&#123;y: 1&#125;  // x:0 is implicit, set named field, error for unamed setting Vertex&#123;1&#125;    v3 = Vertex&#123;&#125;      // x:0 and y:0 and z:0 if unset fields!!!    p  = &amp;Vertex&#123;1, 2, 3&#125; // has type *Vertex)func structDemo() &#123;    var pt *Vertex    // as pt is pointer, must use &amp;     pt = &amp;Vertex&#123;1, 3, 4,// , is needed if &#125; is at newline!!        &#125;    fmt.Println(v1, p, v2, v3, *pt)        // array of struct    var pa []*Vertex    pa = []*Vertex &#123;// type of each element        &#123;4, 5, 6&#125;, //  not &amp; even it&#x27;s pointer type!!!    &#125;    fmt.Println(v1, p, v2, v3, *pt, *pa[0])&#125;structDemo()

&#123;1 2 3&#125; &amp;&#123;1 2 3&#125; &#123;0 1 0&#125; &#123;0 0 0&#125; &#123;1 3 4&#125;
&#123;1 2 3&#125; &amp;&#123;1 2 3&#125; &#123;0 1 0&#125; &#123;0 0 0&#125; &#123;1 3 4&#125; &#123;4 5 6&#125;

functionAlways remember Go is compiled language, hence, each parameter and return value must have a type, NO default value supported for parameter func test(x=12, y), Unsupported named parameter call like test(y=12, x=13).
function can return any number of results  
func add(x int, y int) int &#123;	return x + y&#125;// omit type if use the same typefunc add(x, y int) int &#123;	return x + y&#125;// named return value, like declared a var z at top of the functionfunc add(x, y int) (z int) &#123;    // use z directly	z = x + y    return  // no explicit, return named value, but return directive is a must!!!&#125;// return two valuesfunc add(x, y int) (int, int) &#123;	z = x + y    return z, x&#125;

deferA defer statement defers the execution of a function until the surrounding function returns.
The deferred call’s arguments are evaluated immediately, but the function call is not executed until the surrounding function returns. deferred function calls are pushed onto a stack. When a function returns, its deferred calls are executed in last-in-first-out order.
package mainimport &quot;fmt&quot;func Get() int &#123;    i := 0    defer func() &#123;        i = i + 10    &#125;()    return i&#125;func main() &#123;    fmt.Println(Get())&#125;..0

 

Note  

defer GetPerson().GetName() only the last call GetName() is deferred, GetPerson() is called immediately!!!
deferred call’s arguments are evaluated immediately
deferred function should no return, if wants return value, use channel, if deferred function has return value, it’s not captured!!!
deferred function executes after return statement!!!
As go is compile, hence deferred may not be pushed to stack, if code not reach it!!!.

package mainimport &quot;fmt&quot;func add(a, b int) chan int &#123;    resCh := make(chan int)    go func() &#123;        resCh &lt;- a + b    &#125;()        return resCh&#125;func main() &#123;    resCh := add(1, 2)    res := &lt;-resCh    fmt.Println(&quot;1 + 2 =&quot;, res)&#125;main()

1 + 2 = 3

package mainimport &quot;fmt&quot;func test(cool bool) &#123;    if cool &#123;        return    &#125;    defer func() &#123;        fmt.Printf(&quot;defer %v\n&quot;, cool)    &#125;()&#125;func main() &#123;    test(true) // no defer executes as cool is true!!!    test(false)&#125;main()

defer false

function objectFunction is an object, so it can be used as argument or return value
import &quot;fmt&quot;func add(x, y int) int &#123;    return x + y&#125;// unnamed functionvar hadd = func(x, y int) int &#123;    return x + y&#125;// func(int, int) int: is (function) type!func test(fn func(int, int) int) int &#123;    return fn(1, 2)&#125;fmt.Println(test(add))fmt.Println(test(hadd))

function closurefunction closure is a function that returns another function, but you can NOT define a function in another function like this
func test() &#123;    func embed() &#123; // Compile Error!!!    &#125;        hembed := func() &#123; // OK as hembed is a variable which points to unnamed function    &#125;&#125;

closure Return unnamed function
func adder() func(int) int &#123; // return value is a function	sum := 0	return func(x int) int &#123; // unnamed function		sum += x // always access var defined at its wrapper which is like a static variable!!!		return sum	&#125;&#125;


package mainimport &quot;fmt&quot;// hello is function wrapper which takes wrapped function as parameter// return the wapper function which is used by caller// inside the wrapper, add internal logical, then call wrapped functionfunc hello(fn func(string) error) func(string) &#123;    return func(name string) &#123;        fmt.Print(&quot;wrapper: &quot;)        fn(name)    &#125;&#125;func greet(name string) error &#123;    fmt.Println(&quot;hello&quot;, name)    return nil&#125;func main() &#123;    wp := hello(greet)    wp(&quot;tom&quot;)&#125;main()

wrapper: hello tom

variadic function(dynamic parameters)In Go, a function that can accept a dynamic number of arguments is called a Variadic function. Below is the syntax for variadic function. Three dots are used as a prefix before type.
dynamic parameters with same type  
// same type, ... is closer to type, it&#x27;s a new type ...int// numbers behaves like a slice, packed parametersfunc add(numbers ...int) int &#123; // must be the last parameter of a function!!!    sum := 0    for _, num := range numbers &#123; // numbers is like a slice        sum += num    &#125;    return sum&#125;add()add(1,2)add(1,2,3,4)var numbers := []int&#123;2,3,5&#125;add(numbers...) // call with slice, expand slice, same as add(2, 3, 5), unpacked parameter

dynamic parameters for different types
func test() &#123;    handle(1, &quot;abc&quot;)    handle(&quot;abc&quot;, &quot;xyz&quot;, 3)    handle(1, 2, 3, 4)&#125;// interface&#123;&#125; for any type as it has no method defined// ...interface&#123;&#125; behaves like a new typefunc handle(params ...interface&#123;&#125;) &#123;    fmt.Println(&quot;Handle func called with parameters:&quot;)    for _, param := range params &#123;        fmt.Printf(&quot;%v\n&quot;, param) // print the value of special type    &#125;&#125;


import &quot;fmt&quot;// function as an argumentfunc test(fn func(int, int) int) int &#123;    return fn(1, 2)&#125;// function as return valuefunc adder() func(int) int &#123;    sum := 0    return func(x int) int &#123; // unnamed function        sum += x        return sum    &#125;&#125;func funcDemo() &#123;    // defer function    defer fmt.Println(&quot;boy&quot;)    defer fmt.Println(&quot;girl&quot;)    fmt.Println(&quot;hello&quot;)        // declare a function object    hadd := func(x, y int) int &#123;        return x + y    &#125;    fmt.Println(test(hadd))        // closure    f1 := adder() // f1 has its own copy of sum, all call f1 shares the same sum.    fmt.Println(f1(1))    fmt.Println(f1(1))        f2 := adder() // f2 has its own copy of sum    fmt.Println(f2(1))&#125;funcDemo()

hello
3
1
2
1
girl
boy

function typeThink function signature(without name) as a type, you can declare variable, parameter, new type based on function signature.
package mainimport &quot;fmt&quot;// new type based on function typetype HelloFn func(string) error// parameter with function typefunc hello(fn func(string) error) &#123;    // declare a function var    var pn func(string) error    pn = fn    pn(&quot;jason&quot;)    // declare a function var    var pn1 HelloFn    pn1 = fn    pn1(&quot;jason&quot;)&#125;func greet(name string) error &#123;    fmt.Println(&quot;hello&quot;, name)    return nil&#125;func main() &#123;    hello(greet)&#125;main()

hello jason
hello jason

function parameterParameter passing is same like C except for array, for array it’s copy of the whole array, not array pointer is passed!!! 
For slice even pointer is passed in function, if you append new element in that slice, the caller does not know either, see below explanations

the underlying array reached its capacity, a new slice created to replace the origin one, obviously the origin slice will not be modified.
the underlying array has not reached its capacity, and was modified. BUT the field len of the slice was not overwritten because the slice was passed by value. As a result, the origin slice will not aware its len was modified, which result in the slice not modified.

package mainimport &quot;fmt&quot;func test_slice(s []string) &#123;    s[1] = &quot;b&quot;&#125;func test_slice2(s []string) &#123;    // append means add an element at last, same like s[len(s)] = &quot;c&quot;    // if cap is smaller, a new underlay memory is created.    s = append(s, &quot;c&quot;)    fmt.Println(s, len(s))&#125;func main() &#123;    s := make([]string, 2)    s[0] = &quot;a&quot;    test_slice(s)    fmt.Println(s, len(s))    // as now len(s) = 2    test_slice2(s)    // s does not see the &#x27;c&#x27; as it&#x27;s added to a new larger slice!!!    // new memory is not returned    fmt.Println(s, len(s))    //=============================================================    s1 := make([]string, 0, 2)    s1 = append(s1, &quot;x&quot;)    // no new memory is created, but you still not see &#x27;c&#x27;    // because the len is not update, as it&#x27;s passed by value!!!    /* imagin slice as        Slice &#123;            int len;            char *s; // underlay memory pointer        &#125;    */    test_slice2(s1)    fmt.Println(s1, len(s1))&#125;main()

[a b] 2
[a b c] 3
[a b] 2
[x c] 2
[x] 1

flow controlforGo has only one looping construct, the for loop, NO while, until etc.
The basic for loop has three components separated by semicolons:

the init statement: executed before the first iteration
the condition expression: evaluated before every iteration
the post statement: executed at the end of every iteration

*Note

Unlike other languages like C, Java, or JavaScript. For Go there are no parentheses surrounding the three components of the for statement but the braces { } are always required.

 

import &quot;fmt&quot;for i := 0; i &lt; 10; i++ &#123; // is is only visible in this loop    if i == 5 &#123;        continue    &#125;    if i == 8 &#123;        break    &#125;    fmt.Println(i)&#125;for i := 0; i &lt; 10; &#123; // is is only visible in this loop    if i == 5 &#123;        continue    &#125;    if i == 8 &#123;        break    &#125;    fmt.Println(i)    i++;&#125;var i intfor ; i &lt; 10; i++&#123;    if i == 5 &#123;        continue    &#125;    if i == 8 &#123;        break    &#125;    fmt.Println(i)&#125;

 

use for as while as init and post statements are optional
sum := 0for ; sum &lt; 100; &#123;    sum += 10;&#125;// short way, same like while in Cfor sum &lt; 100 &#123;    sum += 10;&#125;

 

infinite loop
for &#123;&#125;

multiple assignments
a := []int&#123;1, 2, 3, 4, 5, 6&#125;for i, j := 0, len(a) - 1; i &lt; j; i, j = i + 1, j - 1 &#123;    a[i], a[j] = a[j], a[i]&#125;


range with for
package mainimport &quot;fmt&quot;type Person struct &#123;    name string&#125;func main() &#123;    ps := []Person&#123;        &#123;name: &quot;a&quot;&#125;,    &#125;    for _, p := range ps &#123;        p.name = &quot;b&quot; // p is copy of each element    &#125;    // nothing changed    fmt.Println(ps)    for i := 0; i &lt; len(ps); i++ &#123;        ps[i].name = &quot;b&quot;    &#125;    fmt.Println(ps)           &#125;# go run test.go [&#123;a&#125;][&#123;b&#125;]

NOTE: when reach the loop end, the index is different!!!
package mainimport &quot;fmt&quot;func main() &#123;    a := []int&#123;1, 2, 3&#125;    i := 0    for ; i &lt; len(a); i++ &#123;        // i == 3 when out of range    &#125;    fmt.Printf(&quot;out loop index: %d\n&quot;, i)    i = 0    for i, _ = range a &#123;        // i == 2 when out of range    &#125;    fmt.Printf(&quot;out loop index: %d\n&quot;, i)&#125;# go run test.goout loop index: 3out loop index: 2

ifGo’s if statements is like its for loops; the expression may not be surrounded by parentheses ( ) but the braces &#123; &#125; are required.
The if statement can start with a short statement to execute before the condition.
Variables declared by the statement are only in scope until the end of the if. it’s also available inside any of the else blocks.
 

import &quot;fmt&quot;if i := 0; i == 0 &#123; // scope of i is block, after if, it&#x27;s out of scope    fmt.Println(&quot;equal&quot;)&#125; else &#123;    fmt.Println(&quot;i:&quot;, i)&#125;if 1 &#123; // compile error!!! as 1 is not bool type    fmt.Println(&quot;1&quot;)&#125;


NOTE

non-boolean type can NOT be used as if condition!!!

switchThe expressions need not to be constants or even integers, the cases are evaluated top to bottom until a match is found, and if the switch has no expression it switches on true.
Go’s switch is like the one in C, C++, Java, JavaScript, and PHP, except that Go only runs the selected case (implicit break at the end), not all the cases that follow. In effect, the break statement that is needed at the end of each case in those languages is provided automatically in Go but if you want to break in the middle of this case, break is required. Another important difference is that Go’s switch cases need not be constants, and the values involved need not be integers.
 

Switch cases evaluate cases from top to bottom, stopping when a case succeeds, auto break if matched  
import &quot;fmt&quot;switch i &#123;case 0: // auto breakcase f(): //does not call f if i==0    break // break at end no effect as if there is no break here!!!case 1:    if 2 &gt;1 &#123;        break // break here below does not run    &#125;    fmt.Println(&quot;reach end of case&quot;)&#125;

 

Switch without conditionSwitch without a condition is the same as switch true.
This construct can be a clean way to write long if-then-else chains.
t := 15switch &#123;case t &gt; 10:    fmt.Println(10)case t &gt; 5:    fmt.Println(5)case t &gt; 3:    // with fallthrough, the next case condtion is not checked.    fallthrough // if t==4, fallthrough next one, no matter next condition matches or not, print 18 !!!case t &gt; 18:    fmt.Println(18) // break here if no other fallthroughcase t &gt;= 4:    fmt.Println(4)default:&#125;// different cases has the same actionn := 10switch n &#123;case 2,3:    // do something directly herecase 5,6:    // call do_something() to share the same action&#125;

gotofunc myfunc() &#123;    i := 0    HERE:    fmt.Println(i)    i++    if i &lt; 10 &#123;        goto HERE    &#125;&#125;

break&#x2F;continueBy default, break, continue work for inner loop, but if you want to take effect of outer loop, use label for break, continue.
import &quot;fmt&quot;func greet() string &#123;    return &quot;hello&quot;&#125;func flowDemo() &#123;    // for loop    for i := 0; i &lt; 10; i++ &#123;        if i == 5 &#123;            continue        &#125;                if i == 8 &#123;            break        &#125;        fmt.Print(i)    &#125;        fmt.Println(&quot;\n-------------------------&quot;)        // switch, from top to bottom    switch i := &quot;hello&quot;; i &#123;        case &quot;boy&quot;:            fmt.Println(&quot;boy&quot;)        case greet():            fmt.Println(&quot;hello&quot;)        default: // always put default at last one!!!            fmt.Println(&quot;default&quot;)    &#125;        fmt.Println(&quot;\n-------------------------&quot;)    // use break/continue with label on outer loophere:    for i := 0; i &lt; 2; i++ &#123;        fmt.Println(&quot;i=&quot;, i)        for j := i + 1; j &lt; 3; j++ &#123;            fmt.Println(&quot;j=&quot;,j)            if i == 0 &#123;                continue here // continue the out loop, even here is out, i is initialized for only once!!!!            &#125;            if j == 2 &#123;                break            &#125;        &#125;    &#125;            fmt.Println(&quot;\n-------------------------&quot;)there:    for i := 0; i &lt; 2; i++ &#123;        for j := i + 1; j &lt; 3; j++ &#123;            if j == 1 &#123;                continue            &#125;            fmt.Println(j)            if j == 2 &#123;                break there // break out, no outer next loop            &#125;        &#125;    &#125;&#125;flowDemo()

0123467
-------------------------
hello

-------------------------
i= 0
j= 1
i= 1
j= 2

-------------------------
2

system envimport (    &quot;fmt&quot;    &quot;os&quot;)func envDemo() &#123;    // os.Environ() reutrn a map    for _, v := range os.Environ() &#123;        fmt.Println(v)    &#125;        os.Setenv(&quot;GO&quot;, &quot;/tmp/go&quot;)    fmt.Println(os.Getenv(&quot;GO&quot;))&#125;envDemo()

PATH=/home/data/Anaconda3/envs/py3.9/bin:/opt/llvm/bin:/home/data/Anaconda3/envs/py3.9/bin:/home/data/Anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/home/go:/home/go/bin:/root/.yarn_pkg/bin:/usr/lib64:/usr/local/go/bin:/home/data/Anaconda3/envs/py3.9/libexec/git-core:/root/bin:/root/.yarn_pkg/bin:/home/go/bin:/home/go:/usr/local/go/bin
PWD=/
LANG=en_US.UTF-8
SHLVL=1
_=/usr/bin/env
GO111MODULE=on
GOMODCACHE=/home/go/pkg/mod
GOCACHE=/root/.cache/go-build
GOPATH=/home/go
PYDEVD_USE_FRAME_EVAL=NO
JPY_PARENT_PID=1797
GO=/tmp/go
/tmp/go

small tipsreturn local var from stack is safe in GOReturning an address of a local variable is also safe. 
import &quot;fmt&quot;func test() *int &#123;    var s int = 12    return &amp;s&#125;var p *int = test()fmt.Println(*p)

string vs [] bytestring is the set of byte, conventionally but not necessarily representing UTF-8-encoded text. A string may be empty, but not nil.  

Values of string type are immutable   
Values of []byte are mutable

conversion
s1 := &quot;hello&quot;b := []byte(s1) // new memory allocated!!!    // []byte to strings2 := string(b) // new memory allocated!!!


import &quot;fmt&quot;func test() &#123;    s := &quot;你好&quot; //string    fmt.Println(len(s))        var s = [] rune(&quot;你好&quot;) // as rune = int32, henc two elements for 你好    fmt.Println(s, len(s), string(s))&#125;test()

6
[20320 22909] 2 你好

what does empty mean for each typelet’s focus on these types, string, integer(int, uint etc), pointer, array, slice, map)
import &quot;fmt&quot;func emptyDemo() &#123;    /* default value for each type is empty     * &quot;&quot;      ---&gt;string     * 0       ---&gt;number     * nil     ---&gt;pointer     * nil     ---&gt;slice     * nil     ---&gt;map     * nil     ---&gt;function type like function type: &#x27;type CallBack func()&#x27;     * NO empty for array as it&#x27;s has fixed size, must be set at delcaration     */        var str string          // str := &quot;&quot;    var n int               // n := 0    var p *int              // var p *int = nil    var sc []int            // sc := []int&#123;&#125;    var m map[string]int    // here m is nil can NOT add new element in it    // m := map[string]ing&#123;&#125; m NOT nill, can add new element, like m[&quot;a&quot;] = 2    if str == &quot;&quot; &#123;        fmt.Printf(&quot;string default value: %q \n&quot;, &quot;&quot;)    &#125;    if n == 0 &#123;        fmt.Println(&quot;int default value: 0&quot;)    &#125;    if p == nil &#123;        fmt.Println(&quot;pointer default value: nil&quot;)    &#125;    if sc == nil &#123;        fmt.Printf(&quot;slice defautl value: nil  ([], cap=%d, len=%d)\n&quot;, cap(sc), len(sc))    &#125;    if m == nil &#123;        fmt.Printf(&quot;map defautl value: nil  (&#123;&#125; len=%d)\n&quot;, len(m))    &#125;&#125;emptyDemo()

string default value: &quot;&quot; 
int default value: 0
pointer default value: nil
slice defautl value: nil  ([], cap=0, len=0)
map defautl value: nil  (&#123;&#125; len=0)

get the size of memory for each typepackage mainimport (    &quot;fmt&quot;    &quot;unsafe&quot;)func main() &#123;    a := int16(32)    b := 12          //int 8 bytes on 64-bit machine    c := &quot;h&quot;         //not like C strlen.    d := &#x27;h&#x27;         //int32    var e byte = &#x27;h&#x27; //uint8    f := &amp;b    // f := b + 1       // f is &#x27;i&#x27;    fmt.Printf(&quot;sizeof(%T)=%v\n&quot;, a, unsafe.Sizeof(a))    fmt.Printf(&quot;sizeof(%T)=%v\n&quot;, b, unsafe.Sizeof(b))    fmt.Printf(&quot;sizeof(%T)=%v\n&quot;, c, unsafe.Sizeof(c))    fmt.Printf(&quot;sizeof(%T)=%v\n&quot;, d, unsafe.Sizeof(d))    fmt.Printf(&quot;sizeof(%T)=%v\n&quot;, e, unsafe.Sizeof(e))    fmt.Printf(&quot;sizeof(%T)=%v\n&quot;, f, unsafe.Sizeof(f))&#125;sizeof(int16)=2sizeof(int)=8// it&#x27;s always 16 bytes, no matter how long it&#x27;s, as for string, sizeof(s)==sizeof(s.len)+sizeof(s.pointer)sizeof(string)=16sizeof(int32)=4sizeof(uint8)=1sizeof(*int)=8

when should I use new()new(T) allocates zeroed storage for a new item of type T and returns its address, a value of type *T.
Suggestion, use it as less as possible, as new(T) and &amp;T{} can do the same thing. Both allocate a zero T and return a pointer to this allocated memory. The only difference is, that &amp;T{} doesn’t work for builtin types like int; you can only do new(int).
// without new only form is different!!i: = 10p :=  &amp;i//with new, it&#x27;s one statementp := new(int)type Person struct &#123;    name string&#125;// without newp := &amp;Person &#123;name: &quot;jason&quot;&#125;// with newp := new(Person)Person.name = &quot;jason&quot;

when should I use make()It creates slices, maps, and channels only, and it returns an initialized (not zeroed) value of type T (not *T). The reason for the distinction is that these three types represent, under the covers, references to data structures that must be initialized before use. A slice, for example, is a three-item descriptor containing a pointer to the data (inside an array), the length, and the capacity, and until those items are initialized, the slice is nil. For slices, maps, and channels, make initializes the internal data structure and prepares the value for use

The make built-in function allocates and initializes an object of type slice, map, or chan (only), can be used only for Slice, Map, Channel

Slice: The size specifies the length. The capacity of the slice is equal to its length. A second integer argument may be provided to specify a different capacity; it must be no smaller than thelength. For example, make([]int, 0, 10) allocates an underlying array of size 10 and returns a slice of length 0 and capacity 10 that is backed by this underlying array.  

Map: An empty map(not equal nil) is allocated with enough space to hold the specified number of elements. The size may be omitted, in which case a small starting size is allocated.    

Channel: The channel’s buffer is initialized with the specified buffer capacity. If zero, or the size is omitted, the channel is unbuffered.



Suggestion  

If you know the estimated size of slice or map, use make() to preallocate enough memory
Always use make() for channel

s := []int&#123;&#125;s = append(s, 1)s = append(s, 2) // new underlaying array is allocated.s := make([]int, 0, 2) // cap = 2, len = 0s = append(s, 1)s = append(s, 2) // no new array is allocated.

pointer to array and array of pointers// bad way never use this, use slice insteadfunc updatearray(funarr *[5]int) &#123;&#125;// good wayfunc updateslice(funarr []int) &#123;&#125;// bad way, use slice of pointers insteadvar ptr [MAX]*int;// good wayvar ptr []*int;



package mainimport &quot;fmt&quot;type Person struct &#123;    Name string&#125;func main() &#123;    var pc []*Person    // initialization    pc = []*Person&#123;        &amp;Person&#123;Name: &quot;tom&quot;&#125;,        &#123;Name: &quot;jack&quot;&#125;, //shortway    &#125;    // after initialization    pc = append(pc, &amp;Person&#123;Name: &quot;hak&quot;&#125;)    fmt.Println(pc[0].Name, pc[1].Name, pc[2].Name)&#125;main()

tom jack hak

check type of variablevar1 := 12// only print it typefmt.Printf(&quot;var1 = %T\n&quot;, var1)fmt.Println(&quot;var1 = &quot;, reflect.TypeOf(var1))

variable has same name with packageIn such case, error happens.
package mainimport (    &quot;fmt&quot;    &quot;io/ioutil&quot;    &quot;coding.xx.com/agent/src/server/conf/xml&quot;    // package name is xml)func main() &#123;    // variable name is xml    xml, err := ioutil.ReadFile(&quot;./vm.xml&quot;)    if err != nil &#123;        fmt.Println(err)        return    &#125;        // error!!!!    df := &amp;xml.Domain&#123;&#125;    if err = df.Unmarshal(xml); err != nil &#123;        fmt.Println(err)        return    &#125;&#125;

ref
A Tour of Go
Go cheatsheet
Go basic
Learn GO in Y Minutes
Go examples
StudyGo ZH

]]></content>
      <categories>
        <category>go</category>
        <category>program</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>html_basic</title>
    <url>/2020/07/16/html-basic/</url>
    <content><![CDATA[HTMLblock box and inline box elementBoth block and inline element are box! but with different formatting contexts, say for inline-box you can set padding, background, border for them, but can’t set width and height for inline element.
By default, a block level element&#39;s content is 100% of the width of its parent element, and as tall as its content. Inline elements are as tall as their content, and as wide as their content. You can’t set width or height on inline elements — they just sit inside the content of block level elements
Block-level elements may contain inline elements and (sometimes) other block-level elements. Inherent in this structural distinction is the idea that block elements create &quot;larger&quot; structures than inline elements
Inline elements may contain only data and other inline elements. You can&#39;t put block elements inside inline elements.
The behavior of elements which have a block or inline formatting context is defined like this.

block formatting context  In a block formatting context, boxes are laid out one after the other vertically, beginning at the top of a containing block. The vertical distance between two sibling boxes is determined by the ‘margin’ properties. Vertical margins between adjacent block-level boxes in a block formatting context collapse.
  By default block elements will consume all of the space in the inline direction.


In a block formatting context, each box’s left outer edge touches the left edge of the containing block.

inline formatting context  In an inline formatting context, boxes are laid out horizontally, one after the other, beginning at the top of a containing block. Horizontal margins, borders, and padding are respected between these boxes. The boxes may be aligned vertically in different ways: their bottoms or tops may be aligned, or the baselines of text within them may be aligned. The rectangular area that contains the boxes that form a line is called a line box

Block element

address, article, aside, div, dd, d,t fieldset, footer, form, h1, h6, header, nav, hr, li, ol, p, ul, table etc

Inline element

a, audio, video, button, img, input, select, b, br, i, iframe, label, svg etc.

ALL inline elements and ALL block elements
Global attributeThe global attributes are attributes that can be used with all HTML elements(tag), include custom tag. below are common global attributes.



attribute
Description



style
Specifies an inline CSS style for an element


class
Specifies one or more classnames for an element


id
Specifies a unique id for an element


title
The value of the title attribute will be displayed as a tooltip when you mouse over the element


draggable
Specifies whether an element is draggable or not


data-*
Used to store custom data private to the page or application


empty tagSome HTML elements have no content (like the &lt;br&gt; element). These elements are called empty elements. Empty elements do not have an end tag!

&lt;br&gt;
&lt;hr&gt;
&lt;img&gt;
&lt;input&gt;

Quotes attributes valueSingle or Double Quotes?
Double quotes around attribute values are the most common in HTML, but single quotes can also be used.In some situations, when the attribute value itself contains double quotes, it is necessary to use single quotes
Html LinksWhen you move the mouse over a link, the mouse arrow will turn into a little hand, A link does not have to be text. A link can be an image or any other inline HTML element
The target attribute specifies where to open the linked document.

_self - Default. Opens the document in the same window&#x2F;tab as it was clicked
_blank - Opens the document in a new window or tab

Use mailto: inside the href attribute to create a link that opens the user’s email program (to let them send a new email):
&lt;a href=&quot;mailto:someone@example.com&quot;&gt;Send email&lt;/a&gt; 

Jump to particular element on the same page or another page
&lt;!--C4 is element id on the same page--&gt;&lt;a href=&quot;#C4&quot;&gt;Jump to Chapter 4&lt;/a&gt; &lt;!--C4 is element id on the another page--&gt;&lt;a href=&quot;html_demo.html#C4&quot;&gt;Jump to Chapter 4&lt;/a&gt;

Image vs PictureAlways specify the width and height of an image. If width and height are not specified, the web page might flicker while the image loads. as Browser does not know the width and height, it uses zero, after get the image, the whole page is render again which causes flicker.
The most common use of the &lt;picture&gt; element will be for art direction in responsive designs. Instead of having one image that is scaled up or down based on the viewport width, multiple images can be designed to more nicely fill the browser viewport.
The &lt;picture&gt; element contains two tags: one or more &lt;source&gt; tags and one &lt;img&gt; tag.
The browser will look for the first &lt;source&gt; element where the media query matches the current viewport width, and then it will display the proper image (specified in the srcset attribute). The &lt;img&gt; element is required as the last child of the &lt;picture&gt; element, as a fallback option if none of the source tags matches.
Tip: The &lt;picture&gt; element works “similar” to &lt;video&gt; and &lt;audio&gt;. You set up different sources, and the first source that fits the preferences is the one being used.
 &lt;picture&gt;  &lt;source media=&quot;(min-width:650px)&quot; srcset=&quot;img_pink_flowers.jpg&quot;&gt;  &lt;source media=&quot;(min-width:465px)&quot; srcset=&quot;img_white_flower.jpg&quot;&gt;  &lt;img src=&quot;img_orange_flowers.jpg&quot; alt=&quot;Flowers&quot; style=&quot;width:auto;&quot;&gt;&lt;/picture&gt; 
iframeAn HTML iframe is used to display a web page within a web page.
&lt;iframe src=&quot;https://www.google.com&quot; name=&quot;iframe_a&quot; height=&quot;300px&quot; width=&quot;100%&quot; title=&quot;Iframe Example&quot;&gt;&lt;/iframe&gt;

use iframe as the target of link
&lt;iframe src=&quot;demo_iframe.htm&quot; name=&quot;iframe_a&quot; title=&quot;Iframe Example&quot;&gt;&lt;/iframe&gt;&lt;p&gt;&lt;a href=&quot;https://www.w3schools.com&quot; target=&quot;iframe_a&quot;&gt;W3Schools.com&lt;/a&gt;&lt;/p&gt; 

File Path


Path
Description



&lt;img src&#x3D;”picture.jpg”&gt;
The “picture.jpg” file is located in the same folder as the current page


&lt;img src&#x3D;”images&#x2F;picture.jpg”&gt;
The “picture.jpg” file is located in the images folder in the current folder


&lt;img src&#x3D;”&#x2F;images&#x2F;picture.jpg”&gt;
The “picture.jpg” file is located in the images folder at the root of the current web


&lt;img src&#x3D;”..&#x2F;picture.jpg”&gt;
The “picture.jpg” file is located in the folder one level up from the current folder


HTML EntitiesIf you use the less than (&lt;) or greater than (&gt;) signs in your text, the browser might mix them with tags.
Character entities are used to display reserved characters in HTML.A character entity looks like this in html directly
&amp;entity_name;OR&amp;#entity_number; To display a less than sign (&lt;) we must write: &amp;lt; or &amp;#60;
Useful entities
symbol entities
EmojisEmojis are characters from the UTF-8 character set: 😄 😍 💗, They are letters (characters) from the UTF-8 (Unicode) character set. display like image.

😄 is 128516
😍 is 128525
💗 is 128151

Two place to use UTF-8 code.
&lt;p&gt;&amp;#128512;&lt;/p&gt;

a::after &#123;    content: &quot;\128512&quot;;&#125;

Emoji Unicode Reference
URL encodingURLs can only be sent over the Internet using the ASCII character-set. If a URL contains characters outside the ASCII set, the URL has to be converted.
URL encoding converts non-ASCII characters into a format that can be transmitted over the Internet.URL encoding replaces non-ASCII characters with a “%” followed by hexadecimal digits.URLs cannot contain spaces. URL encoding normally replaces a space with a plus (+) sign, or %20.
FormForm elements have &lt;input&gt; &lt;select&gt; &lt;textarea&gt;
Each input field must have a name attribute to be submitted.If the name attribute is omitted, the data of that input field will not be sent at all, as we have to send name and value pairs with Post method.
Different input types, actually, some built-in validation for each type and styes for that

&lt;input type&#x3D;”button”&gt;
&lt;input type&#x3D;”checkbox”&gt;
&lt;input type&#x3D;”color”&gt;
&lt;input type&#x3D;”date”&gt;
&lt;input type&#x3D;”datetime-local”&gt;
&lt;input type&#x3D;”email”&gt;
&lt;input type&#x3D;”file”&gt;
&lt;input type&#x3D;”hidden”&gt;
&lt;input type&#x3D;”image”&gt;
&lt;input type&#x3D;”month”&gt;
&lt;input type&#x3D;”number”&gt;
&lt;input type&#x3D;”password”&gt;
&lt;input type&#x3D;”radio”&gt;
&lt;input type&#x3D;”range”&gt;
&lt;input type&#x3D;”reset”&gt;
&lt;input type&#x3D;”search”&gt;
&lt;input type&#x3D;”submit”&gt;
&lt;input type&#x3D;”tel”&gt;
&lt;input type&#x3D;”text”&gt;
&lt;input type&#x3D;”time”&gt;
&lt;input type&#x3D;”url”&gt;
&lt;input type&#x3D;”week”&gt;

Attribute 	Descriptionchecked 	Specifies that an input field should be pre-selected when the page loads (for type=&quot;checkbox&quot; or type=&quot;radio&quot;)disabled 	Specifies that an input field should be disabledmax             Specifies the maximum value for an input fieldmaxlength 	Specifies the maximum number of character for an input fieldmin 	        Specifies the minimum value for an input fieldpattern 	Specifies a regular expression to check the input value againstreadonly 	Specifies that an input field is read only (cannot be changed)required 	Specifies that an input field is required (must be filled out)size 	        Specifies the width (in characters) of an input fieldstep 	        Specifies the legal number intervals for an input fieldvalue 	        Specifies the default value for an input field

SVG in htmlSVG defines vector-based graphics in XML format, the HTML &lt;svg&gt; element is a container for SVG graphics.
&lt;svg width=&quot;300&quot; height=&quot;200&quot;&gt;  &lt;polygon points=&quot;100,10 40,198 190,78 10,78 160,198&quot;  style=&quot;fill:lime;stroke:purple;stroke-width:5;fill-rule:evenodd;&quot; /&gt;&lt;/svg&gt;

videoThere are three supported video formats: MP4, WebM, and Ogg by browser built-in player. MP4 is popular than others as most browser supports it.
 &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;  &lt;source src=&quot;movie.mp4&quot; type=&quot;video/mp4&quot;&gt;  &lt;source src=&quot;movie.ogg&quot; type=&quot;video/ogg&quot;&gt;Your browser does not support the video tag.&lt;/video&gt; 

audioThere are three supported audio formats: MP3, WAV, and OGG by browser built-in player, mp3 is more popular than others.
 &lt;audio controls&gt;  &lt;source src=&quot;horse.ogg&quot; type=&quot;audio/ogg&quot;&gt;  &lt;source src=&quot;horse.mp3&quot; type=&quot;audio/mpeg&quot;&gt;Your browser does not support the audio element.&lt;/audio&gt; 
PluginPlug-ins are computer programs that extend the standard functionality of the browser, 
Plug-ins were designed to be used for many different purposes:

To run Java applets
To run Microsoft ActiveX controls
To display Flash movies
To display maps
To scan for viruses
To verify a bank id

Hence Plugin-in can do more than &lt;video&gt; and &lt;audio&gt;, and browser supports it long time ago, there are two type to include plugin &lt;object&gt; Microsoft standard and &lt;embed&gt; way, always use later as it’s supported by most browsers, even Microsoft browser.
&lt;embed type=&quot;video/webm&quot; src=&quot;video.mp4&quot; width=&quot;400&quot; height=&quot;300&quot;&gt; 

HTML built-in APIAPI is used by JS to get information, like GEO, Storage, Drap&#x2F;Drop
The getCurrentPosition() method is used to return the user’s position. then you and print it or show it in a Map
Drag and Droop
 &lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt;&lt;script&gt;function allowDrop(ev) &#123;  ev.preventDefault();&#125;function drag(ev) &#123;  ev.dataTransfer.setData(&quot;text&quot;, ev.target.id);&#125;function drop(ev) &#123;  ev.preventDefault();  var data = ev.dataTransfer.getData(&quot;text&quot;);  ev.target.appendChild(document.getElementById(data));&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;div1&quot; ondrop=&quot;drop(event)&quot; ondragover=&quot;allowDrop(event)&quot;&gt;&lt;/div&gt;&lt;img id=&quot;drag1&quot; src=&quot;img_logo.gif&quot; draggable=&quot;true&quot; ondragstart=&quot;drag(event)&quot; width=&quot;336&quot; height=&quot;69&quot;&gt;&lt;/body&gt;&lt;/html&gt; 

Web StorageHTML web storage provides two objects for storing data on the client:

window.localStorage -   stores data with no expiration date
window.sessionStorage - stores data for one session (data is lost when the browser tab is closed)

// StorelocalStorage.setItem(&quot;lastname&quot;, &quot;Smith&quot;);localStorage.lastname = &quot;Smith&quot;;// Retrievedocument.getElementById(&quot;result&quot;).innerHTML = localStorage.getItem(&quot;lastname&quot;);//localStorage.lastnamelocalStorage.removeItem(&quot;lastname&quot;);//Session storageif (sessionStorage.clickcount) &#123;  sessionStorage.clickcount = Number(sessionStorage.clickcount) + 1;&#125; else &#123;  sessionStorage.clickcount = 1;&#125;document.getElementById(&quot;result&quot;).innerHTML = &quot;You have clicked the button &quot; +sessionStorage.clickcount + &quot; time(s) in this session.&quot;;

EventsEvents

Window event
Form events
Keyboard events
Mouse events
Clipboard events
Drap events
Media events

ALL Events
REF
HTML cheatsheet

]]></content>
      <categories>
        <category>html</category>
        <category>tag</category>
      </categories>
      <tags>
        <tag>html</tag>
        <tag>tag</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo usage</title>
    <url>/2019/09/25/hexo-post/</url>
    <content><![CDATA[How to use Hexo tag and Next taghexo search setting

post tags and categories in Hexo front matterlet’s explain them in short samples

tagtags written in this way, no order, no level, flat, each line takes one tag

tags:- taga- tagb


categoriescategories written in two ways, with order, level, not flat, same level categories must in different linecategories:- [parentA, childA]- parentB

Hexo tag and Next tag used in contentTag plugins are different from post tags. They are ported from Octopress and provide a useful way for you to quickly add specific content to your posts
 is an extension to  which makes writting post easily.
Actually, they will be converted to  at last. you can use both of them at same time, here I only list the frequent one used in my posts.
Basic MarkDown tags
Line breaksTo create a line break (&lt;br&gt;), end a line with two or more spaces, and then type return.

blockquote Next extended it as noteTo create a blockquote, add a &gt; in front of a paragraph.

image  
  ![Tux, the Linux mascot](/assets/images/tux.png)
Horizontal RulesTo create a horizontal rule, use three or more asterisks (***), dashes (—), or underscores (___) on a line by themselves.


Note: Many Markdown applications allow you to use HTML tags in Markdown-formatted textMarkdown Basic
Hexo tags&#123;% link text url %&#125;&#123;% post_link filename %&#125;
Next tags
note used to group content
&#123;% note [class] [no-icon] %&#125;Any content (support inline tags too.io).&#123;% endnote %&#125;[class]   : default | primary | success | info | warning | danger.

tabs used to group of content with tabs
&#123;% tabs Unique name %&#125;&lt;!-- tab [Tab caption] --&gt;Any content (support inline tags too).&lt;!-- endtab --&gt;&#123;% endtabs %&#125;

label used to color Text
&#123;% label [class]@Text %&#125;[class] : default | primary | success | info | warning | danger.          &#x27;@Text&#x27; can be specified with or without space          E.g. &#x27;success @text&#x27; similar to &#x27;success@text&#x27;.          If not specified, default class will be selected.

]]></content>
      <categories>
        <category>hexo</category>
        <category>tags</category>
      </categories>
      <tags>
        <tag>hexo tag</tag>
        <tag>next tag</tag>
        <tag>hexo search</tag>
      </tags>
  </entry>
  <entry>
    <title>html_dom</title>
    <url>/2020/07/20/html-dom/</url>
    <content><![CDATA[HTMLDOMWhen a web page is loaded, the browser creates a Document Object Model of the page, the HTML DOM model is constructed as a tree of Objects

Everything in the DOM is a node(text can be a node as well)
&lt;!--document node--&gt;&lt;html&gt;&lt;!--  &lt;h1                        class=&quot;big&quot;&gt;             hello                     &lt;/h1&gt;  h1:element node(object)    attribute node(object)   text noe(object)--&gt;&lt;h1 class=&quot;big&quot;&gt;hello&lt;/h1&gt;&lt;/html&gt;

The HTML DOM is a standard object model and programming interface for HTML. It defines:

The HTML elements as objects
The properties of all HTML elements
The methods to access all HTML elements
The events for all HTML elements

Document objectThe document object represents your web page, it’s root node of the tree, Here are methods commonly used.



Method
Description



document.getElementById(id)
Find an element by element id


document.getElementsByTagName(name)
Find elements by tag name


document.getElementsByClassName(name)
Find elements by class name


element.setAttribute(attribute, value)
Change the attribute value of an HTML element


document.createElement(element)
Create an HTML element


document.removeChild(element)
Remove an HTML element


document.appendChild(element)
Add an HTML element


document.replaceChild(new, old)
Replace an HTML element


document.write(text)
Write into the HTML output stream


document.getElementById(id).onclick &#x3D; function(){code}
Adding event handler code to an onclick event


Some attributes of document root you should know



Property
Description



document.anchors
Returns all &lt;a&gt; elements that have a name attribute


document.cookie
Returns the document’s cookie


document.forms
Returns all &lt;form&gt; elements


document.images
Returns all &lt;img&gt; elements


document.links
Returns all &lt;area&gt; and &lt;a&gt; elements that have a href attribute


document.title
Returns the &lt;title&gt; element


document.URL
Returns the complete URL of the document


element objectFind an element through Document object or another element, if the method is called under an element, it checks child of that element, not root.



Method
Description



element.getElementById(id)
Find an element by element id


element.getElementsByTagName(name)
Find elements by tag name


element.getElementsByClassName(name)
Find elements by class name


element.hasAttributes()
Return true if has any attribute


element.hasAttribute(“class”)
Return true only if has such attribute


element.setAttribute(attribute, value)
Change the attribute value of an HTML element


element.removeAttribute(attribute)
Remove a particular attribute


element.hasChildNodes()
Return true if has at least one child


element.insertBefore(new, exist)
Insert child before another existed child


element.removeChild(element)
Remove an HTML element


element.appendChild(element)
Add an HTML element


element.replaceChild(new, old)
Replace an HTML element


document.write(text)
Write into the HTML output stream


element.getElementById(id).onclick &#x3D; function(){code}
Adding event handler code to an onclick event


element.addEventListener(“click”, click_handler)
Adding event handler for click event


element.removeEventListener(event_listener)
Removing event listener


element.querySelector(“p.intro”)
Return first element by CSS selector


element.querySelectorAll(“p.intro”)
Return all elements by CSS selector


element.focus()
Get focus


add a new element
&lt;div id=&quot;div1&quot;&gt;  &lt;p id=&quot;p1&quot;&gt;This is a paragraph.&lt;/p&gt;  &lt;p id=&quot;p2&quot;&gt;This is another paragraph.&lt;/p&gt;&lt;/div&gt;&lt;script&gt;var para = document.createElement(&quot;p&quot;);var node = document.createTextNode(&quot;This is new.&quot;);para.appendChild(node);var element = document.getElementById(&quot;div1&quot;);element.appendChild(para);&lt;/script&gt; 

Attribute for element



attribute
Description



element.attributes
Return all attributes


element.className
Return value of ‘class’ attribute


element.id
Return Id attribute


element.innerHTML
Return inner HTML


element.innerText
Only return value of ALL Text nodes include children


element.nodeName
Return tag name like p, iframe


element.parentNode
Return parent node


element.firstChild
Return first direct child node


element.lastChild
Return last direct child node


element.nextSibling
Return next sibling node


element.previousSibling
Return previous sibling node


element.childNodes
Return direct child nodes


element.style.property
Set&#x2F;Return css style


EventEvent object is passed to event handler when event happens, actually the passed event is not only Event object, but more specific one which is child of Event object, more specific means more meta data about the event, here is the inheritance of events
Event Inheritance
Event object has below attribute



Attribute
Description



bubbles
Return True if event is bubble event


currentTarget
Return current Listener target


target
Return element who triggered such event


type
return type of event


More specific events, refer to Events API.
Note: as event could bubble, hence currentTarget may not be target
There are two ways to add event handler, but addEventListener() method attaches an event handler to an element without overwriting existing event handlers. You can add many different types of event handlers to one element. You can also add many event handlers of the same type to one element, i.e two “click” events.The addEventListener() method makes it easier to control how the event reacts to bubbling.
// function only takes event object as the input or nothingfunction myFunction(ev) &#123;  alert (&quot;Hello World!&quot;);&#125;element.addEventListener(&quot;click&quot;, myFunction);element.addEventListener(&quot;click&quot;, function(ev)&#123; alert(&quot;Hello World!&quot;); &#125;); element.addEventListener(&quot;click&quot;, function()&#123; alert(&quot;Hello World!&quot;); &#125;); 

&lt;!--  pass H1 object to change Text or nothing--&gt;&lt;h1 onclick=&quot;changeText(this)&quot;&gt;Click on this text!&lt;/h1&gt;&lt;h1 onclick=&quot;changeText()&quot;&gt;Click on this text!&lt;/h1&gt;

NodeList vs HTMLCollectionA NodeList object is a list (collection) of nodes extracted from a document, while HTMLCollection is only for HTML element(tag), NodeList is for inner text as well which is also a node object.
HTMLCollection returned

getElementsByTagName()
getElementsByClassName()

NodeList returned

All browsers return a NodeList object for the property childNodes. 
Most browsers return a NodeList object for the method querySelectorAll()

Same

Both an HTMLCollection object and a NodeList object is an array-like list (collection) of objects.
Both have a length property defining the number of items in the list (collection).
Both provide an index (0, 1, 2, 3, 4, …) to access each item like an array.

Different

HTMLCollection items can be accessed by their name, id, or index number.
NodeList items can only be accessed by their index number.
Only the NodeList object can contain attribute nodes and text nodes.

BOMThe Browser Object Model (BOM) allows JavaScript to “talk to” the browser, it’s information about browser, it has several objects to represent such info

window
window.screen or screen for short
window.location
window.history
window.navigator

JQueryjQuery was designed to handle Browser Incompatibilities and to simplify HTML DOM Manipulation, Event Handling, Animations, and Ajax, it depends on Dom native API above.
JQuery uses CSS selector to find element
//import jquery first.var myElement = $(&quot;#id01&quot;);var all_p = $(&quot;p&quot;)var class_elements = $(&quot;.intro&quot;)

Operation(API) on the selected element

.text   get&#x2F;set the content of selected element $(“#bsd”).text(“new”);
.html   get&#x2F;set the entire html of selected element $(“#bsd”).html()
.before add a html element(with tag) before the selected element
.after  add an element after the selected element
.append add a html element(with tag) inside the selected element
.attr   get&#x2F;set the given attr of an html element $(“#bsd”).attr(“given_attr”) $(“$bsd”).attr(“given_attr”, “given_value”);
.css    get&#x2F;set the style of the selected elements
.parent get the parent element of the selected element $(“#bsd”).parent().
.each   callback on each of the selected groups($(“a”), $(“.className”);AJAX.ajaxSetup()set defaults for AJAx-related parameters such as caching, methods, error handling, among others, this is used for all AJAX call, also you can set them separately by .ajax()

All subsequent Ajax calls using any function will use the new settings, unless overridden by the individual calls, until the next invocation of $.ajaxSetup().
.ajax() parameters are map with key:value, specify method(GET,POST etc) the url, callback functions when error, successful, data type and data cache response etc!!
load()        get()       post         getJson()  +             |            |              +  |             |            |              |  |             |            |              |  +-------------+------+-----+--------------+                       |                       +                     ajax()   low level API
function successFun(data, status) &#123;&#125;function errorFun(request, status, errorthrown)  &#123;&#125;$.ajax(&#123;    type: &quot;GET&quot;,    url: &quot;json.php&quot;,    dataType: &quot;json&quot;, //The type of data that you&#x27;re expecting back from the server.    username: &quot;luo&quot;,    password: &quot;ps&quot;,    //change headers before send    beforeSend: function( xhr ) &#123;        xhr.overrideMimeType( &quot;text/plain; charset=x-user-defined&quot; );    &#125;    success: successFun,    error: errorFun,    timeout: 5000  //5s);$.ajax(&#123;    type: &quot;POST&quot;,    url: &quot;url/ds&quot;,    contentType: &quot;application/json&quot;,  //the type of data you send to the server    data: &#x27;&#123;&quot;a&quot;:1&#125;&#x27;, //json format as a string    success: successFun,    error: errorFun,    timeout: 5000  //5s);
Ref
W3C DOM
JQuery cheatsheet
Dom Event Interface
W3C ALL Events

]]></content>
      <categories>
        <category>html</category>
        <category>dom</category>
      </categories>
      <tags>
        <tag>html</tag>
        <tag>dom</tag>
        <tag>event</tag>
      </tags>
  </entry>
  <entry>
    <title>html-faq</title>
    <url>/2020/07/17/html-faq/</url>
    <content><![CDATA[FAQHtml attribute vs CSS propertyHTML attribute defines the information about html, Tags are different, some are for img, some for text,so the attributes are grouped into two parts, one is global attributes, the other is tag related,you can set global attributes for any tag, while tag related is only valid for that kind of tag, let’s say you can’t set href attribute for a h1, browser thinks it’s invalid.
CSS property is different thing, it’s defined by CSS, every html element can set any CSS property for itself, CSS property is inside style attribute of CSS tag for inline style, see below example
CSS property’s value if not set could be

default value
inherited from its parent

Tag attribute value is empty if not set.
&lt;!--style is tag attribute while color, text-algin are css properties--&gt;&lt;h1 class=&#x27;h1&#x27; style=&quot;color:blue;text-align:center&quot;&gt;This is a header&lt;/h1&gt;
custom tagThere are two ways to create a custom tag, one is by js, the other is in html directly
&lt;style&gt;s_tag[test_attr]&#123;  color: green;  display: block;&#125;&lt;/style&gt;&lt;div style=&quot;color:red&quot;&gt;  &lt;s_tag style=&quot;color: blue&quot;&gt;hi1&lt;/s_tag&gt;  &lt;s_tag&gt;hi2&lt;/s_tag&gt;  &lt;s_tag test_attr=&quot;cool&quot;&gt;h3&lt;/s_tag&gt;&lt;/div&gt;
How browser renders a custom tagBrowser first checks the global attributes as every element can set it and browser knows how to handle such attributes, for other custom attributes, browser does NOT know how to use it, it’s totally used by user, then browser checks the CSS style for the custom element, property value if not set use default value or inherited from its parent.
Note: custom tag with display:inline by default, it’s inline element
custom attribute to built-in tagThe data-* attributes is used to store custom data private to the page or application, the data-* attributes gives us the ability to embed custom data attributes on all HTML elements.
The stored (custom) data can then be used in the page’s JavaScript to create a more engaging user experience (without any Ajax calls or server-side database queries).The data- attributes consist of two parts:*

The attribute name should not contain any uppercase letters, and must be at least one character long after the prefix &quot;data-&quot;
The attribute value can be any string

Note: Custom attributes prefixed with &quot;data-&quot; will be completely ignored by the user agent.
object vs embed vs video&#x2F;audioThe HTML &lt;video&gt; element is used to show a video on a web page by browser built-in player, it’s new feature by HTML5, same thing for &lt;audio&gt; as well.
But &lt;object&gt; and &lt;embed&gt; calls plugins to display them,  supported long time, hence for a video, we should write in this way
 &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;  &lt;source src=&quot;movie.mp4&quot; type=&quot;video/mp4&quot;&gt;  &lt;source src=&quot;movie.ogg&quot; type=&quot;video/ogg&quot;&gt;  &lt;!--fallback to old way--&gt;  &lt;embed type=&quot;video/webm&quot; src=&quot;movie.mp4&quot;&gt;Your browser does not support the video tag.&lt;/video&gt;

select vs datalistThe &lt;select&gt; element defines a drop-down list, used for selecting pre-defined data
&lt;form action=&quot;/action_page.php&quot;&gt;  &lt;label for=&quot;cars&quot;&gt;Choose a car:&lt;/label&gt;  &lt;!-- data list for select element --&gt;  &lt;select id=&quot;cars&quot; name=&quot;cars&quot;&gt;    &lt;option value=&quot;volvo&quot;&gt;Volvo&lt;/option&gt;    &lt;option value=&quot;saab&quot;&gt;Saab&lt;/option&gt;    &lt;option value=&quot;fiat&quot;&gt;Fiat&lt;/option&gt;    &lt;option value=&quot;audi&quot;&gt;Audi&lt;/option&gt;  &lt;/select&gt;  &lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;
The &lt;datalist&gt; element specifies a list of pre-defined options for an &lt;input&gt; element, used for show search history
&lt;form action=&quot;/action_page.php&quot;&gt;  &lt;!--data list for input element with list attribute--&gt;  &lt;input list=&quot;browsers&quot; name=&quot;browser&quot;&gt;  &lt;datalist id=&quot;browsers&quot;&gt;    &lt;option value=&quot;Internet Explorer&quot;&gt;    &lt;option value=&quot;Firefox&quot;&gt;    &lt;option value=&quot;Chrome&quot;&gt;    &lt;option value=&quot;Opera&quot;&gt;    &lt;option value=&quot;Safari&quot;&gt;  &lt;/datalist&gt;  &lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;

AJAX vs SSE vs WebSocketAJAXAn AJAX request is built around the typical HTTP model(HTTP request&#x2F;response Restful API). A request is made by a client and a response is generated by a server, it’s client driven, Each request creates a new HTTP request in the background
Data is transmitted o each HTTP connection based on HTTP protocol.
WebSocketWebSockets are based around an event model. The client and server can emit events and send data to each other whenever they need to, Bi-direction communication, lets say there are 3 users working on a project together; user A, B, and C. User A can update the project by emitting an event to the server. The server can in turn emit an event to user B, and C, informing them that a change has happened and at the same time sending them data about the change. Their browser can then use that data to update their web page.
WebSockets have to establish a connection to a server for data to flow. WebSockets only establish this connection once, then all data is sent over this open WS protocol connection. This means each event being send takes very little resources from both the server and the client because a new connection never has to be established, and ws protocol is small.
It uses HTTP protocol to setup the connection, then switch to WS protocol for data transmission. hence it’s faster.
Difference between Ajax and WebSocket

Web sockets are used to define full duplex communication between different servers and clients. It acts as a means of communication between both these sources and makes an exchange of data possible between them. Web sockets focus upon true concurrency and optimization of performance.

AJAX is an abbreviation for Asynchronous JavaScript and XML. Ajax can be considered as a technology which can be used to create better faster and more interactive applications using XML, HTML, CSS, and JavaScript. Ajax makes use of XHTML, CSS, Document Object Model and JavaScript for dynamic content display.

The distinguishing features of web sockets are as below:

The protocol that is being used by web sockets is standardized which enables real-time communication between the different clients and servers.
Web sockets help to transform the cross-platform standard for real-time communication between client and server.
As there is a pre-defined standard it enables to create new kind of applications. Businesses for real-time web applications can be created speedily. The biggest advantage is that it provides a two-way communication between client and server over a single TCP connection


Ajax Features

Ajax uses different technologies to create the best dynamic pages. It uses XHTML for content, CSS for making presentations look great, document object model and JavaScript for making pages dynamic.
With Ajax, once you submit a form JavaScript makes a request to the server, finds the result and updates the screen. It is never known to the user that there was information transmitted to the server but there actually is information being transmitted.
It also uses XML to format data from receiving server.
It can also be called a web browser which is independent of the web server technology being used. Also, a user can work continuously when a client program is requesting information from the server in the background.


When it comes to Web Socket events there are mainly four events. They are:

Open: acts as a handshake between client and server
Message: happens when the server sends some data. Messages can be plain text messages or binary data
Close: This marks the end of communication between server and client.
Error: When an error occurs, when a communication channel is opened then the error event occurs


Ajax also supports events and actions. The steps that happen when an event occurs are as below:

A XMLHttpRequest object is created.
This object is then configured.
The object then makes an asynchronous request to the webserver.
The web server returns results which contain the XML document.
The object calls the callback() function and processes the result.
Once all this is done the HTML DOM is updated.


AJax is more secure than Websocket.


So if you need bi-direction, low latency, real-time, use WebSocket, otherwise Ajax, but Ajax is driven by client, so server wants to notify update to client like upgrade notification, stock price etc, Ajax can’t do, hence SSE comes into play.
SSEServer-Sent Events (SSE) allow a web page to get updates from a server. One-way MessagingA server-sent event is when a web page automatically gets updates from a server.
This was also possible before, but the web page would have to ask if any updates were available(ajax poll). With server-sent events, the updates come automatically.
Suggestion: Traditional Way, AJAX + SSE, if need low latency, real-time, WebSocket which is new.
how dom event worksThe event object is created when the event first happens; it travels with the event on its journey through the DOM. We can use this object to access a wealth of information about the event that has occurred:

type (string) This is the name of the event.
target (node) This is the DOM node where the event originated.
currentTarget (node) This is the DOM node that the event callback is currently firing on.
bubbles (boolean) This indicates whether this is a “bubbling” event.
preventDefault (function) This prevents any default behavior from occurring that the user agent (i.e. browser) might carry out in relation to the event (for example, preventing a click event on an &lt;a&gt; element from loading a new page).
stopPropagation (function) This prevents any callbacks from being fired on any nodes further along the event chain, but it does not prevent any additional callbacks of the same event name from being fired on the current node.
stopImmediatePropagation (function) This prevents any callbacks from being fired on any nodes further along the event chain, including any additional callbacks of the same event name on the current node.
cancelable (boolean) This indicates whether the default behavior of this event can be prevented by calling the event.preventDefault method.
defaultPrevented (boolean) This states whether the preventDefault method has been called on the event object.
isTrusted (boolean) An event is said to be “trusted” when it originates from the device itself, not synthesized from within JavaScript.
eventPhase (number) This number represents the phase that the event is currently in: none (0), capture (1), target (2) or bubbling (3). We’ll go over event phases next.
timestamp (number) This is the date on which the event occurred.

Event Phasethe event flows from the document’s root to the target (i.e. capture phase), then fires on the event target (target phase), then flows back to the document’s root (bubbling phase)

Capture PhaseThe first phase is the capture phase. The event starts its journey at the root of the document, working its way down through each layer of the DOM, firing on each node until it reaches the event target. The job of the capture phase is to build the propagation path, which the event will travel back through in the bubbling phase.
Target PhaseAn event reaching the target is known as the target phase. The event fires on the target node, before reversing and retracing its steps, propagating back to the outermost document level.
In the case of nested elements, mouse and pointer events are always targeted at the most deeply nested element. If you have listened for a click event on a &lt;div&gt; element, and the user actually clicks on a &lt;p&gt; element in the div, then the &lt;p&gt; element will become the event target.
Bubbling PhaseAfter an event has fired on the target, it doesn’t stop there. It bubbles up (or propagates) through the DOM until it reaches the document’s root. This means that the same event is fired on the target’s parent node, followed by the parent’s parent, continuing until there is no parent to pass the event onto.
Note: Most of callback are at Bubbling Phase
why $ is not defined in your scriptMake sure load your script after jquery.
After move a button(negative margin), it can NOT be clickedThis is because after move, button and other tag overlaps, while other tag at the top, in order to make it clickable, change its z-index to high value
.button &#123;    z-index: 100;&#125;

Ref
Inside Dom event

]]></content>
      <categories>
        <category>html</category>
      </categories>
      <tags>
        <tag>html</tag>
      </tags>
  </entry>
  <entry>
    <title>html_sse</title>
    <url>/2020/08/19/html-sse/</url>
    <content><![CDATA[OverviewTraditionally, client sends a request to the server to retrieve data; that is, client requests data from the server. With server-sent events(SSE), it’s possible for a server to send new data to client at any time, but only server can send message, one way communication.
Client with http GET method to setup a connection(keep it), server accepts it and keeps the connection as well, later on server sends event to client by this connection, when server sends event, the event is as TCP payload, not http, no http at all for event sending.

SSE is useful for stock price update, message notification. but new way is to use websocket for server event
The message(must be UTF-8 encoded when sending by server) has below fields, each takes one line when sending. 

event(optional)  A string identifying the type of event described. If this is specified, an event will be dispatched on the browser to the listener for the specified event name; the website source code should use addEventListener() to listen for named events. The onmessage handler is called if no event name is specified for a message.
data(optional), can be text or json format. can be multiple such field  The data field for the message. When the EventSource receives multiple consecutive lines that begin with data:, it concatenates them, inserting a newline character between each one. Trailing newlines are removed.
id  The event ID to set the EventSource object’s last event ID value.

but on client side, you got the parsed data, an object with below fields mapping to these fields, the name may be different
Client Side// open a connection and keep it, if disconnect, browser will reconnect automatically.const evtSource = new EventSource(&quot;/events&quot;);//only handle event:&quot;ping&quot;, data is always a string(text, json format)evtSource.addEventListener(&quot;ping&quot;, function(event) &#123;  const newElement = document.createElement(&quot;li&quot;);  const time = JSON.parse(event.data).time;  newElement.innerHTML = &quot;ping at &quot; + time;  eventList.appendChild(newElement);&#125;);// handle message without typeevtSource.onmessage = function(event) &#123;  const newElement = document.createElement(&quot;li&quot;);  const eventList = document.getElementById(&quot;list&quot;);  newElement.innerHTML = &quot;message: &quot; + event.data;  eventList.appendChild(newElement);&#125;evtSource.onerror = function(err) &#123;  // such as a network timeout or issues pertaining to access control  console.error(&quot;EventSource failed:&quot;, err);&#125;;
Server Side(JS)Events sent by server
data: some text

data: another messagedata: with two lines 

type: userconnectdata: &#123;&quot;username&quot;: &quot;bobby&quot;, &quot;time&quot;: &quot;02:33:48&quot;&#125;

// $yarn add express-ssesse.jsvar express = require(&#x27;express&#x27;);var router = express.Router();var SSE = require(&#x27;express-sse&#x27;);var sse = new SSE();router.get(&#x27;/events&#x27;, sse.init);//you can see sse in any other filemodule.exports = router;

var sse = require(&#x27;./sse&#x27;).sse;let count = 0;setInterval(() =&gt; &#123;  // sse.send(content)  // sse.send(content, eventName);  // sse.send(content, eventName, customID);  //send message to all connections!!!!  //if no sse connection, it does not send at all  sse.send(&#x27;jason&#x27;);  sse.send(&#x27;jason&#x27;, &#x27;name&#x27;);  sse.send(100);  sse.send(&#123;name: &quot;jason&quot;&#125;, &#x27;name&#x27;);&#125;, 10000);]]></content>
      <categories>
        <category>html</category>
        <category>sse</category>
      </categories>
      <tags>
        <tag>html</tag>
        <tag>sse</tag>
      </tags>
  </entry>
  <entry>
    <title>html-form</title>
    <url>/2020/08/18/html-form/</url>
    <content><![CDATA[OverviewAn HTML form is used to collect user input. The user input is most often sent to a server for processing.


FormForm always has a &lt;form&gt; element and several input elements and submit button inside this form, when user clicks the submit button, user input is sent to server,  but Form fields(input, select etc)do not necessarily have to appear in a form tag. You can put them anywhere in a page. Such form-less fields cannot be submitted (only a form as a whole can).
how data is sent depends on form attributes. there are several form attributes

action:  specify where to send the form-data when submitted.

method:  specify how to send form-data (the form-data is sent to the page specified in the action attribute).The form-data can be sent as URL variables (with method=&quot;get&quot;) or as HTTP post transaction(body) (with method&#x3D;”post”).
  Get Method Note

Appends form-data into the URL in name/value pairs
The length of a URL is limited (about 3000 characters)
Never use GET to send sensitive data! (will be visible in the URL)
Useful for form submissions where a user wants to bookmark the result
GET is better for non-secure data, like query strings in Google

  Post Method Note

Appends form-data inside the body of the HTTP request (data is not shown in URL)
Has no size limitations
Form submissions with POST cannot be bookmarked


enctype: specifies how the form-data should be encoded when submitting it to the server, can  be used only if method&#x3D;”post”. as for ‘get’ it’s encoded with application&#x2F;x-www-form-urlencoded

application&#x2F;x-www-form-urlencoded: Default. All characters are encoded before sent (spaces are converted to “+” symbols, and special characters are converted to ASCII HEX values)
multipart&#x2F;form-data: No characters are encoded. This value is required when you are using forms that have a file upload control
text&#x2F;plain: Spaces are converted to &quot;+&quot; symbols, but no special characters are encoded


target: Specifies where to display the response that is received after submitting the form. by default a new page


SubmitAfter the button is clicked, the ‘submit’ event is triggers, the default submit handler collects all input element with its name and value, encodes them and sends them to server, after gets the response from the server, it opens a new page with response display in that page, this is the default behavior, if you do not like this way, you want to handle the response by yourself, you can overwrite the submit handler, in your own handlers, send the form data by ajax by yourself, something like this.
//with Jquery helperfunction postcode(event) &#123;  //prevent default  event.preventDefault();  /*    get attr from form    get all inputs name and value by    serialize() URL-encoded(jquery object API) or serializeArray() json format    this here is Dom element, not Jquery object!!!  */  //convert to jquery object  var form = $(this);  $.post(form.attr(&#x27;action&#x27;), form.serialize(),    function (res, textStatus, jqXHR) &#123;        //handler response here    &#125;);&#125;//get the form by ID, reset submit event$(&#x27;#code-form&#x27;).submit(postcode);

File UploadIf form has file input, method must set with post and encode with multipart/form-data
&lt;form method=&quot;POST&quot; action=&quot;/upload-profile-pic&quot; enctype=&quot;multipart/form-data&quot;&gt;  &lt;div&gt;    &lt;label&gt;Select your profile picture:&lt;/label&gt;    &lt;input type=&quot;file&quot; name=&quot;profile_pic&quot;&gt;  &lt;/div&gt;  &lt;div&gt;    &lt;input type=&quot;submit&quot; name=&quot;btn_upload_profile_pic&quot; value=&quot;Upload&quot;&gt;  &lt;/div&gt;&lt;/form&gt;

Uppy.jsUse Uppy.js which is more powerful than form way.
&lt;div id=&quot;drag-drop-area&quot;&gt;&lt;/div&gt;
var uppy = Uppy.Core(&#123;  //lots of option you can set, refer to official site  id: &quot;uppy&quot;,  autoProceed: false,  //upload more files  allowMultipleUploads: true,  debug: false,  restrictions: &#123;    // file limition    maxFileSize: 5000000,    minFileSize: null,    maxNumberOfFiles: 5,    minNumberOfFiles: null,    allowedFileTypes: [&quot;image/*&quot;]  &#125;&#125;);//Use Dashboard.uppy.use(Uppy.Dashboard, &#123;  inline: true,  height: 570,  target: &quot;#drag-drop-area&quot;&#125;);//use multipart/form-data encoding  uppy.use(Uppy.XHRUpload, &#123;    endpoint: &#x27;/upload-profile-pic&#x27;,    method: &#x27;POST&#x27;,    // set the name field that could be used by server    // like &lt;input name=&#x27;profile_pic&#x27; /&gt;    formData: true,    fieldName: &#x27;profile_pic&#x27;&#125;);uppy.on(&quot;complete&quot;, (result) =&gt; &#123;  console.log(    &quot;Upload complete! We’ve uploaded these files:&quot;,    result.successful  );&#125;);]]></content>
      <categories>
        <category>html</category>
        <category>tag</category>
      </categories>
      <tags>
        <tag>form</tag>
      </tags>
  </entry>
  <entry>
    <title>html-websocket</title>
    <url>/2021/09/15/html-websocket/</url>
    <content><![CDATA[IntroductionWebSocket is a protocol providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C.
WebSocket is designed to be implemented in web browsers and web servers, but it can be used by any client or server application. The WebSocket Protocol is an independent TCP-based protocol. Its only relationship to HTTP is that its handshake is interpreted by HTTP servers as an Upgrade request. The WebSocket protocol makes more interaction between a browser and a web server possible, facilitating the real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the browser without being solicited by the client, and allowing for messages to be passed back and forth while keeping the connection open. In this way, a two-way (bi-directional) ongoing conversation can take place between a browser and the server. The communications are done over TCP port number 80, which is of benefit for those environments which block non-web Internet connections using a firewall. Similar two-way browser-server communications have been achieved in non-standardized ways using stopgap technologies such as Comet.


The WebSocket protocol specification defines ws and wss as two new uniform resource identifier (URI) schemes that are used for unencrypted and encrypted connections, respectively. Apart from the scheme name and fragment (# is not supported), the rest of the URI components are defined to use URI generic syntax.
WebSockets have to establish a connection to a server for data to flow. WebSockets only establish this connection once by HTTP Method, then all data is sent over this open WS protocol connection(ws as tcp payload). This means each event being sent takes very little resources from both the server and the client because a new connection never has to be established, and ws protocol is small.
It uses HTTP protocol to setup the connection, then switch to WS protocol for data transmission. actually when lower TCP connection is setup, client sends HTTP method to server, this is handled by websocket, hence the TCP connection is not closed, now client and server can send ws protocl for data transmission)(TCP+ws+payload)
So it’s very useful for server driver event, like to notify update to client(upgrade notification, stock price update etc)
Websocket eventWhen it comes to Web Socket events there are mainly four events. They are:

Open: acts as a handshake between client and server
Message: happens when the server sends some data. Messages can be plain text messages or binary data
Close: This marks the end of communication between server and client.
Error: When an error occurs, when a communication channel is opened then the error event occurs

Connectino setupclient request
GET / HTTP/1.1Host: 192.168.43.135:12345Connection: UpgradePragma: no-cacheCache-Control: no-cacheUpgrade: websocketOrigin: file://Sec-WebSocket-Version: 13User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36Accept-Encoding: gzip, deflate, sdchAccept-Language: zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4Sec-WebSocket-Key: bKdPyn3u98cTfZJSh4TNeQ==Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits 

server response
HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: 4EaeSCkuOGBy+rjOSJSMV+VMoC0=WebSocket-Origin: file://WebSocket-Location: ws://192.168.43.135:12345/ 

Data transmission  
websocket protocol is very short, its header only takes 8 bytes, two import ones

Opcode: playload type(text, binary)
Payload length: len of payload

server to client  
Frame 7: 79 bytes on wire (632 bits), 79 bytes captured (632 bits)Ethernet II, Src: Vmware_8a:3d:a7 (00:0c:29:8a:3d:a7), Dst: Vmware_c0:00:08 (00:50:56:c0:00:08)Internet Protocol Version 4, Src: 192.168.43.135, Dst: 192.168.43.1Transmission Control Protocol, Src Port: 12345, Dst Port: 50999, Seq: 205, Ack: 510, Len: 25WebSocket    1... .... = Fin: True    .000 .... = Reserved: 0x0    .... 0001 = Opcode: Text (1)    0... .... = Mask: False    .001 0111 = Payload length: 23    PayloadJavaScript Object NotationLine-based text data    Welcome, 192.168.43.1 ! 

client to server  
Frame 9: 72 bytes on wire (576 bits), 72 bytes captured (576 bits)Ethernet II, Src: Vmware_c0:00:08 (00:50:56:c0:00:08), Dst: Vmware_8a:3d:a7 (00:0c:29:8a:3d:a7)Internet Protocol Version 4, Src: 192.168.43.1, Dst: 192.168.43.135Transmission Control Protocol, Src Port: 50999, Dst Port: 12345, Seq: 510, Ack: 230, Len: 18WebSocket    1... .... = Fin: True    .000 .... = Reserved: 0x0    .... 0001 = Opcode: Text (1)    1... .... = Mask: True    .000 1100 = Payload length: 12    Masking-Key: e17e8eb9    Masked payload    PayloadJavaScript Object NotationLine-based text data    test message 

Examplesclient side in browser you can use other web socket client without browser
var ws = new WebSocket(&quot;wss://echo.websocket.org&quot;);ws.onopen = function(evt) &#123;   console.log(&quot;Connection open ...&quot;);   ws.send(&quot;Hello WebSockets!&quot;);&#125;;ws.onmessage = function(evt) &#123;  console.log( &quot;Received Message: &quot; + evt.data);&#125;;// blob(binary)ws.binaryType = &quot;blob&quot;;ws.onmessage = function(e) &#123;  console.log(e.data.size);&#125;;ws.send(&#x27;your message&#x27;);// ArrayBufferws.binaryType = &quot;arraybuffer&quot;;ws.onmessage = function(e) &#123;  console.log(e.data.byteLength);&#125;;ws.onclose = function(evt) &#123;  console.log(&quot;Connection closed.&quot;);&#125;;    

server side
// use websocket library to setup a serverimport &#123; WebSocketServer &#125; from &#x27;ws&#x27;;// when client const wss = new WebSocketServer(&#123; port: 8080 &#125;);wss.on(&#x27;connection&#x27;, function connection(ws) &#123;  ws.on(&#x27;message&#x27;, function incoming(message) &#123;    console.log(&#x27;received: %s&#x27;, message);  &#125;);  ws.send(&#x27;something&#x27;);&#125;);

websocket library
websocket packet wireshark

]]></content>
      <categories>
        <category>html</category>
        <category>websocket</category>
      </categories>
      <tags>
        <tag>html</tag>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title>http-apache</title>
    <url>/2020/02/11/http-apache/</url>
    <content><![CDATA[IntroductionmodulesApache provides lots of   to user, these modules provide lots of directives you can use in conf file through these, user can modify the behavior of apache like modify request, response header etc.
check enabled modules
$ ls /etc/apache2/mods-enabled/access_compat.load  authn_core.load  authz_user.load  deflate.load  filter.load       mpm_prefork.load  php5.load           ssl.conf...

check available modules
$ ls /etc/apache2/mods-available/access_compat.load  authz_core.load       charset_lite.load  file_cache.load           macro.load        proxy_ajp.load       remoteip.load          speling.load...

enable a mod
# just add a symbol link in /etc/apache2/mods-enabled/ to  /etc/apache2/mods-available/$ ln -s /etc/apache2/mods-available/macro.load /etc/apache2/mods-enabled/

create web server by pythonIn this case, you can add any header and any body to client, fully controlled by your self.
define your own simple server
#! /usr/bin/env python3.5from http.server import HTTPServer, BaseHTTPRequestHandlerclass SimpleHTTPRequestHandler(BaseHTTPRequestHandler):  def do_GET(self):    # only add status line, server and date headers    self.send_response(200)    # add response header by yourself    #self.send_header(&quot;Content-Length&quot;, &quot;0&quot;)    # flush the header buffer    self.end_headers()    self.wfile.write(b&#x27;Hello, world!\n&#x27;)httpd = HTTPServer((&#x27;localhost&#x27;, 8000), SimpleHTTPRequestHandler)httpd.serve_forever()

use built-in simple server
from http.server import SimpleHTTPRequestHandlerfrom socketserver import TCPServerclass GetHandler(SimpleHTTPRequestHandler):    def do_GET(self):        # static web server, rewrite like above for dynamic        SimpleHTTPRequestHandler.do_GET(self)httpd = TCPServer((&quot;&quot;, 8000), GetHandler)httpd.serve_forever()
FAQenable keep-aliveedit &#x2F;etc&#x2F;apache2&#x2F;apache2.conf with below
## KeepAlive: Whether or not to allow persistent connections (more than# one request per connection). Set to &quot;Off&quot; to deactivate.#KeepAlive On# MaxKeepAliveRequests: The maximum number of requests to allow# during a persistent connection. Set to 0 to allow an unlimited amount.# We recommend you leave this number high, for maximum performance.#MaxKeepAliveRequests 100## KeepAliveTimeout: Number of seconds to wait for the next request from the# same client on the same connection. when timeout, apache will close the connection#KeepAliveTimeout 500

return application&#x2F;json for xx.json fileAddType &#x27;application/json; charset=UTF-8&#x27; .json

modify response headermodify header feature is provided by mod_headers.so, first you should enable such module by ln -s /etc/apache2/mods-available/headers.load /etc/apache2/mods-enabled/
# apache.confHeader set Host &quot;www.test.com&quot;;

Remember to restart service service apache2 restart
more details about 
]]></content>
      <categories>
        <category>http</category>
        <category>http server</category>
      </categories>
      <tags>
        <tag>apache</tag>
        <tag>http server</tag>
      </tags>
  </entry>
  <entry>
    <title>html_jquery</title>
    <url>/2020/07/20/html-jquery/</url>
    <content><![CDATA[JQueryjQuery takes a lot of common tasks that require many lines of JavaScript code to accomplish, and wraps them into methods that you can call with a single line of code.
jQuery greatly simplifies JavaScript programming but run it make sure

load your script after jquery
runs it when document is ready

The jQuery library contains the following features:

HTML&#x2F;DOM manipulation
CSS manipulation
HTML event methods
Effects and animations
AJAX
Utilities

use it on your websiteInclude jQuery from a CDN, like Google
&lt;head&gt;&lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;

$(document).ready(function()&#123;    //$(&quot;p&quot;) select all &lt;p&gt; elements    //set its click callback!    /* if you use raw method, you need to     * get all elements by document.getElementByTagName(&quot;p&quot;)     * then iterate each element and set it e.click=     * jquery method can add event for one or an array based on selector     */    $(&quot;p&quot;).click(function()&#123;        // this set display: none        $(this).hide();    &#125;)&#125;)

syntaxThe jQuery syntax is tailor-made for selecting HTML elements and performing some action on the element(s).
Basic syntax is: $(selector).action()

$ sign to define&#x2F;access jQuery
(selector) to “query (or find)” HTML elements
jQuery action() to be performed on the element(s)

Examples:
//The optional speed parameter specifies the speed of the hiding/showing, and can take the following values: &quot;slow&quot;, &quot;fast&quot;, or milliseconds$(this).hide(1000)  //hides the current element.$(&quot;p&quot;).hide(&quot;slow&quot;) //hides all &lt;p&gt; elements slowly$(&quot;.test&quot;).hide()   //hides all elements with class=&quot;test&quot;.$(&quot;#test&quot;).hide()   //hides the element with id=&quot;test&quot;.

selectorIt’s based on the existing CSS Selectors, and in addition, it has some own custom selectors css selector
change effectshide&#x2F;show vs fadeIn&#x2F;fadeOut slideDown&#x2F;slideUp are used to display or hide element with different effects.

Fadeout is a function that will change the opacity of an element to 0, time can be a parameter of the function (but also has a default value if unspecified), opposite of fadeIn() that will change opacity to 1.   
with fdeOut() you see the element from clear to unclear slowly, at last not see it(opacity changes)


Hide function will just set display:none to the element, opposite to show() that will just set display:block.
With hide(), you see the element from clear to disappear.



$(&quot;button&quot;).click(function()&#123;  $(&quot;p&quot;).toggle();  //$(&quot;p&quot;).toggle(1000); speed&#125;);

With jQuery you can fade an element in and out of visibility.jQuery has the following fade methods:

fadeIn()
fadeOut()
fadeToggle()
fadeTo()

The jQuery fadeTo() method allows fading to a given opacity (value between 0 and 1)
$(&quot;button&quot;).click(function()&#123;  $(&quot;#div1&quot;).fadeToggle();  $(&quot;#div2&quot;).fadeToggle(&quot;slow&quot;);  $(&quot;#div3&quot;).fadeToggle(3000);&#125;);

With jQuery you can create a sliding effect on elements.jQuery has the following slide methods:

slideDown()
slideUp()
slideToggle(

$(&quot;#flip&quot;).click(function()&#123;  $(&quot;#panel&quot;).slideToggle();&#125;);

The jQuery animate() method is used to create custom animationsOnly properties containing numeric values!Syntax:
$(selector).animate(&#123;params&#125;,speed,callback);

The required params parameter defines the CSS properties to be animated
//The following example demonstrates a simple use of the animate() method; it moves a &lt;div&gt; element to the right, until it has reached a left property of 250px:$(&quot;button&quot;).click(function()&#123;  $(&quot;div&quot;).animate(&#123;left: &#x27;250px&#x27;&#125;);&#125;);//param defined the result should be from the orginal

$(&quot;button&quot;).click(function()&#123;  $(&quot;div&quot;).animate(&#123;    left: &#x27;250px&#x27;,    opacity: &#x27;0.5&#x27;,    height: &#x27;150px&#x27;,    width: &#x27;150px&#x27;  &#125;);&#125;);$(&quot;button&quot;).click(function()&#123;  $(&quot;div&quot;).animate(&#123;    left: &#x27;250px&#x27;,    height: &#x27;+=150px&#x27;,    width: &#x27;+=150px&#x27;  &#125;);&#125;);$(&quot;button&quot;).click(function()&#123;  var div = $(&quot;div&quot;);  div.animate(&#123;left: &#x27;100px&#x27;&#125;, &quot;slow&quot;);  div.animate(&#123;fontSize: &#x27;3em&#x27;&#125;, &quot;slow&quot;);&#125;);

The jQuery stop() method is used to stop an animation or effect(slide) before it is finished.
with all aboveJavaScript statements are executed line by line. However, with effects, the next line of code can be run even though the effect is not finished. This can create errors.
To prevent this, you can create a callback function.
A callback function is executed after the current effect is finished.
Typical syntax: $(selector).hide(speed,callback);
$(&quot;button&quot;).click(function()&#123;  $(&quot;p&quot;).hide(&quot;slow&quot;, function()&#123;    alert(&quot;The paragraph is now hidden&quot;);  &#125;);&#125;);

action ChainThere is a technique called chaining, that allows us to run multiple jQuery commands, one after the other, on the same element(s).
Tip: This way, browsers do not have to find the same element(s) more than once.
$(&quot;#p1&quot;).css(&quot;color&quot;, &quot;red&quot;).slideUp(2000).slideDown(2000);

elementThree simple, but useful, jQuery methods for DOM manipulation are:

text() - Sets or returns the text content of selected elements
html() - Sets or returns the content of selected elements (including HTML markup)
val() - Sets or returns the value of form fields

change attribute//GET$(&quot;button&quot;).click(function()&#123;  alert($(&quot;#w3s&quot;).attr(&quot;href&quot;));&#125;);//SET$(&quot;button&quot;).click(function()&#123;  $(&quot;#w3s&quot;).attr(&quot;href&quot;,&quot;image/2.png&quot;);&#125;);//SET multiple attr$(&quot;button&quot;).click(function()&#123;  $(&quot;#w3s&quot;).attr(&#123;    &quot;href&quot; : &quot;https://www.w3schools.com/jquery/&quot;,    &quot;title&quot; : &quot;W3Schools jQuery Tutorial&quot;  &#125;);&#125;);

change styleQuery has several methods for CSS manipulation. We will look at the following methods:

addClass()    - Adds one or more classes to the selected elements
removeClass() - Removes one or more classes from the selected elements
toggleClass() - Toggles between adding&#x2F;removing classes from the selected elements
css()         - Sets or returns the style attribute

$(&quot;button&quot;).click(function()&#123;  $(&quot;#div1&quot;).addClass(&quot;row&quot;);&#125;);//GET$(&quot;p&quot;).css(&quot;background-color&quot;);//SET$(&quot;p&quot;).css(&quot;background-color&quot;, &quot;yellow&quot;);$(&quot;p&quot;).css(&#123;&quot;background-color&quot;: &quot;yellow&quot;, &quot;font-size&quot;: &quot;200%&quot;&#125;);//also set do this with css$(&quot;button&quot;).click(function()&#123;  $(&quot;#div1&quot;).width(500).height(500);&#125;);

add&#x2F;del&#x2F;replace elementWe will look at four jQuery methods that are used to add new content:

append() - Inserts content at the end of the selected elements(as end child)
prepend() - Inserts content at the beginning of the selected elements(as first child)
after() - Inserts content after the selected elements(as siblings)
before() - Inserts content before the selected elements(as siblings)
replaceWith() - replace selected element with new one(element)
html()- replace children of selected element with new child(element)
text()- the content will be seen only as text, text(‘&lt;h1&gt;hello&lt;&#x2F;h1&gt;’), h1 has no meaning!!!

Content here can be text or html element!
$(&quot;p&quot;).append(&quot;Some appended text.&quot;);function appendText() &#123;  var txt1 = &quot;&lt;p&gt;Text.&lt;/p&gt;&quot;;               // Create element with HTML  var txt2 = $(&quot;&lt;p&gt;&lt;/p&gt;&quot;).text(&quot;Text.&quot;);   // Create with jQuery  var txt3 = document.createElement(&quot;p&quot;);  // Create with DOM  txt3.innerHTML = &quot;Text.&quot;;  $(&quot;body&quot;).append(txt1, txt2, txt3);      // Append the new elements&#125;

Remove Elements&#x2F;Content

remove() - Removes the selected element (and its child elements)
empty() - Removes the child elements from the selected element

Filter the Elements to be Removed
$(&quot;p&quot;).remove(&quot;.test, .demo&quot;);

parent&#x2F;childThe parent() method returns the direct parent element of the selected element.The parents() method returns all ancestor elements of the selected element, all the way up to the document’s root element (&lt;html&gt;).
The children() method returns all direct children of the selected element.
$(document).ready(function()&#123;  $(&quot;div&quot;).children();&#125;);

Sibling

siblings()
next()
nextAll()
nextUntil()
prev()
prevAll()
prevUntil()

$(document).ready(function()&#123;  $(&quot;h2&quot;).siblings();&#125;);$(document).ready(function()&#123;  $(&quot;h2&quot;).siblings(&quot;p&quot;);&#125;);

The first(), last(), eq(), filter() and not() Methods
The most basic filtering methods are first(), last() and eq(), which allow you to select a specific element based on its position in a group of elements.
Other filtering methods, like filter() and not() allow you to select elements that match, or do not match, a certain criteria.
$(document).ready(function()&#123;  $(&quot;div&quot;).first();&#125;);//The eq() method returns an element with a specific index number of the selected elements.//The index numbers start at 0, so the first element will have the index number 0 and not 1. The following example selects the second &lt;p&gt; element (index number 1):$(document).ready(function()&#123;  $(&quot;p&quot;).eq(1);&#125;);//The filter() method lets you specify a criteria. Elements that do not match the criteria are removed from the selection, and those that match will be returned.//The following example returns all &lt;p&gt; elements with class name &quot;intro&quot;:$(document).ready(function()&#123;  $(&quot;p&quot;).filter(&quot;.intro&quot;);&#125;);

eventsJquery provides lots of methods to attach callback for an event like, but all of them are shorthand of $(‘selector’).on() which is low level API. the callback and the default callback runs in this order


jQuery click-handlers



onclick-handlers



native&#x2F;default behavior (calling the link, writing it to window.location)



but if you call e.preventDefault() in jquery handler, the native&#x2F;default behavior isn’t performed by the browser

$(‘selector’).click()
$(‘selector’).hover()
$(‘selector’).change()
$(‘selector’).focus()
$(‘selector’).keyup() etc
$(‘selector’).mousedown() etc
$(‘selector’).submit()

//Stupid way!!!//iterate each html element and add addEventListener$(&#x27;button&#x27;).each(function() &#123;    //this is html element, not jquery object    this.addEventListener(&#x27;click&#x27;, clickCallback);&#125;)// always use this way!!!$(&#x27;button&#x27;).click(clickCallback);

The hover() method takes two functions and is a combination of the mouseenter() and mouseleave() methods.
The first function is executed when the mouse enters the HTML element, and the second function is executed when the mouse leaves the HTML element:
$(&quot;#p1&quot;).hover(function()&#123;  alert(&quot;You entered p1!&quot;);&#125;,function()&#123;  alert(&quot;Bye! You now leave p1!&quot;);&#125;);

The blur() method attaches an event handler function to an HTML form field.
The function is executed when the form field loses focus:
$(&quot;input&quot;).blur(function()&#123;  $(this).css(&quot;background-color&quot;, &quot;#ffffff&quot;);&#125;);

AjaxjQuery provides several methods for AJAX functionality.
With the jQuery AJAX methods, you can request text, HTML, XML, or JSON from a remote server using both HTTP Get and HTTP Post - And you can load the external data directly into the selected HTML elements of your web page!
load$(selector).load(URL,data,callback);The required URL parameter specifies the URL you wish to load.The optional data parameter specifies a set of query string key&#x2F;value pairs to send along with the request.The optional callback parameter is the name of a function to be executed after the load() method is completed
&lt;h2&gt;jQuery and AJAX is FUN!!!&lt;/h2&gt;&lt;p id=&quot;p1&quot;&gt;This is some text in a paragraph.&lt;/p&gt;

$(&quot;#div1&quot;).load(&quot;demo_test.txt&quot;);$(&quot;button&quot;).click(function()&#123;  $(&quot;#div1&quot;).load(&quot;demo_test.txt&quot;, function(responseTxt, statusTxt, xhr)&#123;    if(statusTxt == &quot;success&quot;)      alert(&quot;External content loaded successfully!&quot;);    if(statusTxt == &quot;error&quot;)      alert(&quot;Error: &quot; + xhr.status + &quot;: &quot; + xhr.statusText);  &#125;);&#125;);

get$.get(URL,callback)
$(&quot;button&quot;).click(function()&#123;  $.get(&quot;demo_test.asp&quot;, function(data, status)&#123;    alert(&quot;Data: &quot; + data + &quot;\nStatus: &quot; + status);  &#125;);&#125;);

post$.post(URL,data,callback)The required URL parameter specifies the URL you wish to request.The optional data parameter specifies some data to send along with the request.The optional callback parameter is the name of a function to be executed if the request succeeds.The following example uses the $.post() method to send some data along with the request:
$(&quot;button&quot;).click(function()&#123;  $.post(&quot;demo_test_post.asp&quot;,  &#123;    name: &quot;Donald Duck&quot;,    city: &quot;Duckburg&quot;  &#125;,  function(data, status)&#123;    alert(&quot;Data: &quot; + data + &quot;\nStatus: &quot; + status);  &#125;);&#125;);

get jsonGet Json
$(&quot;button&quot;).click(function()&#123;  $.getJSON(&quot;demo_ajax_json.js&quot;, function(data,result)&#123;    $.each(result, function(i, field)&#123;      $(&quot;div&quot;).append(field + &quot; &quot;);    &#125;);    //Json.parse(data); No need this as getJSON    //already parsed the json!!!  &#125;);&#125;);

pluginIn addition, jQuery has a lot lot plugins, with these plugins you can do!!

UI like bootstrap (static) not better than
Form validation
Animation
Special effects

REF
jquery examples
jquery cheatsheet

]]></content>
      <categories>
        <category>html</category>
        <category>jquery</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title>http-protocol-cache</title>
    <url>/2019/12/13/http-protocol-cache/</url>
    <content><![CDATA[OverviewThe performance of web sites and applications can be significantly improved by reusing previously fetched resources. Web caches reduce latency and network traffic and thus lessen the time needed to display a representation of a resource. By making use of HTTP caching, Web sites become more responsive.
There are several kinds of caches: these can be grouped into two main categories: private or shared caches. A shared cache is a cache that stores responses for reuse by more than one user. A private cache is dedicated to a single user.




Cache whatHTTP caching is optional, but reusing a cached resource is usually desirable. However, common HTTP caches are typically limited to caching responses to GET and may decline other methods. The primary cache key(may have secondary key like Vary header etc) consists of the request method and target URI (oftentimes only the URI is used as only GET requests are caching targets). Common forms of caching entries are:

Successful results of a retrieval request: a 200 (OK) response to a GET request containing a resource like HTML documents, images or files.
Permanent redirects: a 301 (Moved Permanently) response.
Error responses: a 404 (Not Found) result page.
Incomplete results: a 206 (Partial Content) response.
Responses other than GET if something suitable for use as a cache key is defined.

A cache entry might also consist of multiple stored responses differentiated by a secondary key, if the request is target of content negotiation.
Cache-Control headerThe Cache-Control HTTP&#x2F;1.1 general-header field is used to specify directives for caching mechanisms in both requests and responses.
No caching
Cache-Control: no-store
Cache but revalidate
Cache-Control: no-cache
Validation
When using the “must-revalidate” directive, the cache must verify the status of the stale resources before using it and expired ones should not be used
Cache-Control: must-revalidate
ExpirationThe most important directive here is “max-age=&lt;seconds&gt;“ which is the maximum amount of time a resource will be considered fresh. Contrary to Expires, this directive is relative to the time of the request.
Cache-Control: max-age=600
Freshnesscache should only be updated when client requests the uri(server never notifies client when resource changes), Before this expiration time, the resource is fresh; after the expiration time, the resource is stale. Eviction algorithms often privilege fresh resources over stale resources. Note that a stale resource is ignored ; when the cache receives a request for a stale resource, it forwards this request with a If-None-Match to check if it is in fact still fresh. If so, the server returns a 304 (Not Modified) header without sending the body of the requested resource, saving some bandwidth.


cache with Vary headerThe Vary HTTP response header determines how to match future request headers to decide whether a cached response can be used rather than requesting a fresh one from the origin server. without vary header, you can think key as just domain + uri, with vary header, cache key: domain + uri + Vary header.
When a cache receives a request that can be satisfied by a cached response that has a Vary header field, it must not use that cached response unless all header fields as nominated by the Vary header match in both the original (cached) request and the new request.
]]></content>
      <categories>
        <category>http-protocol</category>
        <category>cache</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>cache-control</tag>
      </tags>
  </entry>
  <entry>
    <title>http-cdn</title>
    <url>/2022/04/29/http-cdn/</url>
    <content><![CDATA[OverviewA content delivery network (CDN) is a group of servers set up in different locations worldwide to provide web content over a wide geographic area much faster.
CDN, also known as “distribution networks,” offers several points of presence (PoP) outside the origin server. This enables websites to better manage traffic by handling user requests more quickly, providing an overall better experience.
In short, you’re using a CDN every time you visit a high-traffic site such as Amazon or catch up with your friends on Facebook. Their data centers keep allows to bring such content much more quickly regardless of the geographic location of individual users or the main website server.
By spreading the delivery systems out over a large area, websites can reduce bandwidth consumption and page load times, shaving precious seconds off the time it takes to handle multiple user requests.


CND Insidehow it works
When the browser makes a DNS request for a domain name that is handled by a CDN, there is a slightly different process than with small, one-IP sites. The server handling DNS requests for the domain name looks at the incoming request to determine the best set of servers to handle it. At it’s simplest, the DNS server does a geographic lookup based on the DNS resolver’s IP address and then returns an IP address for an edge server that is physically closest to that area. 
Edge servers are proxy caches that work in a manner similar to the browser caches. When a request comes into an edge server, it first checks the cache to see if the content is present. The cache key is the entire URL including query string (just like in a browser). If the content is in cache and the cache entry hasn’t expired, then the content is served directly from the edge server.
If, on the other hand, the content is not in the cache or the cache entry has expired, then the edge server makes a request to the origin server to retrieve the information. The origin server is the source of truth for content and is capable of serving all of the content that is available on the CDN. When the edge server receives the response from the origin server, it stores the content in cache based on the HTTP headers of the response
why needs it?To begin, CDNs are networks of servers strategically distributed in many parts of the globe. Each server acts as a content delivery tool and they primarily function to reduce latency—the period it takes for a web page to load in entirety. In other words, CDNs improve one’s browsing experience because they speed things up.
For example, let’s assume your site is hosted by a data center in Washington. When you don’t have a CDN, each of your visitors will have to download all of your website files from Washington. This isn’t a problem if your site visitors are from Vancouver, Pennsylvania, or Maryland. But what if they’re from Australia? Data does travel quickly, but that’s still 9,756 miles!
Simply put, no matter how updated and inventive your hardware is, distance is bound to delay your website’s loading speed for your users who are in distant parts of the world. This is exactly why websites that get good traffic need the technology. CDNs are one solution to retain customers. When you run a business, content delivery is an activity you can’t overlook. Otherwise, you’re only going to lose money.
What Can a CDN Do?Modern CDNs can handle numerous IT tasks, helping you to:

Improve page load speed
Handle high traffic loads
Block spammers, scrapers and other bad bots
Localize coverage without the cost
Reduce bandwidth consumption
Load balance between multiple servers
Protect your website from DDoS attacks
Secure your application
And more

FAQWhich is CNAMEA canonical name record (CNAME) is used in the Domain Name System (DNS) to create an alias from one domain name to another domain name, usually used for CND network, you may need to create alias of your origin server at DNS provider, so taht dns lookup for your domain returns CNAME(provided by CDN vendor), then dns lookup for CNAME will get closer IP(edge server) provided CND provider.
$ nslookup&gt; set type=A&gt; cyun.tech# check cname&gt; set type=CNAME&gt; cyun.tech# check TXT&gt; set type=TXT&gt; cyun.tech# check name server&gt; set type=NS&gt; cyun.tech

How to Enable CDN for your website?
login your CDN vendor
enable CDN for your website in CDN console, enter webiste domain and Origin server ip then CDN network can get content from it for each first request.
update DNS name server in DNS provider with name server provider by CDN vendor, so taht dns lookup for your domain will the edge server.Or
Add CNAME(provided by CDN vendor) in your DNS provider to redirect dns lookup to edge server.

Ref
what is cdn

]]></content>
      <categories>
        <category>http</category>
        <category>cdn</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>cdn</tag>
      </tags>
  </entry>
  <entry>
    <title>http-protocol-connection</title>
    <url>/2019/12/16/http-protocol-connection/</url>
    <content><![CDATA[OverviewHTTP mostly relies on TCP for its transport protocol, providing a connection between the client and the server. In its infancy(early stage like http1.0), HTTP used a single model to handle such connections. These connections were short-lived: a new one created each time a request needed sending, and closed(server does) once the answer was sent
But in HTTP&#x2F;1.x, there are several models: short-lived connections, persistent connections, and HTTP pipelining(never used in practice, but the foundation of http2.0(pipeline mode)), The persistent-connection model keeps connections opened between successive requests, reducing the time needed to open new connections. The HTTP pipelining model goes one step further, by sending several successive requests without even waiting for an answer, reducing much of the latency in the network.



short-lived connectionThe default one in HTTP&#x2F;1.0, is short-lived connections. Each HTTP request is completed on its own connection; this means a TCP handshake happens before each HTTP request and these are serialized.
The TCP handshake itself is time-consuming, but a TCP connection adapts to its load, becoming more efficient with more sustained (or warm) connections. Short-lived connections do not make use of this efficiency feature of TCP, and performance degrades from optimum by persisting to transmit over a new, cold connection.
This model is the default model used in HTTP/1.0 (if there is no Connection header, or if its value is set to close). In HTTP/1.1, short-lived model is only used when the Connection header is sent with a value of close.
persistent-connection(keep-alive)A persistent connection is one which remains open for a period of time, and can be reused for several requests, saving the need for a new TCP handshake, and utilizing TCP’s performance enhancing capabilities. This connection will not stay open forever: idle connections are closed after some time (a server may use the Keep-Alive header to specify a minimum time the connection should be kept open).
Persistent connections also have drawbacks; even when idling they consume server resources, and under heavy load, DoS attacks can be conducted. In such cases, using non-persistent connections, which are closed as soon as they are idle, can provide better performance.
In HTTP/1.1, persistence is the default, and the Connection header is not a must(but it is often added as a defensive measure against cases requiring a fallback to HTTP/1.0).
ConclusionIf server does not allow keepalive(like apache, can be configed), no matter http1.0 or http1.1, server sends response; then (close the connection)FIN, if server allows keepalive, if client sends Connection header, uses the way that client suggested, if client no preference, by default, http1.0 send FIN(no keepalive), http1.1 not FIN(keepalive).
Improved connection management allows considerable boosting of performance in HTTP. With HTTP&#x2F;1.1 or HTTP&#x2F;1.0, using a persistent connection at least until it becomes idle leads to the best performance.
]]></content>
      <categories>
        <category>http-protocol</category>
        <category>connection</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>keep-alive connection</tag>
      </tags>
  </entry>
  <entry>
    <title>http-protocol-faq</title>
    <url>/2019/12/13/http-protocol-faq/</url>
    <content><![CDATA[FAQsHere are list of faqs that are asked for web beginner, like what Vary is and difference between Content-Encoding and Transfer-Encoding, what conditional request is etc.

How response Vary header is usedIn some case, server may return different responses for the same uri based on some info from request  headers, the based on headers are put in Vary header, one typical example is web server returns different web pages for mobile and desktop, but the uri is same, that means response depends on User-Agent request header, so put User-Agent in Vary response header, when proxy(nginx always cache response) receives the response, it (nginx) stores the response with key (uri, User-Agent), next time when a new request comes in with same uri but different user-agent, proxy should request a fresh one from origin server, not use cached data, as the User-Agent(secondary key when searching cache) is different.
response depends on User-Agent and Cookie

Vary: User-Agent, Cookie

In one word, it tells downstream proxies how to match future request headers to decide whether the cached response can be used rather than requesting a fresh one from the origin server.
what q&#x3D;0.6 mean in header valueAs for some headers, like Accept-Language, Accept it could contain multiple values, in that case which value should be used by server, q(relative quality factor) is used for this, it indicates preference of each value, server should use highest one(q value is larger), when sends response to client, here is an example:
Accept-Language: en-us,en;q=0.5
en-us uses default q&#x3D;1 which is higher than en, so when server sends response to client, it should use en-us Language if it supports.
Rules

without q provided use default q=1
high value means much prefer
value range [0, 1]
format value;q=0.5

how Etag is generatedEtag(response header) is an identifier for a specific version of a resource(mostly for static resource), often a hash value of a resource, it’s generated by web server based on attributes(size, inode, modified-time etc) of the resource.
ETag: &quot;737060cd8c284d8af7ad3082f209582d&quot;
headers related to resume from break pointIn order to support this, client can request partial range of content, while server must support partial request, there are three headers, two response headers Accept-Ranges, Content-Range, one request header Range.

Accept-Ranges indicates if server supports partial request
Range indicates which range client wants to get
Content-Range indicates which range that server sends it to client.

server declares it supports
Accept-Ranges: bytes
client request a range
Range: bytes=500-999
server send the conent of that range
Content-Range : bytes 500-999/1234
Expires vs Cache-Control(max-age value) headerCache-Control was introduced in HTTP/1.1 and offers more options than Expires. They can be used to accomplish the same thing.
The data value for Expires is an HTTP date whereas Cache-Control max-age lets you specify a relative amount of time so you could specify ‘X hours after the page was requested’.
If a response includes a Cache-Control field with the max-age directive, a recipient MUST ignore the Expires field.
Cache-Control: max-age=3600
Expires: Tue, 18 Jul 2017 16:07:23 GMT
Always use Cache-Control as it offers more options
Ways to do conditional requestConditional request means client provides condition to server, server checks the condition if matched, sends the resource, otherwise, only sends header with special status code.
Old way(http1.0)
If-Unmodified-Since and If-Modified-Since, where the client sends a timestamp of the resource.
http 1.1
If-Modified and If-None-Modified, where the client sends an ETag representation of the resource
Difference:
Dates can be ordered, ETags can not.
This means that if some resource was modified a year ago, but never since, and we know it. Then we can correctly answer an If-Unmodified-Since request for arbitrary dates the last year and agree that sure… it has been unmodified since that date.
An Etag is only comparable for identity. Either it is the same or it is not. If you have the same resource as above, and during the year the docroot has been moved to a new disk and filesystem, giving all files new inodes but preserving modification dates. And someone had based the ETags on file’s inode number. Then we can’t say that the old ETag is still okay, without having a log of past-still-okay-ETags.
So I don’t see them as one obsoleting the other. They are for different situations. Either you can easily get a Last-Modified date of all the data in the page you’re about to serve, or you can easily get an ETag for what you will serve.
If you have a dynamic webpage with data from lots of db lookups it might be difficult to tell what the Last-Modified date is without making your database contain lots of modification dates. But you can always make an md5 checksum of the result rendered page.
When supporting these cache protocols I definitely go for only one of them, never both.
TE and Transfer-Encoding headerThe TE request header specifies the transfer encodings the user agent is willing to accept. (you could informally call it Accept-Transfer-Encoding, which would be more intuitive).
TE: chunked
The Transfer-Encoding response header specifies the form of encoding used to safely transfer the payload body to the user
#chunked, only for Http1.1Transfer-Encoding: chunked#no encoding at transfor levelTransfer-Encoding: identity

In which case chunked is used
Regards to chunked encoding, there is one important response header Trailer, it allows the sender to include additional fields(header) at the end of chunked messages in order to supply metadata that might be dynamically generated while the message body is sent, such as a message integrity check, digital signature, or post-processing status.
Note: The TE request header needs to be set to &quot;trailers&quot; to allow trailer fields.
Chunked encoding is useful when larger amounts of data are sent to the client and the total size of the response may not be known until the request has been fully processed. For example, when generating a large HTML table resulting from a database query or when transmitting large images.
Data is sent in a series of chunks. The Content-Length header is omitted in this case and at the beginning of each chunk you need to add the length of the current chunk in hexadecimal format, followed by ‘\r\n’ and then the chunk itself, followed by another ‘\r\n’. The terminating chunk is a regular chunk, with the exception that its length is zero. It is followed by the trailer, which consists of a (possibly empty) sequence of entity header fields.
A chunked response looks like this:
HTTP/1.1 200 OKContent-Type: text/plainTransfer-Encoding: chunkedTrailer: Expires7\r\nMozilla\r\n9\r\nDeveloper\r\n7\r\nNetwork\r\n0\r\nExpires: Wed, 21 Oct 2015 07:28:00 GMT\r\n\r\n

Accept-Encoding and Content-EncodingThe Content-Encoding entity header is used to compress the media-type. When present, its value indicates which encodings were applied to the entity-body. It lets the client know how to decode in order to obtain the media-type referenced by the Content-Type header.
The recommendation is to compress data as much as possible and therefore to use this field, but some types of resources, such as jpeg images, are already compressed
The Accept-Encoding request HTTP header advertises which content encoding, usually a compression algorithm, the client is able to understand.
Accept-Encoding: gzipAccept-Encoding: compressAccept-Encoding: deflate# As long as the identity value, meaning no encoding in some case like for image format.Accept-Encoding: identity

Note: browser will decompress payload and show uncompressed web page to user
Content-Encoding vs Transfer-EncodingContent-Encoding is how content is encoding, like if the web page is 100k, it’s better to encode it with gzip to reduce the payload, when server gets the encode data(or server encodes it by itself), the server may decide to transfer the gzip data(Content-Encoding: gzip) with chunked format(Transfer-Encoding: chunked), that’s what they are,  they apply at different levels, Transfer-Encoding is hop by hop, it may change during transferring proxy, while Content-Encoding is end-to-end proxy never touch the payload!
Without Content-Encoding, assume it&#39;s uncompressed, without Transfer-Encoding, assume it&#39;s not chunked, but must has Content-length if has body
method PUT(whole update) vs POST(new) vs PATCH(part update)The POST method is used to submit an entity to the specified resource, often causing a change in state or side effects on the server. plan to create new, if you run many times with same uri, many new objects may be created with same value
POST /questions
The PUT method replaces all current representations of the target resource with the request payload, or create new one if not found, plan to replace, if you run many times with same uri, there is only one objects created, as PUT must provide a identity
PUT /questions/&#123;question-id&#125;
The PATCH method is used to apply partial modifications to a resource.
method                          PATCH     POST         PUTRequest has body	            Yes       Yes          YesSuccessful response has body	Yes       Yes          NOSafe	                        No        NO           NOIdempotent	                    No        NO           YesCacheable	                    No        NO           NOAllowed in HTML forms	        No        Yes          NOSafe:       no side effectIdempotent: same result if ran many times

A POST request is typically sent via an HTML form and results in a change on the server, in this case, it only supports three content types

application&#x2F;x-www-form-urlencoded: the keys and values are encoded in key-value tuples separated by &#39;&amp;&#39;, with a &#39;=&#39; between the key and the value. Non-alphanumeric characters in both keys and values are percent encoded: this is the reason why this type is not suitable to use with binary data (use multipart&#x2F;form-data instead)

POST /test HTTP/1.1Host: foo.exampleContent-Type: application/x-www-form-urlencodedContent-Length: 27field1=value1&amp;field2=value2


multipart&#x2F;form-data: each value is sent as a block of data (“body part”), with a user agent-defined delimiter (“boundary”) separating each part. The keys are given in the Content-Disposition header of each part.

text&#x2F;plain


When the POST request is sent via a method other than an HTML form — like via an XMLHttpRequest(like in script) — the body can take any type
GET VS HEADThe GET method requests a representation of the specified resource. Requests using GET should only retrieve data.
The HEAD method asks for a response identical to that of a GET request, but without the response body.
HEAD same as GET but without body returned
OPTIONS methodThe OPTIONS method is used to describe the communication options for the target resource.The client can specify a URL for the OPTIONS method, or an asterisk (*) to refer to the entire server.
Identifying allowed request methods
$ curl -X OPTIONS http://example.org -iHTTP/1.1 200 OKDate: Mon, 16 Dec 2019 02:57:20 GMTContent-Type: text/htmlContent-Length: 0Connection: keep-aliveServer: Apache/2.4.7 (Ubuntu)Allow: GET,HEAD,POST,OPTIONS
how to create a forever http connectionTo open a connection that never dies until close it by explicitly

Websocket
send http request with transfer-encoding: Chunked, but never set terminating chunk 0\r\n

In the above two ways, after get respone from sever, server will not close it, the close happends only when client closes it or client sends \0\r\n to server
]]></content>
      <categories>
        <category>http-protocol</category>
        <category>faq</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>faq</tag>
      </tags>
  </entry>
  <entry>
    <title>http-protocol-cookie</title>
    <url>/2019/12/13/http-protocol-cookie/</url>
    <content><![CDATA[OverviewAn HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to the user&#39;s web browser. The browser may store it and send it back with the next request to the same server. Typically, it’s used to tell if two requests come from the same browser — keeping a user logged-in, for example. It remembers stateful information for the stateless HTTP protocol.


Cookies are mainly used for three purposes:

Session managementLogins, shopping carts, game scores, or anything else the server should remember, stateful.
PersonalizationUser preferences, themes, and other settings
TrackingRecording and analyzing user behavior

Note: if you just want to store something in client, use storage APIs, as cookies are sent with every request if valid, so they can worsen performance, Web storage API (localStorage and sessionStorage) and IndexedDB.
Cookie typesAn expiration date or duration can be specified, after which the cookie is no longer sent. Additionally, restrictions to a specific domain and path can be set, limiting where the cookie is sent. sent cookie or not depends on uri, domain, path etc
session cookie
HTTP/2.0 200 OKContent-type: text/htmlSet-Cookie: yummy_cookie=chocoSet-Cookie: tasty_cookie=strawberry[page content]
GET /sample_page.html HTTP/2.0Host: www.example.orgCookie: yummy_cookie=choco; tasty_cookie=strawberry

cookie without Expires or Max-Age is session cookie will be deleted when the client shuts down.
Permanent cookies
Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT
Secure and HttpOnly
A secure cookie is only sent to the server with an encrypted request over the HTTPS protocol.
HttpOnly cookies are inaccessible to JavaScript’s Document.cookie API
Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly
CSRF token and SameSite
SameSite cookies let servers require that a cookie shouldn’t be sent with cross-site (where Site is defined by the registrable domain) requests, which provides some protection against cross-site request forgery attacks (CSRF).
Set-Cookie: key=value; SameSite=Strict
CSRF tokens are used to protect against CSRF attacks
A CSRF token is a unique, secret, unpredictable value that is generated by the server-side application and transmitted to the client in such a way that it is included in a subsequent HTTP request made by the client. When the later request is made, the server-side application validates that the request includes the expected token and rejects the request if the token is missing or invalid.
CSRF tokens can prevent CSRF attacks by making it impossible for an attacker to construct a fully valid HTTP request suitable for feeding to a victim user. Since the attacker cannot determine or predict the value of a user’s CSRF token, they cannot construct a request with all the parameters that are necessary for the application to honor the request.
Set-Cookie: CSRF=e8b667; Secure; Domain=example.com, A CSRF token should be included in &lt;form&gt; elements via a hidden input field.
what’s csrf attack
Scope of cookies(when to send cookie)The Domain and Path directives define the scope of the cookie: cookie is sent only when it matches domain and path.
Domain specifies allowed hosts to receive the cookie. If unspecified, it defaults to the host of the current document location, excluding subdomains. If Domain is specified, then subdomains are always included.
For example, if Domain&#x3D;mozilla.org is set, then cookies are included on subdomains like developer.mozilla.org.
Path indicates a URL path that must exist in the requested URL in order to send the Cookie header. The %x2F (“&#x2F;“) character is considered a directory separator, and subdirectories will match as well.
For example, if Path=/docs is set, these paths will match:
/docs/docs/Web//docs/Web/HTTP

see cookie for a website
]]></content>
      <categories>
        <category>http-protocol</category>
        <category>cookie</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>cookie</tag>
        <tag>local storage</tag>
      </tags>
  </entry>
  <entry>
    <title>http-protocol-headers</title>
    <url>/2019/12/13/http-protocol-headers/</url>
    <content><![CDATA[OverviewHTTP headers let the client and the server pass additional information with an HTTP request or response. An HTTP header consists of its case-insensitive name followed by a colon (:), then by its value. Whitespace before the value is ignored


Headers can be grouped according to their contexts:

General headers apply to both requests and responses, but with no relation to the data transmitted in the body. like Date, Cache-Control or Connection.
Request headers contain more information about the resource to be fetched, or about the client requesting the resource.
Response headers hold additional information about the response, like its location or about the server providing it.
Entity headers contain information about the body of the resource, like its content length or MIME type. like Content-Length, Content-Language, Content-Encoding.

GET /home.html HTTP/1.1Host: developer.mozilla.orgUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) Gecko/20100101 Firefox/50.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-US,en;q=0.5Accept-Encoding: gzip, deflate, brReferer: https://developer.mozilla.org/testpage.htmlConnection: keep-aliveUpgrade-Insecure-Requests: 1If-Modified-Since: Mon, 18 Jul 2016 02:36:04 GMTIf-None-Match: &quot;c561c68d0ba92bbeb8b0fff2a9199f722e3a621a&quot;Cache-Control: max-age=0

End-to-end headers
These headers must be transmitted to the final recipient of the message: the server for a request, or the client for a response. Intermediate proxies must retransmit these headers unmodified and caches must store them. like Content-Encoding.
Hop-by-hop headersThese headers are meaningful only for a single transport-level connection, and must not be retransmitted by proxies or cached. like Transfer-Encoding.
Common General HeaderDate
The Date general HTTP header contains the date and time at which the message was originated.Date: Wed, 21 Oct 2015 07:28:00 GMT
Connection
The Connection general header controls whether or not the network connection stays open after the current transaction finishes.
Connection: keep-alive
Cache-Control
The Cache-Control general-header field is used to specify directives for caching mechanisms in both requests and responses. Caching directives are unidirectional, meaning that a given directive in a request is not implying that the same directive is to be given in the response
Cache-Control: public, max-age=31536000
Common Request HeaderHOST
The Host header is mandatory in HTTP&#x2F;1.1 requests, domain of the server with&#x2F;without port, if no port provided, for http, it’s 80, https, 443.
Host: gethttp.info
Referer
tells the server where the requested URL came from. It will almost always be another URL, or else empty for a direct request, the Referer is the URL to the original page, let’s one web page depends on css&#x2F;js, when you get css&#x2F;js, the referer points to the web page.
Referer: https://www.quora.com/profile/Lee-Dowthwaite
User-Agent
User-Agent identifies the requesting system
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36
Accept
The Accept header(server side: Content-type) is how a client (browser or application) tells the server what kind of content it can accept in the HTTP response. The content types are comma-separated, and take the form type&#x2F;subtype such as text&#x2F;html, application&#x2F;json or audio&#x2F;mpeg.
Accept: text/html, text/plain; q=0.6, */*; q=0.1Accept: application/graphql, application/json; q=0.8, application/xml; q=0.7
Accept-Encoding
Accept-Encoding(server side: Content-Encoding) defines what type of content encoding (usually a compression algorithm) the client can accept in the response body, Note(server should use the suggested if it can do)
Client can also explicitly prohibits this encoding by specifying identity;q=0, in this server must not use this encoding.
Accept-Encoding: br, gzip;q=0.9, deflate;q=0.8, *;q=0.1
Accept-Language
The HTTP Accept-Language header tells the server the client’s preferred natural language
Accept-Language: en-GB, en-US, en;q=0.9
Authorization
The HTTP Authorization header specifies the authorization scheme and any associated data or token, and carries that data as a header payload.
Authorization: Basic ZmFsa2VuOmpvc2h1YTU= # encode64(user:password)
JWT or OAuth 2.0 token
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzY290Y2guaW8iLCJleH
a digest (or hash) value is created from a predetermined combination of the username, password, and some information from the server, including random (or “nonce”) values
Authorization: Digest adfsa23f31fsa32f2f # hash(user, password, extra_info)
Cookie
The Cookie request-header field value contains a name&#x2F;value pair of information stored for that URL.Cookie: name1=value1;name2=value2
Common Response HeaderAge
The Age response-header field conveys the sender’s estimate of the amount of time since the response (or its revalidation) was generated at the origin server
Age: 100 # 100s
Location
The Location response-header field is used to redirect the recipient to a location other than the Request-URI for completion
Location: http://www.tutorialspoint.org/http/index.htm
Retry-After
The Retry-After response-header field can be used with a 503 (Service Unavailable) response to indicate how long the service is expected to be unavailable to the requesting client
Retry-After: 120
Server
The Server response-header field contains information about the software used by the origin server to handle the request
Server: Apache/2.2.14 (Win32)
Set-Cookie
The Set-Cookie response-header field contains a name&#x2F;value pair of information to retain for this URL.
Set-Cookie: name1=value1,name2=value2; Expires=Wed, 09 Jun 2021 10:18:14 GMT; Domain=xx.com; Path=/doc
WWW-Authenticate
The WWW-Authenticate response-header field must be included in 401 (Unauthorized) response messages
Common Entity HeadersAllow
The Allow entity-header field lists the set of methods supported by the resource identified by the Request-URI.
Allow: GET, HEAD, PUT
Content-Encoding
The Content-Encoding entity-header field is used as a modifier to the media-type.
Content-Encoding: gzip
Content-Language
The Content-Language entity-header field describes the natural language(s) of the intended audience for the enclosed entity
Content-Language: en
Content-Length
Content-Length: 3495
Content-MD5
The Content-MD5 entity-header field may be used to supply an MD5 digest of the entity for checking the integrity of the message upon receipt.
Content-MD5  : 8c2d46911f3f5a326455f0ed7a8ed3b3
Content-Type
The Content-Type entity-header field indicates the media type of the entity-body sent to the recipient or, in the case of the HEAD method, the media type that would have been sent, had the request been a GET.
Content-Type: text/html; charset=ISO-8859-4
ExpiresThe Expires entity-header field gives the date&#x2F;time after which the response is considered stale
Expires: Thu, 01 Dec 1994 16:00:00 GMT
Last-ModifiedThe Last-Modified entity-header field indicates the date and time at which the origin server believes the variant was last modified.
Last-Modified: Tue, 15 Nov 1994 12:45:26 GMT
Ref
header faq
common headers
all headers wiki 
all headers mdn
all content-type

]]></content>
      <categories>
        <category>http-protocol</category>
        <category>header</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>header</tag>
      </tags>
  </entry>
  <entry>
    <title>http-protocol-overview</title>
    <url>/2019/12/13/http-protocol-overview/</url>
    <content><![CDATA[OverviewHTTP is an extensible protocol which has evolved over time. It is an application layer protocol that is sent over TCP, or over a TLS-encrypted TCP connection,



HTTP is a client-server protocol: requests are sent by one entity, the user-agent (or a proxy on behalf of it). Most of the time the user-agent is a Web browser, but it can be anything, for example a robot that crawls the Web to populate and maintain a search engine index.
Each individual request is sent to a server, which handles it and provides an answer, called the response. Between the client and the server there are numerous entities, collectively called proxies, which perform different operations and act as gateways or caches, for example.

These can be transparent, forwarding on the requests they(proxy) receive without altering them in any way, or non-transparent, in which case they will change the request in some way before passing it along to the server.
Proxies may perform numerous functions:

caching (the cache can be public or private, like the browser cache)
filtering (like an antivirus scan or parental controls)
load balancing (to allow multiple servers to serve the different requests)
authentication (to control access to different resources)
logging (allowing the storage of historical information)

HTTP is an extensible protocol that is easy to use. The client-server structure, combined with the ability to simply add headers, allows HTTP to advance along with the extended capabilities of the Web.
Though HTTP/2 adds some complexity, by embedding HTTP messages in frames to improve performance, the basic structure of messages has stayed the same since HTTP&#x2F;1.0
Http1.x messageHTTP requests, and responses, share similar structure and are composed of:

A start-line describing the requests to be implemented, or its status of whether successful or a failure. This start-line is always a single line.
An optional set of HTTP headers specifying the request, or describing the body included in the message.
A blank line indicating all meta-information for the request have been sent.
An optional body containing data associated with the request (like content of an HTML form), or the document associated with a response. The presence of the body and its size is specified by the start-line and HTTP headers.

The start-line and HTTP headers of the HTTP message are collectively known as the header of the requests, whereas its payload is known as the body.


http2.0In the first half of the 2010s, Google demonstrated an alternative way of exchanging data between client and server, by implementing an experimental protocol SPDY. This amassed interest from developers working on both browsers and servers. Defining an increase in responsiveness, and solving the problem of duplication of data transmitted, SPDY served as the foundations of the HTTP/2 protocol.
The HTTP&#x2F;2 protocol has several prime differences from the HTTP&#x2F;1.1 version:

It is a binary protocol rather than text. It can no longer be read and created manually.  Despite this hurdle, improved optimization techniques can now be implemented.
It is a multiplexed protocol. Parallel requests can be handled over the same connection, removing the order and blocking constraints of the HTTP&#x2F;1.x protocol.
It compresses headers. As these are often similar among a set of requests, this removes duplication and overhead of data transmitted.
It allows a server to populate data in a client cache, in advance of it being required, through a mechanism called the server push.

The next major version of HTTP, HTTP/3, will use QUIC instead TCP/TLS for the transport layer portion.
QUICKQUIC (pronounced ‘quick’) is a general-purpose transport layer network protocol initially designed by Jim Roskind at Google, it establishes a number of multiplexed connections between two endpoints over User Datagram Protocol (UDP). This works hand-in-hand with HTTP&#x2F;2’s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently, and hence independent of packet losses involving other streams. In contrast, HTTP&#x2F;2 hosted on Transmission Control Protocol (TCP) can suffer head-of-line-blocking delays of all multiplexed streams if any of the TCP packets are delayed or lost.
QUIC’s secondary goals include reduced connection and transport latency, and bandwidth estimation in each direction to avoid congestion. It also moves congestion control algorithms into the user space at both endpoints, rather than the kernel space, which it is claimed will allow these algorithms to improve more rapidly
In June 2015, an Internet Draft of a specification for QUIC was submitted to the IETF for standardization. A QUIC working group was established in 2016. In October 2018, the IETF’s HTTP and QUIC Working Groups jointly decided to call the HTTP mapping over QUIC “HTTP&#x2F;3” in advance of making it a worldwide standard.
]]></content>
      <categories>
        <category>http-protocol</category>
        <category>overview</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_APIServer_access</title>
    <url>/2021/06/08/k8s-apiserver-access/</url>
    <content><![CDATA[IntroductionHere let’s foucs on how Kubernets controll access to API server, When a request reaches the API, it goes through several stages, illustrated in the following diagram.



Stage of Access controlTransport securityIn a typical Kubernetes cluster, the API serves on port 443, protected by TLS. The API server presents a certificate and client can present a TLS client certificate at this stage as well, but it’s an optional(verify client is not a must).
AuthenticationThe cluster creation script or cluster admin configures the API server to run one or more Authenticator modules.
All Kubernetes clusters have two categories of users: service accounts managed by Kubernetes, and normal users.

client certificates, password, and plain tokens, bootstrap tokens as normal user
JSON Web Tokens (used for service accounts).

To manually create a service account, use the kubectl create serviceaccount (NAME) command. This creates a service account in the current namespace and an associated secret(auto created).
$ kubectl create serviceaccount jenkins$ kubectl get secret jenkins-token-1yvwg -o yamlapiVersion: v1kind: ServiceAccountmetadata:  # ...secrets:- name: jenkins-token-1yvwg$ kubectl get secret jenkins-token-1yvwg -o yamlapiVersion: v1data:  ca.crt: (APISERVER&#x27;S CA BASE64 ENCODED)  namespace: ZGVmYXVsdA==  token: (BEARER TOKEN BASE64 ENCODED)kind: Secretmetadata:  # ...type: kubernetes.io/service-account-token

AuthorizationIn Kubernetes, you must be authenticated (logged in) before your request can be authorized (granted permission to access).
It evaluates all of the request attributes against all policies and allows or denies the request. All parts of an API request must be allowed by some policy in order to proceed. This means that permissions are denied by default.
Request Attributes

user - The user string provided during authentication.
group - The list of group names to which the authenticated user belongs.
extra - A map of arbitrary string keys to string values, provided by the authentication layer.
API - Indicates whether the request is for an API resource.
Request path - Path to miscellaneous non-resource endpoints like &#x2F;api or &#x2F;healthz.
API request verb - API verbs like get, list, create, update, patch, watch, delete, and deletecollection are used for resource requests. To determine the request verb for a resource API endpoint, see Determine the request verb.
HTTP request verb - Lowercased HTTP methods like get, post, put, and delete are used for non-resource requests.
Resource - The ID or name of the resource that is being accessed (for resource requests only) – For resource requests using get, update, patch, and delete verbs, you must provide the resource name.
Subresource - The subresource that is being accessed (for resource requests only).
Namespace - The namespace of the object that is being accessed (for namespaced resource requests only).
API group - The API Group being accessed (for resource requests only). An empty string designates the core API group

Kubernetes sometimes checks authorization for additional permissions using specialized verbs.

RBAC  bind and escalate verbs on roles and clusterroles resources in the rbac.authorization.k8s.io API group.
Authentication  impersonate verb on users, groups, and serviceaccounts in the core API group, and the userextras in the authentication.k8s.io API group

Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode.

ABAC - Attribute-based access control (ABAC) defines an access control paradigm whereby access rights are granted to users through the use of policies which combine attributes together. The policies can use any type of attributes (user attributes, resource attributes, object, environment attributes, etc). To learn more about using the ABAC mode, see ABAC Mode

RBAC - Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within an enterprise. In this context, access is the ability of an individual user to perform a specific task, such as view, create, or modify a file. To learn more about using the RBAC mode, see RBAC Mode



When specified RBAC (Role-Based Access Control) uses the rbac.authorization.k8s.io API group to drive authorization decisions, allowing admins to dynamically configure permission policies through the Kubernetes API.


Webhook - A WebHook is an HTTP callback: an HTTP POST that occurs when something happens; a simple event-notification via HTTP POST. A web application implementing WebHooks will POST a message to a URL when certain things happen. To learn more about using the Webhook mode, see Webhook Mode.

Admission controlAn admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.
Admission controllers may be “validating”, “mutating”, or both. Mutating controllers may modify the objects they admit; validating controllers may not.
For example.

EventRateLimit  mitigates the problem where the API server gets flooded by event requests.

LimitRanger  will observe the incoming request and ensure that it does not violate any of the constraints enumerated in the LimitRange object in a Namespace.


Check permission# check for current user$ kubectl auth can-i create deployments --namespace dev# check for other user: dave$ kubectl auth can-i list secrets --namespace dev --as dave
Ref
API control access

]]></content>
  </entry>
  <entry>
    <title>http-protocol-response-code</title>
    <url>/2019/12/13/http-protocol-response-code/</url>
    <content><![CDATA[OverviewHTTP response status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes:

Informational responses (100–199)
Successful responses (200–299)
Redirects (300–399)
Client errors (400–499) detected by server
Server errors (500–599)


Let’s focus on some common status codes that’s used for each group.
Informational responseActually, you rarely see such response.
Successful response200 OK
The HTTP 200 OK success status response code indicates that the request has succeeded. A 200 response is cacheable by default.
The meaning of a success depends on the HTTP request method:

GET: The resource has been fetched and is transmitted in the message body.
HEAD: The entity headers are in the message body.
POST: The resource describing the result of the action is transmitted in the message body.

The successful result of a PUT or a DELETE is often not a 200 OK but a 204 No Content (or a 201 Created when the resource is uploaded for the first time).
201 Created
The HTTP 201 Created success status response code indicates that the request has succeeded and has led to the creation of a resource. The new resource is effectively created before this response is sent back and the new resource is returned in the body of the message, its location being either the URL of the request, or the content of the Location header.
The common use case of this status code is as the result of a POST request.
202 Accepted
The HyperText Transfer Protocol (HTTP) 202 Accepted response status code indicates that the request has been received but not yet acted upon. It is non-committal, meaning that there is no way for the HTTP to later send an asynchronous response indicating the outcome of processing the request. It is intended for cases where another process or server handles the request, or for batch processing.
203 Non-Authoritative Information
This response code means the returned meta-information is not exactly the same as is available from the origin server, but is collected from a local or a third-party copy. This is mostly used for mirrors or backups of another resource. Except for that specific case, the “200 OK” response is preferred to this status.
204 No Content
The HTTP 204 No Content success status response code indicates that the request has succeeded, but that the client doesn&#39;t need to go away from its current page. A 204 response is cacheable by default. An ETag header is included in such a response.
The common use case is to return 204 as a result of a PUT request, updating a resource, without changing the current content of the page displayed to the user. If the resource is created, 201 Created is returned instead. If the page should be changed to the newly updated page, the 200 should be used instead
205 Reset Content
The HTTP 205 Reset Content response status tells the client to reset the document view, so for example to clear the content of a form, reset a canvas state, or to refresh the UI.
206 Partial Content
The HTTP 206 Partial Content success status response code indicates that the request has succeeded and has the body contains the requested ranges of data, as described in the Range header of the request.
Redirects301 Moved Permanently
The URL of the requested resource has been changed permanently. The new URL is given in the response.
HTTP/1.1 301 Moved PermanentlyLocation: http://www.example.org/index.asp# new url
302 Found
The HyperText Transfer Protocol (HTTP) 302 Found redirect status response code indicates that the resource requested has been temporarily moved to the URL given by the Location header. A browser redirects to this page but search engines don’t update their links to the resource as it’s temporarily.
304 Not Modified
The HTTP 304 Not Modified client redirection response code indicates that there is no need to retransmit the requested resources. It is an implicit redirection to a cached resource. This happens when the request method is safe, like a GET or a HEAD request, or when the request is conditional and uses a If-None-Match or a If-Modified-Since header.
The equivalent 200 OK response would have included the headers Cache-Control, Content-Location, Date, ETag, Expires, and Vary.
Client errors400 Bad Request
The server could not understand the request due to invalid syntax.
401 Unauthorized
Although the HTTP standard specifies “unauthorized”, semantically this response means “unauthenticated”. That is, the client must authenticate itself to get the requested response.
403 Forbidden
The client does not have access rights to the content; that is, it is unauthorized, so the server is refusing to give the requested resource. Unlike 401, the client&#39;s identity is known to the server.
404 Not Found
The server can not find requested resource. In the browser, this means the URL is not recognized. In an API, this can also mean that the endpoint is valid but the resource itself does not exist. Servers may also send this response instead of 403 to hide the existence of a resource from an unauthorized client. This response code is probably the most famous one due to its frequent occurrence on the web.
405 Method Not Allowed
The request method is known by the server but has been disabled and cannot be used. For example, an API may forbid DELETE-ing a resource. The two mandatory methods, GET and HEAD, must never be disabled and should not return this error code.
406 Not Acceptable
This response is sent when the web server, after performing server-driven content negotiation, doesn&#39;t find any content that conforms to the criteria given by the user agent.
407 Proxy Authentication Required
This is similar to 401 but authentication is needed to be done by a proxy.
408 Request Timeout
This response is sent on an idle connection by some servers, even without any previous request by the client. It means that the server would like to shut down this unused connection. This response is used much more since some browsers, like Chrome, Firefox 27+, or IE9, use HTTP pre-connection mechanisms to speed up surfing. Also note that some servers merely shut down the connection without sending this message.
410 Gone
This response is sent when the requested content has been permanently deleted from server, with no forwarding address. Clients are expected to remove their caches and links to the resource. The HTTP specification intends this status code to be used for “limited-time, promotional services”. APIs should not feel compelled to indicate resources that have been deleted with this status code.
411 Length Required
Server rejected the request because the Content-Length header field is not defined and the server requires it.
413 Payload Too Large
Request entity is larger than limits defined by server; the server might close the connection or return an Retry-After header field.
414 URI Too Long
The URI requested by the client is longer than the server is willing to interpret.
415 Unsupported Media Type
The media format of the requested data is not supported by the server, so the server is rejecting the request.
416 Requested Range Not Satisfiable
The range specified by the Range header field in the request can’t be fulfilled; it’s possible that the range is outside the size of the target URI’s data.
429 Too Many Requests
The user has sent too many requests in a given amount of time (“rate limiting”).
431 Request Header Fields Too Large
The server is unwilling to process the request because its header fields are too large. The request may be resubmitted after reducing the size of the request header fields.
Server errors500 Internal Server Error
The server has encountered a situation it doesn’t know how to handle, This error response is a generic &quot;catch-all&quot; response. Usually, this indicates the server cannot find a better 5xx error code to response
501 Not Implemented
The request method is not supported by the server and cannot be handled. The only methods that servers are required to support (and therefore that must not return this code) are GET and HEAD.
502 Bad Gateway
The HyperText Transfer Protocol (HTTP) 502 Bad Gateway server error response code indicates that the server, while acting as a gateway or proxy, received an invalid response from the upstream server.
503 Service Unavailable
The server is not ready to handle the request. Common causes are a server that is down for maintenance or that is overloaded. Note that together with this response, a user-friendly page explaining the problem should be sent. This responses should be used for temporary conditions and the Retry-After: HTTP header should, if possible, contain the estimated time before the recovery of the service. The webmaster must also take care about the caching-related headers that are sent along with this response, as these temporary condition responses should usually not be cached.
504 Gateway Timeout
The HyperText Transfer Protocol (HTTP) 504 Gateway Timeout server error response code indicates that the server, while acting as a gateway or proxy, did not get a response in time from the upstream server that it needed in order to complete the request.
]]></content>
      <categories>
        <category>http</category>
        <category>status-code</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>status-code</tag>
      </tags>
  </entry>
  <entry>
    <title>javascript_basic</title>
    <url>/2020/07/22/javascript-basic/</url>
    <content><![CDATA[Javascript
The single quote&#x2F;double quotes patterns are identical in terms of functionality
; can be ignore if unnecessary, but it’s not a good habit.
There’s no Integer in JavaScript (except BigInt), all are float(64 bits).
Index starts from 0, like for String and Array.



JS behaves like C style for supports operators.
+   Add-   Minus*   Multiple/   Divid%   Modulo&amp;   AND|   OR^   XOR~   NOT&lt;&lt;  Shift Left&gt;&gt;  Shift Right With Sign&gt;&gt;&gt; Shift Right With Zero Fill==  Equal!=  Not equal=== Equal using strict methods both value and type are same!== Not equal using strict methods&gt;   Greater than&lt;   Less than&gt;=  Greater than or equal to&lt;=  Less than or equal toin  check key of an objectinstanceof  Is an instance of an Objectdelete Removes a property from an object who owns it, the inherited property can NOT deleted.void   Returns undefinedtypeof Returns a string representing the data type++  Increments a number--  Decrements a number+   Converts the operand to a number-   Negates the operand~   Bitwise NOT!   Logical NOT*=  Multiplies the left operand by the right operand/=  Divides the left operand by the right operand%=  Provides the division remainder (modulus) of the left and right operand+=  Adds the right operand to the left operand-=  Subtracts the right operand from the left operand&lt;&lt;=     Bitwise left shift&gt;&gt;=     Bitwise right shift&gt;&gt;&gt;=    Bitwise unsigned right shift&amp;=  Bitwise AND^=  Bitwise XOR|=  Bitwise OR

printing in JSPrinting is an easy way to show result, there are several ways to print variable value. here we go.

in one lineconsole.log(&quot;hello \            jason&quot;);
in multiple linesconsole.log(&quot;hello \n jason&quot;);console.log(`hellojason`);//template string
show one parameter at last positionconst name = &quot;jason&quot;;console.log(&quot;hello&quot;, name); # a space is added between after &#x27;hello&#x27; automaticallyconsole.log(&quot;hello %s&quot;, name); # no auto added space
show multiple parameters at different positionsconst msg = &quot;welcome&quot;;console.log(&quot;hello %s msg %s&quot;, name, msg);
variable can be at any positionconsole.log(name, &quot;hi&quot;); #has auto added space before &#x27;hi&#x27;console.log(name, &quot;hi&quot;, name); # has auto added space before and after &#x27;hi&#x27;console.log(name, &quot;hi %s&quot;, name); # NOT support, %s must be in the first string if hasconsole.log(name + &quot;hi %s&quot;, name); # support
template stringconsole.log(`$&#123;last_name&#125; hi`); // last_name is name of a variable

Types==========================javascript types=============================================                                type                                |                                |               primitive--------+----+object                |                     |    +-----------+-----+               +---------+-------+--------+------+----------+    |                 |               |         |       |        |      |          |    |                 |               |         |       |        +      |          +  string              number        Number     String   Array   Map     Math Custom(class)    +                   +              ^         ^    |                   |    auto-box  |         |    |                   +--------------+         |    |                                            |    |                 auto-box                   |    +--------------------------------------------+    always use primitive string and number as they have high performance even in es6!======================================================================================

auto-boxingjavaScript has two main type categories, primitive(string, number, boolean) and object.
// primitiveconst s = &#x27;test&#x27;;// object, ss&#x27;s value can NOT change as well even it&#x27;s an object.const ss = new String(&#x27;test&#x27;);
A primitive is converted to its wrapper type(auto-boxing) only when a method of the wrapper type is invoked, when auto-boxing happens, a temporary object is created and destroyed after that call.
It’s a primitive data type. It has no methods, it is nothing more than a pointer points to a raw data memory reference, which explains the much faster random access speed than object.
So what happens when you do s.charAt(i) for instance?
s[0] == s.charAt(0)

Since s is not an instance of String, JavaScript will auto-box s, which has typeof string to its wrapper type, String, with typeof object or more precisely.
The auto-boxing behaviour casts a back and forth to its wrapper type as needed.
If you want to force the auto-boxing or to cast a primitive to its wrapper type, you can use Object.prototype.valueOf, but the behaviour is different. Based on a wide letiety of test scenarios auto-boxing only applies the ‘required’ methods, without altering the primitive nature of the letiable. Which is why you get better speed.
Set on wrapped temporary object has no lasting effect as that temporary is destroyed soon after
const str = &#x27;hello&#x27;;str.age = 10; // a temporary String object is created and set.console.log(str.age); //undefined as the temporary object is destroyed.

Numbers and Datesfunction number_date() &#123;    const a2 = 0b11;    const a16 = 0x10;    const n_number = 12;    const n_object = new Number(12) //object    console.log(a2, a16, n_number, n_object);    //Math class    console.log(Math.PI, Math.random(), Math.round(4.3), Math.min(1, 2, 3), Math.max(...[1, 2, 3]))    //Date class library    const today = new Date();    console.log(today.toString(), today.getMilliseconds(), today.getHours(), )    console.log(3 / 2);    console.log(Math.floor(3 / 2));    //random float number from [0-1)    console.log(Math.random());    //random integer from [0-10)    console.log(Math.floor(Math.random() * 10));    console.log(1 + 2 == 3); // true    console.log(0.1 + 0.2 == 0.3); // false, never compare float!!!&#125;number_date()

3 16 12 [Number: 12]
3.141592653589793 0.3273509254139666 4 1 3
Wed Nov 04 2020 18:26:24 GMT+0800 (China Standard Time) 700 18
1.5
1
0.8324244498379996
2
true
false

function string_number_convert() &#123;    // use parseInt and parseFloat which are more poweroff than Number()    console.log(Number(&#x27;123&#x27;));    console.log(Number(&#x27; 123&#x27;));    //not a number    console.log(Number(&#x27;123a&#x27;)); //NaN    console.log(parseInt(&#x27;123&#x27;, 10));    console.log(parseInt(&#x27; 123&#x27;, 10));    // still get a number    console.log(parseInt(&#x27; 123a&#x27;, 10)); // 123    //convert string to number    //valueOf(valueOf is called when number is needed) is called implicitly,    console.log(&#x27;3&#x27; - 1);    // call valueOf explicitly.    console.log(&#x27;3&#x27;.valueOf() - 1);    // convert number to string    const num = 123;    console.log(num.toString());    console.log(String(num));    console.log( 2 + &#x27;hello&#x27;); //toString is called implicitly when string is needed.&#125;string_number_convert()

123
123
NaN
123
123
123
2
2
123
123
2hello

Special number NaNfunction sepcial_number() &#123;    console.log(&#x27;special number: Infinity and -Infinity,NaN are numbers \nNaN: any number of other numeric operations that don not yield a meaningful result.&#x27;);    console.log(Infinity);    console.log(-Infinity);    console.log(typeof NaN);    // NaN is for **numeric operations** that don not yield a meaningful result    console.log(0/0);    console.log(&#x27;a&#x27; * 2);    // Strings cannot be divided, multiplied, or subtracted    console.log(&#x27;a&#x27; - 1);    // NaN meaningless result, hence NaN==NaN is false!!!    console.log(NaN == NaN)    // check its value if it&#x27;s NaN or Not    console.log(Number.isNaN(NaN));    console.log(Number.isNaN(&#x27;1&#x27;));    console.log(Number.isNaN(&#x27;a&#x27;));    console.log(Number.isNaN(1));    // isNaN check if the parameter is number or not    // not a number    console.log(isNaN(&#x27;1&#x27;));    console.log(isNaN(&#x27;a&#x27;));    console.log(isNaN(1));&#125;sepcial_number()

special number: Infinity and -Infinity,NaN are numbers
NaN: any number of other numeric operations that don not yield a meaningful result.
Infinity
-Infinity
number
NaN
NaN
NaN
false
true
false
false
false
false
true
false

StringSting index starts with 0, not support -1 as the last index.
There are two ways to create a string, one uses string primitive, the other uses String object.
let s_hi = &quot;Hello&quot;;let s_hi_o = new String(&quot;Hello&quot;)//The String object is a wrapper around the string primitive data type
But you can call any of the methods of the String object on a string literal value—JavaScript automatically converts the string literal to a temporary String object, calls the method, then discards the temporary String object
strings are immutable array-like objects, you can’t change individual characters.
There is another big feature for string is template string, you access var from string directly, more like Bash, it’s easy to create complex string by accessing var.
let name = &quot;jason&quot;;console.log(&quot;hi $&#123;name&#125;&quot;);let msg = &quot;hi $&#123;name&#125;&quot;;consolg.log(msg);
run js from string
function eval_demo() &#123;    eval(&#x27;var x = 10;&#x27;);    console.log(x);&#125;eval_demo();


function string_demo() &#123;    //convert 100 to string    let s_n = String(100);    console.log(s_n, &quot;hi number &quot; + 100);    let s_hi = &quot;9hello, boy boy &quot;;    s_hi[0] = &quot;B&quot;; //no effect, but no exception.    // last index: -1, not supported by function!!! s_hi[-1] not supported!!!    console.log(s_hi, s_hi[0], s_hi.length,                // auto-box happens                &#x27;\n&#x27; + s_hi.indexOf(&quot;ll&quot;), //check substring index                &#x27;\n&#x27; + s_hi.slice(1, -1),  //slice, from index 1 to index -1                &#x27;\n&#x27; + s_hi.substr(1, 2),  //from index 1, include 2 bytes                &#x27;\n&#x27; + s_hi.toUpperCase(),                &#x27;\n&#x27; + s_hi.replace(&quot;ll&quot;, &quot;xx&quot;),//replace substring, support regular pattern                &#x27;\n&#x27; + s_hi.startsWith(&quot;he&quot;),                &#x27;\n&#x27; + s_hi.endsWith(&quot;xx&quot;),                &#x27;\n&#x27; + s_hi.includes(&quot;ll&quot;),                &#x27;\n&#x27; + s_hi.split(&quot;,&quot;),                &#x27;\n&#x27; + s_hi.trim(),//trim tab, space from start and end.                &#x27;\n&#x27; + s_hi.repeat(3)               ); //repeat string&#125;string_demo()

100 hi number 100
9hello, boy boy  9 16
3
hello, boy boy
he
9HELLO, BOY BOY
9hexxo, boy boy
false
false
true
9hello, boy boy
9hello, boy boy
9hello, boy boy 9hello, boy boy 9hello, boy boy

some String  APIs support regexwhen use with regex with such API, patter must be written in way /pattern/, not &quot;pattern&quot;.

search: check if match return -1 if no match
match:  got the matched value, return null if no match
replace: repalce matched part, return a new string

match: return an object that looks like an array, if no matched, null returned

index: the start matched index
[0]: the full matched string
[1]…[n]: the substring matched in ()

function string_regex_api() &#123;    let s_hi = &quot;9hello, boy boy &quot;;    console.log(s_hi.search(/[0-9]h/g), s_hi.search(/[0-9]b/g));    // The $1 and $2 in the replacement string refer to the parenthesized groups in    // the pattern. $1 is replaced by the text that matched against the first group, $2    // by the second, and so on, up to $9. The whole match can be referred to with $&amp;.    console.log(s_hi.replace(/(hello), (boy)/g, &#x27;$2, $1&#x27;));//NOT &#x27;/boy/g&#x27;    console.log(s_hi.replace(&#x27;boy&#x27;, &#x27;girl&#x27;)); // only first replace without pattern    let matched = s_hi.match(/9([a-z]*)/);    console.log(matched.index, matched[0], matched[1]);    //dynamic pattern, must create RegExp explictly with dynamic value.    let pattern = &#x27;9[a-z]&#x27;;    let reg = new RegExp(pattern);    console.log(s_hi.match(reg)[0])&#125;string_regex_api()

0 -1
9boy, hello boy
9hello, girl boy
0 9hello hello
9h

ArrayArray element can have different types, but most time, it stores same type.
Like string, there are two ways to create an array, but both created object
let array_p = [&#x27;a&#x27;, 2];let array_o = new Array([&quot;a&quot;, 2]);//let array_o = new Array();array_o[2] = &quot;c&quot;;
Array supports method same like Python

pop(), push(elm) at end
shift(), unshift(elm) at head
reverse, sort
join
find: return the first element, filter use callback for testing, return a new filtered array.
indexOf&#x2F;lastIndexOf: return index or last of the element, -1 if not found
includes: like indexOf, but return true or false.
forEach, map, reduce, filter always use this if possible to avoid for loop
forEach,map, reduce, filter provides second argument(thisArgs) used as this by callback handler


slice(subarray, copy array if no parameter), concat(combine two arrays): new array is returned, orignal unchanged
some&#x2F;every: return true if all&#x2F;some pass test.
Array.isArray(obj): return true if an arry

check an element in an array with several ways

for to loop every element
indexOf
includes

NOTE: index from 0 and element can be any type, not same
function check_array_type() &#123;    // array is an object with constructor == Array    var arr = [];    console.log(typeof arr);    if (typeof arr == &#x27;object&#x27; &amp;&amp; arr.constructor == Array) &#123;       console.log(&#x27;var arr is an array&#x27;);    &#125;&#125;check_array_type()function array_demo() &#123;    function is_number(val) &#123;        return !isNaN(val);    &#125;    let array_p = [1, &#x27;a&#x27;, 2];    array_p.push(3); //add one at tail    array_p.shift(); //remove one from head    console.log(array_p,                //auto-box                array_p.concat([3, 4, 5]),                array_p.slice(),  //copy array                array_p.slice(1),                array_p.slice(1,2), //sub array index from [1, 2)                array_p.toString(),                array_p.find(is_number), //first number in array                array_p.indexOf(3),                array_p.indexOf(4),                array_p.includes(3),                array_p.filter(is_number),                array_p.join(&#x27;;&#x27;));    // good way to use but can not break at some point    array_p.forEach(function(elm) &#123;        console.log(elm);    &#125;)    //better new way to iterator array, can break at some point    for(let elm of array_p) &#123;        console.log(elm);    &#125;    for(let elm in array_p) &#123;        console.log(elm); //print index 0, 1, 2    &#125;    //map, reduce, filter    let array_n = [1, 2, 3];    //map takes every element as input and return a value to contruct a new array    console.log(array_n.map((x)=&gt; x+2));    //filter takes every element as input, return true or false, only true element contruct a new array    console.log(array_n.filter((x)=&gt;&#123;if (x&gt;1) return true; else return false;&#125;))    //reduce takes parameter, previous call return value and current element, the final output is last call output    console.log(array_n.reduce((prereturn, x)=&gt;prereturn + x))    console.log(array_n.every(x=&gt;x&gt;2));    console.log(array_n.some(x=&gt;x&gt;2));&#125;array_demo()

object
var arr is an array
[ &#39;a&#39;, 2, 3 ] [ &#39;a&#39;, 2, 3, 3, 4, 5 ] [ &#39;a&#39;, 2, 3 ] [ 2, 3 ] [ 2 ] a,2,3 2 2 -1 true [ 2, 3 ] a;2;3
a
2
3
a
2
3
0
1
2
arrCustom
[ 3, 4, 5 ]
[ 2, 3 ]
6
false
true

delete an element from array


Remove?




An item
array.splice(index, 1)


First item
array.shift()


Last item
array.pop()


What about delete?
Try to avoid delete, causes sparse arrays.


delete do NOT change length and leave empty slot! others NOT
Using delete creates these kinds of holes. It removes an item from the array, but it doesn’t update the length property. This leaves the array in a funny state that is best avoided.
function array_del() &#123;    var array = [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;];    delete array[1];    console.log(array, array.length);    array = [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;];    //first argument is element index!!!, the second is from that index how many elements will be deleted.    array.splice(1, 1);    console.log(array, array.length);&#125;array_del()

[ &#39;a&#39;, &lt;1 empty item&gt;, &#39;c&#39;, &#39;d&#39; ] 4
[ &#39;a&#39;, &#39;c&#39;, &#39;d&#39; ] 3

MAPMap is a litte different with Array, map uses keys as index while Array uses number.
function map_demo() &#123;    let map_o = new Map();    //map_o[&quot;a&quot;] = 1; //this never update size, never use this way!!!    map_o.set(&quot;b&quot;, 2);    //map_o.delete(&quot;b&quot;);//delete map_o[&#x27;b&#x27;]    console.log(map_o,                map_o.size,                map_o.get(&#x27;b&#x27;),                // check key exist                map_o.has(&#x27;b&#x27;));    let i = 0;    let iter = map_o.keys();    while (i &lt; map_o.size) &#123;        let key = iter.next().value;        console.log(&quot;item: %s %s&quot;, key, map_o.get(key));        i++;    &#125;    map_o.forEach(function(value, key) &#123;        //can loop each Map item, but can&#x27;t break/continue somehow!!        console.log(key, value)    &#125;)    //new ways, should we use for map!!!    for(let [k, v] of map_o) &#123;        console.log(k, v);    &#125;    for(let k of map_o.keys()) &#123;        console.log(k);    &#125;&#125;map_demo();

Map &#123; &#39;b&#39; =&gt; 2 &#125; 1 2 true
item: b 2
b 2
b 2
b

EnumThere is no Enum in javacript at all, use object to simulate it
function enum_demo() &#123;    const size = &#123;SMALL: 0, MEDIUM: 1, LARGE: 2&#125;;    var count = [];    count[size.SMALL] = 1;    count[size.MEDIUM] = 2;    count[size.LARGE] = 3;    console.log(count)&#125;enum_demo()

[ 1, 2, 3 ]

Functionarguments
If you pass too many, the extra ones are ignored. If you pass too few, the missing parameters get assigned the value undefined, but you can access all arguments by arguments.
function test_arg(a, b, c = 12)&#123;    console.log(a, b, c,);    for ( let i = 0; i &lt; arguments.length; i++) &#123;        console.log(arguments[i]);    &#125;&#125;test_arg(1);/*1 undefined 121*/test_arg(1, 2, 4, 5);/*1 2 41245*/

Function closureThis feature—being able to reference a specific instance of a local binding inan enclosing scope—is called closure. A function that references bindings fromlocal scopes around it is called a closure
var f2 = (function()&#123;        var counter = 1;        return function()&#123; return counter++;&#125;;&#125;)()// we calls first function(local counter =1) which can be accessed by inner function// after return, the local counter is not freed at all as f2 still have a reference to it!!!//counter as a global to f2f2(); //we call the second functionf2(); //we call the second function

Another case
function makeAdder(a) &#123;  return function(b) &#123;    return a + b;  &#125;;&#125;var add5 = makeAdder(5);add5(6); //

when makeAdder() is called, a scope object is created with one property: a, which is the argument passed to the makeAdder() function. makeAdder() then returns a newly created function. Normally JavaScript’s garbage collector would clean up the scope object created for makeAdder() at this point, but the returned function maintains a reference back to that scope object. As a result, the scope object will not be garbage-collected until there are no more references to the function object that makeAdder() returned.
Arrow functionAs js supports annymous function(a function without name), more over, you can even skip function keyword for annymous function, that’s arrow function =&gt;, like annymous function, you can use arrow function as a callback directly, or save it as function object, call it later on
There are severls rule to define an arrow function.
parameter rule

If there are 0 Parameters, arrow functions use empty parenthesis: () &#x3D;&gt; { statements; }
If there is 1 Parameter, arrow functions can omit the parenthesis around the parameter: parameter &#x3D;&gt; { statements }
If there are 2+ Parameters, parameters go inside parenthesis: (param1, param2, …) &#x3D;&gt; { statements }

return rule

If an arrow function is simply returning a single line of code, you can omit the statement brackets and the return keyword.
if you arrow function does NOT return, say just long or printing, you can NOT omit brackets, that means brackets is not omitted in most case, with brackents if you want to return a value, you must use return keyword, otherwise, undefined is returned.

Definition and Callfunction fun_demo() &#123;    var arrow1 = name=&gt;name;    var arrow2 = ()=&gt;&#123;console.log(&quot;hi&quot;);&#125; //no return undefined returned    var arrow3 = (a, b)=&gt;a+b;    var arrow4 = (a, b)=&gt;&#123;a+b;&#125;; //undefined return as with brackets, need a return value, must use `return`    var arrow5 = (a, b)=&gt;&#123;return a+b;&#125;;    console.log(arrow1(&quot;jason&quot;));    console.log(arrow2()); //print &#x27;hi&#x27; return undefined    console.log(arrow3(1,2))    console.log(arrow4(1,2))    console.log(arrow5(1,2))&#125;fun_demo()

jason
hi
undefined
3
undefined
3

Class&#x2F;Object&#x2F;objectobjectThere are two ways to declare a class ES5 way, ES6(ES2015), object is light weighted Object, object no built-in properties and methods, it’s more effecient thant Object, object behaves like a Map, key:value pairs, but it’s not a map hence object.get(&#39;a&#39;) will report error as object no built-in stuff.

object key must be string, but Map key can be anything.

Object.assign() copies the values (of all enumerable own properties) from one or more source objects to a target object. It has a signature of Object.assign(target, …sources).


The target object is the first parameter and is also used as the return value.

Object.assign() is useful for merging objects or cloning them shallowly.

property
let a = 1, b = 2;// shorthand to create an objectlet ob1 = &#123;a, b&#125;; // same as let ob1=&#123;&#x27;a&#x27;:1, &#x27;b&#x27;:2&#125;;let ob1 = &#123;    name: &#x27;jason&#x27;,    &#x27;your name&#x27;: &#x27;jason&#x27; //&#x27;&#x27; is required as &#x27;your name&#x27; has space&#125;;// two ways to access name property.console.log(ob1.name, ob1[&#x27;name&#x27;]);// only one way for &#x27;your name&#x27; property.console.log(ob1[&#x27;your name&#x27;]);// add one propertyob1.age = 10;// merge two objects(ob1 is returned and modified, the key conflicts, the last source winsObject.assign(ob1, &#123;age: 100, id: 100&#125;);console.log(ob1);// clone an objectlet clone_ob = Object.assign(&#123;&#125;, ob1);console.log(clone_ob);


function object_demo() &#123;    // object instance of it&#x27;s own property and functions    let obj = &#123;        &quot;your name&quot;: &quot;jason&quot;, // NOT ; but , &quot;&quot; is required        id: 16,          // no quote is OK!        get_name_again: function get_name_again() &#123;            return this[&#x27;your name&#x27;];        &#125;,        //shorthand to define a method        get_name() &#123;            return this[&#x27;your name&#x27;];        &#125;,    &#125;    obj.get_name_v2 = function() &#123;        return this[&#x27;your name&#x27;];    &#125;    // two ways to access method    console.log(obj.id, obj.get_name());    console.log(obj.id, obj[&quot;get_name_again&quot;]());    console.log(obj.id, obj.get_name_v2());    for (let key in obj) &#123;        //loop all attributes and its value        console.log(key, obj[key]);    &#125;    let a = 1;    let b = 2;    // shorthand to create an object    let ob1 = &#123;a, b&#125;; // same as let ob1=&#123;&#x27;a&#x27;:1, &#x27;b&#x27;:2&#125;;    console.log(ob1);    let ob2 = &#123;&#x27;a&#x27;: 1, &quot;b&quot;: 2&#125;;    ob2[&#x27;c&#x27;] = 3;    // delete a property    delete ob2.a    console.log(ob2,                ob2.c,                ob2[&#x27;b&#x27;],                 // check key exist                &#x27;b&#x27; in ob2)    //get keys of object, return an array of keys    console.log(Object.keys(ob2));    let ob3 = &#123;a: 1&#125;;    let ob4 = &#123;a: 1&#125;;    // compare object, it compares its address, hence it&#x27;s false.    console.log(ob3 == ob4);&#125;object_demo()

16 jason
16 jason
16 jason
your name jason
id 16
get_name_again [Function: get_name_again]
get_name [Function: get_name]
get_name_v2 [Function]
&#123; a: 1, b: 2 &#125;
&#123; b: 2, c: 3 &#125; 3 2 true
[ &#39;b&#39;, &#39;c&#39; ]
false

ClassES5 class
function Person(name) &#123;    this.name = name;&#125;Person.prototype.get_name = function() &#123;    return this.name;&#125;function Student(name, id) &#123;    Person.call(this, name); // call super class constructor bound to same this as Student.    this.id = id;&#125;// same prototype as parent(extended),hence Student can see get_name as well!!!Student.prototype = Object.create(Person.prototype);Student.prototype.get_id = function() &#123;    return this.id;&#125;Student.prototype.prefix_name = function() &#123;    return &#x27;prefix_&#x27; + this.name;&#125;let s1 = new Student(&#x27;jason&#x27;, 10);console.log(s1.get_name(), s1.prefix_name(), s1.get_id()); // jason prefix_jason 10

With ES6, add keyword like class, construtor, static, super, extends
class Person &#123;    constructor(name) &#123;        this.name = name;    &#125;    // no function keyword    get_name() &#123;        return this.name    &#125;&#125;


function es5_class_demo() &#123;    //constructor function always with UpperCase first letter    function Person(name, id) &#123;        &#x27;use strict&#x27;; // call me without new will throw an error!!        this.name = name;        this.id = id;    &#125;    Person.prototype.get_name = function() &#123;        return this.name;    &#125;    //Outside method defines.    Person.prototype.set_name = function(name) &#123;        this.name = name;    &#125;    // all instances have these, outside the contructor    Person.prototype.age = 10; //static, shared by all instances    let p1 = new Person(&#x27;jason&#x27;, 1);    p1.gender = &#x27;male&#x27;; //only to this instance, add new attribute, do not use this way    let p2 = new Person(&#x27;cob&#x27;, 2);    p2.get_id = function() &#123; //add method to an existing object        return this.id;    &#125;    console.log(p1.get_name());    console.log(p2.name);    console.log(p2.get_id());    p2.set_name(&quot;frank&quot;);    console.log(p2.get_name());    console.log(p2.age);&#125;es5_class_demo()

jason
cob
2
frank
10

function es6_class_demo() &#123;    class Person &#123;        constructor(name) &#123;            this.name = name;        &#125;        get_name() &#123;            return this.name;        &#125;    &#125;    class Student extends Person &#123;        constructor(name, id) &#123;            // call parent constructor!!!            super(name);            this.id = id;            // parent and subclass has the same name property            // actuall the two property are same one!!! on instance object!            // never do this, avoid subclass has same name property with parent!!!            this.name = &#x27;hello&#x27;;        &#125;        get_id() &#123;            return this.id        &#125;        get_prefix_name() &#123;            // call parent&#x27;s function in child            return &#x27;prefix_&#x27; + super.get_name();        &#125;    &#125;    const s1 = new Student(&quot;jason&quot;, 1);    console.log(s1.get_name(), s1.get_id(), s1.get_prefix_name());&#125;es6_class_demo();

hello 1 prefix_hello

loop properties
for…in: which returns all the enumerable properties of an object, regardless of whether they are own properties, or inherited from the prototype chain.
返回对象自身的和继承的可枚举属性（不含 Symbol 属性）。


Object.keys(ob): which returns all own, enumerable properties of an object, an ECMA5 method
返回一个数组，包括自身的（不含继承) 所有可枚举属性（不含 Symbol 属性）。


Object.getOwnPropertyNames(obj): which returns all own properties of an object, both enumerable or not
返回一个数组，包括自身的(不含继承) 所有属性（不含Symbol属性，但包括不可枚举属性）。


Object.getOwnPropertySymbols(obj): which returns all own symbol properties of an object, enumerable or not
返回一个数组。包含对象自身（不含继承）的所有 Symbol 属性。


Reflect.ownKeys(obj) 返回一个数组，包含对象所有的属性(不含继承）&#x3D;&#x3D; 3 + 4 。
obj.hasOwnProperty(prop): check if a property is inherited or actually belongs to that object
obj.propertyIsEnumerable(prop) if a property is enumerable.

use Object.defineProperty() to define non-enumerable property
function loop_properties() &#123;    let get_id = Symbol(&quot;get_id&quot;);    let plain_object = &#123;        id: 12,        name()&#123;return &#x27;plain&#x27;&#125;,        // define a symbol property with [Symbol]        [get_id]() &#123;            return this.id        &#125;    &#125;    //add non-enumerable property, you never see me by for ...in    Object.defineProperty(plain_object, &quot;get_name&quot;, &#123;        value: function() &#123;            return &#x27;get_name&#x27;;        &#125;,        writable: true,        enumerable: false,        configurable: false    &#125;);    for (let attr in plain_object) &#123;        // no get_name printed here as it&#x27;s not enumerable!!!        console.log(attr);    &#125;    let id = Symbol(&quot;id&quot;);    // add a symbol properity to an object.    plain_object[id] = 1000;    // access a symbol function.    console.log(plain_object[get_id]());    console.log(Object.keys(plain_object));    console.log(Object.getOwnPropertyNames(plain_object));    console.log(Object.getOwnPropertySymbols(plain_object));    console.log(Reflect.ownKeys(plain_object));    // plain_object.hasOwnProperty = 10;    // not safe if hasOwnProperty is set    console.log(plain_object.hasOwnProperty(&#x27;id&#x27;));    var hasOwn = &#123;&#125;.hasOwnProperty; //extract or Object.prototype.hasOwnProperty    hasOwn.call(plain_object, &#x27;id&#x27;);&#125;loop_properties()

id
name
12
[ &#39;id&#39;, &#39;name&#39; ]
[ &#39;id&#39;, &#39;name&#39;, &#39;get_name&#39; ]
[ Symbol(get_id), Symbol(id) ]
[ &#39;id&#39;, &#39;name&#39;, &#39;get_name&#39;, Symbol(get_id), Symbol(id) ]
true

function getAllPropertyNames(obj) &#123;  // own, inherited properties, enumerable and non-enumerable!!!  let result = [];  while (obj) &#123;    Object.getOwnPropertyNames(obj).forEach(p =&gt; result.push(p));    result.push(&#x27;#&#x27;);    //get parent prototype!!!    obj = Object.getPrototypeOf(obj);  &#125;  return result;&#125;&#123;    let obj = &#123;      abc: 123,      xyz: 1.234,      foobar: &quot;hello&quot;    &#125;;    console.log(getAllPropertyNames(obj));    // obj1 has no inherited property!!!    let obj1 = Object.create(null);    obj1.abc = 123;    obj1.xyz= 1.234;    obj1.foobar = &quot;hello&quot;;    console.log(getAllPropertyNames(obj1));&#125;

[
  &#39;abc&#39;,              &#39;xyz&#39;,
  &#39;foobar&#39;,           &#39;#&#39;,
  &#39;constructor&#39;,      &#39;__defineGetter__&#39;,
  &#39;__defineSetter__&#39;, &#39;hasOwnProperty&#39;,
  &#39;__lookupGetter__&#39;, &#39;__lookupSetter__&#39;,
  &#39;isPrototypeOf&#39;,    &#39;propertyIsEnumerable&#39;,
  &#39;toString&#39;,         &#39;valueOf&#39;,
  &#39;__proto__&#39;,        &#39;toLocaleString&#39;,
  &#39;#&#39;
]
[ &#39;abc&#39;, &#39;xyz&#39;, &#39;foobar&#39;, &#39;#&#39; ]

SymbolA “symbol” represents a unique identifier, it’s mostly used to avoid conflict, you can image it as a unique string.
// id is a symbol with the description &quot;id&quot;let id1 = Symbol(&quot;id&quot;);  // id1 is not in global registry!!!!let id2 = Symbol(&quot;id&quot;);//They are not equal, but only the description is same for debug meta dataid1 == id2; // false!!!// id is a symbol with the description &quot;id&quot;let id1 = Symbol.for(&quot;name&quot;);  //create if not found in global registry!!let id2 = Symbol.for(&quot;name&quot;);  //check global registry with &#x27;name&#x27; key, return if foundconsole.log(id1 == id2); //truelet id3 = Symbol(&quot;name&quot;);let id4 = Symbol.for(&quot;name&quot;);console.log(id3 == id4); // false
That call checks the global registry, and if there’s a symbol described as key, then returns it, otherwise creates a new symbol Symbol(key) and stores it in the registry by the given key.
Symbols don’t auto-convert to a string, and Symbolic properties do not participate in for..in loop.
function symbol_demo() &#123;    let id1 = Symbol(&quot;id&quot;);    let id2 = Symbol(&quot;id&quot;);    console.log(id1.toString(), id2.toString());    let id = Symbol(&quot;id&quot;);    let name = Symbol(&quot;name&quot;)    let user = &#123; // belongs to third-party code      age: 30,      [name]: &quot;John&quot;,      // MUST use [] to refer to symbol.      //no one else can change the value outside of third-party, it&#x27; safe for library      //without function keyword      [id]()&#123;         return 10;      &#125;,      //with function keyword      get_age: function() &#123;          return this.age;      &#125;    &#125;;    console.log(user[name], user[id]());    //Symbolic properties do not participate in for..in loop.    for(let att in user) &#123;        console.log(att, user[att]);    &#125;    console.log(user.get_age())&#125;symbol_demo()

Symbol(id) Symbol(id)
John 10
age 30
get_age [Function: get_age]
30

Iterator&#x2F;GeneratorIteratorIterator is an object, with next method, it wrapps an object(string, array, map) to make it iterable, so that you can iterate the wrapped object by for/of, most of time, you did not call iterator at all, but now how see how it works.
// String, Array, Map etc has a property Symbol.iterator which returns an Iterator.let okIterator = &quot;OK&quot;[Symbol.iterator]();console.log(okIterator.next());// . &#123;value: &quot;O&quot;, done: false&#125;console.log(okIterator.next());// . &#123;value: &quot;K&quot;, done: false&#125;console.log(okIterator.next());// . &#123;value: undefined, done: true&#125;

Iterator Abstract Interface requiredFirst let simulate an Iterator which has next() method, it returns an object which has two keys value and done
//user is an object.let user = &#123;    age: 10&#125;function makeIterator(array) &#123;    let index = 0;    return &#123;        next() &#123;            return index &lt; array.length?                &#123;value: array[index++], done: false&#125; :                &#123;value: undefined, done: true&#125;;        &#125;    &#125;; /* here return an object instance which has next method */&#125;let iterS = makeIterator([99,88]);console.log(iterS.next());console.log(iterS.next());console.log(iterS.next());let arr1 = [1];let iter = arr1[Symbol.iterator](); /*!get Iterator of array!*/console.log(iter.next())  /* value: 1 done: false */console.log(iter.next())  /* value: undefined, done: true */

Iterator
GeneratorWhen you define a function with function* (placing an asterisk after the word function), it becomes a generator. When you call a generator, it returns an iterator.
Every time you call next on the iterator, the function runs until it hits a yield expression, which pauses it and causes the yielded value to become the next value produced by the iterator. When the function returns (the one in the example never does), the iterator is done.
function generator_demo() &#123;    //Generator!!    function *helloWorldGen()&#123;        let x;        yield &#x27;hello&#x27;;        x = yield  &#x27;world&#x27;;        // called get &#x27;world&#x27; but x undefined if next doesn&#x27;t take parameter for next call!        return x;    &#125;    //generator return an Iterator.    let hw_gen = helloWorldGen();    console.log(hw_gen.next());    console.log(hw_gen.next());    // when next takes parameter, it indicates the last yield express value used internal!    console.log(hw_gen.next(2)); //console.log(hw_gen.next());    for (let elm of helloWorldGen()) &#123;        console.log(elm);    &#125;    let objiter = &#123;        data: [&quot;kk&quot;, &quot;dd&quot;],        //* means a Generator, if you call generator, it returns an Iterator        *[Symbol.iterator]()&#123;            for(let item of this.data) &#123;                //yield like return, but next call still goes here                yield item;            &#125;        &#125;    &#125;;    for (let elm of objiter) &#123;        console.log(elm);    &#125;&#125;generator_demo()

&#123; value: &#39;hello&#39;, done: false &#125;
&#123; value: &#39;world&#39;, done: false &#125;
&#123; value: 2, done: true &#125;
hello
world
kk
dd

Async operationThere are couples of ways of syanc operation, from ES5(callback), ES6(Promise), ES7(async&#x2F;await, really new), most of them just wants to make easier to understand and use, here we go
Promise(ES6)Create Promise by three ways

one is use new call resolve or reject based on condition —&gt;common way
the other two are call resolve() or reject directly

Promise.resolve(&#39;hello&#39;) and Promise.reject(&#39;hello&#39;) set Promise&#39;s state with Resolved or Rejected!
//Basic Implementation of Promise/* 1. each Promise only take one function as its parameter * 2. must provide built-in method, then, resolve, reject * 3. user passed function when create a Promise and call the passed function with built-in resolve, reject methond * 4. built-in then() used to add resolve callbacks and failed callback to proper queue. * 5. built-in resolve and reject called callbacked in the proper queues added by then. * 6. built-in resolve and reject are called by caller at proper time. */function my_promise_demo() &#123;    let MyPromise = function(fn) &#123;      let state = &#x27;pending&#x27;;      let doneList = [];      let failList= [];      this.then = (done ,fail) =&gt; &#123;          switch(state)&#123;              case &#x27;pending&#x27;:                  if(done)                      doneList.push(done);                  if(fail)                      failList.push(fail);                  return this;              // no need to add to the queue as state is settled.              case &#x27;fulfilled&#x27;:                  done();                  return this;              case &#x27;rejected&#x27;:                  fail();                  return this;          &#125;      &#125;      // function keyword can&#x27;t be omitted in function class! can be ignore in class      function tryToJson(obj) &#123;          let value;          try &#123;              value = JSON.parse(obj);          &#125; catch (e) &#123;              value = obj;          &#125;          return value      &#125;      //user calls resolve and rejct with proper value      //then resolve passed such value to done callbacks added by then()      function resolve(newValue) &#123;          state = &#x27;fulfilled&#x27;;          setTimeout(() =&gt; &#123;              let value = tryToJson(newValue);              for (let i = 0; i &lt; doneList.length; i++)&#123;                  //call done hander added by then                  let temp = doneList[i](value);                  if (temp instanceof MyPromise) &#123;                      let newP = temp;                      //done handler returns a new Promise.                      for (i++; i &lt; doneList.length; i++) &#123;                          //add leftlist to new Promise                          newP.then(doneList[i], failList[i]);                      &#125;                  &#125; else &#123;                      //the previous done return value as the parameter for next done handler.                      value = temp;                  &#125;              &#125;          &#125;, 0);      &#125;      function reject(newValue) &#123;          state = &#x27;rejected&#x27;;          setTimeout(() =&gt; &#123;              let value = tryToJson(newValue);              let tempRe = failList[0](value);              if(tempRe instanceof MyPromise)&#123;                  let newP = tempRe;                  for (i=1;i&lt;doneList.length;i++) &#123;                      newP.then(doneList[i],failList[i]);                  &#125;              &#125; else &#123;                  //如果不是promise，执行完当前的fail之后，继续执行doneList                  value = tempRe;                  doneList.shift();                  failList.shift();                  resolve(value);              &#125;          &#125;, 0);      &#125;      fn(resolve,reject);    &#125;    MyPromise.prototype.catch = function(onRejected) &#123;      //catch is just another .then with undefined as input      return this.then(undefined, onRejected)    &#125;    let promiFn = (resolve, reject) =&gt; &#123;      setTimeout(() =&gt; &#123;          resolve(2);      &#125;, 0)    &#125;;    let ps = new MyPromise(promiFn);    ps.then(d =&gt; &#123;      console.log(&quot;resolve:&quot;, d)    &#125;).catch(e =&gt; &#123;      console.log(&quot;error:&quot;, e);    &#125;)&#125;my_promise_demo();

resolve: 2

standard promise API
catch(): registers a handler to be called when the promise is rejected, this callback can also register by .then(success, fail) as second parameter.
.then(success, fail): register success or fail, It returns another promise, which resolves to the value that the handler function returns or, if that returns a promise, waits for that promise and then resolves to its result

function promise_demo() &#123;    let p = new Promise((resolve, reject) =&gt; &#123;      if(1) &#123;        resolve(&quot;resolve&quot;);      &#125; else &#123;        reject(&quot;reject&quot;);      &#125;    &#125;);    //then takes two parameters, success, fail, most of time, we did not set fail, but use catch as well.    p.then(data =&gt; &#123;      console.log(data);    &#125;);    let promise1 = new Promise(function(resolve)&#123;      resolve(2);    &#125;);    promise1.then(function(value)&#123;      return value * 2;    &#125;).then(function(value)&#123;      return value * 2;    &#125;).then(function(value)&#123;      console.log(`$&#123;value&#125;`);    &#125;);    // for each then, it will return a new Promise, and pass the return value to next then    //Promise.all可以接受一个元素为Promise对象的数组作为参数，    //当这个数组里面所有的promise对象都变为!!!resolved or rejected!时，该方法才会返回    //返回值 是所有的Promise的传递的值的数组`)    let p1 = new Promise(function(resolve)&#123;      setTimeout(function()&#123;          resolve(1);      &#125;,300);    &#125;);    let p2 = new Promise(function(resolve)&#123;      setTimeout(function()&#123;          resolve(2);      &#125;,100);    &#125;);    Promise.all([p1,p2]).then(function(value)&#123;      console.log(value); // 打印[1,2]    &#125;);    //Promise.race的含义是只要有一个promise对象进入Resloved或者Rejected状态的话，    //程序就会停止，且会继续后面的处理逻辑；    // `delay`毫秒后执行resolve    function timerPromise(delay)&#123;      return new Promise(function(resolve)&#123;          setTimeout(function()&#123;              console.log(&#x27;set timeout&#x27;,delay);              resolve(delay);          &#125;,delay);      &#125;);    &#125;    // 任何一个promise变为resolve或reject 的话程序就停止运行    Promise.race([      timerPromise(1),      timerPromise(32),      timerPromise(64),      timerPromise(128)    ]).then(function (value) &#123;      console.log(&#x27;reslove timeout&#x27;,value);    // =&gt; 1    &#125;);&#125;promise_demo();

resolve
8
set timeout 1
reslove timeout 1

async&#x2F;await
async ensures that the function returns a promise, and wraps non-promises in it, nothing to do for promise return, console.log(async_function()), it shows promise!!!
await makes JavaScript wait until that promise settles and returns its result
If it’s an error, the exception is generated — same as if throw error were called at that very place.
Otherwise, it returns the resolved result not promise.



Note:

await only works inside an async function, await won’t work in the top-level code as the top-level is not a async function.

await literally makes JavaScript wait until the promise settles, and then go on with the result. that doesn’t cost any CPU resources, because the engine can do other jobs in the meantime: execute other scripts, handle events.


async&#x2F;await function no longer, like a regular JavaScript function, runs from start to completion in one go. Instead, it can be frozen at any point that has an await, and can be resumed at a later time
async&#x2F;await aims to replace Promise.then().catch which is hard to understand
typical use for async&#x2F;wait
async function f() &#123;  try &#123;    let response = await fetch(&#x27;/no-user-here&#x27;);    //json() is async as well! need await    let user = await response.json();  &#125; catch(err) &#123;    // catches errors both in fetch and response.json    alert(err);  &#125;&#125;f();// wait for the array of resultslet results = await Promise.all([  fetch(url1),  fetch(url2),  ...]);

Guide
When we use async/await, we rarely need .then, because await handles the waiting for us. And we can use a regular try..catch instead of .catch. That’s usually (but not always) more convenient
function async_wait_demo() &#123;    async function solve() &#123;        return 1;//wrapper to Promise.resolve(1);    &#125;    async function reject() &#123;        return Promise.reject(2);//no wrapper, as it&#x27;s a promise    &#125;    async function test() &#123;        let s = solve();        let svalue = await s;        console.log(svalue);        try &#123;            let rvalue = await reject();            console.log(rvalue);        &#125; catch(err) &#123;            //err has the value 2            console.log(err);        &#125;        //async always return a Promise(wrapper or not)        return 20;    &#125;    test()    //test().then(data=&gt;console.log(data)); is fine!! should we use it this way?&#125;async function retun_is_promise() &#123;    return 1;&#125;console.log(retun_is_promise());//get resolved promise valueretun_is_promise().then(data=&gt;console.log(&quot;resolved data: &quot;, data));async_wait_demo();

Promise &#123; 1 &#125;

spread destructing operatorThese are special cases for Array and object, shorthand.
function operator_demo() &#123;    //spread using ...    let array1 = [1, 2];    // let array2 = array1.concat(3);    let array2 = [...array1, 3];    console.log(array2);    let ob1 = &#123;&quot;a&quot;: 1&#125;;    //spread using ...    let ob2 = &#123;...ob1, &quot;b&quot;:2&#125;    console.log(ob2)    function rest(a, ...rest) &#123;        console.log(rest);    &#125;    rest(1, 2, 3); //the rest parameters as an array    //destructing, the left, should no more than right, the right is iterable!!!    let [as, bs, cs] = &quot;str&quot;    console.log(as, bs, cs);    let [a, b, c=3] = [1, 2, 4, 5]; // c default is overwritten    console.log(a, b, c);    //other store left attributes with new object    //the var name must be same as key as object has no order,    // you need to which value you want to destruct on left side    let &#123;oa, ob, ...other&#125; = &#123;&quot;oa&quot;:1, &quot;ob&quot;:2, &quot;oc&quot;:3&#125;;    //other is new object with &#123;&quot;oc&quot;: 3&#125;    console.log(oa, ob, other);    function dest(d, [a, b, c=3]) &#123;        console.log(a, b, c);    &#125;    dest(4, [1, 2]);    //must have &#123;&#125; in parameter and pass an object with same key    //a rename a1, c is object, take all left.    function mdest(d, &#123;a:a1, b, ...c&#125;) &#123;        console.log(a1, b, d, c);    &#125;    function adest(d, obj) &#123;        console.log(obj.a, obj.b, d, obj.c);    &#125;    mdest(3, &#123;&quot;a&quot;:1, &quot;b&quot;:2, &quot;c&quot;: 3, d: 4&#125;);    adest(3, &#123;&quot;a&quot;:1, &quot;b&quot;:2, &quot;c&quot;: 3&#125;);&#125;operator_demo()

resolved data:  1
1
2
set timeout 32
[ 1, 2, 3 ]
&#123; a: 1, b: 2 &#125;
[ 2, 3 ]
s t r
1 2 4
1 2 &#123; oc: 3 &#125;
1 2 3
1 2 3 &#123; c: 3, d: 4 &#125;
1 2 3 3

ModulesES5ES5 commonJS way used by node
//one symbol(default)//add.jsfunction add(a, b) &#123;  return a + b;&#125;//defaultmodule.exports = add;//test.js//rename import symbolvar add_two = require(&quot;./add&quot;)var add = require(&quot;./add&quot;)console.log(add(1, 2));console.log(add_two(1, 2));

//several symbols//op.jsfunction add_two(a, b) &#123;  return a + b;&#125;function sub(a, b) &#123;  return a - b;&#125;//rename export symbolsmodule.exports.add = add_two;module.exports.sub = sub;//test.jsvar &#123;add, sub&#125; = require(&#x27;./op&#x27;)console.log(add(1, 2));var sub_two = require(&#x27;./op&#x27;).sub;console.log(sub_two(2, 1));

ES6ES5 has severals way to export and import for a module while ES6 defines a standard way, here we go.It only loads the required at compiling, you can load some exported symbols or all.
import &#123;stat as fileStat, readFile&#125; from &#x27;fs&#x27;;import * as fs from &#x27;fs&#x27;        //load all exported symbols!//fs.count, fs.stat, fs.readFile, fs.number

export ways
//fs.jsexport let count = 0;export function stat() &#123;&#125;export function readFile() &#123;&#125;//Or this waylet count = 0;function stat() &#123;&#125;function readFile()&#123;&#125;// with this way can rename when export, Better Way!!!export &#123;stat, readFile, count as number&#125;;

default exportEach module can only has one default export which has no export name(actuall it’s default name)
//fs.js//no function nameexport default function() &#123;&#125;//use module name as default name, it&#x27;s common way.import fs from &#x27;fs&#x27;;import fs as renamed_stat from fs;import &#123;default as renamed_stat from fs&#125;;

Node finding module# js search module in such path order(current, global)# make sure npm or yarn add module at proper path(base) root@dev:~/shared/github# node&gt; module.paths[ &#x27;/root/windows/shared/github/repl/node_modules&#x27;,  &#x27;/root/windows/shared/github/node_modules&#x27;,  &#x27;/root/windows/shared/node_modules&#x27;,  &#x27;/root/windows/node_modules&#x27;,  &#x27;/root/node_modules&#x27;,  &#x27;/node_modules&#x27;,  &#x27;/root/.node_modules&#x27;,  &#x27;/root/.node_libraries&#x27;,  &#x27;/root/anaconda3/lib/node&#x27; ]# check yarn/npm global module dir$ yarn global dir$ npm ls -g --depth 0

Low-leve API for bufferIn some case, you may want to manipulate on buffer(bytes level) like in network programming, JS supports this by providing low-level API Binary buffer and DataView.
for in vs for of
The for…in statement iterates over the enumerable properties(include inherited) of an object, in an arbitrary order different js engine may produce different result, it used to check attribute(key) of object, the each key is a string!!!

The for…of statement iterates over values that the iterable object(string, array, map) defines to be iterated over, get the value of each iterator object like string, array, map!


Be Careful

Make sure not to modify an object while enumerating its properties with a for...in loop.
Use a while loop or classic for loop instead of a for…in loop when iterating over an object whose contents might change during the loop.

function for_in_of_demo() &#123;    Array.prototype.arrCustom = function() &#123;&#125;;    const iterable = [3, 5, 7];    iterable.foo = &#x27;hello&#x27;;    for (const i in iterable) &#123;      console.log(typeof i, i); //0, 1, 2, &quot;foo&quot;, &quot;arrCustom&quot;      // ther order is not guaranteed, it depends on js engine.    &#125;    for (const i in iterable) &#123;      // arrCustom is not peoprity but prototype(method)      if (iterable.hasOwnProperty(i)) &#123;        console.log(i); // logs 0, 1, 2, &quot;foo&quot;      &#125;    &#125;    for (const i of iterable) &#123;      console.log(i); // logs 3, 5, 7    &#125;&#125;for_in_of_demo()

string 0
string 1
string 2
string foo
string arrCustom
0
1
2
foo
3
5
7

let vs const vs varlet and const declarations allow you to create block-scoped variablesA variable declared with the var keyword is available from the function it is declared in.
// myVarVariable *is* visible out herefor (var myVarVariable = 0; myVarVariable &lt; 5; myVarVariable++) &#123;  // myVarVariable is visible to the whole function&#125;// myVarVariable *is* visible out hereif (true) &#123;    var name = &#x27;Matt&#x27;;    console.log(name); // Matt&#125;console.log(name); // Matt
var specialwithout declaration to access a variable, a global variable is defined after that access, it&#39;s valid non strict mode even it’s not good way to do so.
var Declaration HoistingThe interpreter pulls all variable declarations to the top of its scope. It also allows you to use redundant var declarations without penalty.
function foo() &#123;    console.log(age);    var age = 26; // you not see age value until here    inner(); // you can see inner here as function declaration always lift up!!!    function inner() &#123;        console.log(&#x27;inner&#x27;);    &#125;&#125;foo(); // undefined
equal to below
function foo() &#123;    var age;    console.log(age);    age = 26;&#125;foo(); // undefined


function test() &#123;    // &quot;use strict&quot;;    b = 15;    console.log(b);    console.log(age); // undefined as all var declarations are pulled up to begining of function.    var age = 26;&#125;test()console.log(b)

15
undefined
15
set timeout 64
set timeout 128
[ 1, 2 ]

letlet is not hoisted
console.log(age); // ReferenceError: age is not definedlet age = 26

A let declaration also does not allow for any redundant declarations within a block scope. Doing so will result in an error:
var name;var name;let age;let age; // SyntaxError; identifier &#x27;age&#x27; has already been declared

constfor (const i = 0; i &lt; 10; ++i) &#123;&#125; // TypeError: assignment to constant variablefor (const key in &#123;a: 1, b: 2&#125;) &#123;console.log(key);&#125;// a, bfor (const value of [1,2,3,4,5]) &#123;console.log(value);&#125;// 1, 2, 3, 4, 5

Ref
Reference

]]></content>
      <categories>
        <category>html</category>
        <category>js</category>
      </categories>
      <tags>
        <tag>javascript</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_components</title>
    <url>/2020/06/28/k8s-components/</url>
    <content><![CDATA[KubernetesPodA pod is a group of one or more tightly related containers that will always run together on the same worker node and in the same Linux namespace(s). Each pod is like a separate logical machine with its own IP, hostname, processes, and so on, running a single application.


when deciding whether to put two containers into a single pod or into two separate pods, you always need to ask yourself the following questions:Two container in a single pod?

Do they need to be run together or can they run on different hosts?
Do they represent a single whole or are they independent components?
Must they be scaled together or individually?

Static podStatic pod is a kind of pod created before scheduler&#x2F;api server starts by kubelet, kubelet scans &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F; which is the place for static pod description.
Note: static pod which only runs on master node(s)
$ ls -al /etc/kubernetes/manifests/total 24drwxr-xr-x 2 root root 4096 Jun  2 13:36 .drwxr-xr-x 4 root root 4096 Jun  2 13:36 ..-rw------- 1 root root 2172 Jun  2 13:36 etcd.yaml-rw------- 1 root root 3714 Jun  2 13:36 kube-apiserver.yaml-rw------- 1 root root 3384 Jun  2 13:36 kube-controller-manager.yaml-rw------- 1 root root 1426 Jun  2 13:36 kube-scheduler.yaml# static pod which only runs on master node(s)!!!$ kubectl get pod -n kube-systemNAME                                              READY   STATUS    RESTARTS   AGEetcd-linux-node1.example.com                      1/1     Running   0          114mkube-apiserver-linux-node1.example.com            1/1     Running   0          114mkube-controller-manager-linux-node1.example.com   1/1     Running   0          114mkube-scheduler-linux-node1.example.com            1/1     Running   0          114m

labelsOrganizing pods and all other Kubernetes objects are done through labels, labels are a simple, yet incredibly powerful Kubernetes feature for organizing not only pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors (resources are filtered based on whether they include the label specified in the selector).
A resource can have more than one label, as long as the keys of those labels are unique within that resource. You usually attach labels to resources when you create them, but you can also add additional labels or even modify the values of existing labels later without having to recreate the resource.
A label selector can select resources based on (key, value, equal, not equal) whether the resource

Contains (or doesn’t contain) a label with a certain key
Contains a label with a certain key and value
Contains a label with a certain key, but with a value not equal to the one you specify

AnnotationsA great use of annotations is adding descriptions for each pod or other API object, it is key-value pair, but not used as selector!
liveness probe(check container healthy)Without health check(liveness probes), as long as the process is still running, Kubernetes will consider the container to be healthy, even your application goes into deadlock or infinite loop, K8s will NOT restart it.
Kubernetes can check if a container is still alive through liveness probes, you can specify a liveness probe for each container in the pod’s specification. Kubernetes will periodically execute the probe and restart the container if the probe fails.
Kubernetes can probe a container using one of the three mechanisms:

An HTTP GET probe performs an HTTP GET request on the container’s IP address, a port and path you specify. If the probe receives a response, and the response code doesn’t represent an error (in other words, if the HTTP response code is 2xx or 3xx), the probe is considered successful. If the server returns an error response code or if it doesn’t respond at all, the probe is considered a failure and the container will be restarted as a result.

A TCP Socket probe tries to open a TCP connection to the specified port of the container. If the connection is established successfully, the probe is successful. Otherwise, the container is restarted.

An Exec probe executes an arbitrary command inside the container and checks the command’s exit status code. If the status code is 0, the probe is successful. All other codes are considered failures.


readiness probe(ready to handle request)The readiness probe is invoked periodically and determines whether the specific pod should receive client requests or not.
Three types of readiness probes exist:

An Exec probe, where a process is executed. The container’s status is determined by the process’ exit status code.
An HTTP GET probe, which sends an HTTP GET request to the container and the HTTP status code of the response determines whether the container is ready or not.
A TCP Socket probe, which opens a TCP connection to a specified port of the container. If the connection is established, the container is considered ready.

When a container is started, Kubernetes can be configured to wait for a configurable amount of time to pass before performing the first readiness check. After that, it invokes the probe periodically and acts based on the result of the readiness probe. If a pod reports that it is not ready, it is removed from the service(endpoint). If the pod then becomes ready again, it is re-added to service endpoint.
Readiness vs LivenessUnlike liveness probes, if a container fails the readiness check, it will not be killed or restarted. This is an important distinction between liveness and readiness probes.Liveness probes keep pods healthy by killing off unhealthy containers and replacing them with new, healthy ones, whereas readiness probes make sure that only pods that are ready to serve requests receive them.
If you don’t add a readiness probe to your pods, they’ll become service endpoints almost immediately. If your application takes too long to start listening for incoming connections, client requests hitting the service will be forwarded to the pod while it’s still starting up and not ready to accept incoming connections. Clients will therefore see “Connection refused” types of errors before it’s ready.
volumeMostly a volume is bound to the lifecycle of a pod and will stay in existence only while the pod exists, but depending on the volume type, the volume’s files may remain intact even after the pod and volume disappear, here are list of volume types.

emptyDir—A simple empty directory used for storing transient data, deleted when pod is gone.
hostPath—Used for mounting directories from the worker node’s filesystem into the pod.
gitRepo—A volume initialized by checking out the contents of a Git repository.
nfs—An NFS share mounted into the pod.
persistentVolumeClaim—A way to use a pre or dynamically provisioned persistent storage
others by cloud provider

sharing data between multiple containers in a pod by emptyDir
The volume starts out as an empty directory, the app running inside the pod can then write any files it needs to it, because the volume’s lifetime is tied to that of the pod, the volume contents are lost when the pod is deleted
The emptyDir you used as the volume was created on the actual disk of the worker node hosting your pod, so its performance depends on the type of the node’s disks, but you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory instead of on disk). To do this,set the emptyDir’s medium to Memory like this:
volumes:- name: html  emptyDir:    medium: Memory

share files between host and pod(in the host) by hostPath
If a pod is deleted and the next pod uses a hostPath volume pointing to the same path on the host, the new pod will see whatever was left behind by the previous pod, but only if it’s scheduled to the same node as the first pod, it’s persistent, each pod shares files with node that it’s scheduled to, if pod is deleted and recreated on another node, it does NOT see the previous content as the node is different. this type of volume is useful for DaemonSet pod.
volumes:- name: ca-certs  hostPath:    path: /etc/ssl/certs    # create if dir not exist    # type: FileOrCreate    type: DirectoryOrCreate

share files across nodes by NFS or cloud provider method
volumes:- name: data  nfs:    server: 1.2.3.4    path: /some/path
Persistentvolume and PersistentVolumeClaimAll the persistent volume types above have required the developer of the pod to have knowledge of the actual network storage infrastructure available in the cluster. For example, to create a NFS-backed volume, the developer has to know the actual server the NFS export is located on, we can decouple pods from the underlying storage technology, create a &#39;virtual storage&#39; that takes care of underlying storage technology, let pod uses this &#39;virtual storage&#39;, new resources were introduced. They are Persistentvolumes and PersistentVolumeClaims.
As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume and binds it to the claim, binding is done by Kubernetes not user, you just claim what you wants.

PersistentVolume resources are cluster-scoped and thus cannot be created in a specific namespace, but PersistentVolumeClaims can only be created in a specific namespace, they can then only be used by pods in the same namespace.

here just declare virtual storage disk, mongodb as backend
apiVersion: v1kind: PersistentVolumemetadata:  name: data-pvspec:  capacity:    storage: 1Gi  accessModes:    - ReadWriteOnce    - ReadOnlyMany  persistentVolumeReclaimPolicy: Retain  storageClassName: pv-class # delcare which class I&#x27;m !!!  gcePersistentDisk:    pdName: mongodb    fsType: ext4

apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: data-pvcspec:  storageClassName: pv-class # used to check which PV to use !!!  resources:    requests:      # request 100M from data-pv      storage: 100Mi  accessModes:    - ReadWriteOnce

Use it from pod description
volumes:- name: data  persistentVolumeClaim:    claimName: data-pvc# you need create pv and pvc first when use it in pod like this.

Dynamic provisioning of PersistentVolumesCan K8s create PV for us automatically, user only creates PVC? yes, it’s storageClass, the cluster admin, instead of creating PersistentVolumes, can deploy a PersistentVolume provisioner and define one or more StorageClass objects to let users choose what type of PersistentVolume they want. The users can refer to the StorageClass in their PersistentVolumeClaims and the provisioner will take that into account when provisioning the persistent storage.

Behind Storage class it’s PersistentVolume provisioner who creates PV automatically.!!!
declare StorageClass
apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: fast# auto create pv when needsprovisioner: kubernetes.io/gce-pdparameters:  type: pd-ssd  zone: europe-west1-b

Use storageClass in PVC instead of PV directly
apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: data-pvcspec:  storageClassName: fast  resources:    requests:      storage: 100Mi  accessModes:    - ReadWriteOnce

Use VolumeClaim from pod description
volumes:- name: data  persistentVolumeClaim:    claimName: data-pvc


For all volumes, we need to mount it to pod for use
containers:  - name: kubia    image: luksa/kubia    ports:    - containerPort: 8080    # optional, mount from html to /var/htdocs    volumeMounts:      - name: html        mountPath: /var/htdoc

NamespacesK8s Namespace like a container, limits the scope of the resources, so you can use same resource name in different namespaces, most of resources are namespaced, that means you need to give the namespace name when you list resource, without namespace given, ‘default’ namespace is used, check resource if it’s namespaced by $ kubectl api-resources.
Namespaces allow you to isolate objects into distinct groups, which allow you to operate only on those belonging to the specified namespace, but they don’t provide any kind of isolation of running object, it&#39;s only for viewing
ReplicationReplication is used to create pod with several copies, monitor them, restart them or create new one if fails, make sure the number of running pod equals user desires, it uses label selector to select pod(s).
ReplicationController(deprecated)A ReplicationController’s job is to make sure that an exact number of pods always matches its label selector. If it doesn’t, the ReplicationController takes the appropriate action to reconcile the actual with the desired number.

A ReplicationController has three essential parts:

A label selector, which determines what pods are in the ReplicationController’s scope
A replica count, which specifies the desired number of pods that should be running
A pod template, which is used when creating new pod replicas

Changes to the label selector and the pod template have no effect on existing pods. Changing the label selector makes the existing pods fall out of the scope of the ReplicationController, so the controller stops caring about them.
ReplicaSetIt’s a new generation of ReplicationController, a ReplicaSet behaves exactly like a ReplicationController, but it has more expressive pod selectors, Whereas a ReplicationController’s label selector only allows matching pods that include a certain label, a ReplicaSet’s selector also allows matching pods that lack a certain label or pods that include a certain label key, regardless of its value.
DaemonSetDaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly.
Cases like pods that perform system-level operations. For example, you’ll want to run a log collector and a resource monitor on every node.
Even node can be made unschedulable, preventing pods from being deployed to it. A DaemonSet will deploy pods even to such node, because the **unschedulable attribute is only used by the Scheduler, whereas pods managed by a DaemonSet bypass the Scheduler completely**.

JobJob is similar to the other resources replica, but it allows you to run a pod whose container isn’t restarted when the process running inside finishes successfully. Once it does, the pod is considered complete, in case of failure during running, the job(pod) can be restarted.
In the event of a node failure, the pods on that node that are managed by a Job will be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of the process itself (when the process returns an error exit code), the Job can be configured to either restart the container or not.
By default, Job only runs once successfully, but you can run it more times, each run after another finish, more over, you can run jobs at same time by setting parallelism: 2 to allow run two same jobs at the same time.
CronJobJob resources run their pods immediately when you create the Job resource, but many batch jobs need to be run at a specific time in the future or repeatedly in the specified interval, this is CronJob object. no difference with Job object except when it runs.
Cronjob depends on Job, as Job resources will be created from the CronJob resource at approximately the scheduled time. The Job then creates the pods.

ServiceA Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods(selected by label selector) providing the same service. Each service has an IP address and port that never change while the service exists. no interface for this ip, **ip is virtual just used to create iptable rules**, Clients can open connections to service IP and port, and those connections are then routed to one of the pods(may run on another node) backing that service. randomly selected (or RR selected with ipvs)pod which may or may not be the one running on the node the connection is being made to.
Service does NOT create pod like replication, but uses label selector to select pods(created by rs or deploy) as endpoints. when you create a service with label selector, an EndPoint object is created automatically which holds pod ip lists.
By default service load-balances request by randomly to it’s backend, but you can change its behavior by setting sessionAffinity: ClientIP
ClusterIP type internal serviceFor this kind of service, the service has a fixed cluster IP(auto assigned or manually set), cluster IP means it’s only accessible in the cluster. when such service is created, it only creates iptable nat rule(or ipvs rules), no interface configured with such cluster ip of service.
You have a few ways to make a service accessible externally:

Setting the service type to NodePort—For a NodePort service, each cluster node opens a port on the node itself (hence the name) and redirects traffic received on that port to the underlying service’s endpoint. The service isn’t accessible only at the internal cluster IP and port, but also through a dedicated port on all nodes. you need to know node&#39;s ip to access the service.

Setting the service type to LoadBalancer, an extension of the NodePort type—This makes the service accessible through a dedicated load balancer, provisioned from the cloud infrastructure Kubernetes is running on. The load balancer redirects traffic to the node port across all the nodes. Clients connect to the service through the load balancer’s IP, you only need an load balancer’s ip for accessing the service also NodePort service can be accessed not only through the service’s internal cluster IP, but also through any node’s IP and the reserved node port

Creating an Ingress resource, a radically different mechanism for exposing multiple services through a single public IP address,It operates at the HTTP level (network layer 7) and can thus offer more features than layer 4 services.


node port external service
NodePort service(node ip)—–&gt;Pod(IP)
Even you access one node, the request may be routed to other node which has the service pod, but why we still need loadbalancer service type because when that node fails, your clients can’t access the service anymore, that’s why it makes sense to put a load balancer in front of the nodes to make sure you’re spreading requests across all healthy nodes and never sending them to a node that’s offline at that moment.

loadbalancer type external service
LoadBalancer service—&gt;NodePort service(healthy node)—–&gt;Pod(randomly selected or Client IP Or RR(ipvs) or WRR(ipvs))
Send request to healthy node. it needs cloud provider support!!!

IngressEach service requires its own LoadBalancer with its own public IP address, whereas an Ingress only requires one, even when providing access to dozens of services.
When a client sends an HTTP request to the Ingress, the host and path in the request determine which service the request is forwarded to, to make Ingress resources work, an Ingress controller(pod) needs to be running in the cluster. Different Kubernetes environments use different implementations of the controller.
When you create Ingress resource, actually, you push several lua rules into ingress controller, based on the lua rule, ingress controller sends the request to the proper pod which gets from service definition.
ingress requires clusterIP service as backend, ingress controller sends reqeust to pod directly not sends to clusterIP!!!

configMap, secret and downwardAPIconfigMap and secret are used to pass any data to running container, while downwardAPI is used to pass Kubernetes metadata to running container.
configMapKubernetes allows separating configuration options into a separate object called a ConfigMap, which is a map containing key/value pairs with the values ranging from short literals to full config files, that means the value can be a simple string or content of a file, the contents of the map are passed to containers either as environment variables or files in a volume.

Different between use configMap as env var and volume

env var for short content, while volume for large content.
env var is not updated after container starts, so update configMap, env var no change
volume will be updated if you update configMap

The list of **environment variables also cannot be updated after the pod is created**

secretSecrets are much like ConfigMaps, they’re also maps that hold key-value pairs. They can be used the same way as a ConfigMap.You can

Pass Secret entries to the container as environment variables
Expose Secret entries as files in a volume

secret is for sensitive data secrets are always stored in memory and never written to physical storage. On the master node itself (more specifically in etcd), Secrets used to be stored in decrypted form, which meant the master node needs to be secured to keep the sensitive data stored in Secrets secure.
The contents of a Secret’s entries are shown in different encode(encrypted) formats, whereas those of a ConfigMap are shown in clear text, when you see it by kubectl describe secrets. the showing format is determined by Secret type.

generic secret: Base64-encoded
tls secret: xxx
service-account-token: yyy

When you expose the Secret to a container through a secret volume, the value of the Secret entry is decoded and written to the file in its actual form (regardless if it is plain text or binary). The same is also true when exposing the Secret entry through an environment variable. In both cases, the app doesn’t need to decode it, but can read the file’s contents or look up the environment variable value and use it directly.
downwardAPIKubernetes downwardAPI allows you to pass metadata about the pod and its environment through environment variables or files (in a downwardAPI volume), limits the passed data to running container.

Metadata contains these:

The pod’s name
The pod’s IP address
The namespace the pod belongs to
The name of the node the pod is running on
The name of the service account the pod is running under
The CPU and memory requests for each container
The CPU and memory limits for each container
The pod’s labels
The pod’s annotations

But the metadata exposed is limit, if you need more info about the cluster, talk to the API server directly in the pod, first we need to know restful API before we talk to it, but API server needs authentication, the kubectl proxy command runs a proxy server that accepts HTTP connections on your local machine and proxies them to the API server while taking care of authentication, so you don’t need to pass the authentication token in every request.
DeploymentDeployment aims to upgrade automatically, without deploy, using replicaSet, you need to upgrade(to new image) manually.
With deploy

it will create replicaSet automatically
upgrade automatically at server side
still need create service

upgrade(rolling update way) without deployment
Rolling update: replace old pod one by one with new pod, not replace them at once! it needs two replicaSet for rolling update, old replicaSet scales down while new replicaSet scales up, this could be done by one command kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 kubia-v1 is old replicaSet, kubia-v2 is new will be created after you run such command,
it will do below step by step in client(call API server by kubectl):

create new replicaSet
scale up new replicaSet
scale down old replicaSet

One big issue for this old way is that if you lost network connectivity while kubectl was performing the update, the update process would be interrupted mid-way. Pods and ReplicationControllers would end up in an intermediate state
While compared with deployment, all these actions above are done inside server, no API call, hence if something goes wrong, we can rollback to original state.
A Deployment is a higher-level resource meant for deploying applications and updating them declaratively, instead of doing it through a ReplicationController or a ReplicaSet, which are both considered lower-level concepts.
When you create a Deployment, a ReplicaSet resource is created underneath, the actual pods are created and managed by the Deployment’s ReplicaSets, not by the Deployment directly.
Creating a Deployment isn’t that different from creating a ReplicationController. A Deployment is also composed of a label selector, a desired replica count, and a pod template(like replicaset). In addition to that, it also contains a field, which specifies a deployment strategy that defines how an update should be performed when the Deployment resource is modified.
Default strategy is to perform a rolling update (the strategy is called RollingUpdate, no service down, good way). The alternative is the Recreate strategy, which deletes all the old pods at once and then creates new ones
Recreate strategy causes all old pods to be deleted before the new ones are Recreate created. Use this strategy when your application doesn’t support running multiple versions in parallel and requires the old version to be stopped completely before the new one is started. This strategy does involve a short period of time when your app becomes completely unavailable.
You should use rolling strategy only when your app can handle running both the old and new version at the same time.
Refkubernetes-in-action
]]></content>
      <categories>
        <category>k8s</category>
        <category>overview</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_autoscale</title>
    <url>/2021/06/08/k8s-autoscale/</url>
    <content><![CDATA[IntroductionAutoscaling allows you to dynamically adjust to demand without intervention from the individuals in charge of operating the cluster.
Kubernetes autoscaling helps optimize resource usage and costs by automatically scaling a cluster up and down in line with demand.
Kubernetes enables autoscaling at the cluster/node level as well as at the pod level.


AutoscalerAutoscaling eliminates the need for constant manual reconfiguration to match changing application workload levels. Kubernetes can autoscale by adjusting the capacity (vertical autoscaling) and number (horizontal autoscaling) of pods, and&#x2F;or by adding or removing nodes in a cluster (cluster autoscaling).


There are actually three autoscaling features for Kubernetes: Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and Cluster Autoscaler. Let’s take a closer look at each and what they do.
Horizontal Pod Autoscaler(HPA)Horizontal scaling, which is sometimes referred to as “scaling in&#x2F;out,” allows Kubernetes administrators to dynamically (i.e., automatically) increase or decrease the number of running pods as your application’s usage changes.
A cluster operator declares their target usage for metrics, such as CPU or memory utilization, as well their desired maximum and minimum desired number of replicas,. The cluster will then reconcile the number of replicas accordingly, and scale up or down the number of running pods based on their current usage and the desired target.

Vertical Pod Autoscaler(VPA)VPA refers to adding more resources (such as CPU or memory) to an existing machine. it’s sometimes referred to as “scaling up&#x2F;down”
The cluster operator declares their target usage for metrics, such as CPU or memory utilization. The cluster will then reconcile the size(CPU, Memory) of the service’s pod or pods based on their current usage and the desired target.
Vertical Pod Autoscaler (VPA) frees the users from necessity of setting up-to-date resource limits and requests for the containers in their pods. When configured, it will set the requests automatically based on usage and thus allow proper scheduling onto nodes so that appropriate resource amount is available for each pod. It will also maintain ratios between limits and requests that were specified in initial containers configuration.
It can both down-scale pods that are over-requesting resources, and also up-scale pods that are under-requesting resources based on their usage over time.
Autoscaling is configured with a Custom Resource Definition object called VerticalPodAutoscaler. It allows to specify which pods should be vertically autoscaled as well as if&#x2F;how the resource recommendations are applied.

For each VPA resource,  there are three modes in which VPAs operate:

“Auto”: VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently this is equivalent to “Recreate” (see below). Once restart free (“in-place”) update of pod requests is available, it may be used as the preferred update mechanism by the “Auto” mode. NOTE: This feature of VPA is experimental and may cause downtime for your applications.
“Recreate”: VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendation (respecting the Pod Disruption Budget, if defined). This mode should be used rarely, only if you need to ensure that the pods are restarted whenever the resource request changes. Otherwise prefer the “Auto” mode which may take advantage of restart free updates once they are available. NOTE: This feature of VPA is experimental and may cause downtime for your applications.
“Initial”: VPA only assigns resource requests on pod creation and never changes them later.
“Off”: VPA does not automatically change resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.

VPA does not modify the template in the deployment, but the actual requests of the pods are updated
containers:- name: nginx  image: nginx:1.13.12  ports:  - containerPort: 80  resources:    requests:      memory: &quot;64Mi&quot;      cpu: &quot;250m&quot;    limits:      memory: &quot;128Mi&quot;      cpu: &quot;500m&quot;

enable VPA
# step1: enable metric server https://github.com/kubernetes-incubator/metrics-server# step2: install VerticalPodAutoscaler resource and it&#x27;s controller$ git clone https://github.com/kubernetes/autoscaler.git$ cd autoscaler/vertical-pod-autoscaler$ unset $REGISTRY$ unset $TAG# start VPA controller etc$ ./hack/vpa-up.sh  # openssl must be version 1.1.1 or higher$ kubectl create -f examples/hamster.yaml# The above command creates a deployment with 2 pods, each running a single container that requests 100 millicores and tries to utilize slightly above 500 millicores. The command also creates a VPA config pointing at the deployment. VPA will observe the behavior of the pods and after about 5 minutes they should get updated with a higher CPU request$ kubectl get vpa$ kubectl describe vpa...apiVersion: &quot;autoscaling.k8s.io/v1&quot;                                             kind: VerticalPodAutoscaler                                                     metadata:                                                                         name: hamster-vpa                                                             spec:                                                                             targetRef:                                                                        apiVersion: &quot;apps/v1&quot;                                                           kind: Deployment                                                                name: hamster                                                                 resourcePolicy:                                                                   containerPolicies:                                                                - containerName: &#x27;*&#x27;                                                              minAllowed:                                                                       cpu: 100m                                                                       memory: 50Mi                                                                  maxAllowed:                                                                       cpu: 1                                                                          memory: 500Mi                                                                 controlledResources: [&quot;cpu&quot;, &quot;memory&quot;]        updatePolicy:    updateMode: &quot;Auto&quot; # debug$ kubectl -n kube-system get pods|grep vpa$ kubectl -n kube-system get deploy|grep vpa$ kubectl get customresourcedefinition| grep verticalpodautoscalers# stop using VPA# Note that if you stop running VPA in your cluster, the resource requests for the pods already modified by VPA will not change, but any new pods will get resources as defined in your controllers (i.e. deployment or replicaset) and not according to previous recommendations made by VPA.$ ./hack/vpa-down.sh
Note

Updating running pods is an experimental feature of VPA. Whenever VPA updates the pod resources the pod is recreated, which causes all running containers to be restarted. The pod may be recreated on a different node

Vertical Pod Autoscaler should not be used with the Horizontal Pod Autoscaler (HPA) on CPU or memory at this moment. However, you can use VPA with HPA on custom and external metrics.

VPA recommendation might exceed available resources (e.g. Node size, available size, available quota) and cause pods to go pending. This can be partly addressed by using VPA together with Cluster Autoscaler.

Multiple VPA resources matching the same pod have undefined behavior.


Cluster Autoscaler(CA)HPA and VPA essentially make sure that all of the services running in your cluster can dynamically handle demand while not over-provisioning during slower usage periods. That’s a good thing.
It’s what allows for the autoscaling of the cluster itself, increasing and decreasing the number of nodes available for your pods to run on.
Cluster Autoscaler will reach out to a cloud provider’s API and scale up or down the number of nodes attached to the cluster accordingly, so it’s different config for different cloud providers, only works in cloud env.
Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:

there are pods that failed to run in the cluster due to insufficient resources(to increase node)
there are nodes in the cluster that have been underutilized for an extended period of time and their pods can be placed on other existing nodes.(decrease node, migrate pod to other nodes).

Here is an example for Alibaba Cloud.
Cloud provider that has cluster autoscaler provided

AWS
GKE
Azure
AliCloud…

Ref
autoscaling concept
official autoscaler
official HPA
official VPA
official cluster autoscaler
VPA design
k8s autoscaling

]]></content>
      <categories>
        <category>k8s</category>
        <category>autoscale</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_cmd</title>
    <url>/2020/06/28/k8s-cmd/</url>
    <content><![CDATA[K8s commandsKubernetes resources are usually created&#x2F;edited by posting a JSON or YAML manifest to the Kubernetes REST API endpoint, as with this manifest, you have more control of the resource, also it is a bit complex, hence K8S provides specific commands to create&#x2F;edit limited properties of resources.


create&#x2F;delete resourceyou can create resource by generic API with yaml file or you can create it by specific command for specific resource
$ kubectl create configmap| deployment| namespace| secret| job $options$ kubectl create -f xxx.yaml

modify resource
NOTE

apply manages applications through files defining Kubernetes resources. It creates and updates resources in a cluster through running kubectl apply. This is the recommended way of managing Kubernetes applications on production.

apply behaves like create if resource does not exist, otherwise, modify it, but the yaml must be full definition of a resource, so modification always needs kubectl get deploy kubia -o yaml, then modify the field, then apply it back.


$ kubectl apply -f ./my-manifest.yaml            # create resource(s)$ kubectl apply -f ./my1.yaml -f ./my2.yaml      # create from multiple files$ kubectl apply -f ./dir                         # create resource(s) in all manifest files in dir$ kubectl apply -f https://git.io/vPieo          # create resource(s) from url

creating from json&#x2F;yaml fileMore control of resources, it’s a bit complex, but for each resource type, only a few properties are a must ,you only need to set that part, please refer to each resource type to see how to create each type from json&#x2F;yaml.
get help for each resource, properties
$ kubectl explain podsKIND:     PodVERSION:  v1DESCRIPTION:     Pod is a collection of containers that can run on a host. This resource is     created by clients and scheduled onto hosts.FIELDS:   apiVersion	&lt;string&gt;     APIVersion defines the versioned schema of this representation of an     object. Servers should convert recognized schemas to the latest internal     value, and may reject unrecognized values. More info:     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources   kind	&lt;string&gt;     Kind is a string value representing the REST resource this object     represents. Servers may infer this from the endpoint the client submits     requests to. Cannot be updated. In CamelCase. More info:     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds...$ kubectl explain pod.metadata...$ kubectl explain pod.metadata.labels...

K8s mostly used resourcesNAME                              SHORTNAMES      NAMESPACED   KIND componentstatuses                 cs              false        ComponentStatus configmaps                        cm              true         ConfigMap namespaces                        ns              false        Namespace nodes                             no              false        Node persistentvolumeclaims            pvc             true         PersistentVolumeClaim persistentvolumes                 pv              false        PersistentVolume pods                              po              true         Pod replicationcontrollers            rc              true         ReplicationController resourcequotas                    quota           true         ResourceQuota secrets                                           true         Secret services                          svc             true         Service daemonsets                        ds              true         DaemonSet deployments                       deploy          true         Deployment replicasets                       rs              true         ReplicaSet cronjobs                          cj              true         CronJob jobs                                              true         Job events                            ev              true         Event ingresses                         ing             true         Ingress ...

generic options for all resources
--all-namespaces-n=kube-system-n kube-system-o wide-o json
cluster
context related
# context is groups of running info, higher than ns# context has cluster and default ns for it$ sudo kubectl config current-context$ sudo kubectl config get-contextsCURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE*         minikube   minikube   minikube   default# create a new context and switch to it, must create test-ns before it$ sudo kubectl config set-context test-ctx --namespace=test-ns --user=minikube --cluster=minikube$ sudo kubectl config get-contextsCURRENT   NAME       CLUSTER    AUTHINFO   NAMESPACE*         minikube   minikube   minikube   default          test-ctx   minikube   minikube   test-ns$ sudo kubectl config use-context test-ctx

nodes related
# show all nodes(master and worker)$ sudo kubectl get no -o wideNAME   STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIMEdev    Ready    control-plane,master   17h   v1.20.2   10.116.5.201   &lt;none&gt;        Ubuntu 18.04.5 LTS   4.15.0-139-generic   docker://20.10.5# get details about node$ kubectl describe nodes$ kubectl describe node devName:               devRoles:              masterLabels:             beta.kubernetes.io/arch=amd64                    beta.kubernetes.io/os=linux                    kubernetes.io/arch=amd64...CPU INO /OS INFO/ Pod Running# label related# set and filter with label$ kubectl label node dev disk=ssd$ kubectl get node -l disk=ssd

cluster info
$ kubectl cluster-infoKubernetes master is running at https://192.168.1.1:8443KubeDNS is running at https://192.168.1.1:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
addon related
$ minikube addons list|-----------------------------|----------|--------------||         ADDON NAME          | PROFILE  |    STATUS    ||-----------------------------|----------|--------------|| ambassador                  | minikube | disabled     || dashboard                   | minikube | enabled ✅   || default-storageclass        | minikube | enabled ✅   || efk                         | minikube | disabled     || freshpod                    | minikube | disabled     || gvisor                      | minikube | disabled     || helm-tiller                 | minikube | disabled     || ingress                     | minikube | disabled     || ingress-dns                 | minikube | disabled     || istio                       | minikube | disabled     |# enable ingress plugin$ minikube addons enable ingressThe &#x27;ingress&#x27; addon is enabled.# after ingress is enabled, a ingress pod is running listen on 80/443 by nginx# and meanwhile docker-proxy starts as well on the node which ingress pod runs.# so that if you access that node on port 80, it will proxy the traffic to ingress container# which performs ingress rules and selects the proper endpoint depends on rule$ sudo kubectl get po -n kube-system -o wide -l app.kubernetes.io/name=ingress-nginxNAME                                        READY   STATUS      RESTARTS   AGE   IP           NODE   NOMINATED NODE   READINESS GATESingress-nginx-controller-797b4b5ff7-jk8wp   1/1     Running     0          10h   172.17.0.6   dev    &lt;none&gt;           &lt;none&gt;$ ps -ef | grep docker-proxyubuntu   19579  1156  0 01:44 pts/1    00:00:00 grep -i -n --color docker-proxyroot     26472  6646  0 Mar25 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 443 -container-ip 172.17.0.6 -container-port 443root     26485  6646  0 Mar25 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.6 -container-port 80# docker-proxy just adds iptable rules based on parameters for ingress pod.
Namespacecreate ns from yaml
apiVersioin: v1kind: Namespacemetadata:    name: test-ns

$ kubectl create -f ns.yaml# create a namespace from command line$ kubectl create namespace test-ns# show all namespaces$ kubectl get ns# delete ns will delete resource in that namespace as well!! like Pod, replicaSet, deployment etc$ kubectl delete test-ns# delete all pods in default ns$ kubectl delete pods --all# delete all resource in default ns, after the delete, the ns is still there!!!$ kubectl delete all --all
PODcreate standalone pod(no replicaset, no deployment involved) from yaml
apiVersion: v1kind: Podmetadata:    name: kubia-mannual    # speicify namespace of this pod    namespace: default    labels:        run: kubiaspec:    # without nodeSelector, pod can be scheduled to any node    nodeSelector:      # select scheduled node(s) with such label      # which has highest priority, select pod then create deployment(like Daemset on the selected node)      disk: &quot;ssd&quot;    containers:    - image: luksa/kubia      name: kubia-mannual      ports:      # it just says the container is listening on 8080      # even without it, you still can access 8080 for this pod      - containerPort: 8080        protocol: TCP    # add liveness probe      livenessProbe:        httpGet:          path: /          port: 8080        initialDelaySeconds: 10

# this creates a standalone pod(no replicacontroller, no replicaset, no deployment)# even for standalone pod, if it dies, will be restarted by k8s$ kubectl create -f pod.yaml
create standalone pod from cmd
$ kubectl run kubia-t --image=luksa/kubia --port=8080

# get pods from default namespace$ kubectl get pods# show ip of a given pod/all pods$ kubectl get pods -o wide$ kubectl get pods kubia -o wide# more detail about pods$ kubectl describe pods$ kubectl describe pods kube-apiserver-dev-alokb# output pod conf with yaml or json$ kubectl get pod kubia -o yaml$ kubectl get pod kubia -o json# get logs of a given pod$ kubectl logs kubia-mannual# add container as log prefix if pod multiple containers$ kubectl logs kubia-mannual --prefix# get logs of a container(add container name) in a given pod(pod may have multiple containers)$ kubectl logs kubia-mannual -c kubia-mannual# access service in pod(pod is listening on 8080) without service resource# way 1:$ curl http://$pod_ip:8080# way 2:# forward host port to container port, after run it hangs(added iptable rule on host)$ kubectl port-forward kubia-mannual 8888:8080# open another terminal$ curl localhost:8888# delete pod/pods$ kubectl delete pods kubia# delete all pods and service in current namespace$ kubectl delete pods,svc --all# delete pod with label selector$ kubectl delete pods -l app=kubia# run cmd in specific container of a pod$ kubectl exec $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125; -- $&#123;CMD&#125;# run bash of a pod (if only one container, container name can be omitted)# same like $ docker exec -it $container -- bash$ kubectl exec -it $&#123;POD_NAME&#125; -- bash# modify any field of POD$ kubectl edit po kubia

Label related
$ kubectl get po --show-labels# modify an existing label of a pod$ kubectl label po kubia-mannual run=jason --overwrite# add a new label to a pod$ kubectl label po kubia-mannual release=v1# show pod with specific labels(filter)$ kubectl get po -l run=jason$ kubectl get po -l run!=jason$ kubectl get po -l &#x27;run in (jason, test)&#x27;$ kubectl get po -l &#x27;run notin (jason, test)&#x27;$ kubectl get po -l run# not have run label$ kubectl get po -l &#x27;!run&#x27;# similar like label, but used only for description$ kubectl annotate pod kubia-manual author=&quot;jason&quot;
Replicacreate rc from yaml
apiVersion: v1# Old type, deprecated!!!kind: ReplicationControllermetadata:  name: kubiaspec:  replicas: 3  template:    metadata:      # create pod with such label      labels:        app: kubia    spec:      containers:        - name: kubia          image: luksa/kubia          ports:          - containerPort: 8080

# only way to create rc is from yaml$ kubectl create -f rc.yaml$ kubectl get rc$ kubectl describe rc kubia# scale a rc, run less or more pod of this rc$ kubectl scale rc kubia --replicas=3# change the label of a pod can move it in or out of scope of replication controller# modify any field of rc$ kubectl edit rc kubia# delete rc, pod controlled by it will be deleted as well$ kubectl delete rc kubia# only delete rc, keep pod running$ kubectl delete rc kubia --cascade=false

create rs from yaml
apiVersion: apps/v1kind: ReplicaSetmetadata:  name: kubiaspec:  replicas: 3  # label selector  selector:    matchLabels:      app: kubia    # expression selector    # matchExpressions:    #   - key: app    #     operator: In    #     values:    #       - kubia    # In—Label’s value must match one of the specified values.    # NotIn—Label’s value must not match any of the specified values.    # Exists—Pod must include a label with the specified key (the value isn’t important). When using this operator, you shouldn’t specify the values field.    # DoesNotExist—Pod must not include a label with the specified key. The values property must not be specified.    # If you specify multiple expressions, all those expressions must evaluate to true for the selector to match a pod.  template:    metadata:      # create pod with such label      labels:        app: kubia    spec:      containers:        - name: kubia          image: luksa/kubia          ports:          - containerPort: 8080          # optional, mount from html to /var/htdocs          volumeMounts:            - name: html              mountPath: /var/htdocs        # optional part        volumes:        # emptyDir type, shared by containers in the same pod!!!        - name: html          emptyDir: &#123;&#125;

$ kubectl create -f rs.yaml# expose from host, add iptable rules to rc$ kubectl expose rc kubia --port=80 --target-port=8000$ kubectl get rs$ kubectl describe rs kubia# scale a rs, run less or more pod of this rs$ kubectl scale rc kubia --replicas=3# change the label of a pod can move it in or out of scope of ReplicaSet# modify any field of rs$ kubectl edit rs kubia# delete rs, pod controlled by it will be deleted as well$ kubectl delete rs kubia# only delete rs, keep pod running$ kubectl delete rs kubia --cascade=false
DaemonSetapiVersion: apps/v1kind: DaemonSetmetadata:  # DaemonSet name is kubia  # pod name is kubia-xxx  name: kubiaspec:  # no replica as only one such pod for each worker node  # label selector  selector:    matchLabels:      app: kubia  template:    metadata:      # create pod with such label      labels:        app: kubia    spec:      containers:        - name: kubia          image: luksa/kubia          ports:          - containerPort: 8080

$ kubectl create -f ds.yaml$ kubectl get ds$ sudo kubectl get dsNAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGEkubia    1         1         1       1            1           &lt;none&gt;          10s$ kubectl describe ds kubia# change the label of a pod can move it in or out of scope of DaemonSet# modify any field of ds$ kubectl edit ds kubia# delete ds, pod controlled by it will be deleted as well$ kubectl delete ds kubia# only delete rs, keep pod running$ kubectl delete ds kubia --cascade=false
Job, CronJobapiVersion: batch/v1kind: Jobmetadata:  name: batch-jobspec:  # there two are optional, by default they are 1  # by default, run once and one pod runs  completions: 5  parallelism: 2  template:    metadata:      labels:        app: batch-job    spec:      restartPolicy: OnFailure      containers:        - name: main          image: luksa/batch-job

$ kubectl create -f jb.yaml$ kubectl get jobsNAME        COMPLETIONS   DURATION   AGEbatch-job   0/1           11s        11s# After a while job is done$ kubectl get jobNAME        COMPLETIONS   DURATION   AGEbatch-job   1/1           2m11s      2m24s$ kubectl describe jobs batch-job# modify any field of job$ kubectl edit jobs batch-job# after the job run successfully, job and pod are not deleted but show status completed$ kubectl delete jobs batch-job

apiVersion: batch/v1beta1kind: CronJobmetadata:  name: batch-job-every-15spec:  schedule: &quot;0,15,30,45 * * * *&quot;  jobTemplate:    spec:      template:        metadata:          labels:            app: batch-job        spec:          restartPolicy: OnFailure          containers:            - name: main              image: luksa/batch-jobschedule has the same format as linux cron job┌───────────── minute (0 - 59)│ ┌───────────── hour (0 - 23)│ │ ┌───────────── day of the month (1 - 31)│ │ │ ┌───────────── month (1 - 12)│ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;│ │ │ │ │                                   7 is also Sunday on some systems)│ │ │ │ ││ │ │ │ │* * * * **	any value,	value list separator-	range of values/	step values

$ kubectl create -f cjb.yaml$ kubectl get cjNAME                 SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGEbatch-job-every-15   0,15,30,45 * * * *   False     0        &lt;none&gt;          11s$ kubectl describe cronjobs batch-job-every-15# modify any field of cjob$ kubectl edit cronjobs batch-job-every-15$ kubectl delete cronjobs batch-job-every-15

ServiceClusterIP service
apiVersion: v1kind: Servicemetadata:  name: kubiaspec:  # optional, default select pod randomly  sessionAffinity: ClientIP  # ClusterIP automatically assigned when created or set it manually like below!!!  # clusterIP: 10.2.2.1  ports:    # service port 80---&gt;container port 8080    - port: 80      targetPort: 8080    # if multiple ports for a service, each port must have a name    # - name: http    #   port: 80    #   targetPort: 8080    # - name: https    #   port: 443    #   targetPort: 8443    # pods(in the same namespace) behind the service  selector:    app: kubia

$ kubectl create -f service.yaml# default service only has cluster ip, no external-ip$ kubectl get svcNAME    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGEkubia   ClusterIP   10.102.75.1   &lt;none&gt;        80/TCP    9s# show endpoint Object of all service# endpoint Object is create automatically when service is creating.$ kubectl get epNAME    ENDPOINTS                                           AGEkubia   172.17.0.11:8080,172.17.0.12:8080,172.17.0.7:8080   72s$ kubectl describe ep kubia$ kubectl edit ep kubia$ kubectl describe svc kubia$ kubectl delete svc kubia$ kubectl edit svc kubia$ kubectl get po -o widekubia-6555dff44f-h8kfd                1/1     Running     0          4m49s   172.17.0.11   dev    &lt;none&gt;           &lt;none&gt;kubia-6555dff44f-k55fd                1/1     Running     0          4m49s   172.17.0.7    dev    &lt;none&gt;           &lt;none&gt;kubia-6555dff44f-mvsqj                1/1     Running     0          4m49s   172.17.0.12   dev    &lt;none&gt;           &lt;none&gt;

let’s check the core (nat) rules generated on the node after a service is created
# default/kubia is service name, 10.105.109.83 is service cluster ipChain KUBE-SERVICES (2 references) pkts bytes target     prot opt in     out     source               destination   11   660 KUBE-SVC-L5EAUEZ74VZL5GSC  tcp  --  any    any     anywhere             10.105.109.83        /* default/kubia: cluster IP */ tcp dpt:http# three pods target with random modeChain KUBE-SVC-L5EAUEZ74VZL5GSC (1 references) pkts bytes target     prot opt in     out     source               destination    2   120 KUBE-SEP-PERL6L2Q363TRRKS  all  --  any    any     anywhere             anywhere             /* default/kubia: */ statistic mode random probability 0.33333333349    3   180 KUBE-SEP-5SWG4BFKF7ZYVF5R  all  --  any    any     anywhere             anywhere             /* default/kubia: */ statistic mode random probability 0.50000000000    6   360 KUBE-SEP-5ZBQGZX3PSGHKG5K  all  --  any    any     anywhere             anywhere             /* default/kubia: */# on pod rule 172.17.0.6 pod ip with port 8080Chain KUBE-SEP-PERL6L2Q363TRRKS (1 references) pkts bytes target     prot opt in     out     source               destination    0     0 KUBE-MARK-MASQ  all  --  any    any     172.17.0.6           anywhere             /* default/kubia: */    2   120 DNAT       tcp  --  any    any     anywhere             anywhere             /* default/kubia: */ tcp to:172.17.0.6:8080

NodeType service
apiVersion: v1kind: Servicemetadata:  name: kubiaspec:  type: NodePort  ports:    # service port 80---&gt;container port 8080    - port: 80      targetPort: 8080      nodePort: 30000  selector:    app: kubia

# create service from one command line$ kubectl expose deployment kubia --type=NodePort --port=80 --target-port=8080# when you create a nodePort service,# it creates clusterIP service automatically# (but this is not true for ingress, you have to create clusterIP service manually),# then add extra iptables rules to clusterIp service# if nodePort service exists, no need to create ClusterIP service,otherwise error: services &quot;xxxx&quot; already exists# clusterIp service is also some iptables rules.$ kubectl create -f nodeport.yaml# nodePort service$ kubectl get svcNAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGEkubia   NodePort   10.104.24.124   &lt;none&gt;        80:30000/TCP   8s# curl node port, 80 is service port$ curl 10.109.176.199:80$ curl $Node_IP:30000

NodePort service---&gt;ClusterIP service---&gt;PodLoadBalancer service---&gt;NodePort service---&gt;ClusterIP service---&gt;Pod
IngressMust enable ingress controller firstly
$ minikube addons enable ingress

apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: kubia# no ingress port needed# ingress listens on 80 for http# ingress listens on 443 for httpsspec:  rules:  - host: kubia.example.com    http:      paths:      - path: /        pathType: Exact        backend:          service:            name: kubia # must create ClusterIP service manually!!!            port:              number: 80

$ kubectl create -f in.yaml$ kubectl get ingressName:             kubiaNamespace:        test-ns# ingress IP, it&#x27;s public IP!!!(node IP)Address:          10.116.5.201Default backend:  default-http-backend:80 (&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)Rules:  Host               Path  Backends  ----               ----  --------  kubia.example.com                     /   kubia:80 (172.17.0.11:8080,172.17.0.12:8080,172.17.0.7:8080)  # kubia:80 is clusterIP service with three endpoints  # Ingress depends on cluster service to get the endpoints and use it directly!!!Annotations:         &lt;none&gt;Events:  Type    Reason  Age   From                      Message  ----    ------  ----  ----                      -------  Normal  CREATE  81s   nginx-ingress-controller  Ingress test-ns/kubia  Normal  UPDATE  27s   nginx-ingress-controller  Ingress test-ns/kubia

DeploymentapiVersion: apps/v1kind: Deploymentmetadata:  name: kubiaspec:  replicas: 3  selector:    matchLabels:      app: kubia  template:    metadata:      name: kubia      labels:        app: kubia    spec:      containers:      - image: luksa/kubia:v1        name: nodejs
# create deploy from one command line$ kubectl create deployment kubia --image=luksa/kubia:v1# create dp with --record in version history# have the change to rollback if upgrade fails.$ kubectl create -f dp.yaml --record$ sudo kubectl get deployNAME    READY   UP-TO-DATE   AVAILABLE   AGEkubia   3/3     3            3           21s$ kubectl get deploy kubia$ kubectl describe deploy kubia$ kubectl edit deploy kubia# deployment will create ReplicaSet automatically$ sudo kubectl get rsNAME               DESIRED   CURRENT   READY   AGEkubia-6555dff44f   3         3         3       95s# trigger upgrade by change image version$ kubectl set image deploy kubia nodejs=luksa/kubia:v2# OR$ kubectl edit deploy kubia# scale a deploy$ kubectl scale deploy kubia --replicas=2# check the status when rolling happens$ kubectl rollout status deployment kubia# if the new version has issue, rollback to previous one$ kubectl rollout undo deployment kubia$ kubectl rollout undo deployment kubia --to-revision=1# check the version history of a given deployment when several upgrade happened$ kubectl rollout history deployment kubia$ kubectl rollout pause deployment kubia$ kubectl rollout resume deployment kubia

configMap, downwardAPI, secretcreate configMap from shell
# create configMap from literal, file, dir# --from-file without key will use file name as the key# --from-file with dir will use all files# kubectl create configmap my-cfg --from-file=test.json --from-file=key1=hi.txt --from-file=test_dir/ --from-literal=key2=hello$ kubectl create configmap my-cfg --from-file=conf.cfg --from-literal=id=jason$ kubectl get cmNAME               DATA   AGEmy-cfg             2      17s$ kubectl describe cm my-cfgName:         my-cfgNamespace:    test-nsLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Data====id:----jasonconf.cfg:----name = helloEvents:  &lt;none&gt;# never use yaml to create configMap as you have to copy file content in yaml file!!!
use configMap from env var in pod’s container
# pod definition with value from configMapenv:- name: FIRST_VAR  valueFrom:    configMapKeyRef:      name: my-cfg      key: foo# or all entries from configMap as env varsenvFrom:- prefix: CONFIG_  configMapRef:    name: my-cfg

mount configmap entry as a file keep existing if has same file
# mount configmap entries to a dir in the podapiVersion: v1kind: Podmetadata:    name: kubia-hc    labels:        run: kubiaspec:    containers:    - image: luksa/kubia      name: kubia      # mount individual entries under a dir but keep existing file under that dir      # say mount to /etc, but still keep existing file under etc      volumeMounts:        - name: config          mountPath: /etc/new.json          # subPath says new.json is a file          # without subPath, new.json is treated as a path!!!!          subPath: new.json          readOnly: true    volumes:    - name: config      configMap:        # use configMap declared by configMap        # if no item provided, it will use all entries!!!        name: my-cfg        items:        - key: test.json          # rename it          path: new.json


mount configmap entry as a file hide existing if has same file
# mount configmap entries to a dir in the podapiVersion: v1kind: Podmetadata:    name: kubia-hc    labels:        run: kubiaspec:    containers:    - image: luksa/kubia      name: kubia      volumeMounts:        - name: config          # if this path does not exist, create          # if exists, hide all existing files with configMap entries          mountPath: /var/test          readOnly: true    volumes:    - name: config      configMap:        # use configMap declared by configMap        # if no items provided, it will use all entries!!!        name: my-cfg        items:        - key: test.json          # rename it          path: new.json

secrets# create secrets$ kubectl create secret generic test_sc --from-file=key1=test.json$ kubectl get secrets$ kubectl describe secrets

# secret is mounted in-memory files  volumeMounts:    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount      name: default-token-l5wrk      readOnly: true  volumes:  - name: default-token-l5wrk    secret:      secretName: default-token-l5wrk    # no items lists, all entries in secret are mounted to mountPath!!!

# use secret from envenv:- name: FOO_SECRET  valueFrom:    secretKeyRef:      name: fortune-https      key: foo

downwardAPI
# check Pod manifest to get the field name# use ENV in containerenv:- name: POD_IP  valueFrom:    fieldRef:      fieldPath: status.podIP- name: POD_NAME  valueFrom:    fieldRef:      fieldPath: metadata.name

  # under container with file  volumeMounts:  - name: downward    mountPath: /etc/downwardvolumes:  - name: downward    downwardAPI:      items:      - path: &quot;podName&quot;        fieldRef:          fieldPath: metadata.name      - path: &quot;podIP&quot;        fieldRef:          fieldPath: status.podIP

talk to API server
$ kubectl get svcNAMESPACE              NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGEdefault                kubernetes                           ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                  9d# on the node$ kubectl proxyStarting to serve on 127.0.0.1:8001# proxy will talk to API server(10.96.0.1) and do the authentication, cool$ curl localhost:8001# talk to API server within a pod# as for each pod, we mount# 1. a token(from secret) that can be used to talk with API server# 2. ca: used to verify server&#x27;s certificate.# at /var/run/secrets/kubernetes.io/serviceaccount# inside a container, run below command$ export TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)$ curl -H &quot;Authorization: Bearer $TOKEN&quot; https://kubernetes# another way, you can run another container within a pod, this container runs kubectl proxy# so other container in the same pod, can access localhost:80001 as proxy to talk with API server.
Volumescheck default storageClass provided by minikube
$ sudo minikube addons enable storage-provisioner$ sudo kubectl get pod -n kube-systemNAME                                        READY   STATUS             RESTARTS   AGEstorage-provisioner                         1/1     Running            0          20h$ sudo kubectl get storageClassNAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGEstandard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  19h

declare pvc use storageClass
apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: data-pvcspec:  # use standard storageClass which is created by minikube automatically  storageClassName: standard  resources:    requests:      storage: 100Mi  accessModes:    - ReadWriteOnce

use data-pvc from pod
  volumeMounts:  - name: data    mountPath: /var/storevolumes:- name: data  persistentVolumeClaim:    claimName: data-pvc

$ kubectl get sc$ kubectl describe sc$ kubectl edit sc# create pvc use storageClass$ kubectl create -f pvc.yaml$ kubectl get pvcNAME       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGEdata-pvc   Pending                                      standard       38s# After a while$ kubectl get pvcNAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEdata-pvc   Bound    pvc-0289f274-ab70-4a0f-9a6d-8c7d10bac7fe   100Mi      RWO            standard       35m# pv is created automatically for pvc when it uses storageClass$ kubectl get pvNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGEpvc-a59a4fac-5b1e-44e1-a066-ff33cfc2b1bc   100Mi      RWO            Delete           Bound    default/data-pvc   standard                104s
Ref
kubectl cheatsheet
kubernetes-in-action
Alta3
cron example

]]></content>
      <categories>
        <category>k8s</category>
        <category>overview</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s_command</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_concept</title>
    <url>/2020/06/28/k8s-concept/</url>
    <content><![CDATA[KubernetesKubernetes is a software system that allows you to easily deploy and manage containerized applications on top of it. It relies on the features of Linux containers to run heterogeneous applications without having to know any internal details of these applications and without having to manually deploy these applications on each host.


Because these apps run in containers, they don’t affect other apps running on the same server, which is critical when you run applications for completely different organizations on the same hardware. This is of paramount importance for cloud providers, because they strive for the best possible utilization of their hardware while still having to maintain complete isolation of hosted applications.
Kubernetes enables you to run your software applications on thousands of computer nodes as if all those nodes were a single, enormous computer. It abstracts away the underlying infrastructure and, by doing so, simplifies development, deployment, and management for both development and the operations teams.

But before we go into Kubernetes, let’s first take a look at microservice that drives Kubernetes into our eyes.
MicroserviceToday, big applications are slowly being broken down into smaller, independently running components called microservices. Because microservices are decoupled from each other, they can be developed, deployed, updated, and scaled individually. This enables you to change components quickly and as often as necessary to keep up with today’s rapidly changing business requirements.

Pros:

developed individually with the language(c, C++, python, Go etc) you like.
deployed individually, deploy multiple key service for performance and HA
update individually, upgrade some microservices that needed.
scale individually, for key microsevices that have the bottleneck.

Cons:

With large microservices, it needs more effort to deploy, upgrade, scale, handle failure, we have to do this manually or write our own script.

Why needs Kubernetes:We need automation, which includes automatic scheduling of those components to our servers, automatic configuration, supervision, and failure-handling. This is where Kubernetes comes in, with kubernetes, you do not need to do it manually or write your own script.
Core ConceptA Kubernetes cluster is composed of many nodes, which can be split into two types:

The master node, which hosts the Kubernetes Control Plane that controls and manages the whole Kubernetes system
Worker nodes that run the actual applications you deploy


THE CONTROL PLANEThe Control Plane is what controls the cluster and makes it function. It consists of multiple components that can run on a single master node or be split across multiple nodes and replicated to ensure high availability. These components are

The Kubernetes API Server, which you and the other Control Plane componentscommunicate with
The Scheduler, which schedules your apps (assigns a worker node to each deployable component of your application)
The Controller Manager, which performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures, and so on
etcd, a reliable distributed data store that persistently stores the cluster configuration.

The components of the Control Plane hold and control the state of the cluster, but they don’t run your applications. This is done by the (worker) nodes.
THE NODESThe worker nodes are the machines that run your containerized applications. The task of running, monitoring, and providing services to your applications is done by the following components:

Docker, rkt, or another container runtime, which runs your containers
The Kubelet, which talks to the API server and manages containers on its node
The Kubernetes Service Proxy (kube-proxy), which load-balances network traffic between application components

Master and worker node talk by kubelet trough API server, one end is kubelet, the other end is API server
Actually, each components run in container as well.

run an application by k8sTo run an application in Kubernetes, you first need to package it up into one or more container images, push those images to an image registry, and then post a description of your app to the Kubernetes API server
The description includes information such as the container image or images that contain your application components, how those components are related to each other, and which ones need to be run co-located (together on the same node) and which don’t. For each component, you can also specify how many copies (or replicas) you want to run. Additionally, the description also includes which of those components provide a service to either internal or external clients and should be exposed through a single IP address and made discoverable to the other components
When the API server processes your app’s description, the Scheduler schedules the specified groups of containers onto the available worker nodes based on computational resources required by each group and the unallocated resources on each node at that moment. The Kubelet on those nodes then instructs the Container Runtime (Docker, for example) to pull the required container images and run the containers.
keep application runningOnce the application is running, Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided.
Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will select new nodes for all the containers that were running on the node and run them on the newly selected nodes.scaleWhile the application is running, you can decide you want to increase or decrease the number of copies, and Kubernetes will spin up additional ones or stop the excess ones.

RefKubernetes-in-actionKubernete StorageKubernete Concepts
]]></content>
      <categories>
        <category>k8s</category>
        <category>overview</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_cri_oci</title>
    <url>/2021/06/09/k8s-cri-oci/</url>
    <content><![CDATA[Introduction

The OCI or Open Containers Initiative is an organization that creates container standards. The OCI runtime spec defines the API of a low-level container runtime and the OCI image spec defines what a “Docker image” actually is.
The Kubernetes project has also defined a number of standards. Relevant for this article is the CRI: the Container Runtime Interface. This interface defines how Kubernetes talks with a high-level container runtime.



Companies involved

CRI-O and podman is created by RedHat for OpenShift, it’s open source now.
containerd and runc maintained by community
gVisor is developed by Google
Kata-runtime comes from Intel and Hyper.sh for security reason, it’s start special VM!!!!

Container Runtimelow level container runtimeAn OCI runtime is relatively simple. You give it the root filesystem of the container and a json file describing core properties of the container, and the runtime spins up the container and connects it to an existing network using a pre-start hook.
At the moment, we have three main OCI runtimes or low-level container runtimes.

runc, which is the default for most tools such as Docker and Podman. This is based on the code initially donated by Docker.
kata-run from the “Kata Containers” project, which aims to provide much better security and isolation between containers by running each container in a lightweight VM. It’s a merge of the runv and Intel Clear Containers projects.
gVisor is created by Google. It provides better isolation by running each container in a tight security sandbox.

high-level container runtime  

Actually creating the network of a container.
Managing container images.
Preparing the environment of a container.
Managing local&#x2F;persistent storage.

There are also three main high-level container runtimes.

containerd is a CRI-compatible container runtime which was donated to the CNCF by Docker. It is currently the default in many Kubernetes distributions such ad Canonical’s Charmed Kubernetes. It supports all OCI-compliant runtimes and has a special shim for kata-run.

CRI-O is a bridge between Kubernetes and OCI-compliant runtimes created by Red Hat. It has the big advantage that it gets released in lock-step with Kubernetes itself. Each CRI-O version is compatible with the Kubernetes version that has the same version number. This runtime is the default in OpenShift.

Docker itself can also be used as a CIR-compatible container runtime using the docker-shim. However, many Kubernetes distributors are moving away from this solution, due to the added unnecessary complexity of Docker.


kubernetes CRI(leaded by Kubernetes[Google])The CRI was released in December 2016 by Kubernetes (Container Runtime Interface). In order to avoid subsequent compatibility, Kubernetes attempted to support another container runtime project led by CoreOS, rkt, but needed to write a lot of compatible code, etc. Other runtimes have implemented maintenance work, so a single CRI interface has been released. Any runtime that supports CRI can be explicitly used as Kubernetes’ underlying runtime;, kubelet calls same API(CRI implemented by runtime) to manage container.
Default CRI history


OCI(leaded by Docker)In 2015, Docker and other leaders in the container industry jointly founded the Open Container Initiative (OCI) (it is also a project under the Linux Foundation).
Two specifications are primarily used by OCI:

Runtime-spec: How to run the package when the container is running on the file system defined.
image-spec: How to create a package on a file system that can be run by OCI.

As an original work, Docker donated its own container image format and runtime (now runc) to OCI.
OCI has nothing related to Kubernetes, it defines the specification how to create images and start container.
K8s# If you don&#x27;t specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of well known Unix domain sockets.# Runtime	Path to Unix domain socket# Docker	    /var/run/dockershim.sock# containerd	/run/containerd/containerd.sock# CRI-O	        /var/run/crio/crio.sock# NOTE: If both Docker and containerd are detected, Docker takes precedence.# If any other two or more runtimes are detected, kubeadm exits with an error.# check runtime used by kubelet, check runtime parameter$ ps -ef | grep kubelet # --container-runtime string     Default: `docker`# The container runtime to use. Possible values: `docker`, `remote`.# --container-runtime-endpoint string     Default: `unix:///var/run/dockershim.sock`# possible values: &#x27;/run/containerd/containerd.sock&#x27;, &#x27;/var/run/crio/crio.sock&#x27;

multiple runtime supportA k8s cluster can have multiple runetime!!!
apiVersion: node.k8s.io/v1alpha1  # RuntimeClass is defined in the node.k8s.io API groupkind: RuntimeClassmetadata:  name: myclass  # The name the RuntimeClass will be referenced by  # RuntimeClass is a non-namespaced resourcespec:  runtimeHandler: myconfiguration  # The name of the corresponding CRI configurationapiVersion: v1kind: Podmetadata:  name: mypodspec:  runtimeClassName: myclass  # ...


standalone containerdocker is best choice for this, but this is another choice to try. why redhat deos not use docker for openshift

Use Podman managing pods and containers. It’s a CLI tool which is very similar to docker. It uses libpod which uses runc in backend and is fully compatible with “Docker Images”.
Use Buildah for building “Docker Images”. It supports building containers from DockerFiles, but you can also build them with simple shell scripts!
Use CRI-O for running containers with Kubernetes. If you want to debug pods and containers maintained by Kubernetes, you can use the crictl tool instead of the docker commands.

Note: Podman can’t be used as kubernetes runtime
Ref
history of k8s with docker

setup runtime for k8s


]]></content>
      <categories>
        <category>k8s</category>
        <category>cri</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_device_plugin</title>
    <url>/2024/07/26/k8s-device-plugin/</url>
    <content><![CDATA[OverviewKubernetes provides a device plugin framework that you can use to advertise system hardware resources to the Kubelet which then reports resource to API server.
Instead of customizing the code for Kubernetes itself, vendors can implement a device plugin that you deploy either manually or as a DaemonSet. The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters, and other similar computing resources that may require vendor specific initialization and setup.
The workflow of the device plugin is divided into two parts:

Resource reporting upon startup and monitoring after starts
Scheduling and running during usage



Report and Monitor Resources
Each hardware device is managed by the related device plugin, which is connected as a client to the device plugin manager of the kubelet through gRPC and reports to the kubelet the UNIX socket API version and device name to which it listens.

Step 1: The device plugin is registered to interact with Kubernetes. Multiple devices may exist on a node. The device plugin, as a client, reports the following information to the kubelet: 
(1) name of the device managed by the device plugin, such as a GPU or RDMA,
(2) file path of the UNIX socket to which the device plugin listens so that the kubelet can call the device plugin.


Step 2: The device plugin starts a gRPC server. Then, the device plugin acts on behalf of the gRPC server to provide services to the kubelet. The listening address and API version are provided in Step 1.
Step 3: After the gRPC server is started, the kubelet establishes a persistent connection to ListAndWatch of the device plugin to discover the device ID and check the device health. The device plugin notifies the kubelet when a device is unhealthy. If the unhealthy device is idle, the kubelet removes it from the schedulable device list. If the unhealthy device is used by a pod, the kubelet does not do anything because killing the pod is a high-risk action.
Step 4: The kubelet exposes these devices to the status of the node and sends the device quantity to the Kubernetes API server. The scheduler implements scheduling based on this information.

The kubelet reports only the GPU quantity to the Kubernetes API server. The device plugin manager of the kubelet stores the GPU ID list and assigns the GPU IDs to devices. The Kubernetes global scheduler does not see the GPU ID list, only the GPU quantity.
Resource Allocation
When a pod wants to use a GPU, it declares the GPU resource and required quantity in Resource.limits, such as nvidia.com/gpu: 1 in pod’s spec. Kubernetes finds the node that meets the required GPU quantity, subtracts the number of GPUs on the node by 1, and binds the pod and the node.
After the binding is complete, the node-matched kubelet creates a container. When the kubelet finds that the resource specified in the pod’s container request is a GPU, it enables the internal device plugin manager to select an available GPU from the GPU ID list and assigns the GPU to the container.
The kubelet sends an Allocate request to the device plugin. The request includes the device ID list that contains the GPU to be assigned to the container.
After receiving the Allocate request, the device plugin finds the device path, driver directory, and environment variables related to the device ID, and returns the information to the kubelet through an Allocate response.
The kubelet assigns a GPU to the container based on the received device path and driver directory. Then, Docker creates a container as instructed by the kubelet. The created container includes a GPU. Finally, the required driver directory is mounted. This completes the process of assigning a GPU to a pod in Kubernetes.
]]></content>
      <categories>
        <category>k8s</category>
        <category>deviceplugin</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_faq</title>
    <url>/2020/06/28/k8s-faq/</url>
    <content><![CDATA[Podswhy pod is in pending status?# pods === pod$ kubectl get pod$ kubectl get pods# there are many reason for this check why, use describe which gives status and events$ kubectl describe pods kubia-mannualName:         kubia-mannualNamespace:    defaultPriority:     0Node:         &lt;none&gt;Labels:       run=kubiaAnnotations:  &lt;none&gt;Status:       PendingIP:IPs:          &lt;none&gt;Containers:...Events:  Type     Reason            Age                 From               Message  ----     ------            ----                ----               -------  Warning  FailedScheduling  6s (x4 over 2m45s)  default-scheduler  0/1 nodes are available: 1 node(s) didn&#x27;t match node selector.

pod is restarted, how can I see previous container log$ kubectl logs mypod --previous
how can I check why pod is restart$ kubectl describe po mypod...Events:  Normal   Scheduled  2m30s                default-scheduler        Successfully assigned default/kubia-hc to dev-alokb  Warning  Unhealthy  54s (x3 over 74s)    kubelet, dev-alokb       Liveness probe failed: HTTP probe failed with statuscode: 500  Normal   Killing    53s                  kubelet, dev-alokb       Container kubia-hc failed liveness probe, will be restarted  ...

how can I check liveness setting for a pod$ kubectl describe po mypod...Liveness:       http-get http://:8080/ delay=10s timeout=1s period=10s #success=1 #failure=3...

how pod the service internal cluster ip after it knows the service name?$ kubectl get svcNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGEkubia        ClusterIP   10.108.36.232   &lt;none&gt;        80/TCP    10m
by environmentif pod is created after service, several variables are passed to pod by env
# check env on a pod which is backend of a service$ kubectl exec kubia-vtzwq -- env | grep KUBIAKUBIA_SERVICE_PORT=80KUBIA_PORT_80_TCP_PROTO=tcpKUBIA_PORT_80_TCP_ADDR=10.108.36.232KUBIA_PORT_80_TCP=tcp://10.108.36.232:80KUBIA_PORT=tcp://10.108.36.232:80KUBIA_SERVICE_HOST=10.108.36.232KUBIA_PORT_80_TCP_PORT=80
by DNS serverActually, each pod configure a internal DNS server(dns service at kube-system)
$ kubectl exec kubia-vtzwq -- cat /etc/resolv.confnameserver 10.96.0.10search default.svc.cluster.local svc.cluster.local cluster.local ...
Here dns server is a service cluster ip of kube-system
$ kubectl get svc -n=kube-system -o wideNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE     SELECTORkube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   7d14h   k8s-app=kube-dns# show all pod with labels$ kubectl get po -n=kube-system --show-labels# OR# filter with selector two pods behind the dns service$ kubectl get po -l k8s-app=kube-dns -n=kube-systemNAME                       READY   STATUS    RESTARTS   AGEcoredns-66bff467f8-77978   1/1     Running   0          7d14hcoredns-66bff467f8-mtqbt   1/1     Running   0          7d14h$ kubectl exec kubia-vtzwq -- curl kubia:80

how to support tls in Ingress$ openssl genrsa -out tls.key 2048$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj/CN=kubia.example.com$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key# later on use the secret in Ingress like this

apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: kubiaspec:  tls:  - hosts:    - kubia.example.com    # secrete resource that created before    # only enable ssl when access ingress, but ingress controller to backend still use Http!!!    secretame: tls-secret  rules:    - host: kubia.example.com      http:        paths:        - path: /          backend:            serviceName: kubia-nodeport            servicePort: 80
why needs headless service(no ClusterIP)In some case, you want to know all endpoints of a service by internal DNS lookup, if service has ClusterIP, so dns server only returns the ClusterIP of the service, but for headless service, it returns several,  each record each for a endpoint.
For headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them.
DNS server here provides service&lt;—–&gt;IP mapping

cluster service, return ClusterIP
headless service, return IP(s) of each endpoint(pod).

apiVersion: v1kind: Servicemetadata:  name: kubiaspec:  # headless service, no ClusterIP  clusterIP: None  ports:    # service port 80---&gt;container port 8080    - port: 80      targetPort: 8080  selector:    app: kubia

directly talk to api server by restful API, skip kubectlIn some cause, we may no kubectl installed or we want to use restful API directly talking with api server from source source code like js&#x2F;python, there are API reference with example, but first you need to authenticate with server first, either by your self, or use kube-proxy which will do this for you.
# on one node$ sudo kubectl proxy --address=0.0.0.0 --disable-filter=trueStarting to serve on [0.0.0.0]:8001# curl&lt;---kubectl proxy&lt;-----api server------&gt;$ curl http://localhost:8001/apis/apps/v1/namespaces/default/deployments&#123;  &quot;kind&quot;: &quot;DeploymentList&quot;,  &quot;apiVersion&quot;: &quot;apps/v1&quot;,  &quot;metadata&quot;: &#123;    &quot;resourceVersion&quot;: &quot;44164&quot;  &#125;,  &quot;items&quot;: [    &#123;      &quot;metadata&quot;: &#123;        &quot;name&quot;: &quot;kubia&quot;,        &quot;namespace&quot;: &quot;default&quot;,        &quot;uid&quot;: &quot;80400a5b-0db9-4997-b1a5-b097181b97af&quot;,        &quot;resourceVersion&quot;: &quot;42156&quot;,        &quot;generation&quot;: 2,        &quot;creationTimestamp&quot;: &quot;2021-03-28T11:55:17Z&quot;,        &quot;annotations&quot;: &#123;          &quot;deployment.kubernetes.io/revision&quot;: &quot;1&quot;        &#125;,...
how to use host network directly for podapiVersion: apps/v1kind: DaemonSetmetadata:  name: traefik-ingress-lb  namespace: kube-system  labels:    k8s-app: traefik-ingress-lbspec:  selector:    matchLabels:      k8s-app: traefik-ingress-lb  template:    metadata:      labels:        k8s-app: traefik-ingress-lb        name: traefik-ingress-lb    spec:      terminationGracePeriodSeconds: 60      # use host network diretly      hostNetwork: true      restartPolicy: Always      serviceAccountName: ingress      containers:      - image: traefik:v1.7.20        name: traefik-ingress-lb

how to limit resource used by containerThere are two kinds resource check for K8S pod, but it’s set at each containers of that pod separately.

request: schedule to node only if it can proivde these resource, used by scheduler
limits: how much cpu&#x2F;memory container can use

containers:- name: nginx  image: nginx:1.13.12  ports:  - containerPort: 80  resources:    requests:      memory: &quot;64Mi&quot;      cpu: &quot;250m&quot;    limits:      memory: &quot;128Mi&quot;      cpu: &quot;500m&quot;
Operator vs HelmThey both serve the same basic goal of helping to install and configure apps in Kubernetes. But they do so in different ways. Depending on factors like how much control you want and how important ongoing application lifecycle management is, an operator might be better than a Helm chart, or vice versa.

A Kubernetes Operator is a method of packaging, deploying and managing a Kubernetes application. Essentially, an Operator is a custom controller that extends the functionality of the Kubernetes API. This is accomplished by introducing new custom resources or by modifying existing ones.
Kubernetes Operators let you handle complex applications more easily.Operators are designed to handle tasks like upgrades, backups and failover in a more automated and reliable way. They can even make complex decisions based on the state of your cluster.


A Helm chart is a package that contains the resources necessary for deploying an application on Kubernetes using a packaging format called charts. You can install Helm charts with (you guessed it!) Helm, an application package manager and configuration management tool for Kubernetes.


Which one to chooseIf you need to manage complex, stateful applications and require a high degree of automation, Kubernetes Operator may be the better choice. On the other hand, if you’re looking for a simpler tool to help with application deployment or if you’re new to Kubernetes, Helm may be a more suitable option.
Vanilla Kubernetes vs Distribution KubernetesVanilla Kubernetes is the upstream version of Kubernetes that is maintained by the Kubernetes community. It is the most basic and pure form of Kubernetes, without any modifications or additions. Vanilla Kubernetes is also known as stock Kubernetes or plain Kubernetes.
👍Pros  

Flexible: highly flexible and customizable, allowing users to create and deploy a wide range of applications and workloads
Cost-effective: free and open-source, which means that users do not have to pay for support or maintenance fees

👎Cons  

Steep learning curve: can be difficult to learn and use, especially for users who are new to container orchestration
Complex deployment and management: deploying and managing vanilla Kubernetes requires more technical expertise and manual configurationDistribution Kubernetes

A Kubernetes distribution is a packaged version of Kubernetes that includes additional features, tools, integrations, and support. A Kubernetes distribution aims to simplify the installation, configuration, management, and operation of Kubernetes clusters.
🔴Types of Kubernetes Distributions  

Enterprise Kubernetes: designed for large enterprises and offer additional features such as advanced security, scalability, and manageability. Examples of enterprise Kubernetes distributions include Red Hat OpenShift, IBM Cloud Kubernetes Service, and Google Kubernetes Engine (GKE)

Community Kubernetes: designed for the open-source community and offer a range of features and tools that are not available in Vanilla Kubernetes. Examples of community Kubernetes distributions include CoreOS, Fedora, and openSUSE

Cloud-Native Kubernetes: designed for cloud-native applications and offer additional features such as serverless computing, service mesh, and observability. Examples of cloud-native Kubernetes distributions include AWS Elastic Kubernetes Service (EKS), Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE)

Hybrid Kubernetes: designed for organizations that need to run Kubernetes on-premises and in the cloud. Examples of hybrid Kubernetes distributions include VMware Tanzu, OpenStack, and Kubernetes on AWS


👍Pros  

Easier to use: provide a more streamlined and simplified experience for users, with additional tools and features that make it easier to deploy and manage applications
Additional features: offer additional features and capabilities, such as advanced security, scalability, and manageability
Support and maintenance: typically supported and maintained by the vendor, which means that users can receive support and updates more quickly and easily
Integration with other tools: integrate with other tools and services, such as monitoring, logging, and security solutions, which can simplify the deployment and management of applications
Security and compliance: some distributions come with built-in security features and compliance controls, making it easier for organizations to meet industry regulations and protect their applications and data

👎Cons  

Less flexible: they are designed to meet specific needs and use cases. This can limit the range of applications and workloads that can be deployed
Vendor lock-in: can lead to vendor lock-in, as users may become dependent on the vendor’s tools and featuresAdditional cost: often come with additional costs, such as support and maintenance fees, which can add to the overall cost of using Kubernetes

]]></content>
      <categories>
        <category>k8s</category>
        <category>FAQ</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>k8s_faq</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_ha</title>
    <url>/2021/06/08/k8s-ha/</url>
    <content><![CDATA[IntroductionOne of the main advantages of Kubernetes is how it brings greater reliability and stability to the container-based distributed application, through the use of dynamic scheduling of containers. But, how do you make sure Kubernetes itself stays up when a component or its master node goes down?
Kubernetes High-Availability is about setting up Kubernetes, along with its supporting components in a way that there is no single point of failure. A single master cluster can easily fail, while a multi-master cluster uses multiple master nodes, each of which has access to same worker nodes. In a single master cluster the important component like API server, controller manager lies only on the single master node and if it fails you cannot create more services, pods etc. However, in case of Kubernetes HA environment, these important components are replicated on multiple masters(usually three masters) and if any of the masters fail, the other masters keep the cluster up and running.


HA topologyThere two options for configuring the topology of your highly available (HA) Kubernetes clusters.
You can set up an HA cluster:

With stacked control plane nodes, where etcd nodes are colocated with control plane nodes
With external etcd nodes, where etcd runs on separate nodes from the control plane

Stacked etcd topologyA stacked HA cluster is a topology where the distributed data storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by kubeadm that run control plane components.
Each control plane node runs an instance of the kube-apiserver, kube-scheduler, and kube-controller-manager. The kube-apiserver is exposed to worker nodes using a load balancer.
Each control plane node creates a local etcd member and this etcd member communicates only with the kube-apiserver of this node. The same applies to the local kube-controller-manager and kube-scheduler instances.
This topology couples the control planes and etcd members on the same nodes
Proc:

It is simpler to set up than a cluster with external etcd nodes, and simpler to manage for replication.

Cons

A stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a control plane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.

This is the default topology in kubeadm. A local etcd member is created automatically on control plane nodes when using kubeadm init and kubeadm join –control-plane.

External etcd topologyAn HA cluster with external etcd is a topology where the distributed data storage cluster provided by etcd is external to the cluster formed by the nodes that run control plane components.
Like the stacked etcd topology, each control plane node in an external etcd topology runs an instance of the kube-apiserver, kube-scheduler, and kube-controller-manager. And the kube-apiserver is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the kube-apiserver of each control plane node.
This topology decouples the control plane and etcd member.Proc:

It provides an HA setup where losing a control plane instance or an etcd member has less impact and does not affect the cluster redundancy as much as the stacked HA topology.

Cons:

This topology requires twice the number of hosts as the stacked HA topology. A minimum of three hosts for control plane nodes and three hosts for etcd nodes are required for an HA cluster with this topology.


API server LB and HA
Note  

use HA proxy to expose virtual IP that’s used by worker node for connection, behind HA proxy is the real API server that serves the request.
use Keepalived for HA proxy HA, if one HA proxy is down, the other takes over.

Ref
K8s HA topology
HA cluster setup Kubeadm
external etcd cluster

]]></content>
      <categories>
        <category>k8s</category>
        <category>HA</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>HA</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_kata_container</title>
    <url>/2021/06/09/k8s-kata-container/</url>
    <content><![CDATA[more effort
]]></content>
  </entry>
  <entry>
    <title>k8s_operator</title>
    <url>/2021/06/08/k8s-operator/</url>
    <content><![CDATA[Overview
WhenConsider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other declarative API. it’s mostly used for complex stateful application.
HowCustom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself. Once a custom resource is installed, users can create and access its objects using kubectl, just as they do for built-in resources like Pods.
Operator patternThe combination of a custom resource API and a control loop is called the Operator pattern, The Operator pattern is used to manage specific, usually stateful, applications.
Kubernetes provides two ways to add custom resources to your cluster:

CRDs are simple and can be created without any programming.
API Aggregation requires programming, but allows more control over API behaviors like how data is stored and conversion between API versions.

CRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.
Typically, CRDs are a good fit if:

You have a handful of fields
You are using the resource within your company, or as part of a small open-source project (as opposed to a commercial product)

Operator patternCRDThe CustomResourceDefinition(CRD) API resource allows you to define custom resources. Defining a CRD object creates a new custom resource with a name and schema that you specify.
Kubernetes client libraries can be used to access custom resources. Not all client libraries support custom resources. The Go and Python client libraries do.
When you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify. The CRD can be either namespaced or cluster-scoped, as specified in the CRD’s scope field. As with existing built-in objects, deleting a namespace deletes all custom objects in that namespace. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.
resourcedefinition.yaml
apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata:  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;  name: crontabs.stable.example.comspec:  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;  group: stable.example.com  # list of versions supported by this CustomResourceDefinition  versions:    - name: v1      # Each version can be enabled/disabled by Served flag.      served: true      # One and only one version must be marked as the storage version.      storage: true      schema:        openAPIV3Schema:          type: object          properties:            spec:              type: object              # we can add more to each filed or property, like              # - default value              # - validation              # - limit              properties:                cronSpec:                  type: string                image:                  type: string                replicas:                  type: integer  # either Namespaced or Cluster  scope: Namespaced  names:    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;    plural: crontabs    # singular name to be used as an alias on the CLI and for display    singular: crontab    # kind is normally the CamelCased singular type. Your resource manifests use this.    kind: CronTab    # shortNames allow shorter string to match your resource on the CLI    shortNames:    - ct

$ kubectl apply -f resourcedefinition.yaml# Then a new namespaced RESTful API endpoint is created at:/apis/stable.example.com/v1/namespaces/*/crontabs/...# then you can create custom object with these API.# my-crontab.yamlapiVersion: &quot;stable.example.com/v1&quot;kind: CronTabmetadata:  name: my-new-cron-objectspec:  cronSpec: &quot;* * * * */5&quot;  image: my-awesome-cron-image$ kubectl apply -f my-crontab.yaml$ kubectl get crontab

Delete CRDWhen you delete a CustomResourceDefinition, the server will uninstall the RESTful API endpoint and delete all custom objects stored in it.
More detail to define CRD, refer to CRD guide.
controllercustom controller is a controller who is control loop that watches the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
custom controller is also client of the Kubernetes API that act on a Custom Resource, custom controller written with Go or Python runs a daemon to watch Custom Resource through API server and takes proper action to make it in desired state.
In order to write custom controller, you have to learn detail about client library, kubernetes provides client library for various programming languages, like Go, Python, Java, JS etc.
Samples:

Here is a Go client sample.
Here is a Go controller sample

Tools of writing operatorOperator involves defining CRD and writing custom controller, it’s little complex if starting from zero, as there are lots of common logic for custom controller, hence there are some tools you can use to write your own cloud native Operator.

Charmed Operator Framework
kubebuilder
KUDO (Kubernetes Universal Declarative Operator)
Metacontroller along with WebHooks that you implement yourself
Operator Framework (most popular one)
shell-operator

REF
Operator framework
k8s Operator
Operator hub, operator samples
awesome operators, samples
Sample controller without tool
CRD

]]></content>
      <categories>
        <category>k8s</category>
        <category>operator</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>crd</tag>
        <tag>operator</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_minikube</title>
    <url>/2021/03/24/k8s-minikube/</url>
    <content><![CDATA[minikubeThere are several tools to setup Vanilla Kubernetes, like kubeadm, minikube, Kubespray, while minikube is to setup Vanilla Kubernetes locally, focusing on making it easy to learn and develop for Kubernetes.


# start k8s cluster$ minikube start# for china use this one$ sudo minikube start --driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --force --image-mirror-country=cn$ minikube pause$ minikube stop$ minikube statusminikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: ConfiguredtimeToStop: Nonexistent$ minikube delete --all$ minikube addons list# enable ingress plugin$ minikube addons enable ingressThe &#x27;ingress&#x27; addon is enabled.# after ingress is enabled, an ingress pod is running listen on 80 by nginx# and meanwhile docker-proxy starts as well on the node which ingress pod runs.# so that if you access that node on port 80, it will proxy the traffic to ingress container# which performs ingress rules and selects the proper endpointroot     10678  1009  0 Jun30 ?        00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.6 -container-port 80$ kubectl cluster-infoKubernetes control plane is running at https://10.116.5.201:8443KubeDNS is running at https://10.116.5.201:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy# after cluster starts, check or deploy pod, service$ kubectl get po -ANAMESPACE     NAME                          READY   STATUS             RESTARTS   AGEkube-system   coredns-54d67798b7-f6wq8      1/1     Running            0          11hkube-system   etcd-dev                      1/1     Running            0          11hkube-system   kube-apiserver-dev            1/1     Running            0          11hkube-system   kube-controller-manager-dev   1/1     Running            0          11hkube-system   kube-proxy-fsd9r              1/1     Running            0          11hkube-system   kube-scheduler-dev            1/1     Running            0          11hkube-system   storage-provisioner           0/1     ImagePullBackOff   0          11h# Create a sample deployment and expose it on port 8080:$ kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4$ kubectl expose deployment hello-minikube --type=NodePort --port=8080# check service$ kubectl get services hello-minikube# map to host port 7080$ kubectl port-forward service/hello-minikube 7080:8080$ curl http://localhost:7080/

troubleshootingdashboard is not working# run minikube command with more logs for troubleshooting$ minikube addons enable dashboard$ minikube addons enable metrics-server# if pod is not ready due to pull image error like this$ kubectl get po -n kubernetes-dashboard --show-labelsNAME                                         READY   STATUS             RESTARTS   AGE   LABELSdashboard-metrics-scraper-8554f74445-rnq6d   0/1     ImagePullBackOff   0          15m   k8s-app=dashboard-metrics-scraper,pod-template-hash=8554f74445kubernetes-dashboard-6c87f58d7c-48gk7        0/1     ImagePullBackOff   0          15m   gcp-auth-skip-secret=true,k8s-app=kubernetes-dashboard,pod-template-hash=6c87f58d7c# change the image for these pods# registry.cn-hangzhou.aliyuncs.com/lxm-k8s/metrics-scraper:v1.0.6# registry.cn-hangzhou.aliyuncs.com/lixunan/kubernetes-dashboard:v2.1.0$ sudo kubectl set image deploy kubernetes-dashboard kubernetes-dashboard=registry.cn-hangzhou.aliyuncs.com/lixunan/kubernetes-dashboard:v2.1.0 -n  kubernetes-dashboard$ sudo kubectl set image deploy dashboard-metrics-scraper dashboard-metrics-scraper=registry.cn-hangzhou.aliyuncs.com/lxm-k8s/metrics-scraper:v1.0.6 -n kubernetes-dashboard# local access from this machine$ minikube dashboard🤔  Verifying dashboard health ...🚀  Launching proxy ...🤔  Verifying proxy health ...http://127.0.0.1:43909/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/# remote access by proxy, all interfaces$ sudo kubectl proxy --address=&#x27;0.0.0.0&#x27; --disable-filter=true# http://10.117.5.21:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/#/overview?namespace=default$ kubectl get po -n kubernetes-dashboard --show-labels$ kubectl describe po -l k8s-app=kubernetes-dashboard -n kubernetes-dashboard$ kubectl logs -l k8s-app=kubernetes-dashboard -n kubernetes-dashboard
Ref
Get Started

]]></content>
      <categories>
        <category>k8s</category>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_network_cni</title>
    <url>/2021/06/08/k8s-network-cni/</url>
    <content><![CDATA[IntroductionCNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted. Because of this focus, CNI has a wide range of support and the specification is simple to implement.
A CNI plugin is responsible for inserting a network interface into the container network namespace(e.g., one end of a virtual ethernet (veth) pair) and making any necessary changes on the host (e.g., attaching the other end of the veth into a bridge). It then assigns an IP address to the interface and sets up the routes consistent with the IP Address Management section by invoking the appropriate IP Address Management (IPAM) plugin.
Main tasks  

🔴insert interface in container
🔴assign ip address to container
🔴setup routes or iptables rules


Plugins provided by CNI projectsinterface

bridge：create a bridge and add container to it
loopback：create a loopback
ptp: create veth pair
vlan: create a vlan dev

IPAM：IP assignment

host-local：create and maintain a local ip database

Meta:

flannel：create flannel dev and set route for cross node communication
tuning：adjust the seeting for a dev
portmap：map hostport to container port using iptables.

PluginKubernets network modelKubernetes imposes the following fundamental requirements on any networking implementation for pods on the same node

pods on a node can communicate with all pods on all nodes without NAT
agents on a node (e.g. system daemons, kubelet) can communicate with all pods on that node

But it does NOT impose pods comunication between nodes, this is done by Network plugins which mostly implement CNI, so that kubernetes calls CNI standard API to config its network.

CNI plugins

Calico: Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads
Cilium: Cilium is open source software for providing and transparently securing network connectivity between application containers. Cilium is L7&#x2F;HTTP aware and can enforce network policies on L3-L7 using an identity based security model that is decoupled from network addressing, and it can be used in combination with other CNI plugins
Flannel: Flannel is a very simple overlay network that satisfies the Kubernetes requirements
OVN: OVN is an opensource network virtualization solution developed by the Open vSwitch community. It lets one create logical switches, logical routers, stateful ACLs, load-balancers etc to build different virtual networking topologies.



ACI: Cisco Application Centric Infrastructure offers an integrated overlay and underlay SDN solution that supports containers, virtual machines, and bare metal servers
Antrea: It leverages Open vSwitch as the networking data plane.
AWS VPC CNI for Kubernetes
Azure CNI for Kubernetes
Google Compute Engine (GCE)

👍How CNI plugin is called  

When the container runtime expects to perform network operations on a container, it (like the kubelet in the case of K8s) calls the CNI plugin with the desired command(ADD, DEL etc)
The container runtime also provides related network configuration and container-specific data to the plugin.
The CNI plugin performs the required operations and reports the result.

FlannelThe CNI plugin is selected by passing Kubelet the --network-plugin=cni command-line option. Kubelet reads a file from --cni-conf-dir (default /etc/cni/net.d) and uses the CNI configuration from that file to set up each pod’s network. The CNI configuration file must match the CNI specification, and any required CNI plugins referenced by the configuration must be present in --cni-bin-dir (default /opt/cni/bin).
CNI Related Files

Kubelet has --network-plugin=cni command-line option
cni conf file at --cni-conf-dir (default /etc/cni/net.d)
cni plugin(binary) at --cni-bin-dir (default /opt/cni/bin).
IPAM of host-local for used IP address at /var/lib/cni/networks/cbr0

Ref
Networking
CNI Project
CNI introduction
CNI website

]]></content>
      <categories>
        <category>k8s</category>
        <category>CNI</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_service_deep</title>
    <url>/2021/06/08/k8s-service-deep/</url>
    <content><![CDATA[ServiceA Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods(selected by label selector) providing the same service.  service has an IP address and port that never change while the service exists, but Pod address could change during upgrade, or pod is removed or deleted during scale, hence we SHOULD NOT access pod address directly for a service, we need a dedicated ip for the cased mentioned, that’s why service comes in.
More details about service, refer to k8s service
enable source ip persistence for a serviceIf you want to make sure that connections from a particular client are passed to the same Pod each time, you can select the session affinity based on the client’s IP addresses by setting service.spec.sessionAffinity to &quot;ClientIP&quot; (the default is &quot;None&quot;). You can also set the maximum session sticky time by setting service.spec.sessionAffinityConfig.clientIP.timeoutSeconds appropriately. (the default value is 10800, which works out to be 3 hours).

kube-proxykube-proxy is a key component of any Kubernetes deployment.  Its role is to load-balance traffic that is destined for services (via cluster IPs and node ports) to the correct backend pods.  Kube-proxy can run in one of three modes, each implemented with different data plane technologies: userspace, iptables, or IPVS.  
The userspace mode is very old, slow, and definitely not recommended!  we DO NOT discuss it here.
iptables vs IPVS

IPVS has better performance with larger service and pods
IPVS has more algorithms then iptables
IPVS supports server health checking and connection retries, etc.

Note  

cluser ip of service, pod ip and endpoint are assigned by controller manager
kube-proxy watches apiserver for service and endpoint object, then update iptables or IPVS rules.
kube-proxy runs in each node(kube-system namespace)

Why not use round-robin DNS to replace kube-proxy?A question that pops up every now and then is why Kubernetes relies on proxying to forward inbound traffic to backends. What about other approaches? For example, would it be possible to configure DNS records that have multiple A values (or AAAA for IPv6), and rely on round-robin name resolution?
There are a few reasons for using proxying for Services:

There is a long history of DNS implementations not respecting record TTLs, and caching the results of name lookups after they should have expired.
Some apps do DNS lookups only once and cache the results indefinitely.
Even if apps and libraries did proper re-resolution, the low or zero TTLs on the DNS records could impose a high load on DNS that then becomes difficult to manage

IptablesIn this mode, kube-proxy watches the Kubernetes control plane for the addition and removal of Service and Endpoint objects. For each Service, it installs iptables rules, which capture traffic to the Service’s clusterIP and port, and redirect that traffic to one of the Service’s backend sets. For each Endpoint object, it installs iptables rules which select a backend Pod.

By default, kube-proxy in iptables mode chooses a backend at random.If kube-proxy is running in iptables mode and the first Pod that’s selected does not respond, the connection fails, there is no try next pod!!!
When access service by cluster ip(inside cluster), OUTPUT chain is checked., while when access service by NodePort address, PREROUTING chain is checked, but both will jump to KUBE-SERVICE chain created by kube-proxy, more detail see section below enable iptables for kube-proxy.
Ipvsn IPVS mode, kube-proxy watches Kubernetes Services and Endpoints, calls netlink interface to create IPVS rules accordingly and synchronizes IPVS rules with Kubernetes Services and Endpoints periodically. This control loop ensures that IPVS status matches the desired state. When accessing a Service, IPVS directs traffic to one of the backend Pods.
The IPVS proxy mode is based on netfilter hook function that is similar to iptables mode, but uses a hash table as the underlying data structure and works in the kernel space. That means kube-proxy in IPVS mode redirects traffic with lower latency than kube-proxy in iptables mode, with much better performance when synchronising proxy rules. Compared to the other proxy modes, IPVS mode also supports a higher throughput of network traffic.
IPVS provides more options for balancing traffic to backend Pods; these are:

rr: round-robin
lc: least connection (smallest number of open connections)
dh: destination hashing
sh: source hashing
sed: shortest expected delay
nq: never queue


When creating a ClusterIP type Service, IPVS proxier will do the following three things:

Make sure a dummy interface exists in the node, defaults to kube-IPVS0
Bind Service IP addresses to the dummy interface
Create IPVS virtual servers for each Service IP address respectively

configenable iptables modeWhen access service by cluster ip(inside cluster), OUTPUT chain is checked., while when access service by NodePort address, PREROUTING chain is checked, but both will jump to KUBE-SERVICE chain created by kube-proxy.
Ipvs# set from beginning when create cluster Cluster Created by Kubeadm# conf file for kubeadm init...kubeProxy:  config:    mode: &quot;&quot;...# change after kube-proxy runs$ kubectl edit configmaps kube-proxy -n kube-system$ kubectl get  svcNAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGEnginx-1623168286   NodePort    10.1.172.5   &lt;none&gt;        80:31067/TCP   9m11s# service cluster ip(10.1.172.5:80) and  (nodeaddress:31067) for the service$ kubectl get epNAME               ENDPOINTS            AGEnginx-1623168286   10.2.2.18:8080       9m2s# check KUBE-SERVICE chain$ iptables -nv -L PREROUTING -t natChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target     prot opt in     out     source               destination         25960 2231K KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */$ iptables -nv -L OUTPUT  -t natChain OUTPUT (policy ACCEPT 39 packets, 2451 bytes) pkts bytes target     prot opt in     out     source               destination          986K   60M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */ # nat table of kube-service chain, rule for cluster-ip named with KUBE-SVC-XXX and nodePort(stay at last rule) $ iptables -nv -L KUBE-SERVICES -t natChain KUBE-SERVICES (2 references) pkts bytes target     prot opt in     out     source               destination             0     0 KUBE-SVC-YTBFCGJW6SOUTSSA  tcp  --  *      *       0.0.0.0/0            10.1.172.5           /* default/nginx-1623168286:http cluster IP */ tcp dpt:80  303 18611 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL$ iptables -nv -L KUBE-NODEPORTS  -t natChain KUBE-NODEPORTS (1 references) pkts bytes target     prot opt in     out     source               destination             0     0 KUBE-SVC-YTBFCGJW6SOUTSSA  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-1623168286:http */ tcp dpt:31067# SVC backend rule named with KUBE-SEP-XXX$ iptables -nv -L KUBE-SVC-YTBFCGJW6SOUTSSA -t natChain KUBE-SVC-YTBFCGJW6SOUTSSA (2 references) pkts bytes target     prot opt in     out     source               destination             0     0 KUBE-SEP-NUA5P77FXIMWW66U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-1623168286:http */$ iptables -nv -L KUBE-SEP-NUA5P77FXIMWW66U  -t natChain KUBE-SEP-NUA5P77FXIMWW66U (1 references) pkts bytes target     prot opt in     out     source               destination             0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/nginx-1623168286:http */ tcp to:10.2.2.20:8080# real backend

enable IPVS mode# load module &lt;module_name&gt;$ modprobe -- ip_vs$ modprobe -- ip_vs_rr$ modprobe -- ip_vs_wrr$ modprobe -- ip_vs_sh$ modprobe -- nf_conntrack_ipv4# OR (use nf_conntrack instead of nf_conntrack_ipv4 for Linux kernel 4.19 and later)$ modprobe -- nf_conntrack $ lsmod | grep -e ip_vs -e nf_conntrack_ipv4#---------------------------------------------------------------------------# set from beginning when create cluster Cluster Created by Kubeadm# conf file for kubeadm init...kubeProxy:  config:    mode: ipvs...# change after kube-proxy runs$ kubectl edit configmaps kube-proxy -n kube-system# install ipvs tool to check ipvs rule# Ubuntu18$ apt-get install ipvsadm# Centos7$ yum install -y ipvsadm# after create a service check ipvs rule$ ipvsadm -ln$ kubectl get  svcNAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGEnginx-1623168286   NodePort    10.1.172.5   &lt;none&gt;        80:31067/TCP   9m11s# service cluster ip(10.1.172.5:80) and  (nodeaddress:31067) for the service$ kubectl get epNAME               ENDPOINTS            AGEnginx-1623168286   10.2.2.18:8080       9m2s# pods that provides nginx service# For cluster-IP, kube-proxy configure it at virtual device: kube-ipvs0 # and create one rule for cluster-ip# for nodePort 31067, kube-proxy create several rules, each for one address of interfaces on the node#$ ip addr8: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default     link/ether 9a:f2:1d:c0:84:ec brd ff:ff:ff:ff:ff:ff    inet 10.1.172.5/32 scope global kube-ipvs0       valid_lft forever preferred_lft forever# $ ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn# each address of all device on that nodeTCP  192.168.56.11:31067 rr  -&gt; 10.2.2.18:8080               Masq    1      0          0              # cluster ip     TCP  10.1.172.5:80 rr  -&gt; 10.2.2.18:8080               Masq    1      0          0       # each address of all device on that nodeTCP  10.2.0.0:31067 rr  -&gt; 10.2.2.18:8080               Masq    1      0          0         TCP  10.2.0.1:31067 rr  -&gt; 10.2.2.18:8080               Masq    1      0          0         TCP  127.0.0.1:31067 rr  -&gt; 10.2.2.18:8080               Masq    1      0          0         TCP  172.17.0.1:31067 rr  -&gt; 10.2.2.18:8080               Masq    1      0          0         

NOTEWhen kube-proxy starts in IPVS proxy mode, it verifies whether IPVS kernel modules are available. If the IPVS kernel modules are not detected, then kube-proxy falls back to running in iptables proxy mode.
debug kube-proxy# check process running$ ps -ef | grep kube-proxy$ kubectl get pod -n kube-system | grep kube-proxy$ kubectl get configmaps -n kube-system# check conf for kube-proxy, mode used and parameter for each mode$ kubectl describe configmaps kube-proxy -n kube-system

corednsKubernetes DNS schedules a DNS Pod and Service on the cluster, and configures the kubelets to tell individual containers to use the DNS Service’s IP to resolve DNS names.
Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name. By default, a client Pod&#39;s DNS search list includes the Pod&#39;s own namespace and the cluster&#39;s default domain.
You can (and almost always should) set up a DNS service for your Kubernetes cluster using an add-on.
A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services and creates a set of DNS records for each one. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name.
For example, if you have a Service called my-service in a Kubernetes namespace my-ns, the control plane and the DNS Service acting together create a DNS record for my-service.my-ns. Pods in the my-ns namespace should be able to find the service by doing a name lookup for my-service (my-service.my-ns would also work).
Pods in other namespaces must qualify the name as my-service.my-ns. These names will resolve to the cluster IP assigned for the Service.
Kubernetes also supports DNS SRV (Service) records for named ports. If the my-service.my-ns Service has a port named http with the protocol set to TCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for http, as well as the IP address.
Ref
service example

IPVS VS iptables

IPVS in k8s

coredns

kube-proxy

service inside


]]></content>
      <categories>
        <category>k8s</category>
        <category>service</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_service_mesh</title>
    <url>/2021/12/15/k8s-service-mesh/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>k8s_schedule_inside</title>
    <url>/2021/06/08/k8s-schedule-inside/</url>
    <content><![CDATA[more more effort
]]></content>
  </entry>
  <entry>
    <title>k8s_pkg_manager</title>
    <url>/2021/06/07/k8s-pkg-manager/</url>
    <content><![CDATA[IntroductionHelm is the best way to find, share, and use software built Kubernetes, a software in k8s may be one separate deployment or several deployments that works together to provides service to user, Helm manages these yaml files with concept Chart, Chart is a bundle of yaml files and other files related to the software, Chart helps you define, install, and upgrade, rollback even the most complex Kubernetes application, Chart likes deb package which creates package with xx.deb, while to create an application for k8s by Helm, you need to create Chart files with fixed layout and follow its syntax.


For Helm, there are three important concepts:

The chart is a bundle of information necessary to create an instance of a Kubernetes application. mostly for complex application, but for deployment only, no auto scale, no monitor with Chart.
The config contains configuration information that can be merged into a packaged chart to create a releasable object.
🟢 A release is a running instance of a chart, combined with a specific config. with release version, we can update&#x2F;rollback a chart

CommandIn order to use helm, we need to install it firstly with below command
# Centos7 to install snap$ yum install -y snapd$ systemctl enable --now snapd.socket$ ln -s /var/lib/snapd/snap /snap$ sudo snap install helm --classic
🔴 As helm installs application in k8s(create deployment), hence in order to use it, you MUST have kubernetes and kubectl installed before. there are required!!!
Helm and K8s compatibility
Helm Version	Supported Kubernetes Versions3.5.x	        1.20.x - 1.17.x3.4.x	        1.19.x - 1.16.x3.3.x	        1.18.x - 1.15.x3.2.x	        1.18.x - 1.15.x3.1.x	        1.17.x - 1.14.x3.0.x	        1.16.x - 1.13.x2.16.x	        1.16.x - 1.15.x2.15.x	        1.15.x - 1.14.x2.14.x	        1.14.x - 1.13.x2.13.x	        1.13.x - 1.12.x2.12.x	        1.12.x - 1.11.x2.11.x	        1.11.x - 1.10.x2.10.x	        1.10.x - 1.9.x2.9.x	        1.10.x - 1.9.x...

k8s version supported by Helm
Command used for helm$ sudo snap install helm --classic#------------------------------------repo related----------------------------------# Hub is groups of package from many repos!!!# Official Hub: https://artifacthub.io/$ helm search hub mysql# search from official hub which will tell you which repo has such package!!!# then add the right repo with below command# add a chart repo, hence you can search, download, install charts from there$ helm repo add bitnami https://charts.bitnami.com/bitnami$ helm repo listNAME                	URL                                               bitnami              	https://charts.bitnami.com/bitnami$ helm repo update       # Make sure we get the latest list of charts$ helm search repo mysql # search from user added repo#-----------------------------------install/uninstall package-----------------------$ helm show chart bitnami/mysql$ helm show values bitnami/mysql# get all chart info$ helm show all bitnami/mysql# get deps of this chart$ helm dep list bitnami/mysql$ helm install bitnami/mysql --generate-name# Whenever you install a chart, a new release is created.# So one chart can be installed multiple times into the same cluster.# And each can be independently managed and upgraded.# install with custom value, note myvals.yaml is merged with default!!!!$ helm show values bitnami/mysql$ helm install --generate-name --values=./myvals.yaml bitnami/mysql# if only change few values, you can pass it from commandline$ helm install --generate-name --set auth.database=&quot;my_db&quot; bitnami/mysqlOR# another way to install a package# 1. first only download(default path: ~/.cache/helm/repository/mysql.tgz)# 2. start package from chart file manually$ helm install ~/.cache/helm/repository/mysql.tgz --generate-name# show installed chart$ helm ls# uninstall a chart$ helm uninstall mysql-1612624192# check chart status$ helm status mysql-1612624192# get yaml for this application$ helm get manifest mysql-1612624192

Build a pkg with HelmA chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.
Charts are created as files laid out in a particular directory tree. They can be packaged into versioned archives to be deployed.
wordpress/  Chart.yaml          # A YAML file containing information about the chart  LICENSE             # OPTIONAL: A plain text file containing the license for the chart  README.md           # OPTIONAL: A human-readable README file  values.yaml         # The default configuration values for this chart  values.schema.json  # OPTIONAL: A JSON Schema for imposing a structure on the values.yaml file  charts/             # A directory containing any charts upon which this chart depends.  crds/               # Custom Resource Definitions  templates/          # A directory of templates that, when combined with values,                      # will generate valid Kubernetes manifest files.  templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes

Details about charts, refer to charts guidline
Helm Chart templates are written in the Go template language, with the addition of 50 or so add-on template functions from the Sprig library and a few other specialized functions.
All template files are stored in a chart’s templates&#x2F; folder. When Helm renders the charts, it will pass every file in that directory through the template engine
Predefined Values
Values that are supplied via a values.yaml file (or via the –set flag) are accessible from the .Values object in a template. But there are other pre-defined pieces of data you can access in your templates.
The following values are pre-defined, are available to every template, and cannot be overridden. As with all values, the names are case sensitive.

Release.Name: The name of the release (not the chart)
Release.Namespace: The namespace the chart was released to.
Release.Service: The service that conducted the release.
Release.IsUpgrade: This is set to true if the current operation is an upgrade or rollback.
Release.IsInstall: This is set to true if the current operation is an install.
Chart: The contents of the Chart.yaml. Thus, the chart version is obtainable as Chart.Version and the maintainers are in Chart.Maintainers.
Files: A map-like object containing all non-special files in the chart. This will not give you access to templates, but will give you access to additional files that are present (unless they are excluded using .helmignore). Files can be accessed using { { index .Files “file.name” } } or using the { {.Files.Get name } } function. You can also access the contents of the file as byte using { { .Files.GetBytes } }
Capabilities: A map-like object that contains information about the versions of Kubernetes ({ { .Capabilities.KubeVersion } }) and the supported Kubernetes API versions ({ { .Capabilities.APIVersions.Has “batch&#x2F;v1” } })

NOTE: Any unknown Chart.yaml fields will be dropped. They will not be accessible inside of the Chart object. Thus, Chart.yaml cannot be used to pass arbitrarily structured data into the template. The values file can be used for that, though.
Steps to create your own chart
$ helm create mychart# modify template and values below# syntax check$ helm lint mychart# package it with xx.tgz$ helm package mychart

Ref
Helm Hub
Helm Website

]]></content>
      <categories>
        <category>k8s</category>
        <category>package</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>libvirt-chardev</title>
    <url>/2022/04/08/libvirt-chardev/</url>
    <content><![CDATA[Serial&#x2F;Parallel&#x2F;ConsoleSerial Port vs Parallel PortSerial port is used to connect a serial device to the computer and capable of transmitting one bit at a time.
serial port used for

Mouse - One of the most commonly used devices for serial ports, usually used with computers with no PS&#x2F;2 or USB ports and specialty mice.

Parallel port is used to connect a parallel device to the computer and capable of transmitting 8 bits at a time.parallel port used for?
Today, the parallel port has widely been replaced by the USB port. However, below is a listing of various hardware components that were used with the parallel port

Printer - The most common use for the parallel port.
Scanner - Another commonly used parallel device is a parallel port scanner. Parallel port scanners are a popular alternative to SCSI scanners because of how easy they are to install.

The main difference between a serial port and a parallel port is that a serial port transmits data one bit after another, while a parallel port transmits all 8 bits of a byte in parallel. Thus a parallel port transmits data much faster than a serial port.
The ports COM1 and COM2 on your computer are serial ports and the LPT1 port is a parallel port.

Common serial port(builtin motherboard&#x2F;isa serial) names are /dev/ttyS0, &#x2F;dev&#x2F;ttyS1, etc. Then around the year 2000 came the USB bus with names like /dev/ttyUSB0 and &#x2F;dev&#x2F;ttyACM1 (for the ACM modem on the USB bus). Multiport serial card used somewhat differnt names (depending on the brand) such as /dev/ttyE5.
# check how many serial ports that a PC has$ ls /dev/ttyS*/dev/ttyS0  /dev/ttyS1  /dev/ttyS2  /dev/ttyS3# if it&#x27;s usb serial port(converted usb port to serial port)$ ls /dev/USB*/dev/ttyUSB0  /dev/USB1 /dev/USB2 /dev/USB3# check perallel ports$ ls /dev/lp*



ConsoleThe physical interface of console port is RJ45(but not ethernet port which is also RJ45 like management port), the other end is always PC serial port. you can’t telnet&#x2F;SSH into a console port. Console port is an “up close and personal” port:  You need to have physical access to the appliance in order to use the console port.









PARAMETER
CONSOLE PORT
MANAGEMENT PORT


IP address Assignment
Can’t give IP address to console Port
IP address can be given to a management port


Communication Type
Asynchronous
Synchronous


Remote access via Telnet&#x2F;SSH
No
Yes


Access required
Physical access to device required
IP reachability and TCP port 23 (for  telnet) or  TCP port 443 (for SSH) required or HTTP (80)


Segregation type
Physically separate connection
Generally a VRF based traffic segregation


Maximum Speed
0.1 Mbps (115200 bps)
1 Gbps


Connectivity Type
Serial , DB9 , Rj45
RJ45


Management type
Out of Band Management
Out of Band Management


Boot Sequence
Shows Boot sequence
Does not show boot sequence


SNMP, Logging on interface
No SNMP, syslog configurable on console interface
SNMP, syslog configurable on management interface


Application required
HyperTerminal Telnet&#x2F;SSH
Web GUI


Console (Serial) Port: Connect to a PC with a serial adapter cable, so that we can get output from console to PC which connected with serial port.

console vs tty vs pty(pts)
tty &#x3D; text input&#x2F;output environment(tty is an environment)
console &#x3D; physical terminal

TTY
In unix terminology, a tty is a particular kind of device file which implements a number of additional commands (ioctls) beyond read and write. In its most common meaning, terminal is synonymous with tty. Some ttys are provided by the kernel on behalf of a hardware device, for example with the input coming from the keyboard and the output going to a text mode screen, or with the input and output transmitted over a serial line. Other ttys, sometimes called pseudo-ttys(pts), are provided (through a thin kernel layer) by programs called terminal emulators, such as Ssh (which connects a terminal on one machine with programs on another machine), Expect (for scripting terminal interactions), etc.
The console appears to the operating system as a (kernel-implemented) tty. On some systems, such as Linux and FreeBSD, the console appears as several ttys (special key combinations switch between these ttys); just to confuse matters, the name given to each particular tty can be “console”, ”virtual console”, ”virtual terminal”, and other variations.
PTYA pty is a pseudo-terminal - it’s a software implementation that appears to the attached program like a terminal, but instead of communicating directly with a “real” terminal, it transfers the input and output to another program. For example, when you ssh in to a machine and run ls, the ls command is sending its output to a pseudo-terminal, the other side of which is attached to the SSH daemon. A pts is the slave part of a pty. A ptmx is the master part of a pty
/dev/ptmx The idea of ptmx is that your application creates a virtual console for communication with other applications or with the operating system. By opening ptmx, an application gets a filed descriptor (basically a number) which gives your application the possibility to communicate over a virtual terminal with other applications. These other applications can open your terminal by opening &#x2F;dev&#x2F;pts&#x2F;12345 for example.
The only function of /dev/ptmx is to provide your application with a file descriptor of a newly created /dev/pts/ device which can be used by echo.
PTS

Stands for pseudo terminal slave.
A pts is the slave part of a pty.
A pty (pseudo terminal device) is a terminal device which is emulated by an other program (example: xterm, screen, or ssh are such programs).
&#x2F;dev&#x2F;pts contains entries corresponding to devices. &#x2F;dev&#x2F;pts is a special directory that is created dynamically by the Linux kernel. The contents of the directory vary with time and reflect the state of the running system.
The entries in &#x2F;dev&#x2F;pts correspond to pseudo-terminals (or pseudo-TTYs, or PTYs).
In laymen terms the primary difference between TTY and PTS is the type of connection to the computer. TTY ports are direct connections to the computer such as a keyboard&#x2F;mouse or a serial connection to the device. PTS connections are SSH connections or telnet connections. All of these connections can connect to a shell which will allow you to issue commands to the computer.

CharDevA character device provides a way to interact with the virtual machine. Paravirtualized consoles, serial ports, parallel ports and channels are all classed as character devices and so represented using the same syntax.
To specify the consols, channel and other devices configuration settings, use a management tool to make the following changes to the domain XML
&lt;devices&gt;  &lt;!--serial device --&gt;  &lt;serial type=&#x27;file&#x27;&gt;                                                          &lt;source path=&#x27;/tmp/serial.log&#x27;/&gt;                                           &lt;target type=&#x27;isa-serial&#x27; port=&#x27;0&#x27;&gt;  &lt;!--/dev/ttyS0--&gt;                                         &lt;model name=&#x27;isa-serial&#x27;/&gt;                                                &lt;/target&gt;                                                                   &lt;alias name=&#x27;serial0&#x27;/&gt;                                                   &lt;/serial&gt;                                                                   &lt;!--console for early boot message--&gt;  &lt;console type=&#x27;file&#x27;&gt;                                                         &lt;source path=&#x27;/tmp/serial.log&#x27;/&gt;                                           &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt;   &lt;!--/dev/ttyS0--&gt;                                     &lt;alias name=&#x27;serial0&#x27;/&gt;                                                   &lt;/console&gt;                                                                  &lt;!--virtio console--&gt;  &lt;!--/dev/hvc0, after system boot and loaded virtio driver--&gt;  &lt;console type=&#x27;pty&#x27;&gt;                                                          &lt;source path=&#x27;/dev/pts/5&#x27;/&gt;                                                 &lt;target type=&#x27;virtio&#x27; port=&#x27;1&#x27;/&gt;                                            &lt;alias name=&#x27;console1&#x27;/&gt;                                                  &lt;/console&gt;                                                                  &lt;!--virtio console--&gt;  &lt;channel type=&#x27;unix&#x27;&gt;                                                         &lt;source mode=&#x27;bind&#x27; path=&#x27;/tmp/agent&#x27;/&gt;                                     &lt;target type=&#x27;virtio&#x27; name=&#x27;virtio.serial.port0&#x27; state=&#x27;disconnected&#x27;/&gt;     &lt;alias name=&#x27;channel1&#x27;/&gt;                                                    &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt;           &lt;/channel&gt;                                                                  &lt;channel type=&#x27;pty&#x27;&gt;                                                          &lt;source path=&#x27;/dev/pts/3&#x27;/&gt;                                                 &lt;target type=&#x27;virtio&#x27; name=&#x27;virtio.serial.port1&#x27; state=&#x27;disconnected&#x27;/&gt;     &lt;alias name=&#x27;channel2&#x27;/&gt;                                                    &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;3&#x27;/&gt;           &lt;/channel&gt;                                                                &lt;/devices&gt;

In each of these directives, the top-level element name (parallel, serial, console, channel) describes how the device is presented to the guest virtual machine. The guest virtual machine interface is configured by the target element.
Console(which is a serial as well)libvirt supports two console types: serial and virtio, you can use one or both at same time, but dynamic adding is not supported for serial console, virsh attach is not allowed for it, Using the serial type for the console, you get the usual /dev/ttyS0 console. Using virtio, you get /dev/hvc0 as the console device

serial console(NOTE: the backend can be pts or file)&lt;console type=&#x27;pty&#x27;&gt; &lt;!--on host auto select one--&gt;  &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;!--insde vm /dev/ttyS0--&gt;  &lt;alias name=&#x27;console0&#x27;/&gt;&lt;/console&gt;
virtio console(NOTE: the backend can be pts or file)&lt;console type=&#x27;pty&#x27;&gt;  &lt;source path=&#x27;/dev/pts/5&#x27;/&gt;&lt;!--specify the pts, not auto selecting--&gt;  &lt;target type=&#x27;virtio&#x27; port=&#x27;1&#x27;/&gt;&lt;!--inside vm /dev/hvc0--&gt;  &lt;alias name=&#x27;console1&#x27;/&gt;&lt;/console&gt;      

####################check console################################## on the guest$ echo hello &gt; /dev/console# on the host$ tail -f /tmp/serial.loghello# on host(quit screen ctrl + a, then press \)$ screen /dev/pts/5# on guest$ echo hello &gt;/dev/hvc0####################check serial################################# on the guest$ echo hello &gt;/dev/ttyS0# on the host$ tail -f /tmp/serial.loghello####################check channel################################# check virtio-ports with name set by user(actuall it&#x27;s link to /dev/vport0p2)$ ls -al  /dev/virtio-ports/total 0drwxr-xr-x.  2 root root  100 Aug  8 09:10 .drwxr-xr-x. 19 root root 3220 Aug  8 09:10 ..lrwxrwxrwx.  1 root root   11 Aug  8 09:10 virtio.serial.port0 -&gt; ../vport0p2lrwxrwxrwx.  1 root root   11 Aug  8 09:10 virtio.serial.port1 -&gt; ../vport0p3$ ls /sys/class/virtio-ports/vport0p3/dev  device  name  power  subsystem  uevent$ cat /sys/class/virtio-ports/vport0p3/name virtio.serial.port1# on host$ screen /dev/pts/3hello# inside guest$ echo hello &gt;/dev/vport0p3# on host$ socat - UNIX-CONNECT:/tmp/agent hello# inside guest$ echo hello &gt;/dev/vport0p2]]></content>
      <categories>
        <category>libvirt</category>
        <category>char</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s_tools</title>
    <url>/2021/03/24/k8s-tools/</url>
    <content><![CDATA[Kube Cluster Deployment Tools1. KubesprayKubespray provides a set of Ansible roles for Kubernetes deployment and configuration. Kubespray can use AWS, GCE, Azure, OpenStack or a bare metal Infrastructure as a Service (IaaS) platform. Kubespray is an open-source project with an open development model. The tool is a good choice for people who already know Ansible as there’s no need to use another tool for provisioning and orchestration. Kubespray uses kubeadm under the hood.
Link: https://github.com/kubernetes-incubator/kubespray


2. MinikubeMinikube allows you to install and try out Kubernetes locally. The tool is a good starting point for Kubernetes exploration. Easily launch a single-node Kubernetes cluster inside a virtual machine (VM) on your laptop. Minikube is available on Windows, Linux, and OSX. In just 5 minutes you will be able to explore Kubernetes’ main features. Launch the Minikube dashboard straight-from-the-box with just one command.
Link: https://github.com/kubernetes/minikube
3. KubeadmKubeadm is a Kubernetes distribution tool since version 1.4. The tool helps to bootstrap best-practice Kubernetes clusters on existing infrastructure. Kubeadm cannot provision infrastructure for you though. Its main advantage is the ability to launch minimum viable Kubernetes clusters anywhere. Add-ons and networking setup are both out of Kubeadm’s scope though, so you will need to install this manually or using another tool.
Link: https://github.com/kubernetes/kubeadm
4. KopsKops helps you create, destroy, upgrade, and maintain production-grade, highly available Kubernetes clusters from the command line. Amazon Web Services (AWS) is currently officially supported, with GCE in beta support, and VMware vSphere in alpha, and other platform support is planned. Kops allows you to control the full Kubernetes cluster lifecycle; from infrastructure provisioning to cluster deletion.
Link: https://github.com/kubernetes/kops
Monitoring Tools10. KubeboxKubebox is a terminal console for Kubernetes cluster which allows you to manage and monitor your cluster-live status with nice, old-school interface. Kubebox shows your pod resource usage, cluster monitoring, and container logs, etc. Additionally, you can easily navigate to the desired namespace and execute into the desired container for fast troubleshooting&#x2F;recovery.
Link: https://github.com/astefanutti/kubebox
11. Kubernetes Operational View (Kube-ops-view)Kube-ops-view is a read-only system dashboard for multiple K8s clusters. With Kube-ops-view you can easily navigate between your cluster and monitor nodes as well as your pod’s healthiness. Kube-ops-view animates some Kubernetes processes such as pod creation and termination. 
Link: https://github.com/hjacobs/kube-ops-view
12. KubetailKubetail is a small bash script which allows you to aggregate logs from multiple pods into one stream. The initial Kubetail version doesn’t have filtering or highlighting features, but there is an additional Kubetail fork on Github. This can form and perform logs coloring using multitail tools.
Link:https://github.com/johanhaleby/kubetailhttps://github.com/aks/kubetail
13. KubewatchKubewatch is a Kubernetes watcher which can publish K8s events to the team communication app, Slack. Kubewatch runs as a pod inside Kubernetes clusters and monitors changes that occur in the system. You can specify the notifications you want to receive by editing the configuration file.
Link: https://github.com/bitnami-labs/kubewatch
14. Weave ScopeWeave Scope is a troubleshooting and monitoring tool for Docker and Kubernetes clusters. It can automatically generate applications and infrastructure topologies which can help you to identify application performance bottlenecks easily. You can deploy Weave Scope as a standalone application on your local server&#x2F;laptop, or you can choose the Weave Scope Software as a Service (SaaS) solution on Weave Cloud. With Weave Scope, you can easily group, filter or search containers using names, labels, and&#x2F;or resource consumption.
Link: https://www.weave.works/oss/scope/
15. PrometheusPrometheus monitoring has fast become the go-to tool for Kubernetes monitoring tool. It offers a multi-dimensional data model and a very user-accessible format and protocols. Exposing Prometheus metrics in Kubernetes is a pretty straightforward task. The data scraped is human readable, in a self-explanatory format, and published using a standard HTTP transport.
Link: https://prometheus.io/
16. SearchlightSearchlight by AppsCode is a Kubernetes operator for Icinga. Searchlight periodically runs various checks on Kubernetes clusters and alerts you via email, SMS or chat if something goes wrong. Searchlight includes a default suite of checks written specifically for Kubernetes. Also, it can enhance Prometheus monitoring with external black-box monitoring and serves as a fallback in case internal systems completely fail.
Link: https://github.com/appscode/searchlight
17.cAdvisorCAdvisor is installed by default on all cluster nodes to collect metrics for Kubernetes about running containers and nodes. CAdvisor Kubelet exposes these metrics through Kubelet APIs (with a default of one-minute resolution). The Metrics Server identifies all available nodes and calls Kubelet API to get containers and nodes resources usage before exposing the metrics through Kubernetes aggregation API.
Link: https://github.com/google/cadvisor
18. Kube-state-metricskube-state-metrics generates metrics from Kubernetes API objects without modification by listening to the Kubernetes API server. It doesn’t examine the health of individual Kubernetes components so much as it focuses on the health of the various objects inside, such as deployments, nodes and pods.
Link: https://github.com/kubernetes/kube-state-metrics
19. Sumo Logic AppThe Sumo Logic Kubernetes App offer complete visibility into the worker nodes within your clusters, as well as for their application logs. The app allows users to monitor and troubleshoot container health, replication, load balancing, pod state and hardware resource allocation. The App utilizes Falco events to monitor and detect anomalous container, application, host, and network activity.
Link: https://www.sumologic.com/application/kubernetes/
20. DynatraceDynatrace OneAgent is container-aware and comes with built-in support for out-of-the-box monitoring of Kubernetes. Dynatrace provides full-stack monitoring for Kubernetes, i.e. monitoring from the application down to the infrastructure layer. However, if you don’t have access to the infrastructure layer, Dynatrace provides also the option of application-only monitoring.
Link: https://www.dynatrace.com/
Testing21. Kube-monkeyKube-monkey is the Kubernetes’ version of Netflix’s Chaos Monkey. Kube-monkey is a tool that follows the principles of chaos engineering. It can delete K8s pods at random, check services are failure-resilient, and contribute to your system’s healthiness. Kube-monkey is also configured by a TOML file where you can specify which app is to be killed and when to practice your recovery strategies.
Link: https://github.com/asobti/kube-monkey
22. K8s-testsuiteK8s-testsuite is made up of 2 Helm charts which work for network bandwidth testing and load testing a single Kubernetes cluster. Load tests emulate simple web-servers with loadbots which run as a Kubernetes microservice based on the Vegeta. Network tests use iperf3 and netperf-2.7.0 internally and run three times. Both sets of tests generate comprehensive log messages with all results and metrics.
Link: https://github.com/mrahbar/k8s-testsuite
23. Test-infraTest-infra is a collection of tools for Kubernetes testing and results verification. Test-infra includes a few dashboards for displaying history, aggregating failures, and showing what is currently testing. You can enhance your test-infra suite by creating your own test jobs. Test-infra can perform end-to-end Kubernetes testing with full Kubernetes lifecycle emulation on different providers using the Kubetest tool.
Link: https://github.com/kubernetes/test-infra
24. SonobuoySonobuoy allows you to understand your current Kubernetes cluster state by running a set of tests in an accessible and non-destructive manner. Sonobuoy generates informative reports with detailed information about cluster performance. Sonobuoy supports 3 Kubernetes minor versions: the current release and 2 minor versions before. Sonobuoy Scanner is a browser-based tool which allows you to test Kubernetes clusters in a few clicks, but the CLI version has a bigger set of tests available.
Link: https://sonobuoy.io/
25. PowerfulSealPowerfulSeal is a tool similar to Kube-monkey and follows the Principles of Chaos Engineering. PowerfulSeal can kill pods and remove&#x2F;add VMs from or to your clusters. In contrast to Kube-monkey, PowefulSeal has an interactive mode which allows you to manually break specific cluster components. Also, PowefulSeal doesn’t need external dependencies apart from SSH.
Link: https://github.com/bloomberg/powerfulseal
Security26. TriremeTrireme is a flexible and straightforward implementation of the Kubernetes Network Policies. Trireme works in any Kubernetes cluster and allows you to manage traffic between pods from different clusters. The main advantages of Trireme are the lack of a need for any centralized policy management, the ability to easily organize the interaction of the two resources deployed in Kubernetes, and the lack of complexities of SDN, VLAN tags, and subnets (Trireme uses a conventional L3-network).
Link: https://github.com/aporeto-inc/trireme-kubernetes
27. AporetoAporeto provides security for containers, microservices, cloud and legacy applications based on workload identity, encryption, and distributed policies. As Aporeto policies function independently of the underlying infrastructure, security policies can be enabled across Kubernetes clusters or over hybrid environments that include Kubernetes and non-Kubernetes deployments.
Link: https://www.aporeto.com/
28. TwistlockTwistlock continually monitors your applications deployed on K8s for vulnerability and compliance issues, including the underlying host as well as containers and images. In addition, Twistlock Runtime Defense automatically models container behavior, allowing known, good behavior while alerting on or blocking anomalous activity. Finally, Twistlock provides both layer 3 microsegmentation as well as a layer 7 firewall that can protect front end microservices from common attacks.
Link: https://www.twistlock.com/
29. FalcoFalco is a behavioral activity monitor designed to detect anomalous activity in your applications. Falco is based on the Sysdig Project, an open source tool (and now a commercial service), built for monitoring container performance by way of tracking kernel system calls. Falco lets you continuously monitor and detect container, application, host, and network activity with one set of rules.
Link: https://sysdig.com/opensource/falco/
30. Sysdig SecureSysdig Secure, part of the Sysdig Container Intelligence Platform, comes out-of-the-box with unmatched container visibility and deep integrations with container orchestration tools. These include Kubernetes, Docker, AWS ECS, and Apache Mesos. With Sysdig Secure you can Implement service-aware policies, block attacks, analyze your history, and monitor cluster performance. Sysdig Secure is available as cloud and on-premise software offerings.
Link: https://sysdig.com/product/secure/
31. Kubesec.ioKubesec.io is a service which allows you to score Kubernetes resources for security feature usage. Kubesec.io verifies resource configuration according to Kubernetes security best-practices. As a result, you will have total control and additional suggestions for how to improve overall system security. The site also contains plenty of external links related to containers and Kubernetes security.
Link: https://kubesec.io
Helpful CLI Tools32. CabinNOTE: This project is currently not under active development
Cabin functions as a mobile dashboard for the remote management of Kubernetes clusters. With Cabin, users can quickly manage applications, scale deployments, and troubleshoot overall K8s cluster from their Android or iOS device. Cabin is a great tool for operators of K8s clusters as it allows you to perform quick remediation actions in case of incidents.
Link: https://github.com/bitnami-labs/cabin
33. Kubectx&#x2F;KubensKubectx is a small open-source utility tool which enhances Kubectl functionality with the possibility to switch context easily and connect to a few Kubernetes clusters at the same time. Kubens allows you to navigate between Kubernetes namespaces. Both tools have an auto-completion feature on bash&#x2F;zsh&#x2F;fish shells.
Link: https://github.com/ahmetb/kubectx
34. Kube-shellKube-shell increases your productivity when working with kubectl. Kube-shell enables command auto-completion and auto-suggestion. Also, Kube-shell will provide in-line documentation about executed command. Kube-shell even can search and correct commands when wrongly typed. It’s a great tool to increase your performance and productivity in the K8s console.
Link: https://github.com/cloudnativelabs/kube-shell
35. KailKail is short for Kubernetes tail and works for Kubernetes clusters. With Kail, you can tail Docker logs for all matched pods. Kail allows you to filter pods by service, deployment, labels, and other features. Pods will be added (or removed) automatically to the log after a launch if it matches the criteria.
Link: https://github.com/boz/kail
Development Tools36. TelepresenceTelepresence provides the possibility to debug Kubernetes clusters locally by proxy data from your Kubernetes environment to the local process. Telepresence is able to provide access to Kubernetes services and AWS&#x2F;GCP resources for your local code as it will be deployed to the cluster. With Telepresence, Kubernetes counts local code as a normal pod within your cluster.
Link: https://www.telepresence.io/
37. HelmHelm is a package manager for Kubernetes. It is like APT&#x2F;Yum&#x2F;Homebrew, but for Kubernetes. Helm operates with Charts which is an archive set of Kubernetes resource manifests that make up a distributed application. You can share your application by creating a Helm chart. Helm allows you to create reproducible builds and manage Kubernetes manifests easily.
Link: https://github.com/kubernetes/helm
38. JaegerThe Jaeger Operator is an implementation of a Kubernetes Operator and provides another method of packaging, deploying, and managing a Kubernetes application.
Link: https://www.jaegertracing.io/ 
39. turbonomicturbonomic’s kubernetes-as-a-service (KaaS) management capabilities include support for Amazon Elastic Container Service for Kubernetes (EKS), Microsoft Azure Kubernetes Service (AKS), Google Kubernetes Engine (GKE), and Pivotal Container Service (PKS). Self-managing Kubernetes optimizes performance, efficiency, and compliance so IT organizations can scale and accelerate cloud native intiatives.
Link: https://turbonomic.com/product/integrations/kubernetes/
40. SupergiantSupergiant  is an open source collection of utilities that simplify installing and managing your Kubernetes clusters. The Supergiant Kubernetes toolkit is three separate applications: Control, Analyze, and Capacity. Essentially, Supergiant acts as a microservices application that allows using these three tools separately. 
Link: https://supergiant.io/toolkit/
41. KeelKeel allows you to automate Kubernetes deployment updates and can be launched as a Kubernetes service in a dedicated namespace. With such organization, Keel introduces a minimal load on your environment and adds significant robustness. Keel helps to deploy Kubernetes service through labels, annotations, and charts. You just need to specify an update policy for each deployment or Helm release. Keel will automatically update your environment as soon as the new application version is available in the repository.
Link: https://keel.sh/
42. ApolloApollo is an open source application providing teams with self-service UI for creating and deploying their services to Kubernetes. Apollo allows operators to view logs and revert deployments to any point in time with just one click. Apollo has flexible permission models for deployments. Each user can deploy only what he needs to deploy.
Link: https://github.com/logzio/apollo
43. DraftDraft is a tool provided by the Azure team that streamlines application development and deployment into any Kubernetes cluster. Draft creates “inner loops” between code deployment and code commits which significantly speed up the change verification process. With Draft, developers can prepare application Dockerfiles and Helm charts plus deploy applications to a remote or local Kubernetes cluster with two commands.
Link: https://github.com/azure/draft
44. Deis WorkflowNOTE: This project is no longer maintained
Deis Workflow is an open source tool. The Platform as a Service (PaaS) creates additional layers of abstraction on top of Kubernetes clusters. These layers allow you to deploy and&#x2F;or update Kubernetes applications without specific domain knowledge from developers. Workflow builds upon Kubernetes concepts to provide simple, developer-friendly app deployment. Delivered as a set of Kubernetes microservices, operators can easily install the platform. Workflow can deploy new versions of your app with zero downtime.
Link: https://deis.com/workflow/
45. KelKel is an open source PaaS from Eldarion, Inc. which helps to manage Kubernetes applications through the entire lifecycle. Kel provides two additional layers written in Python and Go on top of Kubernetes. Level 0 allows you to provision Kubernetes resources, and Level 1 helps you to deploy any application on K8s.
Link: http://www.kelproject.com/
46. KongKong, previously known as Kong Community (CE), is an open-source scalable API gateway technology initiated by Kong Inc and has a growing community. Kong allows developers to manage authentication, data  encryption, logging, rate limiting and other standard features with Kubernetes that they would expect from a basic API management system. All of this is powered by a straightforward RESTful API, and the platform itself is built on top of the NGINX proxy server and the Apache Cassandra database management system.
Link: https://konghq.com/
Serverless&#x2F;Function Tools48. KubelessKubeless is a Kubernetes-native serverless framework that lets you deploy small bits of code without having to worry about the underlying infrastructure plumbing. Kubeless is aware of Kubernetes resources out-of-the-box and also provides auto-scaling, API routing, monitoring, and troubleshooting. Kubeless fully relies on K8s primitives, so Kubernetes users will also be able to use native K8s API servers and API gateways.
Link: https://github.com/kubeless/kubeless
49. FissionFission is a fast serverless framework for Kubernetes with a focus on developer productivity and high performance. Fission works on a Kubernetes cluster anywhere: on your laptop, in any public cloud, or in a private data-center. You can write your function using Python, NodeJS, Go, C# or PHP, and deploy it on K8s clusters with Fission.
Link: https://fission.io/
50. FunktionNOTE: This project is now sandboxedFor a long time, there was only one Function as a Service (FaaS) implementation available for Kubernetes: Funktion. Funktion is an open source event-driven lambda-style programming model designed for Kubernetes. Funktion is tightly coupled with the fabric8 platform. With Funktion, you can create flows to subscribe from over 200 event sources to invoke your function, including most databases, messaging systems, social media, and other middleware and protocols.
Link: https://github.com/funktionio/funktion
51. IronFunctionIronFunctions is an open source serverless platform or FaaS platform that you can run anywhere. IronFunction is written on Golang and really supports functions in any language. The main advantage of IronFunction is that it supports the AWS Lambda format. Import functions directly from Lambda and run them wherever you want.
Link: https://github.com/iron-io/functions
52. OpenWhiskApache OpenWhisk is a robust open source FaaS platform driven by IBM and Adobe. OpenWhisk can be deployed on a local on-premise device or on the cloud. The design of Apache OpenWhisk means it acts as an asynchronous and loosely-coupled execution environment that can run functions against external triggers. OpenWhisk is available as SaaS solution on Bluemix, or you can deploy a Vagrant-based VM locally.
Link: https://console.bluemix.net/openwhisk/
53. OpenFaaSThe OpenFaaS framework aims to manage serverless functions on Docker Swarm or Kubernetes where it will collect and analyze a wide range of metrics. You can package any process inside your function and use it without repetitive coding or any other routine action. FaaS has Prometheus metrics baked-in, which means it can automatically scale your functions up and down for demand. FaaS natively supports a web-based interface where you can try out your function.
Link: https://github.com/openfaas/faas
54. NuclioNuclio is a serverless project which aims to proceed with high-performance events and large amounts of data. Nuclio can be launched on an on-premise device as a standalone library or inside a VM&#x2F;Docker container. Also, Nuclio supports Kubernetes out of the box. Nuclio provides real-time data processing with maximum parallelism and minimum overheads. You can try out Nuclio on the playground page.
Link: https://github.com/nuclio/nuclio
55. Virtual-KubeletVirtual Kubelet is an open source Kubernetes Kubelet implementation that masquerades as a kubelet for the purposes of connecting Kubernetes to other APIs. Virtual Kubelet allows the nodes to be backed by other services like ACI, Hyper.sh, and AWS, etc. This connector features a pluggable architecture and direct use of Kubernetes primitives, making it much easier to build on.
Link: https://virtual-kubelet.io/
56. FnprojectFnproject is a container native serverless project which supports practically any language and can run almost everywhere. Fn is written on Go, so it is performance-ready and lightweight. Fnproject supports AWS Lambda format style, so you can easily import your Lambda functions and launch it with Fnproject.
Link: http://fnproject.io/
Service Mesh Tools57. IstioIstio is an open source service mesh intended to make it easier to connect, manage and secure traffic between, and observe telemetry about microservices running in containers. Istio is a collaboration between IBM, Google and Lyft. 
Link: https://istio.io/
58. Linkerd + Linkerd2Linkerd (rhymes with “chickadee”) is an open source service mesh tool that makes service-to-service communication reliable, fast and safe. By intercepting network communication within the application, service meshes are able to extract metrics (“telemetry”), apply service-to-service policies and encrypt the exchange. Linkerd2 is an ultralight service mesh from Linkerd that works specifically with Kubernetes.
Links:https://linkerd.io/https://github.com/linkerd/linkerd2
59. Hashicorp’s ConsulConsul is a service networking solution that connects and secure sservices across any runtime platform and public or private cloud. Like the above service mesh technologies, Istio and Linkerd, HashiCorp’s Consul Connect opts for a proxy that’s deployed as a sidecar. The proxy transparently secures communication among microservices and enables policy definition through a concept known as Intentions.
Link: https://www.hashicorp.com/products/consul/
Native Service Discovery60. CoreDNSCoreDNS is a set of plugins written in Go which perform DNS functions. CoreDNS with additional Kubernetes plugins can replace the default Kube-DNS service and implement the specification defined for Kubernetes DNS-based service discovery. CoreDNS can also listen for DNS requests coming in over UDP&#x2F;TCP, TLS, and gRPC.
Link: https://coredns.io/
Native Visualization &amp; Control61. Kubernetes DashboardKubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It is much easier to troubleshoot and monitor K8s clusters with a native dashboard. You need to create a secure proxy channel between your machine and Kubernetes API server to access the dashboard. The native Kubernetes dashboard relies on the Heapster data collector, so it also needs to be installed in the system.
Link: https://github.com/kubernetes/dashboard#kubernetes-dashboard
Cost Management62. ReplexReplex is a namesake governance and cost management platform designed for working in Kubernetes environments. The tool solves the challenges surrounding Kubernetes’ dynamic nature by unifying cost and governance management for deployments in the cloud.
Link: https://www.replex.io/
Ref
k8s useful tools

]]></content>
      <categories>
        <category>k8s</category>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>libvirt-debug</title>
    <url>/2021/07/08/libvirt-debug/</url>
    <content><![CDATA[Overviewlibvirt provides lots of tools to manage VM or virtual disk, let’s take a quick look about them.
VM management

virt-install: install VM etc
virsh: start, stop, destroy VM, monitor and collect stats of VM etc
virt-manager: GUI for manage VM

Virtual disk management(provided by libguestfs)

guestfish: interactive shell to manage disk(–verbose for debug)
guestmount/guestumount: mount&#x2F;umount disk to host path(–verbose for debug)
virt-cat/virt-copy-in/virt-copy-out/virt-format/virt-xxx: commands to manage virtual disk(–berbose for debug)

ALL libvirt tools(included libguestfs tools) communicate with libvirt daemon to manage vm or disk by default, but you can direct with qemu if libvirt is not thre
export LIBGUESTFS_BACKEND=direct virt-sysprep


Tools and frequent used commandrequently used virsh command
$ virsh help dominfo# virsh with remote server$ virsh -c qemu+tcp://172.17.0.3/system xxx# vda target name in conf$ virsh domblkinfo vm100 vdaCapacity:       8589934592Allocation:     200015872Physical:       200015872$ virsh domblkstat vm100 vdavda rd_req 5761vda rd_bytes 133831680vda wr_req 86vda wr_bytes 3244544vda flush_operations 9vda rd_total_times 5365449922vda wr_total_times 599328712vda flush_total_times 15684892# target name in conf$ virsh domifstat vm100 mynet0mynet0 rx_bytes 1748mynet0 rx_packets 29mynet0 rx_errs 0mynet0 rx_drop 0mynet0 tx_bytes 0mynet0 tx_packets 0mynet0 tx_errs 0mynet0 tx_drop 0# get from memballoon driver inside guest$ virsh dommemstat vm100actual 1310720swap_in 0swap_out 0major_fault 175minor_fault 142267unused 1082444available 1210140usable 1099536last_update 1661222057rss 1526228$ virsh domblkerror vm100No errors found$ virsh dominfo vm100Id:             1Name:           vm100UUID:           4a0a3bb3-57cf-4efd-84c7-be9b74b02ccdOS Type:        hvmState:          runningCPU(s):         4CPU time:       94.0sMax memory:     1572864 KiBUsed memory:    1310720 KiBPersistent:     yesAutostart:      disableManaged save:   noSecurity model: noneSecurity DOI:   0$ virsh domjobinfo vm100Job type:         None      $ virsh domstate vm100 --reasonrunning (booted)$ virsh console vm100(exit console `Ctrl+]`)$ virsh dumpxml vm100

virsh examples
# this will install libvirt-client(virsh) and libvirt-daemon(libvirtd)$ yum install -y libvirt# install kvm and bios to start vm$ yum install -y kvm$ yum install -y seabios$ libvirtd --version$ virsh -v# By default virsh will connect with local libvirt by unix socket# but can connect with remote libvirtd# ssh$ virsh --connect qemu+ssh://remote/system# plain tcp$ virsh --connect qemu+tcp://remote:port/system# show the connection$ virsh uri# create a vm, for QEMU and LXC, libvirtd stores vm XML to disk and in memory# DO NOT edit the XML directly from disk at in that way, there is no validation!!!# if you edit the on-disk XML is a VM that simply vanishes the next time libvirtd is restarted after the edit.# The VM disappears from libvirt because the XML has become invalid, after which libvirt can&#x27;t do anything with it$ virsh dumpxml $domain# change domain config from xml, restart it to make it work$ virsh shutdown $domain$ virsh edit $domain$ virsh start $domain# delete a vm and related files$ virsh destroy  $domain$ virsh undefine $domain -remove-all-storage# attach/detach disk to a vm# --target is the device hit, it&#x27;s a must option# NOTE: vdc may be not used by guest if it&#x27;s not the first available device, say, if vdb is free, even you proivdes vdc, vdb is used# that means if you check the config, it shows vdc, but actually, vdb is used inside vm, hence --target should be the first available device if expected as set# check existing device from xml, dev name may be not the name used inside guest$ virsh domblklist centos Target     Source------------------------------------------------vda        /home/data/tmp/vm100.qcow2vdc        /dev/loop0# check the real dev name used inside guest, vda is used by /dev/loop0 while vdb is used for /home/data/tmp/vm100.qcow2$ virsh qemu-agent-command vm100 &#x27;&#123;&quot;execute&quot;:&quot;guest-exec&quot;,&quot;arguments&quot;:&#123;&quot;path&quot;:&quot;lsblk&quot;,&quot;capture-output&quot;:true&#125;&#125;&#x27;&#123;&quot;return&quot;:&#123;&quot;pid&quot;:980&#125;&#125;$ virsh qemu-agent-command vm100 &#x27;&#123;&quot;execute&quot;:&quot;guest-exec-status&quot;,&quot;arguments&quot;:&#123;&quot;pid&quot;:980&#125;&#125;&#x27; &#123;&quot;return&quot;:&#123;&quot;exitcode&quot;:0,&quot;out-data&quot;:&quot;TkFNRSAgIE1BSjpNSU4gUk0gIFNJWkUgUk8gVFlQRSBNT1VOVFBPSU5UCnZkYSAgICAyNTM6MCAgICAwICAzNTBLICAwIGRpc2sgCnZkYiAgICAyNTM6MTYgICAwICAgIDhHICAwIGRpc2sgCuKUlOKUgHZkYjEgMjUzOjE3ICAgMCAgICA4RyAgMCBwYXJ0IC8KcG1lbTAgIDI1OTowICAgIDAgIDI1Nk0gIDAgZGlzayAK&quot;,&quot;exited&quot;:true&#125;&#125;$ echo &quot;TkFNRSAgIE1BSjpNSU4gUk0gIFNJWkUgUk8gVFlQRSBNT1VOVFBPSU5UCnZkYSAgICAyNTM6MCAgICAwICAzNTBLICAwIGRpc2sgCnZkYiAgICAyNTM6MTYgICAwICAgIDhHICAwIGRpc2sgCuKUlOKUgHZkYjEgMjUzOjE3ICAgMCAgICA4RyAgMCBwYXJ0IC8KcG1lbTAgIDI1OTowICAgIDAgIDI1Nk0gIDAgZGlzayAK&quot; | base64 -dNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTvda    253:0    0  350K  0 disk vdb    253:16   0    8G  0 disk └─vdb1 253:17   0    8G  0 part /pmem0  259:0    0  256M  0 disk # attach-disk &lt;domain&gt; &lt;source&gt; &lt;target&gt; [--targetbus &lt;string&gt;] [--driver &lt;string&gt;] [--subdriver &lt;string&gt;] [--iothread &lt;string&gt;] [--cache &lt;string&gt;] [--io &lt;string&gt;] [--type &lt;string&gt;] [--mode &lt;string&gt;] [--sourcetype &lt;string&gt;] [--serial &lt;string&gt;] [--wwn &lt;string&gt;] [--rawio] [--address &lt;string&gt;] [--multifunction] [--print-xml] [--persistent] [--config] [--live] [--current]$ virsh attach-disk --domain centos --source /home/data/tmp/disk.raw --persistent --target vdc# dd if=/dev/zero of=/tmp/disk.raw bs=2M count=5120 status=progress$ virsh attach-disk centos /tmp/disk.raw vdc --persistent$ virsh detach-disk --domain centos --config --target vdb$ virsh detach-disk centos vdb --config# attach/detach interface#  attach-interface &lt;domain&gt; &lt;type&gt; &lt;source&gt; [--target &lt;string&gt;] [--mac &lt;string&gt;] [--script &lt;string&gt;] [--model &lt;string&gt;] [--inbound &lt;string&gt;] [--outbound &lt;string&gt;] [--persistent] [--config] [--live] [--current] [--print-xml] [--managed]# virsh domiflist centos# ovsnet is a network: virsh net-listInterface  Type       Source     Model       MAC-------------------------------------------------------vnet1      bridge     ovsnet     virtio      52:54:00:43:85:03# add a new interface to domain$ virsh attach-interface --domain centos --type network --source ovsnet  --model virtio$ virsh attach-interface --domain centos --type network --source ovsnet  --model virtio --persistent# virsh domiflist centosInterface  Type       Source     Model       MAC-------------------------------------------------------vnet1      bridge     ovsnet     virtio      52:54:00:43:85:03vnet2      bridge     ovsnet     virtio      52:54:00:a6:45:d6$ virsh detach-interface centos bridge --mac 52:54:00:a6:45:d6$ virsh detach-interface centos bridge --mac 52:54:00:a6:45:d6 --persistent

QMP from virsh# 6095 is domain id, vm100 is domain name# HMP protocol$ virsh qemu-monitor-command –hmp 6095 info blockdrive-virtio-disk0: removable=0 file=/export/jvirt/jcs-agent/instances/i-sm6pxr4068/vda backing_file=/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq backing_file_depth=1 ro=0 drv=qcow2 encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0# all supported QMP commands$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123;&quot;execute&quot;: &quot;query-commands&quot;&#125;&#x27;# QMP protocol --pretty means format json output# virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-block&quot;&#125;&#x27;$ virsh qemu-monitor-command  6095 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-block&quot;&#125;&#x27;&#123;  &quot;return&quot;: [    &#123;      &quot;device&quot;: &quot;drive-virtio-disk0&quot;,      &quot;locked&quot;: false,      &quot;removable&quot;: false,      &quot;inserted&quot;: &#123;        &quot;iops_rd&quot;: 0,        &quot;image&quot;: &#123;          &quot;backing-image&quot;: &#123;            &quot;virtual-size&quot;: 42949672960,            &quot;filename&quot;: &quot;/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq&quot;,            &quot;cluster-size&quot;: 65536,            &quot;format&quot;: &quot;qcow2&quot;,            &quot;actual-size&quot;: 24866193408,            &quot;format-specific&quot;: &#123;              &quot;type&quot;: &quot;qcow2&quot;,              &quot;data&quot;: &#123;                &quot;compat&quot;: &quot;1.1&quot;,                &quot;lazy-refcounts&quot;: false              &#125;            &#125;,            &quot;dirty-flag&quot;: false          &#125;,          &quot;virtual-size&quot;: 42949672960,          &quot;filename&quot;: &quot;/export/jvirt/jcs-agent/instances/i-sm6pxr4068/vda&quot;,          &quot;cluster-size&quot;: 65536,          &quot;format&quot;: &quot;qcow2&quot;,          &quot;actual-size&quot;: 21068431360,          &quot;format-specific&quot;: &#123;            &quot;type&quot;: &quot;qcow2&quot;,            &quot;data&quot;: &#123;              &quot;compat&quot;: &quot;1.1&quot;,              &quot;lazy-refcounts&quot;: false            &#125;          &#125;,          &quot;backing-filename&quot;: &quot;/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq&quot;,          &quot;dirty-flag&quot;: false        &#125;,        &quot;iops_wr&quot;: 0,        &quot;ro&quot;: false,        &quot;backing_file_depth&quot;: 1,        &quot;drv&quot;: &quot;qcow2&quot;,        &quot;iops&quot;: 0,        &quot;bps_wr&quot;: 0,        &quot;backing_file&quot;: &quot;/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq&quot;,        &quot;encrypted&quot;: false,        &quot;bps&quot;: 0,        &quot;bps_rd&quot;: 0,        &quot;file&quot;: &quot;/export/jvirt/jcs-agent/instances/i-sm6pxr4068/vda&quot;,        &quot;encryption_key_missing&quot;: false      &#125;,      &quot;type&quot;: &quot;unknown&quot;    &#125;  ],  &quot;id&quot;: &quot;libvirt-8302918&quot;&#125;

QGAIn order to run command inside guest os, we must have two things prepared.

A channel
guest agent runs in guest os to execute the command

Enable the channel, by edit xml of libvirtd with $virsh edit $domain
&lt;devices&gt;  &lt;channel type=&#x27;unix&#x27;&gt;   &lt;source mode=&#x27;bind&#x27;/&gt;   &lt;target type=&#x27;virtio&#x27; name=&#x27;org.qemu.guest_agent.0&#x27;/&gt;  &lt;/channel&gt;&lt;/devices&gt;
Then after save, libvirt will set extra info, the full meta is like this
&lt;channel type=&#x27;unix&#x27;&gt;   &lt;source mode=&#x27;bind&#x27; path=&#x27;/var/lib/libvirt/qemu/channel/target/domain-3-centos/org.qemu.guest_agent.0&#x27;/&gt;   &lt;target type=&#x27;virtio&#x27; name=&#x27;org.qemu.guest_agent.0&#x27; state=&#x27;connected&#x27;/&gt;   &lt;alias name=&#x27;channel0&#x27;/&gt;   &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/channel&gt;
After enable this channel, on host a unix socket is created for the VM at /var/lib/libvirt/qemu/channel/target/domain-3-centos/org.qemu.guest_agent.0, you can also set the path of socket with path attribute in source tag like  &lt;source mode=&#39;bind&#39; path=&#39;/var/lib/libvirt/test.agent.0&#39;/&gt;Inside the domain, a char device(/dev/virtio-ports/org.qemu.guest_agent.0) is exported to user by virtio, write&#x2F;read to char dev inside guest OS, data is available at unix socket on host.
Install agent inside guest vm
$ yum install qemu-guest-agent$ systemctl start qemu-guest-agent$ systemctl enable qemu-guest-agent

QGA commands
# show all supported commands that can run inside domain$ virsh qemu-agent-command $&#123;DOMAIN&#125; --pretty &#x27;&#123;&quot;execute&quot;:&quot;guest-info&quot;&#125;&#x27;# guest-exec# guest-exec-status# guest-get-host-name# guest-get-time# guest-set-user-password# guest-shutdown# guest-get-cpu-usage# guest-get-mem-usage# guest-get-time# File content is encrypted base64!!!# guest-file-seek# guest-file-read# guest-file-write# guest-file-flush# guest-file-close# guest-file-open# open with read only$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-file-open&quot;, &quot;arguments&quot;:&#123;&quot;path&quot;:&quot;/tmp/test.txt&quot;,&quot;mode&quot;:&quot;r&quot;&#125;&#125;&#x27;&#123;&quot;return&quot;:1000&#125;# guest-file-read, you can call these several times to get the whole file data, then close it# this is probably used for large file$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-file-read&quot;, &quot;arguments&quot;:&#123;&quot;handle&quot;:1000&#125;&#125;&#x27;# read with large buffer$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-file-read&quot;, &quot;arguments&quot;:&#123;&quot;handle&quot;:1000, &quot;count&quot;:1000000&#125;&#125;&#x27;# guest-file-close$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-file-close&quot;, &quot;arguments&quot;:&#123;&quot;handle&quot;:1000&#125;&#125;&#x27;# run arbitrary command, run `any` command inside guest$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-exec&quot;,&quot;arguments&quot;:&#123;&quot;path&quot;:&quot;mkdir&quot;,&quot;arg&quot;:[&quot;-p&quot;,&quot;/root/.ssh&quot;],&quot;capture-output&quot;:true&#125;&#125;&#x27;&#123;&quot;return&quot;:&#123;&quot;pid&quot;:911&#125;&#125;# get the content of a file, it opens the file then close it every time# if file is large, output is truncated!!!, so that you can only the first part of a larger file!!!! use guest-file-read instead for large file$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-exec&quot;,&quot;arguments&quot;:&#123;&quot;path&quot;:&quot;cat&quot;,&quot;arg&quot;:[&quot;/var/log/plugin.log&quot;],&quot;capture-output&quot;:true&#125;&#125;&#x27;# response is encrypted base64$ virsh qemu-agent-command $&#123;DOMAIN&#125; &#x27;&#123;&quot;execute&quot;:&quot;guest-exec-status&quot;,&quot;arguments&quot;:&#123;&quot;pid&quot;:911&#125;&#125;&#x27; # base64 decode/encode$ echo &quot;hello&quot; | base64aGVsbG8K$ echo &quot;aGVsbG8K&quot; | base64 -dhello# encode/decode from file$ echo &quot;hello&quot; &gt;test.txt$ base64 ./test &gt;encoded.txt$ base64 -d ./encoded.txthello

Manage virtual disk with toolslibguestfs is a C library tool for creating, accessing and modifying virtual machine disk images. You can look inside the disk images, modify the files they contain, create them from scratch, resize them, and much more.yum install libguestfs libguestfs-toollibguestfs works with any disk image, including ones created in VMware, KVM, qemu, VirtualBox, Xen, and many other hypervisors

guestfish is an interactive shell that you can use from the command line or from shell scripts to access guest virtual machine file systems. All of the functionality of the libguestfs API is available from the shell, guestfish shell provides lots of built-in commands(600+) you can use to access guest file systems, it&#39;s a super-set of virt-xxx command below(--verbose for debug).

virt-copy-in
virt-copy-out
virt-edit
virt-df
virt-tar-in
virt-tar-out
virt-cat
virt-ls
virt-make-fs
virt-filesystems
virt-sysprep

#===========================start a new vm permenantly, destroy it when quit the fish shell====# two ways to run guestfish either by providing a disk image or domain name of libvirt# -i means inspect the image and mount the file systems(mount requires a vm started by guestfish)# 1. guestfish will start a new vm by libvirt and mount domain disk # --verbose to debug issue$ guestfish --ro -d $domain -i&gt;&lt;fs&gt; df-hFilesystem      Size  Used Avail Use% Mounted ontmpfs            96M  112K   96M   1% /run/dev            237M     0  237M   0% /dev/dev/sda1       8.0G  1.2G  6.9G  14% /sysroot# NOTE use TAB for command completion!!!$ virsh list Id    Name                           State---------------------------------------------------- 4     guestfs-cmurc2zne6ot9rt8       running# 2. from a disk image to start a new vm(this will call libvirt to start a new vm as well)$ guestfish -a /path/to/disk/image -i&gt;&lt;fs&gt; df-h#===========================start a new vm temporarily, destroy it when get the command result===$ virt-filesystems -a CentOS-7-x86_64-GenericCloud-1503.qcow2/dev/sda1$ virt-filesystems -d $domain/dev/sda1$ virt-ls -d $domain /home/centos$ virt-tar-out -d $domain /home/centos out.tar.gz$ virt-copy-in -d $domain test.txt /home/centos$ virt-copy-out -d $domain /home/centos/text.txt .########################the new vm start by guest fish######################################## vm100 has its disk /root/jason/vm/test/vm100.qcow2$ guestfish --ro -d vm100 -iWelcome to guestfish, the guest filesystem shell forediting virtual machine filesystems and disk images.Type: &#x27;help&#x27; for help on commands      &#x27;man&#x27; to read the manual      &#x27;quit&#x27; to quit the shellOperating system: CentOS Linux release 7.8.2003 (Core)/dev/sda1 mounted on /&gt;&lt;fs&gt; df$ ps -ef | grep kvm/usr/libexec/qemu-kvm -global virtio-blk-pci.scsi=off -nodefconfig -enable-fips -nodefaults -display none -cpu host -machine accel=kvm:tcg -m 500 -no-reboot -rtc driftfix=slew -no-hpet -global kvm-pit.lost_tick_policy=discard -kernel /var/tmp/.guestfs-0/appliance.d/kernel -initrd /var/tmp/.guestfs-0/appliance.d/initrd -device virtio-scsi-pci,id=scsi -drive file=/tmp/libguestfsKuVs4B/overlay1,cache=unsafe,format=qcow2,id=hd0,if=none -device scsi-hd,drive=hd0 -drive file=/var/tmp/.guestfs-0/appliance.d/root,snapshot=on,id=appliance,cache=unsafe,if=none -device scsi-hd,drive=appliance -device virtio-serial-pci -serial stdio -device sga -chardev socket,path=/tmp/libguestfsKuVs4B/guestfsd.sock,id=channel0 -device virtserialport,chardev=channel0,name=org.libguestfs.channel.0 -append panic=1 console=ttyS0 udevtimeout=6000 udev.event-timeout=6000 no_timer_check acpi=off printk.time=1 cgroup_disable=memory root=/dev/sdb selinux=0 TERM=xterm-256color# kernel: /var/tmp/.guestfs-0/appliance.d/kernel# initrd: /var/tmp/.guestfs-0/appliance.d/initrd# disk /tmp/libguestfsKuVs4B/overlay1 whoes backing file is /root/jason/vm/test/vm100.qcow2!!!$ file /tmp/libguestfsKuVs4B/overlay1/tmp/libguestfsKuVs4B/overlay1: QEMU QCOW Image (v3), has backing file (path /root/jason/vm/test/vm100.qcow2), 8589934592 bytes

NOTE

The guest fish vm started by libvirt by default, you can LIBGUESTFS_BACKEND=direct to start it directly
libvirtd is not a must for guestfish, but if you use -d $domain option, you must start libvirtd as guest fish need to get the info(disk info) from libvirtd

virsh command groupsvm xmlUse virsh to edit VM XML 

Virtual Networks: net-edit, net-dumpxml
Storage Pools: pool-edit, pool-dumpxml
Storage Volumes: vol-edit, vol-dumpxml
Interfaces: iface-edit, iface-dumpxml

Redefining the XML of a running machine will not change anything, the changes will take effect after the next VM start up
Command groupsDomain management
virsh console
virsh define (define a guest domain from xml file)
virsh destroy(immediately terminates a running vm)
virsh undefine(remove cfg for an inactive vm)
virsh domjobinfo(return info about jobs running on a domain)
virsh dumpxml(output current vm info /var/run/libvirt/qemu/$domain.xml to stdout)
virsh edit(edit the XML cfg for a vm located at /etc/libvirtd/qemu/$domain.xml, take affect at next virsh start)
virsh migrate(migrate vm to another host)
virsh suspend&#x2F;resume&#x2F;start&#x2F;shutdown&#x2F;reboot&#x2F;reset&#x2F;save&#x2F;restore
shutdown&#x2F;reboot   gracefully from guest like type command from guest
destroy&#x2F;reset     forcely from outside like press power button


virsh setmem&#x2F;setvcpus
virsh vcpuinfo&#x2F;vcpucount&#x2F;vcpupin
virsh domiflist
virsh attach-device (attach device from an XML file, device can be interface, disk etc)

Domain monitor
virsh domblkinfo &#x2F; dominfo
virsh domblkstat &#x2F;domifstat &#x2F;dommemstat
virsh domstate
virsh list –all

Domain Snapshot
virsh snapshot-create 
virsh snapshot-delete 
virsh snapshot-list
virsh snapshot-revert

Host and Hypervisor
virsh capabilities
virsh nodeinfo
virsh uri

Network Interface
virsh iface-dumpxml
virsh iface-list &#x2F; iface-name &#x2F; iface-mac
virsh iface-define &#x2F; iface-undefine
virsh iface-start &#x2F; iface-destroy
virsh iface-edit

Network
virsh net-create &#x2F; net-destroy
virsh net-edit &#x2F; net-dumpxml
virsh net-define &#x2F; net-undefine
virsh net-start
virsh net-info &#x2F; net-list &#x2F; net-name

Node device
virsh nodedev-create
virsh nodedev-reattach &#x2F; nodedev-dettach
virsh nodedev-destroy
virsh nodedev-dumpxml
virsh nodedev-list

Storage pool
virsh pool-create &#x2F; pool-destroy &#x2F; pool-delete
virsh pool-define &#x2F; pool-undefine
virsh pool-start
virsh pool-list &#x2F; pool-dumpxml
virsh pool-edit &#x2F; pool-info

Storage Volume
virsh vol-create &#x2F; vol-delete
virsh vol-info &#x2F;vol-list

eventlibvirt supports several type events can be monitored by client, they are domain event, qemu monitor event, network event, storage pool event, nodedev event

virsh event(domain event)
virsh qemu-monitor-event
virsh net-event
virsh nodedev-event
virsh pool-event
virsh secret-event

blk
virsh domblklist centos10
virsh attach-disk &#x2F;detach-disk


libvirtd supports three modes of attaching a disk to vm, --config, --live, --persistent by default it’s --live

--config, new disk setting only write to /etc/libvirt/qemu/$doman.xml, only take affect after virsh destroy then virsh start. virsh reset, virsh reboot is not sufficient.
--live, new disk setting only in memory of running vm, you can see it by virsh dumpxml $domain or /var/run/libvirt/qemu/$domain.xml, take affect immediately, no written to /etc/libvirt/qemu/$doman.xml, virsh reset and virsh reboot you still can see it, but if you virsh destroy and virsh start, the new disk is gone as it’s not written to that file, /var/run/libvirt/qemu/$domain.xml is deleted after virsh destroy, &#96;&#96;&#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;$doman.xmlis used forvirsh start&#96;.
--persistent &#x3D;&#x3D; --config + --live, that means new disk setting is in memory(/var/run/libvirt/qemu/$domain.xml) and disk(/etc/libvirt/qemu/$doman.xml)
--current, If the VM is offline, use –config. If the VM is running, use –live.


trobleshootingconf runtime filesRelated Paths:

&#x2F;etc&#x2F;libvirt&#x2F;
&#x2F;etc&#x2F;sysconfig&#x2F;libvirtd(it&#39;s a file)
&#x2F;var&#x2F;run&#x2F;libvirt&#x2F;
&#x2F;var&#x2F;log&#x2F;libvirt&#x2F;
&#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;

# /etc/libvirt/libvirtd.conf: global conf of libvirtd# /etc/sysconfig/libvirtd: override libvirtd.conf# /usr/lib/systemd/system/libvirtd.service： systemd service# /etc/libvirt/qemu/networks/default.xml: default network# /var/run/libvirt: runtime of daemon, like daemon socket/pid of each vm$ ls /var/run/libvirt/hostdevmgr  libvirt-admin-sock  libvirt-sock  libvirt-sock-ro  lxc  network  qemu  storage  virtlockd-sock  virtlogd-sock# hostdevmgr network storage: setting for device, networking(rules), volume(pool, local disk)# libvirt-admin-sock libvirt-sock libvirt-sock-ro: sock for admin, readonly etc# lxc qemu: xml/pid of each instance, will go when boots# virtlockd-sock  virtlogd-sock: sock of locked and virtlogd$ ls /var/run/libvirt/qemu/centos.pid centos.xml # /var/log/libvirt: logs of VM from vm console and part logs from libvirtd$ ls /var/log/libvirt/qemu/centos.log# by default libvirtd writes logs to /var/log/message(centos) /var/log/syslog(ubuntu)# default: log_outputs=&quot;3:syslog:libvirtd&quot;# change it to any file by editing /etc/libvirt/libvirtd.conf#    1: DEBUG                                                                   #    2: INFO                                                                    #    3: WARNING                                                                 #    4: ERROR# only log of level large or equal to 2 are sent to filelog_level = 1log_outputs=&quot;2:file:/var/log/libvirt/libvirtd.log&quot;  # /etc/libvirt: daemon conf and persistent XML of VMS located at qemu/$ ls /etc/libvirt/libvirt-admin.conf  libvirtd.conf      nwfilter/  qemu.conf   virtlogd.conflibvirt.conf        qemu/   virtlockd.conf $ ls /var/lib/libvirtdnsmasq  filesystems/  images/  network/  qemu/# snapshot saves the xml of snapshot, data of snapshot is saved into image by default(internal snapshot)!!# save/ holds the managed save(cpu/memory)$ ls /var/lib/libvirt/qemu/channel/  domain-1-centos  save/  snapshot/# socket of this Vm like qmp socket etc$ ls /var/lib/libvirt/qemu/domain-1-centosmaster-key.aes  monitor.sock# socket of this vm for QGA channel$ ls /var/lib/libvirt/qemu/channel/target/domain-1-centos/org.qemu.guest_agent.0

libvirt log
libvirtd.conf
log_level = 1log_outputs=&quot;1:file:/var/log/libvirt/libvirtd.log&quot;keepalive_interval=60admin_keepalive_interval=60

qemu.conf
# use virtlogd as backendstdio_handler = &quot;logd&quot;

virtlogd.conf
log_level = 1 log_outputs=&quot;1:file:/var/log/libvirt/virtlogd.log&quot; 

FAQDifference between qemu:&#x2F;&#x2F;&#x2F;system and qemu:&#x2F;&#x2F;&#x2F;session, Which one should I use?All ‘system’ URIs (be it qemu, lxc, uml, …) connect to the libvirtd daemon running as root which is launched at system startup. Virtual machines created and run using &#39;system&#39; are usually launched as root
All ‘session’ URIs launch a libvirtd instance as your local user, and all VMs are run with local user permissions.You will definitely want to use qemu:&#x2F;&#x2F;&#x2F;system if your VMs are acting as servers. VM autostart on host boot only works for &#39;system&#39;, and the root libvirtd instance has necessary permissions to use proper networking via bridges or virtual networks
MigrationThere are two primary types of migration with QEMU&#x2F;KVM and libvirt:

Plain migration: The source host VM opens a direct unencrypted TCP connection to the destination host for sending the migration data. Unless a port is manually specified, libvirt will choose a migration port in the range 49152-49215, which will need to be open in the firewall on the remote host.

Tunneled migration: The source host libvirtd opens a direct connection to the destination host libvirtd for sending migration data. This allows the option of encrypting the data stream. This mode doesn’t require any extra firewall configuration, but is only supported with qemu 0.12.0 or later, and libvirt 0.7.2.


For all QEMU&#x2F;KVM migrations, libvirtd must be running on the source and destination host.

For tunneled migration, no extra configuration should be required, you simply need to pass the –tunnelled flag to virsh migrate.

For plain unencrypted migration, the TCP port range 49152-49215 must be opened in the firewall on the destination host.


virsh migrate $domain $REMOTE_HOST_URI --migrateuri tcp://$REMOTE_HOST

How to import a vm from a terminal(no X display)$ virsh net-list$ virt-install --name guest1-rhel7 --memory 2048 --vcpus 2 --disk /home/data/tmp/CentOS-7-x86_64-GenericCloud-1503.qcow2 --import --os-type linux --wait 0 --network default# OR$ virt-install --name guest1-rhel7 --memory 2048 --vcpus 2 --disk /home/data/tmp/CentOS-7-x86_64-GenericCloud-1503.qcow2 --import --os-type linux --wait 0 --network none# must set wait 0, otherwise, it will show &#x27;Domain installation still in progress. Waiting for installation to complete&#x27;

How to install a VM from a terminalHere we use image from remote by –location parameters
-l LOCATION , --location=LOCATIONDistribution tree installtion source. virt-install can recognize certain distribution trees and fetches a bootable kernel/initrd pair to launch the install. The &quot;LOCATION&quot; can take one of the following forms:DIRECTORY    Path to a local directory containing an installable distribution image nfs:host:/path or nfs://host/path    An NFS server location containing an installable distribution image http://host/path    An HTTP server location containing an installable distribution image ftp://host/path    An FTP server location containing an installable distribution image centos 7--location &#x27;http://mirror.i3d.net/pub/centos/7/os/x86_64/&#x27;ubuntu(different versions has different url)--location &#x27;http://archive.ubuntu.com/ubuntu/dists/trusty/main/installer-amd64/&#x27;--location &#x27;http://archive.ubuntu.com/ubuntu/dists/bionic/main/installer-amd64/&#x27;

$ virt-install --name ubuntu --memory 2048 --vcpus 2 --disk size=8 --location &#x27;http://archive.ubuntu.com/ubuntu/dists/bionic/main/installer-amd64/&#x27; --os-type linux --graphics none --extra-args &quot;console=tty0 console=ttyS0,115200n8&quot;

how to change something of disk without boot itvirt-sysprep(only support linux!!!) can reset or unconfigure a virtual machine so that clones can be made from it.  Steps in this process include removing SSH host keys, removing persistent network MAC configuration, and removing user accounts.  virt-sysprep can also customize a virtual machine, for instance by adding SSH keys, users or logos.  Each step can be enabled or disabled as required..
Usage
virt-sysprep [--options] -d domnamevirt-sysprep [--options] -a disk.img [-a disk.img ...]

NOTEvirt-sysprep modifies the guest or disk image in place

The virtual machine must be shut down before.
disk images must not be edited concurrently.
virt-sysprep depends on libvirt running by default
export LIBGUESTFS_BACKEND&#x3D;direct virt-sysprep ..(start qemu without libvirtd)

# NO wildcards supported for below three # --copy SOURCE:DEST  inside vm# --copy-in LOCALPATH:REMOTEDIR   from host to guest# --move SOURCE:DEST  inside vm# --mkdir DIR # --delete PATH support wildcards# --install PKG,PKG.   use guest pkg manager and host network# --uninstall PKG,PKG  use guest pkg manager# This will start a temporary vm$ virt-sysprep --root-password password:$new --uninstall cloud-init --selinux-relabel -a  CentOS-7-x86_64-GenericCloud.qcow2 
How to reset a user password of domain# There are two ways to do this##################Way 1################################################# if qemu-ga is running, we can change it runtime, no need to restart vm# QGA: guest-set-user-password$virsh set-user-password vm100 root root##################Way 2################################################# if no qemu-ga or qemu-ga is not working use this way# shutdown it first$ virsh shutdown $domain# generate your password$ openssl passwd -1 $yourpassword$6$FU5Nl9oxxxx# mount the disk with read/write mode(--verbose for debugging)$ guestfish --rw -a /var/lib/libvirt/images/debian9-vm1.qcow2 -i# oneshot mount$ guestmount -a disk.img -i /tmp/disk --verbose&gt;&lt;fs&gt; vi /etc/shadow# modify passwordroot:$6$FU5Nl9oxxxx:17572:0:99999:7:::&gt;&lt;fs&gt; flush&gt;&lt;fs&gt; quit# OR you can do it with just one command who does simliar operation for you$ virt-sysprep --root-password password:$new -a CentOS-7-x86_64-GenericCloud.qcow2

how to change log level without libvirt restart# 1: DEBUG  # 2: INFO   # 3: WARNING     # 4: ERROR# each source file register log with a tag# filter supports wildcard match# that means libvirt.xxx also matches here!!!# log_filters=&quot;1:libvirt 1:daemon 1:rpc&quot;# log_outputs=&quot;1:file:/var/log/libvirt/libvirtd.log 2:syslog:libvirtd&quot;$ virt-admin daemon-log-filters$ virt-admin daemon-log-filters &quot;1:libvirt 1:daemon 1:rpc&quot;$ virt-admin daemon-log-filtersLogging filters: 1:*libvirt* 1:*daemon* 1:*rpc* $ virt-admin daemon-log-outputsLogging outputs: 1:file:/var/log/libvirt/libvirtd.log$ virt-admin daemon-log-outputs &quot;2:file:/var/log/libvirt/libvirtd.log&quot;
how to enable client library logBy default the client library doesn’t produce any logs and usually usually it’s not very interesting on its own anyway.The library configuration of logging is through 3 environment variables allowing to control the logging behaviour:

LIBVIRT_DEBUG: it can take the four following values:
1 or “debug”: asking the library to log every message emitted, though the filters can be used to avoid filling up the output
2 or “info”: log all non-debugging information
3 or “warn”: log warnings and errors, that’s the default value
4 or “error”: log only error messages


LIBVIRT_LOG_FILTERS: defines logging filters
LIBVIRT_LOG_OUTPUTS: defines logging outputs

$ export LIBVIRT_DEBUG=debug$ export LIBVIRT_LOG_OUTPUTS=&quot;1:file:/tmp/libvirt_client.log&quot;

how to enable libvirtd on tcp without tlslibvirtd.conf
listen_tls = 0listen_tcp = 1auth_tcp = &quot;none&quot;

&#x2F;etc&#x2F;sysconfig&#x2F;libvirtd
LIBVIRTD_ARGS=&quot;--listen&quot;

&#x2F;etc&#x2F;libvirt&#x2F;libvirt.conf
# tell virsh use this uri by defaulturi_default = &quot;qemu+tcp://127.0.0.1:16509/system&quot;
Why doesn’t ‘shutdown’ seem to work?If you are using Xen HVM or QEMU&#x2F;KVM, ACPI must be enabled in the guest for a graceful shutdown to work. To check if ACPI is enabled, run:
virsh dumpxml $your-vm-name | grep acpi
If nothing is printed, ACPI is not enabled for your machine. Use ‘virsh edit’ to add the following XML under :

If your VM is running Linux, the VM additionally needs to be running acpid to receive the ACPI eventsyum install acpid
When &lt;input&gt; device is used&lt;devices&gt;  &lt;input type=&#x27;mouse&#x27; bus=&#x27;usb&#x27;/&gt;&lt;/devices&gt;
Input devices allow interaction with the graphical framebuffer in the guest virtual machine virtual machine. that means.

For Desktop, you should configure mouse input, otherwise, you can only use keyboard
For Server without GUI, mouse input can be disable, as it no effect at all.

reduce disk image file not disk sizeAs most of time, disk is thin-provisioned, disk file grows when needed, but after disk file grew, it can not be resize automatcially even you delete vm inside VM, lots of free space of this disk.
NOTE

The virtual machine must be shut down
disk images must not be edited concurrently.

# the new image is smaller(No snapshot is lost)$ qemu-img convert -O qcow2 centos7.img out.qcow2

run kvm machine from another vmif you plan to run vm from vm by libvirt, make sure nested virtualization is enabled on host, otherwise, you will meet error from libvirtd.log like this invalid argument: could not find capabilities for arch=x86_64 domaintype=kvm, the pass nested virtualization to vm who will plan to start new vm.

enable nested virtualization on host
pass it to vm
from that vm start another kvm machine.

# if you can run vm from another vm, check it by that vm# run this in virtual vm which plans to start another vm.$ virt-host-validate  QEMU: Checking for hardware virtualization                                 : PASS---&gt;nested virtulization is enabled  QEMU: Checking if device /dev/kvm exists                                   : PASS  QEMU: Checking if device /dev/kvm is accessible                            : PASS  QEMU: Checking if device /dev/vhost-net exists                             : PASS  QEMU: Checking if device /dev/net/tun exists                               : PASS  QEMU: Checking for cgroup &#x27;memory&#x27; controller support                      : PASS  QEMU: Checking for cgroup &#x27;memory&#x27; controller mount-point                  : PASS  QEMU: Checking for cgroup &#x27;cpu&#x27; controller support                         : PASS  QEMU: Checking for cgroup &#x27;cpu&#x27; controller mount-point                     : PASS  QEMU: Checking for cgroup &#x27;cpuacct&#x27; controller support                     : PASS  QEMU: Checking for cgroup &#x27;cpuacct&#x27; controller mount-point                 : PASS  QEMU: Checking for cgroup &#x27;cpuset&#x27; controller support                      : PASS  QEMU: Checking for cgroup &#x27;cpuset&#x27; controller mount-point                  : PASS  QEMU: Checking for cgroup &#x27;devices&#x27; controller support                     : PASS  QEMU: Checking for cgroup &#x27;devices&#x27; controller mount-point                 : PASS  QEMU: Checking for cgroup &#x27;blkio&#x27; controller support                       : PASS  QEMU: Checking for cgroup &#x27;blkio&#x27; controller mount-point                   : PASS  QEMU: Checking for device assignment IOMMU support                         : WARN (No ACPI DMAR table found, IOMMU either disabled in BIOS or not supported by this hardware platform)   LXC: Checking for Linux &gt;= 2.6.26                                         : PASS   LXC: Checking for namespace ipc                                           : PASS   LXC: Checking for namespace mnt                                           : PASS   LXC: Checking for namespace pid                                           : PASS   LXC: Checking for namespace uts                                           : PASS   LXC: Checking for namespace net                                           : PASS   LXC: Checking for namespace user                                          : PASS   LXC: Checking for cgroup &#x27;memory&#x27; controller support                      : PASS   LXC: Checking for cgroup &#x27;memory&#x27; controller mount-point                  : PASS   LXC: Checking for cgroup &#x27;cpu&#x27; controller support                         : PASS   LXC: Checking for cgroup &#x27;cpu&#x27; controller mount-point                     : PASS   LXC: Checking for cgroup &#x27;cpuacct&#x27; controller support                     : PASS   LXC: Checking for cgroup &#x27;cpuacct&#x27; controller mount-point                 : PASS   LXC: Checking for cgroup &#x27;cpuset&#x27; controller support                      : PASS   LXC: Checking for cgroup &#x27;cpuset&#x27; controller mount-point                  : PASS   LXC: Checking for cgroup &#x27;devices&#x27; controller support                     : PASS   LXC: Checking for cgroup &#x27;devices&#x27; controller mount-point                 : PASS   LXC: Checking for cgroup &#x27;blkio&#x27; controller support                       : PASS   LXC: Checking for cgroup &#x27;blkio&#x27; controller mount-point                   : PASS

monitor event from command line# for event, check help first$ virsh event --help  NAME    event - Domain Events  SYNOPSIS    event [--domain &lt;string&gt;] [--event &lt;string&gt;] [--all] [--loop] [--timeout &lt;number&gt;] [--list] [--timestamp]  DESCRIPTION    List event types, or wait for domain events to occur  OPTIONS    --domain &lt;string&gt;  filter by domain name, id or uuid    --event &lt;string&gt;  which event type to wait for    --all            wait for all events instead of just one type    --loop           loop until timeout or interrupt, rather than one-shot    --timeout &lt;number&gt;  timeout seconds    --list           list valid event types    --timestamp      show timestamp for each printed event# all events of all domains$ virsh event --all --loop --timestamp# default remote port is 16509$ virsh -c qemu+tcp://172.17.0.3/system  event --event lifecycle --loop$ virsh qemu-monitor-event --help NAME    qemu-monitor-event - QEMU Monitor Events  SYNOPSIS    qemu-monitor-event [--domain &lt;string&gt;] [--event &lt;string&gt;] [--pretty] [--loop] [--timeout &lt;number&gt;] [--regex] [--no-case] [--timestamp]  DESCRIPTION    Listen for QEMU Monitor Events  OPTIONS    --domain &lt;string&gt;  filter by domain name, id or uuid    --event &lt;string&gt;  filter by event name    --pretty         pretty-print any JSON output    --loop           loop until timeout or interrupt, rather than one-shot    --timeout &lt;number&gt;  timeout seconds    --regex          treat event as a regex rather than literal filter    --no-case        treat event case-insensitively    --timestamp      show timestamp for each printed event# all qemu monitor event of all domains$ virsh qemu-monitor-event --loop --timestamp

monitor performance of vmSome platforms allow monitoring of performance of the virtual machine and the code executed inside. To enable the performance monitoring events you can either specify them in the perf element or enable them via virDomainSetPerfEvents API. The performance values are then retrieved using the virConnectGetAllDomainStats
# check perf event status of given vm$ virsh perf vm100 cmt            : disabledmbmt           : disabledmbml           : disabledcpu_cycles     : enabledinstructions   : disabledcache_references: disabledcache_misses   : disabledbranch_instructions: disabledbranch_misses  : disabledbus_cycles     : disabledstalled_cycles_frontend: disabledstalled_cycles_backend: disabledref_cpu_cycles : disabledcpu_clock      : disabledtask_clock     : disabledpage_faults    : disabledcontext_switches: disabledcpu_migrations : disabledpage_faults_min: disabledpage_faults_maj: disabledalignment_faults: disabledemulation_faults: disabled# enable perf event of given vm$ virsh perf vm100 --enable page_faults --live$ virsh perf vm100 --enable cache_misses --live# get event stats$  virsh domstats vm100Domain: &#x27;vm100&#x27;  state.state=1  state.reason=1  cpu.time=544449957934  cpu.user=11320000000  cpu.system=95640000000  balloon.current=2048000  balloon.maximum=2048000  balloon.swap_in=0  balloon.swap_out=0  balloon.major_fault=179  balloon.minor_fault=131463  balloon.unused=1732652  balloon.available=1832708  balloon.usable=1673880  balloon.last-update=1659001628  balloon.rss=488752  vcpu.current=2  vcpu.maximum=2  vcpu.0.state=1  vcpu.0.time=159640000000  vcpu.0.wait=0  vcpu.1.state=1  vcpu.1.time=367810000000  vcpu.1.wait=0  net.count=0  block.count=1  block.0.name=vda  block.0.path=/tmp/vm100.qcow2  block.0.rd.reqs=5746  block.0.rd.bytes=145617408  block.0.rd.times=10162755968  block.0.wr.reqs=324  block.0.wr.bytes=4339200  block.0.wr.times=1200496823  block.0.fl.reqs=158  block.0.fl.times=4497529526  block.0.allocation=3385786368  block.0.capacity=8589934592  block.0.physical=3385360384  perf.cache_misses=11298  perf.page_faults=0

add hotplug cpu to guest(add cpu to guest without rebooting it)you must set maxcpus for qemu to use hotpluggable cpu
# check hotpluggable cpu# no qom-path means it&#x27;s plugged out!!!$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-hotpluggable-cpus&quot;&#125;&#x27;&#123;  &quot;return&quot;: [    &#123;      &quot;props&quot;: &#123;         &quot;core-id&quot;: 1,        &quot;thread-id&quot;: 0,        &quot;node-id&quot;: 0,        &quot;socket-id&quot;: 0      &#125;,      &quot;vcpus-count&quot;: 1,      &quot;type&quot;: &quot;qemu64-x86_64-cpu&quot;    &#125;,    &#123;      &quot;props&quot;: &#123;        &quot;core-id&quot;: 0,        &quot;thread-id&quot;: 0,        &quot;node-id&quot;: 0,        &quot;socket-id&quot;: 0      &#125;,      &quot;vcpus-count&quot;: 1,      &quot;qom-path&quot;: &quot;/machine/unattached/device[0]&quot;,      &quot;type&quot;: &quot;qemu64-x86_64-cpu&quot;    &#125;  ],  &quot;id&quot;: &quot;libvirt-21&quot;&#125;# add a hotpluggable cpu$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;device_add&quot;, &quot;arguments&quot;:&#123;&quot;id&quot;:&quot;cpu-2&quot;, &quot;driver&quot;:&quot;qemu64-x86_64-cpu&quot;, &quot;socket-id&quot;:&quot;0&quot;, &quot;core-id&quot;:&quot;1&quot;, &quot;thread-id&quot;:&quot;0&quot;&#125;&#125;&#x27;# query vcpu details$virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-cpus-fast&quot;&#125;&#x27;# pluggable cpu and unpluggable cpu has different qom-paths!!!# pluggable cpu sits at /machine/peripheral&#123;  &quot;return&quot;: [    &#123;      &quot;arch&quot;: &quot;x86&quot;,      &quot;thread-id&quot;: 17865,      &quot;props&quot;: &#123;        &quot;core-id&quot;: 0,        &quot;thread-id&quot;: 0,        &quot;node-id&quot;: 0,        &quot;socket-id&quot;: 0      &#125;,      &quot;qom-path&quot;: &quot;/machine/unattached/device[0]&quot;,      &quot;cpu-index&quot;: 0,      &quot;target&quot;: &quot;x86_64&quot;    &#125;,    &#123;      &quot;arch&quot;: &quot;x86&quot;,      &quot;thread-id&quot;: 18887,      &quot;props&quot;: &#123;        &quot;core-id&quot;: 1,        &quot;thread-id&quot;: 0,        &quot;node-id&quot;: 0,        &quot;socket-id&quot;: 0      &#125;,      &quot;qom-path&quot;: &quot;/machine/peripheral/cpu-2&quot;,      &quot;cpu-index&quot;: 1,      &quot;target&quot;: &quot;x86_64&quot;    &#125;  ],  &quot;id&quot;: &quot;libvirt-28&quot;&#125;# inside the guest, there are two cpu now!!!

memory deivce and share memorywithout hotplugable memory device&lt;domain&gt;  &lt;maxMemory slots=&quot;4&quot; unit=&quot;GiB&quot;&gt;2&lt;/maxMemory&gt;                                   &lt;cpu&gt;                                                                             &lt;topology sockets=&quot;1&quot; cores=&quot;2&quot; threads=&quot;1&quot;/&gt;                                   &lt;numa&gt;                                                                            &lt;cell id=&quot;0&quot; cpus=&quot;0-1&quot; memory=&quot;1024&quot; unit=&quot;MiB&quot;/&gt;         -----&gt; no need to set &lt;memory&gt;&lt;/memory&gt;, it&#x27;s auto calcualted from &lt;numa&gt;&lt;/numa&gt; if no &lt;numa&gt; must set it!!!    &lt;/numa&gt;                                                                       &lt;/cpu&gt;libvirt auto generate xml /var/run/libvirt/qemu/$domain.xml  &lt;maxMemory slots=&#x27;4&#x27; unit=&#x27;KiB&#x27;&gt;4194304&lt;/maxMemory&gt;                           &lt;memory unit=&#x27;KiB&#x27;&gt;1048576&lt;/memory&gt;                                           &lt;currentMemory unit=&#x27;KiB&#x27;&gt;1048576&lt;/currentMemory&gt;  

check memory device
# total memory info$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-size-summary&quot;&#125;&#x27;&#123;  &quot;return&quot;: &#123;    &quot;base-memory&quot;: 1073741824,    &quot;plugged-memory&quot;: 0  &#125;,  &quot;id&quot;: &quot;libvirt-20&quot;&#125;# details about pluggable memory device$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-devices&quot;&#125;&#x27;&#123;  &quot;return&quot;: [  ],  &quot;id&quot;: &quot;libvirt-21&quot;&#125;

Inside guest check memory
# kernel code used is not counted here$ free              total        used        free      shared  buff/cache   availableMem:         948532       77504      785368       12436       85660      750776Swap:             0           0           0# show result in bytes$ lsmem -a -b$ lsmem -aRANGE                                  SIZE  STATE REMOVABLE BLOCK0x0000000000000000-0x0000000007ffffff  128M online        no     00x0000000008000000-0x000000000fffffff  128M online       yes     10x0000000010000000-0x0000000017ffffff  128M online       yes     20x0000000018000000-0x000000001fffffff  128M online       yes     30x0000000020000000-0x0000000027ffffff  128M online       yes     40x0000000028000000-0x000000002fffffff  128M online        no     50x0000000030000000-0x0000000037ffffff  128M online        no     60x0000000038000000-0x000000003fffffff  128M online        no     7Memory block size:       128MTotal online memory:       1GTotal offline memory:      0B

with hotplugable memory(dimm) from command line&lt;domain&gt;  &lt;maxMemory slots=&quot;4&quot; unit=&quot;GiB&quot;&gt;2&lt;/maxMemory&gt;                                   &lt;cpu&gt;                                                                             &lt;topology sockets=&quot;1&quot; cores=&quot;2&quot; threads=&quot;1&quot;/&gt;                                   &lt;numa&gt;                                                                            &lt;cell id=&quot;0&quot; cpus=&quot;0-1&quot; memory=&quot;1024&quot; unit=&quot;MiB&quot;/&gt;         -----&gt; no need to set &lt;memory&gt;&lt;/memory&gt;, it&#x27;s auto calcualted from &lt;numa&gt;&lt;/numa&gt; if no &lt;numa&gt; must set it!!!    &lt;/numa&gt;                                                                       &lt;/cpu&gt;  &lt;devices&gt;    &lt;memory model=&quot;dimm&quot;&gt;                                                             &lt;target&gt;                                                                          &lt;size unit=&quot;MiB&quot;&gt;256&lt;/size&gt;                                                     &lt;node&gt;0&lt;/node&gt;                                                                &lt;/target&gt;                                                                     &lt;/memory&gt;  &lt;/devices&gt;libvirt auto generate xml /var/run/libvirt/qemu/$domain.xml  &lt;maxMemory slots=&#x27;4&#x27; unit=&#x27;KiB&#x27;&gt;4194304&lt;/maxMemory&gt;                           &lt;memory unit=&#x27;KiB&#x27;&gt;1310720&lt;/memory&gt;              ---&gt; ram(numa) + dimm  &lt;currentMemory unit=&#x27;KiB&#x27;&gt;1310720&lt;/currentMemory&gt;---&gt; the memory you can see in guest with free = ram(numa + dimm)

check memory device
# total memory info$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-size-summary&quot;&#125;&#x27;&#123;  &quot;return&quot;: &#123;    &quot;base-memory&quot;: 1073741824,    &quot;plugged-memory&quot;: 268435456  &#125;,  &quot;id&quot;: &quot;libvirt-19&quot;&#125;# details about pluggable memory device$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-devices&quot;&#125;&#x27;&#123;  &quot;return&quot;: [    &#123;      &quot;type&quot;: &quot;dimm&quot;,      &quot;data&quot;: &#123;        &quot;memdev&quot;: &quot;/objects/memdimm0&quot;,        &quot;hotplugged&quot;: false, -------&gt;as hotpluggable memory device is from command line(xml) not QMP command, so it is not hotplugged!!        &quot;addr&quot;: 4294967296,        &quot;hotpluggable&quot;: true,        &quot;size&quot;: 268435456,        &quot;slot&quot;: 0,        &quot;node&quot;: 0,        &quot;id&quot;: &quot;dimm0&quot;      &#125;    &#125;  ],  &quot;id&quot;: &quot;libvirt-20&quot;&#125;

Inside guest check memory
# kernel code used is not counted here$ free              total        used        free      shared  buff/cache   availableMem:        1210676       82836     1042392       12432       85448     1092196Swap:             0           0           0# lsmem -a -b$ lsmemRANGE                                  SIZE  STATE REMOVABLE BLOCK0x0000000000000000-0x0000000007ffffff  128M online        no     00x0000000008000000-0x000000000fffffff  128M online       yes     10x0000000010000000-0x0000000017ffffff  128M online       yes     20x0000000018000000-0x000000001fffffff  128M online        no     30x0000000020000000-0x0000000027ffffff  128M online        no     40x0000000028000000-0x000000002fffffff  128M online        no     50x0000000030000000-0x0000000037ffffff  128M online        no     60x0000000038000000-0x000000003fffffff  128M online        no     70x0000000100000000-0x0000000107ffffff  128M online        no    320x0000000108000000-0x000000010fffffff  128M online        no    33Memory block size:       128MTotal online memory:     1.3GTotal offline memory:      0B

with hotplugable memory(nvdimm) from command line&lt;domain&gt;  &lt;maxMemory slots=&quot;4&quot; unit=&quot;GiB&quot;&gt;2&lt;/maxMemory&gt;                                   &lt;cpu&gt;                                                                             &lt;topology sockets=&quot;1&quot; cores=&quot;2&quot; threads=&quot;1&quot;/&gt;                                   &lt;numa&gt;                                                                            &lt;cell id=&quot;0&quot; cpus=&quot;0-1&quot; memory=&quot;1024&quot; unit=&quot;MiB&quot;/&gt;         -----&gt; no need to set &lt;memory&gt;&lt;/memory&gt;, it&#x27;s auto calcualted from &lt;numa&gt;&lt;/numa&gt; if no &lt;numa&gt; must set it!!!    &lt;/numa&gt;                                                                       &lt;/cpu&gt;  &lt;devices&gt;    &lt;memory model=&quot;dimm&quot;&gt;                                                             &lt;target&gt;                                                                          &lt;size unit=&quot;MiB&quot;&gt;256&lt;/size&gt;                                                     &lt;node&gt;0&lt;/node&gt;                                                                &lt;/target&gt;                                                                     &lt;/memory&gt;    &lt;memory model=&quot;nvdimm&quot;&gt;                                                           &lt;source&gt;                                                                          &lt;path&gt;/tmp/nvdimm&lt;/path&gt;                                                      &lt;/source&gt;                                                                       &lt;target&gt;                                                                          &lt;size unit=&quot;MiB&quot;&gt;256&lt;/size&gt;                                                     &lt;node&gt;0&lt;/node&gt;                                                                &lt;/target&gt;                                                                     &lt;/memory&gt;              &lt;/devices&gt;libvirt auto generate xml /var/run/libvirt/qemu/$domain.xml  &lt;maxMemory slots=&#x27;4&#x27; unit=&#x27;KiB&#x27;&gt;4194304&lt;/maxMemory&gt;                           &lt;memory unit=&#x27;KiB&#x27;&gt;1572864&lt;/memory&gt;               ----&gt;memory include ram(numa)+dimm+nvdimm  &lt;currentMemory unit=&#x27;KiB&#x27;&gt;1310720&lt;/currentMemory&gt; ----&gt;currentMemory not include nvdimm, but include dimm, this is the value you can see with `lsmem -ab`

check memory device
# total memory info$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-size-summary&quot;&#125;&#x27;&#123;  &quot;return&quot;: &#123;    &quot;base-memory&quot;: 1073741824,    &quot;plugged-memory&quot;: 536870912 ----&gt;count both dimm and nvdimm  &#125;,  &quot;id&quot;: &quot;libvirt-19&quot;&#125;# details about pluggable memory device(dimm and nvdim)$ virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-devices&quot;&#125;&#x27;&#123;  &quot;return&quot;: [    &#123;      &quot;type&quot;: &quot;dimm&quot;,      &quot;data&quot;: &#123;        &quot;memdev&quot;: &quot;/objects/memdimm0&quot;,        &quot;hotplugged&quot;: false, ----&gt;from command line        &quot;addr&quot;: 4294967296,        &quot;hotpluggable&quot;: true,        &quot;size&quot;: 268435456,        &quot;slot&quot;: 0,        &quot;node&quot;: 0,        &quot;id&quot;: &quot;dimm0&quot;      &#125;    &#125;,    &#123;      &quot;type&quot;: &quot;nvdimm&quot;,      &quot;data&quot;: &#123;        &quot;memdev&quot;: &quot;/objects/memnvdimm1&quot;,        &quot;hotplugged&quot;: false,----&gt;from command line        &quot;addr&quot;: 4563402752,        &quot;hotpluggable&quot;: true,        &quot;size&quot;: 268435456,        &quot;slot&quot;: 1,        &quot;node&quot;: 0,        &quot;id&quot;: &quot;nvdimm1&quot;      &#125;    &#125;  ],  &quot;id&quot;: &quot;libvirt-20&quot;&#125;

Inside guest check memory
# kernel code used is not counted here$ free              total        used        free      shared  buff/cache   availableMem:        1210676       82836     1042392       12432       85448     1092196Swap:             0           0           0# total memory is 1.3G as well, nvdimm is not counted$ lsmem -aRANGE                                  SIZE  STATE REMOVABLE BLOCK0x0000000000000000-0x0000000007ffffff  128M online        no     00x0000000008000000-0x000000000fffffff  128M online       yes     10x0000000010000000-0x0000000017ffffff  128M online       yes     20x0000000018000000-0x000000001fffffff  128M online       yes     30x0000000020000000-0x0000000027ffffff  128M online       yes     40x0000000028000000-0x000000002fffffff  128M online        no     50x0000000030000000-0x0000000037ffffff  128M online        no     60x0000000038000000-0x000000003fffffff  128M online        no     70x0000000100000000-0x0000000107ffffff  128M online        no    320x0000000108000000-0x000000010fffffff  128M online        no    33Memory block size:       128MTotal online memory:     1.3GTotal offline memory:      0B# nvdimm is seen as block by guest /dev/pmem0!!!$fdisk -lDisk /dev/pmem0: 268 MB, 268435456 bytes, 524288 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytes

with hotpluggable memory device from QMP$virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;object-add&quot;, &quot;arguments&quot;:&#123;&quot;qom-type&quot;:&quot;memory-backend-ram&quot;,&quot;id&quot;:&quot;mem-dimm2&quot;,&quot;props&quot;:&#123;&quot;size&quot;:268435456&#125;&#125;&#125;&#x27;$virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;device_add&quot;, &quot;arguments&quot;:&#123;&quot;driver&quot;:&quot;pc-dimm&quot;,&quot;id&quot;:&quot;dm2&quot;,&quot;memdev&quot;:&quot;mem-dimm2&quot;&#125;&#125;&#x27;$virsh qemu-monitor-command  vm100 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-memory-devices&quot;&#125;&#x27;&#123;  &quot;return&quot;: [    &#123;      &quot;type&quot;: &quot;dimm&quot;,      &quot;data&quot;: &#123;        &quot;memdev&quot;: &quot;/objects/memdimm0&quot;,        &quot;hotplugged&quot;: false,        &quot;addr&quot;: 4294967296,        &quot;hotpluggable&quot;: true,        &quot;size&quot;: 268435456,        &quot;slot&quot;: 0,        &quot;node&quot;: 0,        &quot;id&quot;: &quot;dimm0&quot;      &#125;    &#125;,    &#123;      &quot;type&quot;: &quot;nvdimm&quot;,      &quot;data&quot;: &#123;        &quot;memdev&quot;: &quot;/objects/memnvdimm1&quot;,        &quot;hotplugged&quot;: false,--------&gt;can NOT be removed from QMP as it is added from command line        &quot;addr&quot;: 4563402752,        &quot;hotpluggable&quot;: true,        &quot;size&quot;: 268435456,        &quot;slot&quot;: 1,        &quot;node&quot;: 0,        &quot;id&quot;: &quot;nvdimm1&quot;      &#125;    &#125;,    &#123;      &quot;type&quot;: &quot;dimm&quot;,      &quot;data&quot;: &#123;        &quot;memdev&quot;: &quot;/objects/mem-dimm2&quot;,        &quot;hotplugged&quot;: true, -----&gt;this can be removed from QMP command!!!        &quot;addr&quot;: 4831838208,        &quot;hotpluggable&quot;: true,        &quot;size&quot;: 268435456,        &quot;slot&quot;: 2,        &quot;node&quot;: 0,        &quot;id&quot;: &quot;dm2&quot;      &#125;    &#125;  ],  &quot;id&quot;: &quot;libvirt-23&quot;&#125;# insde guest$ free              total        used        free      shared  buff/cache   availableMem:        1472816       94176     1292552       12440       86088     1339132Swap:             0           0           0# total memory is increased after add a memory device$ lsmem -aRANGE                                  SIZE  STATE REMOVABLE BLOCK0x0000000000000000-0x0000000007ffffff  128M online        no     00x0000000008000000-0x000000000fffffff  128M online       yes     10x0000000010000000-0x0000000017ffffff  128M online       yes     20x0000000018000000-0x000000001fffffff  128M online       yes     30x0000000020000000-0x0000000027ffffff  128M online       yes     40x0000000028000000-0x000000002fffffff  128M online        no     50x0000000030000000-0x0000000037ffffff  128M online        no     60x0000000038000000-0x000000003fffffff  128M online        no     70x0000000100000000-0x0000000107ffffff  128M online        no    320x0000000108000000-0x000000010fffffff  128M online        no    330x0000000120000000-0x0000000127ffffff  128M online       yes    360x0000000128000000-0x000000012fffffff  128M online       yes    37Memory block size:       128MTotal online memory:     1.5GTotal offline memory:      0B

move qemu-process launched outside to libvirtd control# Attach an externally launched QEMU process to the libvirt QEMU driver. # The QEMU process must have been created with a monitor connection using the UNIX driver. # Ideally the process will also have had the &#x27;-name&#x27; argument specified. $ qemu-kvm -cdrom ~/demo.iso \    -monitor unix:/tmp/demo,server,nowait \    -name foo \    -uuid cece4f9f-dff0-575d-0e8e-01fe380f12ea  &amp;$ QEMUPID=$!$ virsh qemu-attach $QEMUPID
limit IO# blkdeviotune - Set or query a block device I/O tuning parameters#  OPTIONS#    [--domain] &lt;string&gt;  domain name, id or uuid#    [--device] &lt;string&gt;  block device#    --total-bytes-sec &lt;number&gt;  total throughput limit, as scaled integer (default bytes)#    --read-bytes-sec &lt;number&gt;  read throughput limit, as scaled integer (default bytes)#    --write-bytes-sec &lt;number&gt;  write throughput limit, as scaled integer (default bytes)#    --total-iops-sec &lt;number&gt;  total I/O operations limit per second#    --read-iops-sec &lt;number&gt;  read I/O operations limit per second#    --write-iops-sec &lt;number&gt;  write I/O operations limit per second#    --total-bytes-sec-max &lt;number&gt;  total max, as scaled integer (default bytes)#    --read-bytes-sec-max &lt;number&gt;  read max, as scaled integer (default bytes)#    --write-bytes-sec-max &lt;number&gt;  write max, as scaled integer (default bytes)#    --total-iops-sec-max &lt;number&gt;  total I/O operations max#    --read-iops-sec-max &lt;number&gt;  read I/O operations max#    --write-iops-sec-max &lt;number&gt;  write I/O operations max#    --size-iops-sec &lt;number&gt;  I/O size in bytes#    --group-name &lt;string&gt;  group name to share I/O quota between multiple drives#    --total-bytes-sec-max-length &lt;number&gt;  duration in seconds to allow total max bytes#    --read-bytes-sec-max-length &lt;number&gt;  duration in seconds to allow read max bytes#    --write-bytes-sec-max-length &lt;number&gt;  duration in seconds to allow write max bytes#    --total-iops-sec-max-length &lt;number&gt;  duration in seconds to allow total I/O operations max#    --read-iops-sec-max-length &lt;number&gt;  duration in seconds to allow read I/O operations max#    --write-iops-sec-max-length &lt;number&gt;  duration in seconds to allow write I/O operations max#    --config         affect next boot#    --live           affect running domain#    --current        affect current domain# get io parameters of given disk$virsh blkdeviotune vm100 vdatotal_bytes_sec: 0read_bytes_sec : 83886080write_bytes_sec: 83886080total_iops_sec : 0read_iops_sec  : 2000write_iops_sec : 2000total_bytes_sec_max: 0read_bytes_sec_max: 0write_bytes_sec_max: 0total_iops_sec_max: 0read_iops_sec_max: 0write_iops_sec_max: 0size_iops_sec  : 0group_name     : drive-virtio-disk0total_bytes_sec_max_length: 0read_bytes_sec_max_length: 0write_bytes_sec_max_length: 0total_iops_sec_max_length: 0read_iops_sec_max_length: 0write_iops_sec_max_length: 0$virsh blkdeviotune vm100 vda --total-iops-sec 5000# check result$virsh blkdeviotune vm100 vdatotal_bytes_sec: 0read_bytes_sec : 83886080write_bytes_sec: 83886080total_iops_sec : 5000----&gt;changedread_iops_sec  : 0write_iops_sec : 0total_bytes_sec_max: 0read_bytes_sec_max: 0write_bytes_sec_max: 0total_iops_sec_max: 0read_iops_sec_max: 0write_iops_sec_max: 0size_iops_sec  : 0group_name     : drive-virtio-disk0total_bytes_sec_max_length: 0read_bytes_sec_max_length: 0write_bytes_sec_max_length: 0total_iops_sec_max_length: 0read_iops_sec_max_length: 0write_iops_sec_max_length: 0
get device property value# get property of given device$virsh qemu-monitor-command --hmp  vm100 info qom-tree$virsh qemu-monitor-command vm100 --pretty &#x27;&#123;&quot;execute&quot;: &quot;qom-get&quot;, &quot;arguments&quot;: &#123;&quot;path&quot;: &quot;/machine/peripheral/net1&quot;, &quot;property&quot;: &quot;mq&quot;&#125;&#125;&#x27;&#123;  &quot;return&quot;: true,  &quot;id&quot;: &quot;libvirt-39&quot;&#125;

how to login vm by serial console forTo make virsh console vm100 work, you have to do these steps by order

enable console from boot parameter inside vm vm#grubby --update-kernel=ALL --args=&quot;console=ttyS0&quot; vm#reboot

NOTE: without this you can still use console but lost early boot message


enable getty on ttyS0 inside vm
# touch /etc/systemd/system/serial-getty@ttyS0.service[Unit]Description=Serial Getty on %IDocumentation=man:agetty(8) man:systemd-getty-generator(8)Documentation=http://0pointer.de/blog/projects/serial-console.htmlBindsTo=dev-%i.deviceAfter=dev-%i.device systemd-user-sessions.service plymouth-quit-wait.serviceAfter=rc-local.service# If additional gettys are spawned during boot then we should make# sure that this is synchronized before getty.target, even though# getty.target didn&#x27;t actually pull it in.Before=getty.targetIgnoreOnIsolate=yes[Service]ExecStart=-/sbin/agetty --keep-baud 115200,38400,9600 %I $TERMType=idleRestart=alwaysUtmpIdentifier=%ITTYPath=/dev/%ITTYReset=yesTTYVHangup=yesKillMode=processIgnoreSIGPIPE=noSendSIGHUP=yes[Install]WantedBy=getty.target# ln -s /etc/systemd/system/serial-getty@ttyS0.service /etc/systemd/system/getty.target.wants/# systemctl daemon-reload# systemctl start serial-getty@ttyS0.service# systemctl enable serial-getty@ttyS0.service

NOTE: without this you can still use console, but no login promt


virsh stop vm100

edit /etc/libvirt/qemu/vm100.xml to add console(serial type) device
&lt;console type=&#x27;pty&#x27;&gt; &lt;!--on host auto select one--&gt;  &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;!--insde vm /dev/ttyS0--&gt;  &lt;alias name=&#x27;console0&#x27;/&gt;&lt;/console&gt;
service libvirtd restart

virsh start vm100

inside vm $ echo hello &gt;/dev/ttyS0


virsh console vm100

without kernel parameter, you can NOT seeing Starting ...when boots
without getty, you can NOT seeing login promt centos76 login: 
got hello here



how to access console&#x2F;serial without virsh command$ grep redirect  /var/log/libvirt/qemu/vm100.log115:2023-03-24T06:28:28.402166Z qemu-kvm: -chardev pty,id=charconsole1: char device redirected to /dev/pts/5 (label charconsole1)$ screen /dev/pts/5# quit from console(terminate screen session)ctrl + a, then press \
use specific qemu for a vmUse emulator in xml to do this like below.
&lt;devices&gt;                                                                       &lt;emulator&gt;/src/qemu/build/x86_64-softmmu/qemu-system-x86_64&lt;/emulator&gt;  ...&lt;/devices&gt;

node device info# list all pci devices of host(we can passthrough these devices if iommus is enabled on host)$ virsh nodedev-list --cap pcipci_0000_00_00_0pci_0000_00_01_0pci_0000_00_02_0pci_0000_00_03_0pci_0000_00_04_0pci_0000_00_1f_0pci_0000_00_1f_2...# get device infomration$ virsh nodedev-dumpxml pci_0000_00_00_0&lt;device&gt;  &lt;name&gt;pci_0000_00_00_0&lt;/name&gt;  &lt;path&gt;/sys/devices/pci0000:00/0000:00:00.0&lt;/path&gt;  &lt;parent&gt;computer&lt;/parent&gt;  &lt;capability type=&#x27;pci&#x27;&gt;    &lt;domain&gt;0&lt;/domain&gt;    &lt;bus&gt;0&lt;/bus&gt;    &lt;slot&gt;0&lt;/slot&gt;    &lt;function&gt;0&lt;/function&gt;    &lt;product id=&#x27;0x29c0&#x27;&gt;82G33/G31/P35/P31 Express DRAM Controller&lt;/product&gt;    &lt;vendor id=&#x27;0x8086&#x27;&gt;Intel Corporation&lt;/vendor&gt;    &lt;iommuGroup number=&#x27;0&#x27;&gt;      &lt;address domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt;    &lt;/iommuGroup&gt;  &lt;/capability&gt;&lt;/device&gt;

Ref
VM XML format
virsh reference
libvirt Wiki
libvirt FAQ
libvirt API concepts
libvirt API in C
libvirt debug logs
libvirt snapshot
virtulization troubleshooting

]]></content>
      <categories>
        <category>libvirt</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
        <tag>cloud</tag>
        <tag>vm</tag>
      </tags>
  </entry>
  <entry>
    <title>libvirt-design</title>
    <url>/2021/08/24/libvirt-design/</url>
    <content><![CDATA[OverviewLibvirt is collection of software that provides a convenient way to manage virtual machines and other virtualization functionality, such as storage and network interface management. These software pieces include an API library, a daemon (libvirtd), and a command line utility (virsh).
An primary goal of libvirt is to provide a single way to manage multiple different virtualization providers&#x2F;hypervisors.
The libvirt project:

is a toolkit to manage virtualization platforms
is accessible from C, Python, Perl, Go and more
is licensed under open source licenses
supports KVM, QEMU, Xen, Virtuozzo, VMWare ESX, LXC, and more
targets Linux, FreeBSD, Windows and macOS




File layout
.|-- daemon(daemon for accepting rpc call and dispatch it, remote dispatcher)|-- gnulib|   |-- lib(gnulib copied from gnu for stable)|   `-- tests(test cases for gnulib)|-- include|   `-- libvirt(header files of libvirt)|-- po(internationalization, zh-CN, US etc)|-- src(generic virt level API who calls driver API)|   |-- access(---driver for access control, write, read etc)|   |-- admin(--admin program)|   |-- bhyve(BSD hypervisor)|   |-- conf(---Parse xml cpu, memory, device etc)|   |-- cpu(---specific vcpu setting arm, x86 etc)|   |-- esx(VMWare ESX hypervisor)|   |-- hyperv(MS hypervisor)|   |-- interface(---driver for iface)|   |-- libxl(Xen hypervisor with libxenlight tool)|   |-- locking(--lockd daemon: By maintaining the locks in a standalone daemon, the main libvirtd daemon can be restarted without risk of losing locks for VM disk mutual exclusion)|   |-- logging(--logd daemon: logging run as a separate daemon(log from vm console,  not libvirtd daemon logs), the main libvirtd daemon can be restarted without risk of losing logs)|   |-- lxc(kind of linux container system)|   |-- network(---dirver for network)|   |-- node_device(---driver for device)|   |-- nwfilter(---driver for network filter, iptables etc)|   |-- openvz(kind of Linux container system)|   |-- phyp(IBM Power Hypervisor)|   |-- qemu(qemu fully emulated, or qemu-kvm hypervisor)|   |-- remote(---client side remote driver who used rpc to libvirtd, make rpc call)|   |-- rpc(---rpc framework)|   |-- secret(---driver for secrets for storing and retrieving secret information)|   |-- security|   |-- storage(---driver for storage)|   |-- test(---driver for testing)|   |-- uml(user mode linux hypervisor)|   |-- util|   |-- vbox(virtual hypervisor)|   |-- vmware(workstation hypervisor)|   |-- vmx(VMWare file parser)|   |-- vz(Parallels Cloud Server Virtualization Solution)|   |-- xen(xen hypervisor)|   |-- xenapi(xen api)|   `-- xenconfig(xen config)`-- tools(virsh tool)    |-- nss(connect with nss service)nss is tool to configure name services, it lists databases as sources for obtaining that informationnss conf filepasswd:     files ldapshadow:     filesgroup:      files ldaphosts:      dns nis filesethers:     files nisnetmasks:   files nisnetworks:   files nisprotocols:  files nisrpc:        files nisservices:   files nisautomount:  filesaliases:    filesThe virtlockd daemon is a single purpose binary which focuses exclusively on the task of acquiring and holding locks on behalf of running virtual machines. It is designed to offer a low overhead, portable locking scheme can be used out of the box on virtualization hosts with minimal configuration overheads. It makes use of the POSIX fcntl advisory locking capability to hold locks, which is supported by the majority of commonly used filesystems.virtlockd is a lock manager implementation for libvirt. It’s designed to prevent you from starting two virtual machines (eg. on different nodes in your cluster) which are backed by the same writable disk image, something which can cause disk corruption. It uses plain fcntl-based file locking, so it is ideal for use when you are using NFS to share your disk images.



RPClibvirt uses a simple, variable length, packet based RPC protocol. All structured data within packets is encoded using the XDR standard., A program defines a set of procedures that it supports. The procedures can support call+reply method invocation, asynchronous events, and generic data streams. details please refer to libvirt rpc
RPC Frame &#x3D; Len + Data
|~~~   Packet 1   ~~~|~~~   Packet 2   ~~~|~~~  Packet 3    ~~~|~~~+-------+------------+-------+------------+-------+------------+...| n=U32 | (n-4) * U8 | n=U32 | (n-4) * U8 | n=U32 | (n-4) * U8 |+-------+------------+-------+------------+-------+------------+...|~ Len ~|~   Data   ~|~ Len ~|~   Data   ~|~ Len ~|~   Data   ~|~
Data &#x3D; Header + Payload
+-------+-------------+---------------....---+| n=U32 | 6*U32       | (n-(7*4))*U8         |+-------+-------------+---------------....---+|~ Len ~|~  Header   ~|~  Payload     ....  ~|

HeaderThe header contains 6 fields, encoded as signed&#x2F;unsigned 32-bit integers. 
+---------------+| program=U32   |+---------------+| version=U32   |+---------------+| procedure=S32 |+---------------+| type=S32      |+---------------+| serial=U32    |+---------------+| status=S32    |+---------------+



program  This is an arbitrarily chosen number that will uniquely identify the “service” running over the stream. (like rpc server as ‘remote’ service)

version  This is the version number of the program, by convention starting from ‘1’. When an incompatible change is made to a program, the version number is incremented. Ideally both versions will then be supported on the wire in parallel for backwards compatibility. 

procedure  This is an arbitrarily chosen number that will uniquely identify the method call(function provided by server), or event associated with the packet. By convention, procedure numbers start from 1 and are assigned monotonically thereafter. 

type  This can be one of the following enumeration values

call: invocation of a method call
reply: completion of a method call
event: an asynchronous event
stream: control info or data from a stream


serial  This is a number that starts from 1 and increases each time a method call packet is sent. A reply or stream packet will have a serial number matching the original method call packet serial. Events always have the serial number set to 0. 

status  This can one of the following enumeration values

ok: a normal packet. this is always set for method calls or events. For replies it indicates successful completion of the method. For streams it indicates confirmation of the end of file on the stream.
error: for replies this indicates that the method call failed and error information is being returned. For streams this indicates that not all data was sent and the stream has aborted
continue: for streams this indicates that further data packets will be following



]]></content>
      <categories>
        <category>libvirt</category>
        <category>kvm</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
      </tags>
  </entry>
  <entry>
    <title>libvirt-performance</title>
    <url>/2022/01/04/libvirt-performance/</url>
    <content><![CDATA[IntroductionFor performance issue, like a rpc all takes long time, you want to know which function takes much time without modifying source code and restart libvirt, also you want to collect stats of libvirt, here I will share the way to meet this reqirement, the core tool is systemtap, you can also refer to linux systemtap guide which shows how to enable systemtap and write probes. with systemtap, you can insert any code outside application without restarting it or rebuilding it


systemtapSystemtap is a scripting language and tool for dynamically probing or tracing in Linux kernel space or user space. libvirt already has built in markers(static probes) and tapset to simple your use, this feature can be enabled when compile libvirt
$ ../configure --with-dtrace$ make
The tapset is installed at /usr/share/systemtap/tapset/libvirt_* if enabled.
NOTE:

make sure you install systemap, refer to linux systemtap guide

With dtrace enabledwhen dtrace enabled, you can use the markers registered in source code and probe on marker like this, debuginfo is not a must in this way, this is can used for production env!!!
# list all markers$ grep &#x27;probe libvirt.[a-z_0-9.]*&#x27; /usr/share/systemtap/tapset/libvirt_* -oprobe libvirt.event_poll.add_handleprobe libvirt.event_poll.update_handleprobe libvirt.event_poll.remove_handleprobe libvirt.event_poll.dispatch_handleprobe libvirt.event_poll.purge_handleprobe libvirt.event_poll.add_timeoutprobe libvirt.event_poll.update_timeoutprobe libvirt.event_poll.remove_timeoutprobe libvirt.event_poll.dispatch_timeoutprobe libvirt.event_poll.purge_timeoutprobe libvirt.event_poll.runprobe libvirt.dbus.method_callprobe libvirt.dbus.method_errorprobe libvirt.dbus.method_replyprobe libvirt.object.newprobe libvirt.object.refprobe libvirt.object.unrefprobe libvirt.object.disposeprobe libvirt.rpc.socket_newprobe libvirt.rpc.socket_disposeprobe libvirt.rpc.socket_send_fdprobe libvirt.rpc.socket_recv_fdprobe libvirt.rpc.server_client_newprobe libvirt.rpc.server_client_disposeprobe libvirt.rpc.server_client_msg_tx_queueprobe libvirt.rpc.server_client_msg_rxprobe libvirt.rpc.client_newprobe libvirt.rpc.client_disposeprobe libvirt.rpc.client_msg_tx_queueprobe libvirt.rpc.client_msg_rxprobe libvirt.rpc.server_client_auth_allowprobe libvirt.rpc.server_client_auth_denyprobe libvirt.rpc.server_client_auth_failprobe libvirt.rpc.tls_context_newprobe libvirt.rpc.tls_context_disposeprobe libvirt.rpc.tls_context_session_allowprobe libvirt.rpc.tls_context_session_denyprobe libvirt.rpc.tls_context_session_failprobe libvirt.rpc.tls_session_newprobe libvirt.rpc.tls_session_disposeprobe libvirt.rpc.tls_session_handshake_passprobe libvirt.rpc.tls_session_handshake_failprobe libvirt.rpc.keepalive_newprobe libvirt.rpc.keepalive_disposeprobe libvirt.rpc.keepalive_startprobe libvirt.rpc.keepalive_stopprobe libvirt.rpc.keepalive_sendprobe libvirt.rpc.keepalive_receivedprobe libvirt.rpc.keepalive_timeoutprobe libvirt.qemu.monitor_newprobe libvirt.qemu.monitor_refprobe libvirt.qemu.monitor_unrefprobe libvirt.qemu.monitor_closeprobe libvirt.qemu.monitor_send_msgprobe libvirt.qemu.monitor_recv_replyprobe libvirt.qemu.monitor_recv_eventprobe libvirt.qemu.monitor_io_processprobe libvirt.qemu.monitor_io_readprobe libvirt.qemu.monitor_io_writeprobe libvirt.qemu.monitor_io_send_fd# check the args for each marker in order to use that marker# vi /usr/share/systemtap/tapset/libvirt_qemu_probes.stp# vi /usr/share/systemtap/tapset/libvirt_probes.stp

Write probe on marker
# stp is a script but similar like C$ cat qmp.stpprobe begin&#123;  printf(&quot;Start tracing\n&quot;);&#125;# check marker parameters from aboveprobe libvirt.qemu.monitor_send_msg&#123;  printf(&quot;%s QMPs: %s&quot;, tz_ctime(gettimeofday_s()), msg);&#125;$ stap qmp.stp -x `pidof libvirtd`Start tracingWed Jan  5 09:27:28 2022 CST QMPs: &#123;&quot;execute&quot;:&quot;query-version&quot;,&quot;id&quot;:&quot;libvirt-13&quot;&#125;# OR enable probe when starting libvirtd$ stap qmp.stp -c &quot;/usr/sbin/libvirtd&quot;

without dtrace enableWhen dtrace is disabled, there is no markder available, you have to write probe by yourself and build with deubg mode or install debuginfo of libvirt and shared library used by libvirt, without dtrace from libvirt itself, you have more freedom and more control, as you can trace on any function, but more complex as well.
# must build libvirt and shared library with debuginfo first$ cat qmp.stpprobe begin&#123;  printf(&quot;Start tracing\n&quot;)&#125;# check the function parametersprobe process(&quot;/usr/lib64/libvirt/connection-driver/libvirt_driver_qemu.so&quot;).function(&quot;qemuMonitorSend&quot;) &#123;  // can access fields like we did in C  printf(&quot;%s, mon fd: %d, vm: %s, QMPs: %s&quot;, tz_ctime(gettimeofday_s()), $mon-&gt;fd, user_string($mon-&gt;vm-&gt;def-&gt;name), user_string($msg-&gt;txBuffer))&#125;$ stap qmp.stp -x `pidof libvirtd`Start tracingWed Jan  5 10:38:44 2022 CST, mon fd: 22, vm: centos, QMPs: &#123;&quot;execute&quot;:&quot;query-version&quot;,&quot;id&quot;:&quot;libvirt-32&quot;&#125;# OR$ stap qmp.stp -c &quot;/usr/sbin/libvirtd&quot;

perf# perf on given process, include all fucntions related(kernel and userspace)$ perf record -p 21692$ perf report]]></content>
      <categories>
        <category>libvirt</category>
        <category>performance</category>
      </categories>
      <tags>
        <tag>libvirt</tag>
        <tag>dtrace</tag>
        <tag>systemtap</tag>
      </tags>
  </entry>
  <entry>
    <title>linux_iptables_inside</title>
    <url>/2020/11/10/linux-iptables-inside/</url>
    <content><![CDATA[Kernel


Above lost conntrack table which is important for nat
                                   netfilter hooks                                  +-----------&gt; local +-----------+                                  |             process           |                                  |                               |                                  |                               |                                  |                               |                                  |                               v  MANGLE            +-------------+--------+  FILTER            |                      |               +----------------------+    RAW  SECURITY          |        input         |               |                      |    conntrack  SNAT              |                      |               |     output           |    MANGLE  conntrack         |                      |               |                      |                    +------+---------------+               |                      |    DNAT                           ^                               +-------+--------------+    routing                           |                                       |                   FILTER                           |                                       |                   SECURITY                           |            +---------------------+    |         +-------------+     +-----------+                      |                     |    +-------&gt; |             |+--&gt; |pre routing+----  route    -----&gt; |      forward        |              |post routing +----&gt;     |           |      lookup          |                     +------------&gt; |             |     +-----------+                      +---------------------+              +-------------+     RAW                                       MANGLE                         MANGLE     conntrack                                 FILTER                         SNAT                                                                              conntrack     MANGLE                                    SECURITY     DNAT     routing# Security means selinux rules

从逻辑上来说每个Hook Point, 可以有不同的Table，并且Table是有固定优先级的，在同一个Hook的同一个Table 内，rule 是按照添加的先后顺序执行的，但是实现上iptable的每个表只注册了一个公共entry（callback)在某些链表中，也就是虽然每个iptable table支持在不同的point（input，output etc），但是都是统一的入口函数, 然后在这个统一的函数如找到具体是point的rules。
Conceptiptables实现防火墙功能的原理：在数据包经过内核的过程中有五处关键地方，分别是PREROUTING、INPUT、OUTPUT、FORWARD、POSTROUTING(chain)，称为钩子函数，iptables这款用户空间的软件可以在这5处地方写规则，对经过的数据包进行处理，规则一般的定义为“如果数据包头符合这样的条件，就这样处理数据包”, iptables中定义有5条链，说白了就是上面说的5个钩子函数，因为每个钩子函数中可以定义多条规则，每当数据包到达一个钩子函数时，iptables就会从钩子函数中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则iptables将继续检查下一条规则，如果该数据包不符合钩子函数中任一条规则，iptables就会根据该函数预先定义的默认策略来处理数据包
iptables中定义的表，分别表示提供的功能，有filter表（实现包过滤， deny, drop etc）、nat表（实现网络地址转换）、mangle表（实现包修改）、raw表（实现数据跟踪），这些表具有一定的优先级, table priority is fixed at each Hook point.
raw–&gt;mangle–&gt;nat–&gt;filter
Inside each table, rules are executed by order one by one

insert (head)
append (tail)

iptables commandiptables [-t 表] [操作命令] [链] [规则匹配器] [-j 目标动作]
Table
Operation
Condition
Target:

ACCEPT will terminate rule matching in the same table?REJECT, DROP will terminate all, stop here.one more target: 用户自定义的链名：跳转到这条链下，进行匹配
PS：

目标地址转换一般在PREROUTING链上操作
源地址转换一般在POSTROUTING链上操作
if without -t, default use filter table 

exampleshow commands# Please NOTE this may not show all rules!!! as user may create custom chain like docker does# show all rules at DOCKER chain(can be used as target)$ iptables -nv -L PREROUTING -t  natChain PREROUTING (policy ACCEPT 24 packets, 2840 bytes) pkts bytes target     prot opt in     out     source               destination             6   272 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;;iptables -nvL DOCKER  -t $table;done# show all rules at PREROUTING chain$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -nvL PREROUTING -t $table;done# show command used by iptable to add the rule$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -S PREROUTING -t $table;done# show all rules at INPUT chain$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -nvL INPUT -t $table;done$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -S INPUT -t $table;done# show all rules at FORWARD chain$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -nvL FORWARD -t $table;done$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -S FORWARD -t $table;done# show all rules at OUTPUT chain$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -nvL OUTPUT -t $table;done$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -S OUTPUT -t $table;done# show all rules at POSTROUTING chain$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -nvL POSTROUTING -t $table;done$ for table in raw mangle nat filter security; do printf &#x27;=%.0s&#x27; &#123;1..50&#125;; echo -e &quot;\n$&#123;table&#125;...&quot;; iptables -S POSTROUTING -t $table;done######################## Another way to show all rules######################################################################################## show all nat table at different chains(target or hook point)$ iptables -nvL -t nat# show command to replay the rule$ iptables -S  -t nat$ iptables -nvL -t filter$ iptables -nvL -t security$ iptables -nvL -t raw$ iptables -nvL -t mangle# show nat table at given chain(target)$ iptables -nvL PREROUTING -t nat# show command to replay the rule$ iptables -S  PREROUTING -t nat######################## Another way to show all rules#######################################################################################

show iptable rules in particular network namespace
# show all rules in mangle under network namespace bb2fdae4-44bf-40a3-8acc-dfb554d79d76$ alias nsc=&#x27;ip netns exec&#x27;root@dev:~/ping# nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables  -nvL -t mangleChain PREROUTING (policy ACCEPT 111K packets, 5802K bytes) pkts bytes target     prot opt in     out     source               destination1788K  399M DIVERT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            socket 8121  268K DIVERT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            icmptype 0Chain INPUT (policy ACCEPT 1792K packets, 399M bytes) pkts bytes target     prot opt in     out     source               destinationChain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target     prot opt in     out     source               destinationChain OUTPUT (policy ACCEPT 2614K packets, 196M bytes) pkts bytes target     prot opt in     out     source               destinationChain POSTROUTING (policy ACCEPT 2614K packets, 196M bytes) pkts bytes target     prot opt in     out     source               destinationChain DIVERT (2 references) pkts bytes target     prot opt in     out     source               destination1796K  399M MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK set 0x11796K  399M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0Only show PREROUTING rules in mangle tableroot@dev:~/ping# nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables  -nv -L PREROUTING -t mangleChain PREROUTING (policy ACCEPT 111K packets, 5806K bytes) pkts bytes target     prot opt in     out     source               destination1789K  399M DIVERT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            socket 8121  268K DIVERT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            icmptype 0DIVERT is self-defined chain like a hook pointroot@dev:~/ping# nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables  -nv -L DIVERT -t mangleChain DIVERT (2 references) pkts bytes target     prot opt in     out     source               destination1797K  400M MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK set 0x11797K  400M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0show rules with numberroot@dev:~/ping# nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables  -nv -L DIVERT -t mangle --line-numberChain DIVERT (2 references)num   pkts bytes target     prot opt in     out     source               destination1    1798K  400M MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK set 0x12    1798K  400M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0root@dev:~/ping#  nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables  -nvL -t mangle --line-numberChain PREROUTING (policy ACCEPT 111K packets, 5811K bytes)num   pkts bytes target     prot opt in     out     source               destination1    1791K  400M DIVERT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            socket2     8121  268K DIVERT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            icmptype 0Chain INPUT (policy ACCEPT 1795K packets, 399M bytes)num   pkts bytes target     prot opt in     out     source               destinationChain FORWARD (policy ACCEPT 0 packets, 0 bytes)num   pkts bytes target     prot opt in     out     source               destinationChain OUTPUT (policy ACCEPT 2618K packets, 197M bytes)num   pkts bytes target     prot opt in     out     source               destinationChain POSTROUTING (policy ACCEPT 2618K packets, 197M bytes)num   pkts bytes target     prot opt in     out     source               destinationChain DIVERT (2 references)num   pkts bytes target     prot opt in     out     source               destination1    1799K  400M MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK set 0x12    1799K  400M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0# Delete rule by rule numberroot@dev:~/ping# nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables -D PREROUTING  1 -t mangleroot@dev:~/ping# nsc bb2fdae4-44bf-40a3-8acc-dfb554d79d76 iptables  -nvL -t mangle --line-numberChain PREROUTING (policy ACCEPT 93 packets, 5580 bytes)num   pkts bytes target     prot opt in     out     source               destination1     8121  268K DIVERT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            icmptype 0Chain INPUT (policy ACCEPT 0 packets, 0 bytes)num   pkts bytes target     prot opt in     out     source               destinationChain FORWARD (policy ACCEPT 0 packets, 0 bytes)num   pkts bytes target     prot opt in     out     source               destinationChain OUTPUT (policy ACCEPT 52 packets, 3120 bytes)num   pkts bytes target     prot opt in     out     source               destinationChain POSTROUTING (policy ACCEPT 52 packets, 3120 bytes)num   pkts bytes target     prot opt in     out     source               destinationChain DIVERT (1 references)num   pkts bytes target     prot opt in     out     source               destination1    1799K  400M MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK set 0x12    1799K  400M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0

set command# flush all rules in filter table(default)$ iptables -F# flush all rules in mangle table$ iptables -F -t mangle# set default policy in filter table for INPUT chain$ iptables -P INPUT DROP# 阻止来自IP地址x.x.x.x eth0 tcp的包$ iptables -A INPUT -i eth0 -p tcp -s x.x.x.x -j DROP# check rule exist or not$ iptables -C OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT# if no rule $? == 1, otherwise 0$ echo $?# 允许所有来自外部的SSH连接请求，即只允许进入eth0接口，并且目标端口为22的数据包$ iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT$ iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT

save and restore使用iptables-save可以保存到特定文件中
$ iptables-save &gt;/etc/sysconfig/iptables_save

使用iptables-restore可以恢复规则
$ iptables-restore&lt;/etc/sysconfig/iptables_save
iptables match with module conditionAs basic iptables only supports protocol, saddr, sport etc conditions when matching, so some extensions add more conditions when matching packet, these extensions are organized as module like (time, limit, socket, ttl, state, mark etc)
kernel filesnet/netfilter/xt_time.cnet/netfilter/xt_socket.cnet/netfilter/xt_state.c
addrtype 地址类型Format:
[!] --src-type type[,...]      Match source address type[!] --dst-type type[,...]      Match destination address type    --limit-iface-in           Match only on the packet&#x27;s incoming device    --limit-iface-out          Match only on the packet&#x27;s outgoing deviceUNSPECUNICASTLOCALBROADCASTANYCASTMULTICASTBLACKHOLEUNREACHABLEPROHIBITTHROWNATHere are some examples$ iptables  -A OUTPUT -m addrtype --src-type LOCAL -j ACCEPTChain OUTPUT (policy ACCEPT)target     prot opt source               destinationACCEPT     all  --  anywhere             anywhere             ADDRTYPE match src-type LOCAL$ iptables  -A INPUT -m addrtype --dst-type LOCAL -j ACCEPTChain INPUT (policy ACCEPT)target     prot opt source               destinationACCEPT     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL
LOCAL means the address is one the interface, say host has eth0(10.172.1.32), if address is 10.172.1.32 means LOCAL.
limit限制流量：# -m limit --limit 1000/s               设置最大平均匹配速率# -m limit --limit-burst 15             设置一开始匹配的最大数据包数量# -m limit --limit 5/m --limit-burst 15 表示一开始能匹配的数据包数量为15个，每匹配到一个，limit-burst的值减1,所以匹配到15个时，该值为0,以后每过12s，limit-burst的值会加1,表示又能匹配1个数据包$ iptables -A INPUT -i eth0 -m limit --limit 5/m --limit-burst 15 -j ACCEPT$ iptables -A INPUT -i eth0 -j DROP

要点：

–limit-burst的值要比–limit的大
limit本身没有丢弃数据包的功能，因此，需要第二条规则一起才能实现限速的功能

time ：在特定时间内匹配# -m time 	说明# --monthdays day1[,day2] 	在每个月的特定天匹配# --timestart hh:mm:ss 	    在每天的指定时间开始匹配# --timestop hh:mm:ss 	    在每天的指定时间停止匹配# --weekdays day1[,day2] 	    在每个星期的指定工作日匹配，值可以是1-7$ iptables -A INPUT -i eth0 -m time --weekdays 1,2,3,4 -jACCEPT$ iptables -A INPUT -i eth0 -j DROP

ttl：匹配符合规则的ttl值的数据包# 参数 	说明# --ttl -eq 100 	匹配TTL值为100的数据包# --ttl -gt 100 	匹配TTL值大于100的数据包# --ttl -lt 100 	匹配TTL值小于100的数据包$ iptables -A OUTPUT -m ttl --ttl-eq 100 -j ACCEPT

multiport：匹配离散的多个端口iptables –dport and –sport 只支持连续的端口.
# 参数 	说明# --sports port1[,port2,port3] 	匹配源端口# --dports port1[,port2,port3] 	匹配目的端口# --ports port1[,port2,port3] 	匹配源端口或目的端口# 连续的端口$ iptables -A INPUT --sports 5000:5050 -j DROP# 离散的端口$ iptables -A INPUT -m multiport --sports 22，80，8080 -j DROP

state：匹配指定的状态数据包# 参数 	说明# --state value 	value可以为NEW、RELATED（有关联的）、ESTABLISHED、INVALID（未知连接）$ iptables -A INPUT -m state --state NEW，ESTABLISHED -j ACCEPT

mark：匹配带有指定mark值的数据包# 参数 	说明--mark value 	匹配mark标记为value的数据包$ iptables -t mangle -A INPUT -m mark --mark 1 -j DROP

mac：匹配特定的mac地址$ iptables -A FORWARD -m mac --mac-source 00:0C:24:FA:19:80 -j DROP
statistic: 负载均衡--mode mode    Set the matching mode of the matching rule, supported modes are `random and nth`.[!] --probability p    Set the probability for a packet to be randomly matched. It only works with the  random  mode.  p    must be within 0.0 and 1.0. The supported granularity is in 1/2147483648th increments.[!] --every n    Match  one  packet  every  nth  packet.  It  works  only with the nth mode (see also the --packet option).--packet p    Set the initial counter value (0 &lt;= p &lt;= n-1, default 0) for the nth mode.# RR$ iptables -A PREROUTING -t nat -d 172.17.64.8 -m statistic --mode nth --every 2 --packet 0 -j DNAT --to-destination 192.168.100.160$ iptables -A PREROUTING -t nat -d 172.17.64.8  -m statistic --mode nth --every 2 --packet 1 -j DNAT --to-destination 192.168.100.161# random$ iptables -A PREROUTING -t nat -d 172.17.64.8 -m statistic --mode random  --probability 0.5 -j DNAT --to-destination 192.168.100.160$ iptables -A PREROUTING -t nat -d 172.17.64.8  -m statistic --mode random  --probability 0.5 -j DNAT --to-destination 192.168.100.161
iptables with extended target modulesBasically, iptables is very simple, only support ACCEPT, DROP, more targets are provided by modulesHere are frequently used target.

BALANCEThis allows you to DNAT connections in a round-robin way over a given range of destination addresses.
$ iptables -A PREROUTING -t nat -j BALANCE --to-destination 192.168.1.1-192.168.1.10

DNAT
$ iptables -A PREROUTING -t nat -j DNAT --to-destination 192.168.1.1-192.168.1.10$ iptables -A PREROUTING -t nat -j DNAT --to-destination 192.168.1.1-192.168.1.10:1000-2000
Only valid in nat table at PREROUTING and OUTPUT chains

LOGTurn on kernel logging of matching packets
$ iptables -A PREROUTING -t nat -j LOG --log-level info$ iptables -A PREROUTING -t nat -j LOG --log-level info --log-prefix tag1
MARKThis is used to set the netfilter mark value associated with the packet. It is only valid in the mangle tableiptables -A PREROUTING -t mangle -j MARK –set-mark 0x10

MASQUERADEThis target is only valid in the nat table, in the POSTROUTING chain， like SNAT but use address of out dev as the source ip.
$ iptables -A POSTROUTING -t nat -j MASQUERADE
REDIRECT(change the receiving ip(port is optional))This target is only valid in the nat table, in the PREROUTING and OUTPUT chains, and user-defined chains which are only called from those chains. It redirects the packet to the machine itself by changing the destination IP to the primary address of the incoming interface, special DNAT without providing dest by admin!!
# --to-ports port[-port]# This specifies a destination port or range of ports to use$ iptables --table nat --append PREROUTING --protocol tcp --dport 80 --jump REDIRECT --to-ports 8080
REJECTThis is used to send back an error packet in response to the matched packet: otherwise it is equivalent to DROP so it is a terminating TARGET, ending rule traversal. This target is only valid in the INPUT, FORWARD and OUTPUT chains, and user-defined chains which are only called from those chains
# --reject-with type# The type given can be#     icmp-net-unreachable#     icmp-host-unreachable#     icmp-port-unreachable#     icmp-proto-unreachable#     icmp-net-prohibited#     icmp-host-prohibited or#     icmp-admin-prohibited (*)

SNATThis target is only valid in the nat table, in the POSTROUTING chain
# --to-source ipaddr[-ipaddr][:port-port]
comment to a ruleFor a single rule, you can add 256 characters for it to explain what is used for!
# -m comment --comment &quot;comment for this rule&quot;

FAQnegative one condition add ! before it$ iptables -t nat -I POSTROUTING  -s 172.17.0.0/16 ! -o  docker0 -j LOG --log-level info$ iptables -t nat -I POSTROUTING  ! -s 172.17.0.0/16 ! -o  docker0 -j LOG --log-level info

what’s the rule order in one ChainLet’s say there are several rules in one Chain, if the first rule doesn’t match, check next one, what about the rule matches, should we go to check next rule depends on the action of the matched rule.
- ACCEPT:           check next one in the chain- REJECT/DROP:      NOT check next one, return out of iptables- SNAT/DANT:        after SNAT or DNAT, check next one in the chain- RETURN:           just jump out the Chain, next one in this chain is not checked, check next chain

what’s conntrack table  nat is stateful, that means you should save info after nat, conntrack table is actually flow table, the initial use is for NAT, as when you do SNAT  after post routing, you need to save the 5 tuples to a flow entry  orig: src, dst, src_port, dst_port, protocol  nat change: m_src, dst, m_src_port, dst_port, protocol,  when reply packet comes back, it should match the nat_change port, then do DNAT  then goes routing.
  In order to support this, conntrack table was introduced, it insert some callbacks  at NF_HOOK POINT with different priority.
  let’s explain how conntrack fit the example, when you do SNAT after POSTROUTING, one entry is create  at conntrack table, when reply packet comes back, as conntrack table is examined before nat table, hence  we found a matched entry in conntrack table, will never check nat table at all at PREROUTING, skip nat table but check other table if insert rule at PREROUTING
  Note: now conntrack is not only used for nat, but for all connection, even without NAT, you can also delete&#x2F;create&#x2F;update entry in this table by $conntrack command
show command used for each ruledump the command used for all rules with
$ iptables-save...-A PREROUTING -j LOG --log-prefix nat-pre-jason --log-level 6-A PREROUTING -d 172.17.0.1/32 -j LOG --log-prefix nat-pre --log-level 6-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A DOCKER -i docker0 -j RETURN
how custom chain is usedcustom chain is similar like other chains(INPUT, OUTPUT), belongs to a table(filter, nat etc) you can create rules in it, but it has some difference

must create custom chain before creating rule in it, unlike other chains, they are kernel built-in chains
custom chain has no fixed hook point unlike other built-in chains have fixed hook point in kernel, custom chain only works when a rule in other built-in chain refers to it by -j CUSTOM_CHAIN.

# create custom chain$ iptables -t filter -N WEB_CHAIN$ iptables --list...Chain WEB_CHAIN (0 references)target     prot opt source               destination# create a rule in the chain$ iptables -t filter -I WEB_CHAIN -s 1.1.1.1 -j DROP$ iptables -t filter -nvL WEB_CHAINChain WEB_CHAIN (0 references) pkts bytes target     prot opt in     out     source               destination    0     0 DROP       all  --  *      *       1.1.1.1              0.0.0.0/0# other rule refer to custom chain set target as custom chain$ iptables -t filter -I INPUT -p tcp --dport 80 -j WEB_CHAIN$ iptables -t filter -nvL INPUTChain INPUT (policy ACCEPT 154 packets, 11509 bytes) pkts bytes target     prot opt in     out     source               destination    0     0 WEB_CHAIN  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80$ iptables -t filter -nvL WEB_CHAINChain WEB_CHAIN (1 references)----&gt; one reference pkts bytes target     prot opt in     out     source               destination    0     0 DROP       all  --  *      *       1.1.1.1              0.0.0.0/0

ref
iptables command
example commands
kernel netfilter

]]></content>
      <categories>
        <category>linux</category>
        <category>iptables</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title>libvirt-vnc-spice</title>
    <url>/2022/08/03/libvirt-vnc-spice/</url>
    <content><![CDATA[










SPICE
VNC
RDP


BIOS screen display
can
can
Can’t


Full color support
can
can
can


Change resolution
can
can
can


Multi-display
Multi-monitor support (up to 4 screens)
Only one screen
Multi-monitor support


Image transmission
Image and graphics transmission
Image transmission
Image and graphics transmission


Video playback support
GPU acceleration support
Can’t
GPU acceleration support


Audio transmission
Two-way voice can be controlled
Can’t
Two-way voice can be controlled


Mouse control
Both client and server can be controlled
Server-side control
Server-side control


USB transfer
USB can be transmitted over the network
Can’t
USB can be transmitted over the network


Ref- 

spice for newbies
spice user manual
spice agent
usbredir

]]></content>
  </entry>
  <entry>
    <title>linux-kernel-boot</title>
    <url>/2021/11/10/linux-kernel-boot/</url>
    <content><![CDATA[Overviewwhy need intial ram diskMany Linux distributions ship a single, generic Linux kernel image – one that the distribution’s developers create specifically to boot on a wide variety of hardware. The device drivers for this generic kernel image are included as loadable kernel modules because statically compiling many drivers into one kernel causes the kernel image to be much larger, in some cases to cause boot-time crashes or other problems due to probing for inexistent or conflicting hardware. This static-compiled kernel approach also leaves modules in kernel memory which are no longer used or needed, and raises the problem of detecting and loading the modules necessary to mount the root file system at boot time, or for that matter, deducing where or what the root file system is.
To avoid having to hardcode handling for so many special cases into the kernel, an initial boot stage with a temporary root file-system(ram disk with temporary root fs) is used. This temporary root file-system can contain user-space helpers which do the hardware detection, module loading and device discovery necessary to get the real root file-system mounted
initial ramdisk is for loading a temporary root file system into memory, to be used as part of the Linux startup process. initrd and initramfs refer to two different methods of achieving this. Both are commonly used to make preparations before the real root file system can be mounted, if no real root file system is proivdes, the initial ramdisk is used for root file system which is in memory.
The bootloader will load the kernel and initial root file system image into memory and then start the kernel, passing in the memory address of the image. At the end of its boot sequence, the kernel tries to determine the format of the image from its first few blocks of data, which can lead either to the initrd or initramfs scheme. 

initial root file system image format schema  

initrd scheme: the image may be a file system image (optionally compressed), which is made available in a special block device (&#x2F;dev&#x2F;ram) that is then mounted as the initial root file system during boots, The driver for that file system must be compiled statically into the kernel. Many distributions originally used compressed ext2 file system images. Once the initial root file system is up, the kernel executes /linuxrc as its first process; when it exits, the kernel assumes that the real root file system has been mounted and executes &#x2F;sbin&#x2F;init to begin the normal user-space boot process.

A ramdev block device is created(default fixed size 16M). It is a ram-based block device, that is a simulated hard disk that uses memory instead of physical disks.
The initrd file is read and unzipped into the device, as if you did zcat initrd | dd of&#x3D;&#x2F;dev&#x2F;ram0 or something similar.
The initrd contains an image of a filesystem, so now you can mount the filesystem as usual: mount /dev/ram0 /root. Naturally, filesystems need a driver, so if you use ext2, the ext2 driver has to be compiled in-kernel.
exec /linuxrc as first process which mount real root file system, then call &#x2F;sbin&#x2F;init to begin user-space boot process.


initramfs scheme: (available since the Linux kernel 2.6.13), the image may be a cpio archive (optionally compressed). The archive is unpacked by the kernel into a special instance of a tmpfs that becomes the initial root file system. This scheme has the advantage of not requiring an intermediate file system or block drivers to be compiled into the kernel. In the initramfs scheme, the kernel executes /init as its first process that is not expected to exit. 

A tmpfs is mounted: mount -t tmpfs nodev /root. The tmpfs doesn’t need a driver, it is always on-kernel. No device needed, no additional drivers.
The initramfs is uncompressed directly into this new filesystem: zcat initramfs | cpio -i, or similar.
exec /init never exit



initrd and initramfsinitrd schema  

initrd is for Linux kernels 2.4 and lower.
Initrd requires at least one file system driver be compiled into the kernel
A disk created by Initrd has got to have a fixed size
All of the reads&#x2F;writes on Initrd are buffered redundantly (unnecessarily) into main memory

So, initrd is deprecated and is replaced by initramfs.
# inspect initrd image(it&#x27;s disk)$ gunzip initrd.gz$ file -L initrdinitrd: Linux rev 1.0 ext2 filesystem data (mounted or unclean), UUID=6d512aa6-269e-4932-ba2b-83d953559340$ mount ‑t ext2 ‑o loop initrd /mnt/initrd$ ls /mnt/initrdbin  cleanup  dev  drivers.lzm  etc  lib  liblinuxlive  linuxrc  mnt  proc  sbin  sys  tmp  usr  usr.lzm  var

initramfs schema  

initramfs is a Linux 2.6 and above.
This feature is made up from a cpio archive of files that enables an initial root filesystem and init program to reside in kernel memory cache, rather than on a ramdisk, as with initrd filesystems.
with initramfs, you create an archive with the files which the kernel extracts to a tmpfs.
intramfs can increase boot-time flexibility, memory efficiency, and simplicity
dracut is the tool used to create the initramfs image.
initramfs location of init : &#x2F;init

# inspect initramfs(it&#x27;s just root file system)$ file -L initramfs-3.10.0-1160.el7.x86_64.imginitramfs-3.10.0-1160.el7.x86_64.img: gzip compressed data, from Unix, last modified: Wed Sep 29 18:29:57 2021, max compression$ zcat initramfs-3.10.0-1160.el7.x86_64.img | cpio -idmv
create initramfsActually, initramfs is created when you build kernel, you can also create initramfs with other tools without build your kernel.When we first boot, we need at least some tools to start working. This includes the init process and some tools like ls, mount, mv, etc. To get those user space tools you can use BusyBox. BusyBox has many useful commands available for just 1.1MB, it’s a binary that support many command
# use busybox$ busybox ls$ busybox df

create_initramfs.sh
#!/bin/bashARCH=&quot;x86_64&quot;BB_VER=&quot;1.31.0&quot;# Dirsmkdir -p rootcd rootmkdir -p bin dev etc lib mnt proc sbin sys tmp varcd -# Utilsif [ ! -f &quot;root/bin/busybox&quot; ]; then    curl -L &quot;https://www.busybox.net/downloads/binaries/$&#123;BB_VER&#125;-defconfig-multiarch-musl/busybox-$&#123;ARCH&#125;&quot; &gt;root/bin/busyboxficd root/binchmod +x busybox# as this two may be called before init process, hence link itln -s busybox mountln -s busybox shcd -# Init process# init process create soft link at /bin for all commands that busybox support cat &gt;&gt;root/init &lt;&lt; EOF#!/bin/busybox sh/bin/busybox --install -s /binmount -t devtmpfs  devtmpfs  /devmount -t proc      proc      /procmount -t sysfs     sysfs     /sysmount -t tmpfs     tmpfs     /tmpsetsid cttyhack shexec /bin/shEOFchmod +x root/init# initramfs creationcd rootfind . | cpio -ov --format=newc | gzip -9 &gt;../initramfscd -

boot with disk&#x2F;diskless# root filesystem is in an ext2 &quot;hard disk&quot;$ qemu-system-x86_64 -kernel normal/bzImage -drive file=rootfs.ext2# root filesystem is in initramfs$ qemu-system-x86_64 -kernel normal/bzImage -initrd initramfs.img# full command to run$ qemu-system-x86_64  -kernel normal/bzImage -initrd initramfs.img -nographic -append &quot;console=ttyS0&quot;# root filesystem is built in kernel$ qemu-system-x86_64 -kernel with_initramfs/bzImage# Neither -drive nor -initrd are given.# with_initramfs/bzImage is a kernel compiled with options identical to normal/bzImage, except for one: CONFIG_INITRAMFS_SOURCE=initramfs.img pointing to the exact same CPIO as from the -initrd example.

ramfs vs tmpfsA ramdisk is a volatile storage space defined in the RAM memory. all information stored in it will be lost if the device is umounted or  system reboots.
In Linux, ramdisks can be created using the command mount and the filesystems tmpfs and ramfs

Tmpfs: Tmpfs is a temporary file system stored in the RAM memory (and/or swap memory). By specifying this file system with the argument -t of the command mount, you can assign limited memory resources to a temporary file system.

stored in ram and swap memory
ensure a limit
adjusted on the fly via ‘mount -o remount …’
Normal users can be allowed write access to tmpfs mounts!


Ramfs: Ramfs is similar to Tmpfs, it uses ram memory only and the user can’t ensure a limit, and the allocated resource grows dynamically. If the user doesn’t control the ramfs consumption, ramfs will keep using all the memory until hanging or crashing the system

stored in ram only
can not ensure a limit, but traditonal ram disk (&#x2F;dev&#x2F;ramX) has fixed size, default is 16M
cant adjust size on the fly for &#x2F;dev&#x2F;ramX, you need to reboot system or reload kernel module brd.
only root use can access ramfs mounts!



ram disk(ramfs)As mentioned above, they are two ways to create ram disk, one is using ramfs, the other is using tmpfs.  

enable traditional ram disk  old system ram disk is built into kernel with these kernel configs, that means after system boots, you will seee /dev/ram0, /dev/ram1, /dev/ramX which has default fixed size configured during kernel compiling. even you have 16 /dev/ramX, the memory for each block device is not prealloated, memory allocation happens when make fs on that device.  CONFIG_BLK_DEV_RAM=yCONFIG_BLK_DEV_RAM_COUNT=16CONFIG_BLK_DEV_RAM_SIZE=16384
  new system newer system by default compile it as kernel module brd, you need to load this module when using /dev/ramX  CONFIG_BLK_DEV_RAM=mCONFIG_BLK_DEV_RAM_COUNT=16CONFIG_BLK_DEV_RAM_SIZE=16384
change traditonal ram disk size  old system ram disk is built into kernel, there is only one way to change ram disk ize, that’s by appending parmaters to kernel boot line like this.  kernel /vmlinuz-2.6.32.24 ro root=LABEL=/ rhgb quiet ramdisk_size=10485760
  new system ram disk szie can be changed only when brd is loaded  # 1G, one ram disk /dev/ram0$ modprobe brd rd_nr=1 rd_size=1048576
use traditional ram disk  $ mkfs /dev/ram0$ mkdir /mnt/randisk$ mount /dev/ram0 /mnt/ramdisk$ df -hdf -hFilesystem               Size  Used Avail Use% Mounted on.../dev/ram0                 16M  140K   15M   1% /mnt/ramdisk
use ramfs not &#x2F;dev&#x2F;ramX(brd is not needed for this)  $ mount -t ramfs ramfs /tmp/ramdisk
ram disk(tmpfs)Use tmpfs to create ram disk is easy.# memory is allocated only when it&#x27;s used, it&#x27;s not prereserved.$ mount -t tmpfs -o size=10G none /tmp/ramdisk(base) [root@dev github]# df -h...none                      10G     0   10G   0% /tmp/ramdisk(base) [root@dev github]# mountnone on /tmp/ramdisk type tmpfs (rw,relatime,seclabel,size=10485760k)

As when umount data on ram disk is gone, hence in order to save data in ram disk when reboot, we need a service that will copy data from ram disk to hard disk also copy it back to ram disk.
$ vi /lib/systemd/system/ramdisk-sync.service[Unit]# runs before umount service, to make sure, we copy data before umounting# but if you run umount mannually, data is lost.Before=umount.target[Service]Type=oneshotUser=root# root below can be change to any user.ExecStartPre=/bin/chown -Rf root /mnt/ramdisk# when service starts, copy back when system bootsExecStart=/usr/bin/rsync -ar /mnt/ramdisk_backup/ /mnt/ramdisk/# when serivce stops, copy data from ram disk to hard diskExecStop=/usr/bin/rsync -ar /mnt/ramdisk/ /mnt/ramdisk_backup/ExecStopPost=/bin/chown -Rf root /mnt/ramdisk_backupRemainAfterExit=yes[Install]WantedBy=multi-user.target$ vi /etc/fstabtmpfs  /mnt/ramdisk  tmpfs  rw,size=110M  0   0

Acutally, there are some ram disk(tmpfs) created by systemd if you check with df, it’s /run/ and /dev/shm, sys/fs/cgroup with default size(half of total physical memory)
(base) [root@dev etc]# dfFilesystem              1K-blocks     Used Available Use% Mounted ondevtmpfs                  4074576        0   4074576   0% /devtmpfs                     4086484        0   4086484   0% /dev/shmtmpfs                     4086484     9168   4077316   1% /runtmpfs                     4086484        0   4086484   0% /sys/fs/cgroup(base) [root@dev etc]# free              total        used        free      shared  buff/cache   availableMem:        8172968      979092     4939616        9308     2254260     6880632Swap:       8257532           0     8257532(base) [root@dev etc]# mount | grep tmpfsdevtmpfs on /dev type devtmpfs (rw,nosuid,seclabel,size=4074576k,nr_inodes=1018644,mode=755) # with sizetmpfs on /dev/shm type tmpfs (rw,nosuid,nodev,seclabel) # without size half of total memorytmpfs on /run type tmpfs (rw,nosuid,nodev,seclabel,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755)
resize &#x2F;dev&#x2F;shm
Edit file /etc/fstab (with sudo if needed).In this file, try to locate a line like this one : none /dev/shm tmpfs defaults,size=4G 0 0.Case 1 - This line exists in your /etc/fstab file:    Modify the text after size=. For example if you want an 8G size, replace size=4G by size=8G.    Exit your text editor, then run (with sudo if needed) $ mount -o remount /dev/shm.Case 2 - This line does NOT exists in your /etc/fstab file:    Append at the end of the file the line none /dev/shm tmpfs defaults,size=4G 0 0, and modify the text after size=. For example if you want an 8G size, replace size=4G by size=8G.    Exit your text editor, then run (with sudo if needed) $ mount /dev/shm.
REF
linux boot process
rootfs, initramfs, devfs, tmpfs etc

]]></content>
      <categories>
        <category>linux</category>
        <category>initramfs</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>initramfs</tag>
        <tag>tmpfs</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-memory-checker</title>
    <url>/2019/11/29/linux-memory-checker/</url>
    <content><![CDATA[IntroductionThere are two powerful tools for memory check, one is valgrind, the other is address sanitizer, the difference is

valgrind, no need to compile your app, have big impact on performance(down 10-50 times), can’t work well with multiple process, but can detect leak in shared library.
address sanitizer, need to compile your app, has less impact for app run(down 2 times)

Always use address sanitizer if you can recomplie your application!

valgrindvalgrind error when runs
--4636:0: aspacem &lt;&lt;&lt; SHOW_SEGMENTS: out_of_memory (19 segments)--4636:0: aspacem 1 segment names in 1 slots--4636:0: aspacem freelist is empty--4636:0: aspacem (0,4,2) /usr/lib/valgrind/memcheck-amd64-linux--4636:0: aspacem   0: RSVN 0000000000-0003ffffff     64m ----- SmFixed--4636:0: aspacem   1:      0004000000-0037ffffff    832m--4636:0: aspacem   2: FILE 0038000000-003821efff 2224128 r-x-- d=0x802 i=404505  o=0       (0,4)--4636:0: aspacem   3:      003821f000-003841efff 2097152--4636:0: aspacem   4: FILE 003841f000-0038421fff   12288 rw--- d=0x802 i=404505  o=2224128 (0,4)...==4636==     Valgrind&#x27;s memory management: out of memory:==4636==        newSuperblock&#x27;s request for 4194304 bytes failed.==4636==           42,684,416 bytes have already been mmap-ed ANONYMOUS.==4636==     Valgrind cannot continue.  Sorry.==4636====4636==     There are several possible reasons for this.==4636==     - You have some kind of memory limit in place.  Look at the==4636==       output of &#x27;ulimit -a&#x27;.  Is there a limit on the size of==4636==       virtual memory or address space?==4636==     - You have run out of swap space.==4636==     - Valgrind has a bug.  If you think this is the case or you are==4636==     not sure, please let us know and we&#x27;ll try to fix it.==4636==     Please note that programs can take substantially more memory than==4636==     normal when running under Valgrind tools, eg. up to twice or==4636==     more, depending on the tool.  On a 64-bit machine, Valgrind==4636==     should be able to make use of up 32GB memory.  On a 32-bit==4636==     machine, Valgrind should be able to use all the memory available==4636==     to a single process, up to 4GB if that&#x27;s how you have your==4636==     kernel configured.  Most 32-bit Linux setups allow a maximum of==4636==     3GB per process.==4636====4636==     Whatever the reason, Valgrind cannot continue.  Sorry.$ sysctl -w kernel.pax.softmode=1$ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --verbose --log-file=valgrind-out.txt ./test

address sanitizerit comes from google, and gcc supports it from 4.8, so make sure you gcc support it before compiling your program.
how to use it
-fsanitize=address    #开启地址越界检查功能-fno-omit-frame-pointer  #开启后，可以出界更详细的错误信息-fsanitize=leak   #开启内存泄露检查功能可选择-O1或者更高的优化级别编译
this will build nginx with static asan, can NOT detect link in shared library.
build it in nginx
--with-cc-opt=&quot;-O0 -fsanitize=address -fno-omit-frame-pointer -fPIC -g $CC_OPT&quot;--with-ld-opt=&quot;-fsanitize=address -fno-omit-frame-pointer -Wl,-Bsymbolic-functions -fpic -Wl,-z,relro&quot;

check resultsanitizer only prints result when program ends normally. (Hint: Do a kill or killall with normal SIGTERM of the daemons at the end).

$ killall nginx (if nginx runs as a daemon)
check file set by error_log in nginx.conf

==7852==ERROR: LeakSanitizer: detected memory leaksDirect leak of 320 byte(s) in 5 object(s) allocated from:    #0 0x7f92a193b602 in malloc (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x98602)    #1 0x9f7eef in ngx_http_init_connection src/http/ngx_http_request.c:228    #2 0x97fdb3 in ngx_event_accept src/event/ngx_event_accept.c:350    #3 0x9aea6f in ngx_epoll_process_events src/event/modules/ngx_epoll_module.c:937    #4 0x9773a9 in ngx_process_events_and_timers src/event/ngx_event.c:242    #5 0x9a6cca in ngx_worker_process_cycle src/os/unix/ngx_process_cycle.c:989    #6 0x99b93c in ngx_spawn_process src/os/unix/ngx_process.c:213    #7 0x9a25b9 in ngx_start_worker_processes src/os/unix/ngx_process_cycle.c:425    #8 0x9a0fb6 in ngx_master_process_cycle src/os/unix/ngx_process_cycle.c:146    #9 0x8eff1a in main src/core/nginx.c:390    #10 0x7f929da2a82f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)    ...    SUMMARY: AddressSanitizer: 576 byte(s) leaked in 11 allocation(s).

option for AddressSanitizer# check all available options$ ASAN_OPTIONS=help=1 ./testAvailable flags for AddressSanitizer:	symbolize		- If set, use the online symbolizer from common sanitizer runtime to turn virtual addresses to file/line locations.	external_symbolizer_path		- Path to external symbolizer. If empty, the tool will search $PATH for the symbolizer.	allow_addr2line		- If set, allows online symbolizer to run addr2line binary to symbolize stack traces (addr2line will only be used if llvm-symbolizer binary is unavailable.	strip_path_prefix		- Strips this prefix from file paths in error reports.	fast_unwind_on_check		- If available, use the fast frame-pointer-based unwinder on internal CHECK failures.	fast_unwind_on_fatal		- If available, use the fast frame-pointer-based unwinder on fatal errors.	fast_unwind_on_malloc		- If available, use the fast frame-pointer-based unwinder on malloc/free.	handle_ioctl		- Intercept and handle ioctl requests.	malloc_context_size		- Max number of stack frames kept for each allocation/deallocation.	log_path		- Write logs to &quot;log_path.pid&quot;. The special values are &quot;stdout&quot; and &quot;stderr&quot;. The default is &quot;stderr&quot;.....

set log file for AddressSanitizer
$ ASAN_OPTIONS=log_path=/tmp/test ./test# it will create a file named test.$pid and print result to that file
REF
address sanitizer
memory leak detector compare

]]></content>
      <categories>
        <category>linux</category>
        <category>memory-checker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>memory checker</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-ansible-guide</title>
    <url>/2022/09/30/linux-ansible-guide/</url>
    <content><![CDATA[OverviewAnsible is an open source IT automation engine that automates provisioning, configuration management, application deployment, orchestration, and many other IT processes.

Ansible works by connecting to your nodes and pushing out small programs—called modules—to these nodes. Modules are used to accomplish automation tasks in Ansible. These programs are written to be resource models of the desired state of the system. Ansible then executes these modules and removes them when finished.



terminology:

Control node: the host on which you use Ansible to execute tasks on the managed nodes
Managed node: a host that is configured by the control node
Host inventory: a list of managed nodes
Ad-hoc command: a simple one-off task
Playbook: a set of repeatable tasks for more complex configurations
Module: code that performs a particular common task such as adding a user, installing a package, etc.

Concepts$ansible --versionansible 2.9.27  config file = /etc/ansible/ansible.cfg ------&gt; the cfg used  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]  ansible python module location = /usr/lib/python2.7/site-packages/ansible  executable location = /usr/bin/ansible  python version = 2.7.5 (default, Jun 28 2022, 15:30:04) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]# get available modules$ansible-doc -l# list all hosts$ansible all --list-hosts# list hosts in web group$ansible web --list-hosts# get help and example for each module$ansible-doc copy$ansible-doc shell# show inventory graph(higher ansible)$ansible-inventory -i host --graph@all:  |--@ungrouped:  |  |--172.17.0.3# show in json format$ansible-inventory -i host --list&#123;    &quot;_meta&quot;: &#123;        &quot;hostvars&quot;: &#123;&#125;    &#125;,     &quot;all&quot;: &#123;        &quot;children&quot;: [            &quot;ungrouped&quot;        ]    &#125;,     &quot;ungrouped&quot;: &#123;        &quot;hosts&quot;: [            &quot;172.17.0.3&quot;        ]    &#125;&#125;
confdefault configuration file is /etc/ansible/ansible.cfg
The order in which a configuration file is located is as follow.

ANSIBLE_CONFIG (environment variable)
ansible.cfg (per directory)
~&#x2F;.ansible.cfg (home directory)
&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg (global)

All parameters of conf and conf example
# dump the current setting$ansible-config dump# Only show configurations that have changed from the default$ansible-config dump --only-changed$cat ansible.cfg[defaults]# thirdpart to speed up ansible, see ref belowstrategy_plugins = /xxx/mitogen-0.3.3/ansible_mitogen/plugins/strategystrategy = mitogen_linear# the actual number may be less than this(max value) due to cpu and memory used for each fork# suggest: 30-50 for server and less than total cpu numberforks          = 50# do not gather by default, must say gather_facts: Truegathering      = explicit# SSH timeouttimeout = 60log_path = ./log/ansible.log[ssh_connection]# It can result in a very significant performance improvement when enabled. However this conflicts with privilege escalation (become). For example, when using ‘sudo:’ operations you must first disable ‘requiretty’ in /etc/sudoers on all managed hosts, which is why it is disabled by defaultpipelining = True

inventoryThe inventory file contains the IP address or DNS information about the list of managed hosts we want to work with.
Inventory file has a concept called grouping where you will be grouping your resources and run tasks against that group. You can create the inventory file without using groups. In this case, Ansible will use two default groups &quot;all&quot; and &quot;ungrouped&quot;.

ALL GROUP - All resources that are available in the inventory file by default will be assigned to all group.
UNGROUPED - Resources that are not part of any user-defined groups will be automatically assigned to the ungrouped group

inventory pathdefault: /etc/ansible/hosts can be changed in the ansible.cfg Or  by using the -i option on the ansible command
$cat ansible.cfg[defaults]inventory = $HOME/hosts$ansible -i xxx/hosts

inventory file example
# must /etc/hosts for vm1, vm2, vm3 and vm4web1[web]vm2vm3[db]192.168.1.2[log]192.168.1.100# use inventory from default$ansible web -m ping# use explicit inventory from a host$ansible web -m ping -i ./hosts# use explicit inventory from command line$ansible all  -i 172.17.0.2,172.17.0.3 -m ping# NOTE the , is needed for one host$ansible all  -i 172.17.0.2, -m ping
modulesModules (also referred to as “task plugins” or “library plugins”) are discrete units of code that can be used from the command line or in a playbook task. Ansible executes each module, usually on the remote managed node, and collects return values.
Each module supports taking arguments. Nearly all modules take key=value arguments, space delimited. Some modules take no arguments, and the command/shell modules simply take the string of the command you want to run.
Modules should be idempotent, and should avoid making any changes if they detect that the current state matches the desired final state. When used in an Ansible playbook, modules can trigger ‘change events’ in the form of notifying handlers to run additional tasks
# module from command line$ansible webservers -m service -a &quot;name=httpd state=started&quot;# fork 100 process to run$ansible webservers -m ping -f 100$ansible webservers -m command -a &quot;/sbin/reboot -t now&quot;# module from playbook$cat play.yml- name: restart webserver  service:  # module    name: httpd  # parameter    state: restarted # parameter

Top modules ares file, include, template, command, service, shell, lineinfile, copy, yum, user, systemd, cron etc.

file:  Creating different new files is a common task in the server scripts. In Ansible tools, you will find various methods for creating a new file. You can even set different group permission, assign an owner to the file; create a file with content, and more. It sets attributes of directories, symlinks, and files. Besides, it removes symlinks, directories, and file.

ansible test-servers -m file -a &#39;path=/tmp/test state=directory mode=0755&#39;


ping is used when we want to check whether the connection with our hosts defined in the inventory file is established or not.

ansible test-servers -m ping


copy: The copy module is often used in writing playbooks when we want to copy a file(support directory as well) from a remote server to destination nodes.

ansible test-servers -m copy -a &#39;src=/home/knoldus/Personal/blogs/blog3.txt dest=/tmp&#39;
ansible test-servers -m copy -a &#39;src=/home/knoldus/Personal/blogs dest=/tmp&#39;


fetch: Ansible’s fetch module transfers files(not support directory) from a remote host to the local host. This is the reverse of the copy module.

ansible test-servers -m fetch -a &#39;src=/var/log/nginx/access.log dest=fetched&#39;


synchronize: Ansible’s fetch module to push&#x2F;pull directory

ansible all -m synchronize -a &#39;mode=pull src=/export/Data/xcgroup/persistence dest=fetched&#39; pull src to local fetched&#x2F;
ansible all -m synchronize -a &#39;mode=push src=/export/Data/xcgroup/persistence dest=fetched&#39; push src to remote fetched&#x2F;


yum: We use the Yum module to install a service.

ansible test-servers -m yum -a &#39;name=httpd,postfix state=present&#39;


shell: When we want to run UNIX commands then we use shell module

ansible test-servers -m shell -a &#39;ls -la&#39;


script: When we want to run a bunch of commands use script module

ansible test-servers -m script -a &#39;./test.sh&#39;


service: When we want to ensure the state of a service that is service is running we use the service module

ansible test-servers -m service -a &#39;name=httpd state=started&#39;


tempalte: The Template module is used to copy a configuration file from the local system to the host server. It is the same as the copy module, but it dynamically binds group variables defined by us.
  - name: a play  hosts: all  gather_facts: no  vars:    variable_to_be_replaced: &#x27;Hello world&#x27;    inline_variable: &#x27;hello again.&#x27;  tasks:  - name: Ansible Template Example    template:      src: hello_world.j2 # in template, we can use var defined here      dest: /Users/mdtutorials2/Documents/Ansible/hello_world.txt

lineinfile: this is generally used to alter or remove the existing line, insert line, and to replace the lines. Let’s know about the process to insert a line. You can set the file’s path to modify using the path&#x2F; dest parameter. You can insert lines through the line parameter. The line enters to the EOF. However, if the line is already there in the system, it won’t be added.

ansible test-servers -m lineinfile -a &#39;path=/etc/selinux/config regexp=^SELINUX= line=SELINUX=enforcing&#39;


replace: The replace module replaces all instances of a defined string within a file.
  - hosts: 127.0.0.1  tasks:  - name: Ansible replace string example    replace:      path: /etc/ansible/sample.txt      regexp: &#x27;Unix&#x27;      replace: &quot;Linux&quot;- hosts: 127.0.0.1  tasks:  - name: Ansible replace string example    replace:      path: /etc/hosts      regexp:  &#x27;(\s+)server\.myubuntu\.com(\s+.*)?$&#x27;      replace: &#x27;\1server.linuxtechi.info\2&#x27; # use captured tokens

include: When we want to include another playbook in our playbook, then we use the Include module

user: To add a particular user to our module we can use User module


Ad-HocYou can also use Ansible to run ad-hoc commands. To do this, you will need to run a command or call a module directly from the command line. No playbook is used. This is fine for a one time task.
host pattern
# command format# without -m, default module is &#x27;command&#x27; similar like `shell`$ansible [host-pattern] -m [module] -a “[module options]”$ansible all -m copy -a &#x27;src=dvd.repo dest=/etc/yum.repos.d owner=root group=root mode=0644&#x27;# Each node reports SUCCESS and &quot;changed&quot; : `true meaning the module execution was successful # and the file was created/changed`. If we run the command again, # the output will include &quot;changed&quot; : false meaning the file is already present # and configured as required. In other words, # Ansible will only make the required changes if they do not already exist. # This is what is known as &quot;idempotence&quot;.$ansible all -m ansible.builtin.service -a &quot;name=libvirtd state=started&quot;

PlayBook(suggested way)A playbook runs in order from top to bottom. Within each play, tasks also run in order from top to bottom. Playbooks with multiple ‘plays’ can orchestrate multi-machine deployments, running one play on your webservers, then another play on your database servers, then a third play on your network infrastructure, and so on.
Plays consist of an ordered set of tasks to execute against host selections from your Ansible inventory file. Tasks are the pieces that make up a play and call Ansible modules. In a play, tasks are executed in the order in which they are written.  
Ansible includes a “check mode” which allows you to validate playbooks and ad-hoc commands before making any state changes on a system. This shows you what Ansible would do, without actually making any changes. Handlers in Ansible are used to run a specific task only after a change has been made to the system. They are triggered by tasks and run once, at the end of all of the other plays in the playbook
playbook handlers
By default, handlers run after all the tasks in a particular play have been completed. Notified handlers are executed automatically after each of the following sections, in the following order: pre_tasks, roles/tasks and post_tasks. This approach is efficient, because the handler only runs once, regardless of how many tasks notify it. For example, if multiple tasks update a configuration file and notify a handler to restart Apache, Ansible only bounces Apache once to avoid unnecessary restarts.
# basic playbook$cat play.yml- name: Variables playbook                                                        hosts: all                                                                      tasks:                                                                          - name: Install the installed of package &quot;postfix&quot;                              yum:                                                                              name: &quot;postfix&quot;                                                                 state: installed# with variable$cat play.yml- name: Variables playbook                                                        hosts: all                                                                      vars:                                                                               state: installed                                                                user: bob                                                                   tasks:                                                                          - name: Add the user &#123;&#123; user &#125;&#125;                                                   ansible.builtin.user:                                                             name: &quot;&#123;&#123; user &#125;&#125;&quot;                                                          - name: Install the &#123;&#123; state &#125;&#125; of package &quot;postfix&quot;                              yum:                                                                              name: &quot;postfix&quot;                                                                 state: &quot;&#123;&#123; state &#125;&#125;&quot;    # with handler$cat play.yml- name: Verify apache installation  hosts: webservers  vars:    http_port: 80    max_clients: 200  remote_user: root  tasks:    - name: Ensure apache is at the latest version      ansible.builtin.yum:        name: httpd        state: latest    - name: Write the apache config file      ansible.builtin.template:        src: /srv/httpd.j2        dest: /etc/httpd.conf      notify:      - Restart apache # handler    - name: Ensure apache is running      ansible.builtin.service:        name: httpd        state: started  handlers:    - name: Restart apache # defined handler      ansible.builtin.service:        name: httpd        state: restarted# -v verbose -vv, -vvv, -vvvv$ansible-playbook -i host play.yml  -v################### retry for the failed nodes######### play.retry is auto generated of failed nodes$ansible-playbook -i host --limit @play.retry play.yml

conditional taskRun a task only when condition is matched, condition can be custom variable, ansible built-in variable, or result of another task.
- hosts: webservers  remote_user: root  tasks:  - name: Host 192.168.1.101 run this task    debug: &#x27;msg=&quot; &#123;&#123; ansible_default_ipv4.address &#125;&#125;&quot;&#x27;    when: ansible_default_ipv4.address == &quot;192.168.2.101&quot;  - name: memtotal &lt; 500M and processor_cores == 2 run this task    debug: &#x27;msg=&quot;&#123;&#123; ansible_fqdn &#125;&#125;&quot;&#x27;    when: ansible_memtotal_mb &lt; 500 and ansible_processor_cores == 2  - name: all host run this task    shell: hostname    register: info  - name: Hostname is lamp1 Machie run this task    debug: &#x27;msg=&quot;&#123;&#123; ansible_fqdn &#125;&#125;&quot;&#x27;    when: info[&#x27;stdout&#x27;] == &quot;lamp1&quot;  - name: Hostname is startswith l run this task    debug: &#x27;msg=&quot;&#123;&#123; ansible_fqdn &#125;&#125;&quot;&#x27;    when: info[&#x27;stdout&#x27;].startswith(&#x27;l&#x27;)

For more keyword that’s available in playbook, refer to playbook keywords
Error Handling In PlaybooksIgnoring Failed Commands
Generally playbooks will stop executing any more steps on a host that has a task fail. Sometimes, though, you want to continue on. To do so, write a task that looks like this. This feature only works when the task must be able to run and return a value of ‘failed’, ignore_errors still print error output but continue to run next one. but as even fails, it continues to run next task if next task depends on result of the failed task, the result may not be the object descripted in module doc. as the command may be not run on the node at all. but if no ignore error, the result used in next task is always the one expected as otherwise it does not run if the previous one fails.
- name: this will not be counted as a failure  command: /bin/false  register: result  ignore_errors: yes# output like this  fatal: [10.229.225.6]: FAILED! =&gt; &#123;&quot;censored&quot;: &quot;the output has been hidden due to the fact that &#x27;no_log: true&#x27; was specified for this result&quot;&#125;...ignoring

Handlers and Failure
When a task fails on a host, handlers which were previously notified will not be run on that host. This can lead to cases where an unrelated failure can leave a host in an unexpected state. For example, a task could update a configuration file and notify a handler to restart some service. If a task later on in the same play fails, the service will not be restarted despite the configuration change.
You can change this behavior with the --force-handlers command-line option, or by including force_handlers: True in a play, or force_handlers = True in ansible.cfg. When handlers are forced, they will run when notified even if a task fails on that host.
Controlling What Defines Failure
Ansible lets you define what “failure” means in each task using the failed_when conditional. As with all conditionals in Ansible, lists of multiple failed_when conditions are joined with an implicit and, meaning the task only fails when all conditions are met. If you want to trigger a failure when any of the conditions is met, you must define the conditions in a string with an explicit or operator.
- name: Fail task when the command error output prints FAILED  command: /usr/bin/example-command -x -y -z  register: command_result  no_log: true       # never print result of this task.  failed_when: false # never fail this task, but use command_result in another task- name: Fail task when the command error output prints FAILED  command: /usr/bin/example-command -x -y -z  register: command_result  failed_when: &quot;&#x27;FAILED&#x27; in command_result.stderr&quot;- name: Fail task when both files are identical  raw: diff foo/file1 bar/file2  register: diff_cmd  failed_when: diff_cmd.rc == 0 or diff_cmd.rc &gt;= 2- name: Check if a file exists in temp and fail task if it does  command: ls /tmp/this_should_not_be_here  register: result  failed_when:    - result.rc == 0    - &#x27;&quot;No such&quot; not in result.stdout&#x27;

Overriding The Changed Result
When a shell&#x2F;command or other module runs it will typically report “changed” status based on whether it thinks it affected machine state.
Sometimes you will know, based on the return code or output that it did not make any changes, and wish to override the “changed” result such that it does not appear in report output or does not cause handlers to fire:
tasks:  - shell: /usr/bin/billybass --mode=&quot;take me to the river&quot;    register: bass_result    changed_when: &quot;bass_result.rc != 2&quot;  - command: /bin/fake_command    register: result    ignore_errors: True    changed_when:      - &#x27;&quot;ERROR&quot; in result.stderr&#x27;      - result.rc == 2

debug moduleThis module prints statements during execution and can be useful for debugging variables or expressions without necessarily halting the playbook.
Parameters

msg： The customized message that is printed. If omitted, prints a generic message.
var: A variable name to debug. Mutually exclusive with the msg option. Be aware that this option already runs in Jinja2 context and has an implicit &#123;&#123; &#125;&#125; wrapping, so you should not be using Jinja2 delimiters unless you are looking for double interpolation.
verbosity: A number that controls when the debug is run, if you set to 3 it will only run debug when -vvv or above. Default: 0

- name: Get uptime information  ansible.builtin.shell: /usr/bin/uptime  register: result- name: Print return information from the previous task  ansible.builtin.debug:    var: result    verbosity: 2#########################################- name: Task name  stat:    path: /etc/lt.conf  register: register_name- name: Task name  debug:    msg: &quot;The file or directory exists&quot;  when: register_name.stat.exists

Asynchronous Actions and Polling(task level)Ansible runs tasks synchronously by default, if one task fails, the others does not run anymore. It keeps the connection to the remote node open until the task is completed. This means within a playbook, each task blocks the subsequent tasks until the current task completes. 
Some of the long-running tasks could be

Downloading a Big File from URL
Running a Script known to run for a long time
Rebooting the remote server and waiting for it to comeback

This may cause issue. Suppose you have a task in your playbook which takes more than say 10 minutes to execute. This means that the ssh connection between Ansible controller and the target machine should be stable for more than 10 minutes. It may take longer to complete than the SSH session allows for, causing a timeout. One can run the long-running process to execute in the background to perform other tasks concurrently.
To avoid blocking or timeout issues, you can use asynchronous mode to run all of your tasks at once and then poll until they are done.
To enable Asynchronous mode within Ansible playbook we need to use few parameters such as async, poll.

async - async keyword’s value indicates the total time allowed to complete the task.Once that time is over the task will be marked as completed irrespective of the end result. Along with this async also sends the task in the background which can be verified later on its final execution status.

poll - poll keyword allows us to track the status of the job which was invoked by async and running in the background. Its value decides how frequent it would check if the background task is completed or not.

The Poll keyword is auto-enabled whenever you use async and it has a default value as 10 seconds.

When you use poll parameter’s value set to positive Ansible will avoid connection timeouts but will still block the next task in your playbook, waiting until the async task either completes, fails or times out.




NOTE

async without poll, default poll is 10s, set timeout for the task
async with postive poll, same as above, it just sets timeout for the task
async with poll &#x3D;&#x3D; 0, really async, the task marked finished immediately without waiting for it result!!, but if second task depends on first one(in a node), NOT use async!!!

##############--- set time out for a task ###########################- name: async and poll example playbook  hosts: workers  become: true  remote_user: ansible_user  tasks:    - name: update the system packages      command: yum update -y      async: 180 # the total time allowed to complete the package update task      poll: 10 # Polling Interval in Seconds      register: package_update    - name: task-2 to create a test user # will be blocked until first task finished or timedout!!!      user: name=async_test state=present shell=/bin/bash##############--- set time out and async job for a task ###########################- name: async and poll example playbook  hosts: workers  become: true  remote_user: ansible_user  tasks:    - name: sleep for 60 seconds      command: /bin/sleep 60      async: 80 # the total time allowed to complete the sleep task      poll: 0 # No need to poll just fire and forget the sleep command      register: sleeping_node    - name: task-2 to create a test user # will run even the first task is in progress      user: name=async_test-2 state=present shell=/bin/bash      - name: async and poll example playbook  hosts: workers  become: true  remote_user: ansible_user  tasks:    - name: sleep for 20 seconds      command: /bin/sleep 20      async: 30 # the total time allowed to complete the sleep task      poll: 0 # No need to poll just fire and forget the sleep command      register: sleeping_node    - name: task-2 to create a test user      user: name=async_test-2 state=present shell=/bin/bash    # check the async job status    - name: Checking the Job Status running in background      async_status:        jid: &quot;&#123;&#123; sleeping_node.ansible_job_id &#125;&#125;&quot;      register: job_result      until: job_result.finished # Retry within limit until the job status changed to &quot;finished&quot;: 1      retries: 30 # Maximum number of retries to check job status
StrategyWhen running Ansible playbooks, you might have noticed that the Ansible runs every task on each node one by one, it will not move to another task until a particular task is completed on each node, which will take a lot of time, in some cases. By default, the strategy is set to “linear”, we can set it to free.

linear: run the first task of play on all nodes(forks), when the first task finished on all nodes, run the second tasks on all nodes
free:   The nodes who finished the first task, can run the second task without waiting for host who is still running first task. a host that is slow or stuck on a specific task won’t hold up the rest of the hosts and tasks

# play level$cat play.yml- name: free strategy demo  hosts: workers  strategy: free# global setting$cat /etc/ansible/ansible.cfg[defaults]strategy = free

NOTE

if nodes has dependency, use linear, otherwise use free
free only speed up process for play with more than on tasks.

Python API(not frequently used)python api
FQAAnsible playbook hangs during executionRefer to why ansible hangs

SSH timeout
command hangs in remote node

speed up playbooktunning ansible
NameError: name ‘temp_path’ is not definedIn such case, task does not run at all, this is probably there is no disk space on remote node as ansible need to copy module to remote host at /root/.ansible/tmp
change remote_tmpremote_tmp is set by ansible.cfg
$cat /etc/ansible/ansible.cfg...remote_tmp     = ~/.ansible/tmp# or change it from env$ANSIBLE_REMOTE_TEMP=/cedar/.tmp ansible-playbook play.yml -i ./hosts
get verbose outputSomething if gathering is false, it’s hard to see why it fails, in this case, turn it on and run your playbook with -vvv or -vvvv
$cat ansible.cfg[defaults]gathering      = True# OR$cat play.yml- name: perf check  hosts: all  gather_facts: true...# rerun$ansible-playbook -i 10.211.98.106, playbook/check_file.yml -vvv

set python interpreter at remote nodeAs ansible copies python module to remote and runs it at remote node, so that the remote python interpreter should be compatible with the module copied from control node.
# check python on control node where you run ansible to see what python script that is for module$ansible --versionansible [core 2.13.4]  config file = /etc/ansible/ansible.cfg  configured module search path = [&#x27;/root/.ansible/plugins/modules&#x27;, &#x27;/usr/share/ansible/plugins/modules&#x27;]  ansible python module location = /home/data/Anaconda3/envs/py3.9/lib/python3.9/site-packages/ansible -----&gt;ansible modules  ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections  executable location = /home/data/Anaconda3/envs/py3.9/bin/ansible  python version = 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0] -----&gt;ansbile module version  jinja version = 3.1.2  libyaml = True# by default ansible select the remote python interpreter automatically based on its rule# hence it may select a python which is not compatiable, here we can set remote python interpreter explicitly$cat ansible.cfg[defaults]interpreter_python = /usr/bin/python3# OR$cat play.yml- name: a play  hosts: all  gather_facts: no  vars:    ansible_python_interpreter: &#x27;/usr/bin/python3&#x27;

Cannot handle SSH host authenticity prompts for multiple hosts$ ansible-playbook -i conf/app/south.host ./playbook/app/check_file.ymlThe authenticity of host &#x27;10.0.0.4 (10.0.0.4)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:WkPeJUhNdz/MX3zAy536BHZRC/9INGEQWGhsmAPzkEo.Are you sure you want to continue connecting (yes/no)? The authenticity of host &#x27;10.0.0.5 (10.0.0.5)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:UJCEM05W15HZuzOLRpxNli+Qnwei7j84u2lbpVFBqkI.Are you sure you want to continue connecting (yes/no)? 

Solutionedit ansible.cfg with host_key_checking = false
[defaults]host_key_checking = False

forks vs serial vs async
Serial sets a number, a percentage, or a list of numbers of hosts you want to manage at a time.
Async triggers Ansible to run the task in the background which can be checked (or) followed up later, and its value will be the maximum time that Ansible will wait for that particular Job (or) task to complete before it eventually times out or complete.
Ansible works by spinning off forks of itself and talking to many remote systems independently. The forks parameter controls how many hosts are configured by Ansible in parallel.

Suggestion  

SERIAL : Decides the number of nodes process in each tasks in a single run.

Use: When you need to provide changes as batches&#x2F; rolling changes.


FORKS : Maximum number of simultaneous connections Ansible made on each Task.

Use: When you need to manage how many nodes should get affected simultaneously.



serial exampleBy default, with serial set, failing all servers(max fail percentage 100%) from one batch(serial value) will stop whole playbook to run even if there are some servers left in inventory, but this can be tunned with max_fail_percentage.
---- name: test play  hosts: webservers  # serial: 10% or mix these two format  # serial:  #   - 3  #   - 50%  serial: 3  gather_facts: False  tasks:    - name: first task      command: hostname    - name: second task      command: hostname

In the above example, if we had 6 hosts in the group ‘webservers’, Ansible would execute the play completely (both tasks) on 3 of the hosts before moving on to the next 3 hosts:
PLAY [webservers] ****************************************TASK [first task] ****************************************changed: [web3]changed: [web2]changed: [web1]TASK [second task] ***************************************changed: [web1]changed: [web2]changed: [web3]PLAY [webservers] ****************************************TASK [first task] ****************************************changed: [web4]changed: [web5]changed: [web6]TASK [second task] ***************************************changed: [web4]changed: [web5]changed: [web6]PLAY RECAP ***********************************************web1      : ok=2    changed=2    unreachable=0    failed=0web2      : ok=2    changed=2    unreachable=0    failed=0web3      : ok=2    changed=2    unreachable=0    failed=0web4      : ok=2    changed=2    unreachable=0    failed=0web5      : ok=2    changed=2    unreachable=0    failed=0web6      : ok=2    changed=2    unreachable=0    failed=0

without serial---- name: test play  hosts: webservers  gather_facts: False  tasks:    - name: first task      command: hostname    - name: second task      command: hostname

PLAY [webservers] ****************************************TASK [first task] ****************************************changed: [web3]changed: [web2]changed: [web1]changed: [web4]changed: [web5]changed: [web6]TASK [second task] ***************************************changed: [web1]changed: [web2]changed: [web3]changed: [web4]changed: [web5]changed: [web6]PLAY RECAP ***********************************************web1      : ok=2    changed=2    unreachable=0    failed=0web2      : ok=2    changed=2    unreachable=0    failed=0web3      : ok=2    changed=2    unreachable=0    failed=0web4      : ok=2    changed=2    unreachable=0    failed=0web5      : ok=2    changed=2    unreachable=0    failed=0web6      : ok=2    changed=2    unreachable=0    failed=0

with large hosts, forks are decreasing after a whileWith large hosts say 10000+, even set forks with larger number say 64, the ansible creates almost that number, but after a while if you check with ps -ef | grep ansible, the number becomes smaller and smaller with time to solve this 

use serial(in playbook) to split hosts into small batches.
split hosts into several files outside

---- name: test play  hosts: webservers  # serial: 10% or mix these two format  # serial:  #   - 3  #   - 50%  serial: 3  gather_facts: False  tasks:    - name: first task      command: hostname    - name: second task      command: hostname
$ ansible-playbook -i large_host ./playbook/with_serial_enabled.yml# split large host into small ones# 100 lines per small file# -d use number as suffix# host. used as prefix$ split --lines=100 -d large_host host.host.00 host.01$ ansible-playbook -i host.00 ./playbook/without_serial.yml$ ansible-playbook -i host.01 ./playbook/without_serial.yml

compare stdout with numberAs stdout is a string, like &quot;45&quot;, you have to convert it to int to compare it with nubmer
- name: check process fd  hosts: all  tasks:    - name: get fd count of agent      shell: lsof -p $(pidof agent) | wc -l      register: ret      # | to convert string to int      failed_when: ret.stdout | int &gt; 100

Ref
ansible tutorial
ansible quick start
inventory examples
ansible configuration
advanced playbook jinjia template
All Modules Index
top 100 modules
handler examples
operator in ansible
conditionals
mitogen to speedup ansible
playbook keywords
install and upgrade ansible
mitogen for ansible

]]></content>
  </entry>
  <entry>
    <title>linux_kernel_vxlan</title>
    <url>/2021/06/22/linux-kernel-vxlan/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>linux-network-performance</title>
    <url>/2022/05/20/linux-network-performance/</url>
    <content><![CDATA[OverviewsocketThere are two popular tools for inspecting the socket states (netstat and ss)
netstat gets its information from /proc/net directly. It parses the file and prints out information based on it.
ss was written more recently to use the netlink API (it will fall back to proc&#x2F;net if netlink is unavailable). The information in both systems is essentially the same, but netlink API exposes more information than procfs. so try to use ss for socket stats if it’s available.
NOTE: netstat provides other info except socket statistics, like routing table etc.


ss commandThe ss command shows socket information, pretty much like netstat does. but use netlink API, with more details, ss always shows socket with Local address and Remote address even for unix socket!!!
options  

–n, –numeric don’t resolve service names
-r, –resolve : resolve host hostnames.
-l, –listening display listening sockets
-o, –options show timer information
-e, –extended show detailed socket information
-m, –memory show socket memory usage
-p, –processes show process using socket
–s, –summary show socket usage summary
-N, –net switch to the specified network namespace name
-t, –tcp display only TCP sockets
-u, –udp display only UDP sockets
-w, –raw display only RAW sockets
-x, –unix display only Unix domain sockets

# Show all listing tcp sockets including the corresponding process$ ss -tlp# check unix socket# Local Address:Port    /var/lib/openvswitch/port-d3f1ehosfc -565918981# Peer Address:Port     * -564914601 $ ss -xp | grep port-d3f1ehosfcNetid  State      Recv-Q Send-Q Local Address:Port                 Peer Address:Portu_str  ESTAB      0      0      /var/lib/openvswitch/port-d3f1ehosfc -565918981            * -564914601            users:((&quot;qemu-kvm&quot;,pid=26495,fd=24))# get its peer$ ss -xp | grep 564914601u_str  ESTAB      0      0       * -564914601            * -565918981            users:((&quot;ovs-vswitchd&quot;,pid=85046,fd=187))u_str  ESTAB      0      0      /var/lib/openvswitch/port-d3f1ehosfc -565918981            * -564914601            users:((&quot;qemu-kvm&quot;,pid=26495,fd=24))##################### Filter ################################## Show all sockets connecting to 192.168.1.10 on port 443# ss [option] dst [IP Address]$ ss -t dst 192.168.1.10:443# Show all ssh related connection# ss [option] [state] [name of the socet state]# ss [option] dport = :[port number]# ss [option] sport = :[port number]# ss [option] &#x27;( dport = :[port number] or sport = :[port number] )&#x27;$ ss -lt sport = :22$ ss -t state established &#x27;( dport = :ssh or sport = :ssh )&#x27;##################### Filter ################################## memory and backlog size$ ss -ltm

lsofThe lsof utility shows all the currently active file handles(open file) on the system.
# which process(es) open this file$ lsof /some/file# total sockets opened by a process$ lsof -p &lt;PID&gt;# show number ports opened by a process# -P to show port number instead of its common name$ lsof -p &lt;PID&gt; -P# total sockets opened by a givn user$ lsof -u &lt;USERNAME&gt;# other useful command like # who opens port 80# who opens tcp/udp socket$ lsof -i :80# show host and port with ip not name# show all open tcp ports or udp ports$ lsof -i tcp -P$ lsof -i udp -P

Ref
network parameters
receiving data from kernel to user
sending data from user to kernel
network low latency guide
more ss command examples
netstat and ss usage

]]></content>
      <categories>
        <category>linux</category>
        <category>network</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>performance</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-numa</title>
    <url>/2022/04/27/linux-numa/</url>
    <content><![CDATA[OverviewThe NUMA-aware architecture is a hardware design which separates its cores into multiple clusters where each cluster has its own local memory region and still allows cores from one cluster to access all memory in the system. However, if a processor needs to use memory that is not its own memory region, it will take longer to access that (remote) memory. For applications where performance is crucial, preventing the need to access memory from other clusters is critical.

A socket refers to the physical location where a processor package plugs into a motherboard. The processor that plugs into the motherboard is also known as a socket.
A core is an individual execution unit within a processor that can independently execute a software execution thread and maintains its execution state separate from the execution state of any other cores within a processor.
A thread refers to a hardware-based thread execution capability. For example, the Intel Xeon 7560 has eight cores, each of which has hardware that can effectively execute two software execution threads simultaneously, yielding 16 threads.

$ lscpu Architecture:                    aarch64CPU op-mode(s):                  32-bit, 64-bitByte Order:                      Little EndianCPU(s):                          128On-line CPU(s) list:             0-127Thread(s) per core:              1Core(s) per socket:              128Socket(s):                       1NUMA node(s):                    2Vendor ID:                       ARM...




From the hardware perspective, a NUMA system is a computer platform that comprises multiple components or assemblies each of which may contain 0 or more CPUs, local memory, and/or IO buses.  we’ll call the components&#x2F;assemblies ‘cells’ as well.
The cells of the NUMA system are connected together with some sort of system interconnect–e.g., a crossbar or point-to-point link are common types of NUMA system interconnects. Both of these types of interconnects can be aggregated to create NUMA platforms with cells at multiple distances from other cells.
Memory access time and effective memory bandwidth varies depending on how far away the cell containing the CPU or IO bus making the memory access is from the cell containing the target memory.
Linux divides the system’s hardware resources into multiple software abstractions called “nodes”. Linux maps the nodes onto the physical cells of the hardware platform, abstracting away some of the details for some architectures. As with physical cells, software nodes may contain 0 or more CPUs, memory and&#x2F;or IO buses. And, again, memory accesses to memory on “closer” nodes–nodes that map to closer cells–will generally experience faster access times and higher effective bandwidth than accesses to more remote cells.
For each node with memory, Linux constructs an independent memory management subsystem, complete with its own free page lists, in-use page lists, usage statistics and locks to mediate access. In addition, Linux constructs for each memory zone [one or more of DMA, DMA32, NORMAL, HIGH_MEMORY, MOVABLE], an ordered “zonelist”. A zonelist specifies the zones&#x2F;nodes to visit when a selected zone&#x2F;node cannot satisfy the allocation request. This situation, when a zone has no available memory to satisfy a request, is called “overflow” or “fallback”.
By default, Linux will attempt to satisfy memory allocation requests from the node to which the CPU that executes the request is assigned. Specifically, Linux will attempt to allocate from the first node in the appropriate zonelist for the node where the request originates. This is called “local allocation.” If the “local” node cannot satisfy the request, the kernel will examine other nodes’ zones in the selected zonelist looking for the first zone in the list that can satisfy the request.
NUMACPUeach CPU is assigned its own local memory and can access memory from other CPUs in the system.each processor contains many cores with a shared on-chip cache and an off-chip memory and has variable memory access costs across different parts of the memory within a server
MemoryIn Non-Uniform Memory Access (NUMA), system memory is divided into zones (called nodes), which are allocated to particular CPUs or sockets. Access to memory that is local to a CPU is faster than memory connected to remote CPUs on that system. 
Memory allocation policies defines for Numa system

Default(local allocation): This mode specifies that any nondefault thread memory policy be removed, so that the memory policy “falls back” to the system default policy.  The system default policy is “local allocation”—that is, allocate memory on the node of the CPU that triggered the allocation.  nodemask must be specified as NULL.  If the &quot;local node&quot; contains no free memory, the system will attempt to allocate memory from a &quot;near by&quot; node.

Bind: This mode defines a strict policy that restricts memory allocation to the nodes specified in nodemask. If nodemask specifies more than one node, page allocations will come from the node with the lowest numeric node ID first, until that node contains no free memory. Allocations will then come from the node with the next highest node ID specified in nodemask and so forth, until none of the specified nodes contain free memory.  Pages will not be allocated from any node not specified in the nodemask.

Interleave: This mode interleaves page allocations across the nodes specified in nodemask in numeric node ID order.  This optimizes for bandwidth instead of latency by spreading out pages and memory accesses to those pages across multiple nodes.  However, accesses to a single page will still be limited to the memory bandwidth of a single node.

Preferred: This mode sets the preferred node for allocation. The kernel will try to allocate pages from this node first and fall back to “near by” nodes if the preferred node is low on free memory. If nodemask specifies more than one node ID, the first node in the mask will be selected as the preferred node.  If the nodemask and maxnode arguments specify the empty set, then the policy specifies “local allocation” (like the system default policy discussed above).


DebugTasksetThe taskset command is considered the most portable Linux way of setting or retrieving the CPU affinity (binding) of a running process (thread). it only sets cpu affinity of running process, not touch memory allocation.
# start process on given cpu$ taskset -c 0 ./app# change process to run on given cpu$ taskset -p -c 0 $pid$ taskset -c 0 -p $pid # error!!!$ taskset -p -c 0,2 $pid# get process affinity$ taskset -cp $pid$ taskset -p $pid

numactlnumactl can be used to control the NUMA policy for processes, shared memory, or both. One key thing about numactl is that, unlike taskset, you can’t use it to change the policy of a running application.
$ yum install -y numactl# show nodes topology and free/total numa node memory$ numactl -Havailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30node 0 size: 65442 MBnode 0 free: 1903 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31node 1 size: 65536 MBnode 1 free: 17423 MBnode distances:node   0   1   0:  10  21   1:  21  10 # show policy$ numactl --showpolicy: defaultpreferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 cpubind: 0 1 nodebind: 0 1 membind: 0 1 


--interleave=&lt;nodes&gt; policy has the application allocate memory in a round-robin fashion on “nodes.” With only two NUMA nodes, this means memory will be allocated first on node 0, followed by node 1, node 0, node 1, and so on. If the memory allocation cannot work on the current interleave target node (node x), it falls back to other nodes, but in the same round-robin fashion. You can control which nodes are used for memory interleaving or use them all

--interleave=0,1 or --interleave=all


--membind=&lt;nodes&gt; policy forces memory to be allocated from the list of provided nodes

--membind=0 or --menbind=all


--preferred=&lt;node&gt; policy causes memory allocation on the node you specify, but if it can’t, it will fall back to using memory from other nodes.

--preferred=1


--localalloc policy forces allocation of memory on the current node

--localalloc


--cpunodebind=&lt;nodes&gt; option causes processes to run only on the CPUs of the specified node(s)

--cpunodebind=0


--physcpubind=&lt;CPUs&gt; policy executes the process(es) on the list of CPUs provided

--physcpubind=+0-4,8-12



numastatThe numastat tool is provided by the numactl package, and displays memory statistics (such as allocation hits and misses) for processes and the operating system on a per-NUMA-node basis. The default tracking categories for the numastat command are outlined as follows: 

numa_hit

The number of pages that were successfully allocated to this node. 


numa_miss

The number of pages that were allocated on this node because of low memory on the intended node. Each numa_miss event has a corresponding numa_foreign event on another node. 


numa_foreign

The number of pages initially intended for this node that were allocated to another node instead. Each numa_foreign event has a corresponding numa_miss event on another node. 


interleave_hit

The number of interleave policy pages successfully allocated to this node. 


local_node

The number of pages successfully allocated on this node, by a process on this node.


other_node

The number of pages allocated on this node, by a process on another node.



numa_miss vs numa_foreign

If a process running on Node 0 requests memory from Node 1 (its preferred node), but the allocation is completed on Node 0, the numa_miss counter for Node 0 will increase, request memory from A but allocated on B, this is miss!
If a process running on Node 0 requests memory from Local Node 0, but the allocation is completed on Node 1, the numa_foreign counter for Node 0 will increase, request memory from Local but allocated on remote, this is foreign!

numa_foreign vs other_node

numa_foreign is about allocation preference (where memory was intended to be allocated)
other_node is about process locality (where the process is running)
Different perspective!!!

Options

-m Displays system-wide memory usage information on a per-node basis, similar to the information found in &#x2F;proc&#x2F;meminfo
-p pattern|PID Displays per-node memory information for the specified pattern. If the value for pattern is comprised of digits, numastat assumes that it is a numerical process identifier
-s Sorts the displayed data in descending order so that the biggest memory consumers (according to the total column) are listed first
-v Displays more verbose information. Namely, process information for multiple processes will display detailed information for each process

# system wide view of each numa node$ numastat -m# more info about system wide view$ numastat -mvPer-node system memory usage (in MBs):                          Node 0          Node 1           Total                 --------------- --------------- ---------------MemTotal                65442.34        65536.00       130978.34MemFree                  2073.16        17484.08        19557.24MemUsed                 63369.18        48051.92       111421.10Active                  28696.10        29089.88        57785.98Inactive                31393.39        15842.16        47235.55Active(anon)             1541.27         1925.21         3466.48Inactive(anon)           9547.17         1200.94        10748.11Active(file)            27154.82        27164.67        54319.50Inactive(file)          21846.21        14641.22        36487.43Unevictable                 0.00            0.00            0.00Mlocked                     0.00            0.00            0.00Dirty                       0.13            0.02            0.15Writeback                   0.00            0.00            0.00FilePages               58718.86        43096.30       101815.16Mapped                    101.45          222.64          324.10AnonPages                1370.99         1835.88         3206.87Shmem                    9712.72         1288.01        11000.73KernelStack                 9.66            8.83           18.48PageTables                 18.27           18.41           36.68NFS_Unstable                0.00            0.00            0.00Bounce                      0.00            0.00            0.00WritebackTmp                0.00            0.00            0.00Slab                     1441.85         1498.11         2939.95SReclaimable             1312.22         1387.39         2699.61SUnreclaim                129.62          110.72          240.34AnonHugePages             140.00         1238.00         1378.00HugePages_Total             0.00            0.00            0.00HugePages_Free              0.00            0.00            0.00HugePages_Surp              0.00            0.00            0.00# system wide view in MB unit$ numastat -cPer-node numastat info (in MBs):                    Node 0     Node 1       Total                ---------- ---------- -----------Numa_Hit        6889434917 6552564429 13441999346Numa_Miss         19506824   18047982    37554806Numa_Foreign      18047982   19506824    37554806Interleave_Hit         232        230         462Local_Node      6889391241 6552564755 13441955995Other_Node        19550500   18047656    37598156# show numa memory used by process who has command qemu-kvm$ numastat -p qemu-kvm$ numastat -p $pid# sort by total$ numastat -s -p qemu-kvmPer-node process memory usage (in MBs)PID                         Node 0          Node 1           Total-----------------  --------------- --------------- ---------------116345 (qemu-kvm)            73.78           47.09          120.8737545 (qemu-kvm)              3.90          106.01          109.91117212 (qemu-kvm)            74.95           16.20           91.15114870 (qemu-kvm)            74.74            8.19           82.9320080 (qemu-kvm)              5.45           76.87           82.32134180 (qemu-kvm)            59.51           22.23           81.73131889 (qemu-kvm)            76.20            4.18           80.3850070 (qemu-kvm)             50.39           26.17           76.5760596 (qemu-kvm)             22.01           50.14           72.1416097 (qemu-kvm)             68.00            4.11           72.12131511 (qemu-kvm)            44.75           26.11           70.87-----------------  --------------- --------------- ---------------Total                       553.68          387.30          940.98# more detail about of each process# numastat  -v -p qemu-kvmPer-node process memory usage (in MBs) for PID 16097 (qemu-kvm)                           Node 0          Node 1           Total                  --------------- --------------- ---------------Huge                         4.00            0.00            4.00Heap                        45.83            0.00           45.83Stack                        0.04            0.00            0.04Private                     18.14            4.11           22.25----------------  --------------- --------------- ---------------Total                       68.00            4.11           72.12Per-node process memory usage (in MBs) for PID 20080 (qemu-kvm)                           Node 0          Node 1           Total                  --------------- --------------- ---------------Huge                         0.00            4.00            4.00Heap                         2.00           54.64           56.64Stack                        0.00            0.04            0.04Private                      3.44           18.20           21.64----------------  --------------- --------------- ---------------Total                        5.45           76.88           82.32...# list all memory used by a process# numa bind policy for huage page, default policy for others$ cat /proc/82389/numa_maps7f069ab9a000 default file=/usr/lib64/ld-2.17.so anon=1 dirty=1 N0=1 kernelpagesize_kB=47f0588000000 bind:0-1 file=/mnt/huge_2MB/libvirt/qemu/141-i-f2w3m8owht/qemu_back_mem._objects_ram-node0.2IyOfZ\040(deleted) huge dirty=2048 mapmax=4 N0=2048 kernelpagesize_kB=2048# the first column is memory address# the second column is numa node used by this memory#   default means all numa nodes#   bind:0-1 means only numa node 0 and node 1 are allowed for this memory, but we may only used one node.# the third column is file related# the last column is kernel page for this memory# the one before last colum is the page count of this memory on each node# N0=2048 means allocate 2048*kernelpagesize_kB on node 0 $ numastat -p 82389Per-node process memory usage (in MBs) for PID 82389 (qemu-kvm)                           Node 0          Node 1           Total                  --------------- --------------- ---------------Huge                      4096.00            0.00         4096.00Heap                        65.16            0.00           65.16Stack                        0.03            0.00            0.04Private                     32.34            1.21           33.55----------------  --------------- --------------- ---------------Total                     4193.53            1.21         4194.74# total numa stat for all processes(unit: page)$ numastat                            node0           node1numa_hit            271446988685    306962573062numa_miss                 144931     15014353674numa_foreign         15014353674          144931interleave_hit             35352           35604local_node          271445218315    306961782987other_node               1915301     15015143749# huage page on two numa nodes of given process$numastat -p 1144Per-node process memory usage (in MBs) for PID 1144 (qemu-kvm)                           Node 0          Node 1           Total                  --------------- --------------- ---------------Huge                      1752.00         6440.00         8192.00Heap                         1.04           44.79           45.82Stack                        0.00            0.04            0.04Private                     16.66           11.42           28.08----------------  --------------- --------------- ---------------Total                     1769.70         6496.25         8265.95# as you can see N0(node 0) 876*2M and N1(node 1) 3220*2M# numa bind policy$ cat /proc/1144/numa_maps | grep 20487fbd58000000 bind:0-1 file=/mnt/huge_2MB/libvirt/qemu/38-i-y4s8f9zg2o/qemu_back_mem._objects_ram-node0.dlp9WP\040(deleted) huge dirty=4096 mapmax=4 N0=876 N1=3220 kernelpagesize_kB=2048# numa prefer policy for huge page# prefer node 1 but huage page is allocated at node0(N0) 12G and node1(N1) 20G$ cat /proc/140662/numa_maps | grep 1G7f5480000000 prefer:1 file=/mnt/huge_1GB/qemu_back_mem._objects_ram-node0.8Jxnog\040(deleted) huge anon=32 dirty=32 N0=12 N1=20 kernelpagesize_kB=1048576

numadnumad is an automatic NUMA affinity management daemon. It monitors NUMA topology and resource usage within a system in order to dynamically improve NUMA resource allocation and management., it scans all processes of the system within period of time.
Note that when numad is enabled, its behavior overrides the default behavior of automatic NUMA balancing(scheduler)
$ yum install numad$ service nuamd start# log file$ ls /var/log/numad.log/var/log/numad.log$ cat /etc/numad.conf # Config file for numad## Default INTERVAL is 15# modify below to change itINTERVAL=15
lstopolstopo and lstopo-no-graphics are capable of displaying a topological map of the system in a variety of different output formats.  The only difference between lstopo and lstopo-no-graphics is that graphical outputs are only supported by lstopo, to reduce dependencies on external libraries. hwloc-ls is identical to lstopo-no-graphics.
$ lstopo-no-graphics$ lstopo-no-graphics -.ascii$ lstopo topo.png

Ref
numa introduction
numa memory policy
cpu cache hierarchy

]]></content>
      <categories>
        <category>linux</category>
        <category>numa</category>
      </categories>
      <tags>
        <tag>numa</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-performance-faq</title>
    <url>/2022/10/19/linux-performance-faq/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>linux-performance-perf</title>
    <url>/2023/02/22/linux-performance-perf/</url>
    <content><![CDATA[OverviewPerf can do lots of thing, like collect, cache miss, context switch, per-thread, per-cpu etc But it needs kernel supporting, perf is always used for system performace debugging, gperftools for application performance debugging(perf also can take application performance debugging)
Usage    #perf    usage: perf [--version] [--help] COMMAND [ARGS]    The most commonly used perf commands are:     annotate        Read perf.data (created by perf record) and display annotated code     diff            Read two perf.data files and display the differential profile     list            List all symbolic event types     probe           Define new dynamic tracepoints     record          Run a command and record its profile into perf.data     report          Read perf.data (created by perf record) and display the profile     script          Read perf.data (created by perf record) and display trace output     stat            Run a command and gather performance counter statistics     top             System profiling tool.    details about each command    #perf annotate --help    Summary: perf has two ACTIONS &#x27;stat&#x27; and &#x27;record&#x27; for profiling    &#x27;stat&#x27; uses counter while &#x27;record&#x27; uses samples( xx samples/per second)    which can show call graph(stacks)    to collect information about the system, cpu, thread, call graph    As EVENT is the core part of perf that it can monitor, EVENT includes        Hardware event like (cach misses, LL2 cache etc)        Software event like (page fault, context switch etc)        tracepoint predefined in kernel(requires kernel compiled with debugfs)        Dynamic event (create by #perf probe --add )



event overview
# run a command under perf stat$ perf stat [-e &lt;EVENT&gt; | --event=EVENT] [-a] &lt;command&gt;$ perf stat [-e &lt;EVENT&gt; | --event=EVENT] [-a] — &lt;command&gt; [&lt;options&gt;]$ perf stat [-e &lt;EVENT&gt; | --event=EVENT] [-a] record [-o file] — &lt;command&gt; [&lt;options&gt;]# OR attach to a running process$ perf stat [-e &lt;EVENT&gt; | --event=EVENT] [-a] -p $pidimportant options-i: by default perf stat without -i option count all processes, forked (and threads cloned) from target process after starting of counting. With perf stat ./program it will profile all child processes too, and with perf stat -p $PID with attaching to already running $PID it will profile target process and all child processes started after attaching. use -i to disable counting child.# ================= remote cache and remote dram access for multiple numa =======================================#  offcore_response.all_data_rd.llc_miss.remote_dram #       [Counts all demand &amp; prefetch data reads that miss the L3 and the data is returned from remote dram]#  offcore_response.all_data_rd.llc_miss.remote_hit_forward#       [Counts all demand &amp; prefetch data reads that miss the L3 and clean or shared data is transferred from remote cache]#  offcore_response.all_data_rd.llc_miss.remote_hitm #       [Counts all demand &amp; prefetch data reads that miss the L3 and the modified data is transferred from remote cache]#  offcore_response.all_reads.llc_miss.remote_dram   #       [Counts all data/code/rfo reads (demand &amp; prefetch) that miss the L3 and the data is returned from remote dram]#  offcore_response.all_reads.llc_miss.remote_hit_forward#       [Counts all data/code/rfo reads (demand &amp; prefetch) that miss the L3 and clean or shared data is transferred from remote cache]#  offcore_response.all_reads.llc_miss.remote_hitm   #       [Counts all data/code/rfo reads (demand &amp; prefetch) that miss the L3 and the modified data is transferred from remote cache]#  offcore_response.demand_rfo.llc_miss.remote_hitm  #       [Counts all demand data writes (RFOs) that miss the L3 and the modified data is transferred from remote cache]# ================= remote cache and remote dram access for multiple numa =======================================# splitlock$ perf stat -e cpu/event=0xf4,umask=0x10/ -a -I 1000 ===$ perf stat -e sq_misc.split_lock -a -I 1000# run a application that accesss automic memory cross cache-line, henc generate splitlock$ perf stat -e sq_misc.split_lock -a -I 100 ./sp#           time             counts unit events     0.108722215             36,456      sq_misc.split_lock                                               0.214217317             37,401      sq_misc.split_lock                                               0.318182137             37,700      sq_misc.split_lock           # total last level cache$ lscpu | grep &#x27;L3 cache&#x27;L3 cache:              40960K# intel cqm need hardware and kernel support$ cat /proc/cpuinfo | grep rdt_a$ grep  &#x27;RDT_A&#x27; /boot/config-$(uname -r)$ perf stat -a -e intel_cqm/llc_occupancy/ -I 1000 -p $pid$ perf stat -a -e intel_cqm/total_bytes/ -I 1000 -p $pid$ perf stat -a -e intel_cqm/local_bytes/ -I 1000 -p $pid# last level cache occupancy$ perf stat -a -e intel_cqm/llc_occupancy/ -I 1000 sleep 100#           time             counts  unit events     1.004945555         35,061,760 Bytes intel_cqm/llc_occupancy/                                         2.007805514         46,989,312 Bytes intel_cqm/llc_occupancy/                                    $ perf stat -a -e intel_cqm/llc_occupancy/ -I 1000 ./sp#           time             counts  unit events     1.011122701         31,391,744 Bytes intel_cqm/llc_occupancy/                                         2.018970467         42,401,792 Bytes intel_cqm/llc_occupancy/                                    $ perf stat -a -e intel_cqm/total_bytes/ -I 1000 sleep 30#           time             counts unit events     1.007463857               0.00 MB   intel_cqm/total_bytes/                                           2.011665924               0.00 MB   intel_cqm/total_bytes/                       $ perf stat -a -e intel_cqm/local_bytes/ -I 1000 sleep 30#           time             counts unit events     1.008000842               0.00 MB   intel_cqm/local_bytes/                                           2.012502066               0.00 MB   intel_cqm/local_bytes/  # -----For memory bandwith on new kernel(&gt;=4.14), use this way----------# this need cpu and kernel supports$ cat /proc/cpuinfo | grep rdt_a$ grep  &#x27;RESCTRL&#x27; /boot/config-$(uname -r)# Mount the resctrl filesystem.$ mount -t resctrl resctrl -o mba_MBps /sys/fs/resctrl# after mount check$ ls -p /sys/fs/resctrl/cpus  cpus_list  info/  mode  mon_data/  mon_groups/  schemata  size  tasks# ======================host level=====================# Print the number of llc occupancy on the first socket.$ cat /sys/fs/resctrl/mon_data/mon_L3_00/llc_occupancy# Print the number of local bytes on the first socket.$ cat /sys/fs/resctrl/mon_data/mon_L3_00/mbm_local_bytes# Print the number of total bytes on the first socket.$ cat /sys/fs/resctrl/mon_data/mon_L3_00/mbm_total_bytes# Print the number of llc occupancy on the second socket.$ cat /sys/fs/resctrl/mon_data/mon_L3_01/llc_occupancy# Print the number of local bytes on the second socket.$ cat /sys/fs/resctrl/mon_data/mon_L3_01/mbm_local_bytes# Print the number of total bytes on the second socket.$ cat /sys/fs/resctrl/mon_data/mon_L3_01/mbm_total_bytes# ======================host level=====================# ====================== group level=====================# group level mean collect mbm data of all task in this group$ mkdir /sys/fs/resctrl/mon_groups/test$ ls /sys/fs/resctrl/mon_groups/testcpus  cpus_list  mon_data/  tasks$ ls /sys/fs/resctrl/mon_groups/test/mon_data/mon_L3_00llc_occupancy  mbm_local_bytes  mbm_total_bytes# mbm data of this group$ ls /sys/fs/resctrl/mon_groups/test/mon_data/mon_L3_00/  mon_L3_01/# then add task(process to this group)$ echo 245543 /sys/fs/resctrl/mon_groups/test/tasks# ====================== group level=====================# -----For memory bandwith on new kernel(&gt;=4.14), use this way----------

Usage# command help# List all symbolic event types, event has another format: Raw which is number!!!$ perf list [hw|sw|cache|tracepoint|pmu|event_glob]- &#x27;hw&#x27; or &#x27;hardware&#x27; to list hardware events such as cache-misses, etc.- &#x27;sw&#x27; or &#x27;software&#x27; to list software events such as context switches, etc.- &#x27;cache&#x27; or &#x27;hwcache&#x27; to list hardware cache events such as L1-dcache-loads, etc.- &#x27;tracepoint&#x27; to list all tracepoint events, alternatively use subsys_glob:event_glob to filter by tracepoint subsystems such as sched, block, etc- &#x27;pmu&#x27; to print the kernel supplied PMU events.- As a last resort, it will do a substring search in all event names# list the available PMUs$ ls /sys/devices/*/events//sys/devices/cpu/events/:branch-instructions  bus-cycles    cache-references  cycles-ct  el-abort     el-commit    el-start      mem-loads   ref-cycles  tx-capacity  tx-conflictbranch-misses        cache-misses  cpu-cycles        cycles-t   el-capacity  el-conflict  instructions  mem-stores  tx-abort    tx-commit    tx-start/sys/devices/intel_cqm/events/:llc_occupancy          llc_occupancy.scale     llc_occupancy.unit  local_bytes.per-pkg  local_bytes.unit  total_bytes.per-pkg  total_bytes.unitllc_occupancy.per-pkg  llc_occupancy.snapshot  local_bytes         local_bytes.scale    total_bytes       total_bytes.scale/sys/devices/power/events/:energy-cores  energy-cores.scale  energy-cores.unit  energy-gpu  energy-gpu.scale  energy-gpu.unit  energy-pkg  energy-pkg.scale  energy-pkg.unit  energy-ram  energy-ram.scale  energy-ram.unit/sys/devices/uncore_imc_0/events/:cas_count_read  cas_count_read.scale  cas_count_read.unit  cas_count_write  cas_count_write.scale  cas_count_write.unit  clockticks/sys/devices/uncore_imc_1/events/:cas_count_read  cas_count_read.scale  cas_count_read.unit  cas_count_write  cas_count_write.scale  cas_count_write.unit  clockticks/sys/devices/uncore_imc_4/events/:cas_count_read  cas_count_read.scale  cas_count_read.unit  cas_count_write  cas_count_write.scale  cas_count_write.unit  clockticks/sys/devices/uncore_imc_5/events/:cas_count_read  cas_count_read.scale  cas_count_read.unit  cas_count_write  cas_count_write.scale  cas_count_write.unit  clockticks# check Raw of one PMU event$ cat /sys/devices/cpu/events/cache-misses event=0x2e,umask=0x41# L1 cache$ perf stat -e L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores sleep 1# L3 cache$ perf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches sleep 1$ perf stat -e cache-misses sleep 1equal to$ perf stat -e cpu/event=0x2e,umask=0x41/ sleep 1

perf for application performanceEven perf can check application performance, but it does this from system view for that process, collect event, stats for that process, and for call graph, kernel symbol, kernel function is counted as well, not like gperf which does this with user function only.
Suggestionif you want to debug performance issue of application code, gperf is good choice, if you want to know the whole path user level and kernel level of this application, use perf.
User-level application performance  
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void func1() &#123;    int i = 0;    while (i &lt; 100000) &#123;        ++i;    &#125;&#125;void func2() &#123;    int i = 0;    while (i &lt; 200000) &#123;        ++i;    &#125;&#125;void func3() &#123;    int i = 0;    for (; i &lt; 1000; ++i) &#123;        func1();        func2();    &#125;&#125;int main()&#123;    func3();    return 0;&#125;

$ gcc -o test test.c -g -O0$ perf record ./test$ perf report    # To display the perf.data header info, please use --header/--header-only options.    #    # Samples: 2K of event &#x27;cpu-clock&#x27;    # Event count (approx.): 723750000    #    # Overhead  Command      Shared Object                      Symbol    # ........  .......  .................  ..........................    #        66.22%     test  test               [.] func2        33.75%     test  test               [.] func1         0.03%     test  [kernel.kallsyms]  [k] get_page_from_freelist    #    # (For a higher level overview, try: perf report --sort comm,dso)    #$ perf annotate -s func2     Percent |      Source code &amp; Disassembly of test    --------------------------------------------------             :             :             :             :      Disassembly of section .text:             :             :      0000000000400509 &lt;func2&gt;:             :          int i = 0;             :          while (i &lt; 100000) &#123;             :              ++i;             :          &#125;             :      &#125;             :      void func2() &#123;        0.00 :        400509:       push   %rbp        0.00 :        40050a:       mov    %rsp,%rbp             :          int i = 0;        0.00 :        40050d:       movl   $0x0,-0x4(%rbp)             :          while (i &lt; 200000) &#123;        0.05 :        400514:       jmp    40051a &lt;func2+0x11&gt;             :              ++i;       17.48 :        400516:       addl   $0x1,-0x4(%rbp)             :              ++i;             :          &#125;             :      &#125;             :      void func2() &#123;             :          int i = 0;             :          while (i &lt; 200000) &#123;        1.88 :        40051a:       cmpl   $0x30d3f,-0x4(%rbp)       80.59 :        400521:       jle    400516 &lt;func2+0xd&gt;             :              ++i;             :          &#125;             :      &#125;        0.00 :        400523:       pop    %rbp        0.00 :        400524:       retq# Except this, you can monitor system information (event) when running your processes see how many cache miss when executing test$ perf stat -e instructions,cycles,branches,branch-misses,cache-misses ./test#===============attach to a running process===================================# show top cpu usage for a process from system wide view(include kernel symbol)$ sysctl -w kernel.pax.softmode=1$ sysctl -w kernel.kptr_restrict=1$ perf top -p $pid# show cpu counter until ctrl+C$ perf stat -p $pid# show open event for $PID with 5s$ perf stat -e &quot;syscalls:sys_*&quot; -p $PID sleep 10# show open event for $PID until ctrl+C$ perf stat -e &quot;syscalls:sys_*&quot;  -p $PID# recod cpu-clock event for given pid until control + c$ perf record -p $pid# record event for context switch of given pid until control + c$ perf record -e &quot;cs&quot; -p $pid

System-wide performace# show funtion(from different apps or kernel function) which takes much CPU$ perf top# show all EVNETs supported that can be used by &#x27;stat&#x27; or &#x27;record&#x27;$ perf list# show counters in detail of all cpus within 10s$ perf stat -a -d sleep 10# show cpu counter when executing &#x27;ls&#x27; command$ perf stat ls# show open event for all cpu with 5s$ perf stat -e &quot;syscalls:sys_*&quot; -a sleep 5#record all cpus within 10s# (-g graph, call graph)$ perf record -a -g -- sleep 10$ perf record -a  -- sleep 10$ perf record -e &quot;cpu-clock&quot; -a sleep 5$ perf record -e &quot;sched:sched_switch&quot; -a sleep 5# record until ctrl+c$ perf record -e block:block_rq_insert -a$ perf report -n --stdio -i perf.data# (need kernel debug info)$ perf annotate --stdio# add tracepoint at door of tcp_sendmsg$ perf probe --add tcp_sendmsg    Failed to find path of kernel module.    Added new event:        probe:tcp_sendmsg    (on tcp_sendmsg)    You can now use it in all perf tools, such as:        perf record -e probe:tcp_sendmsg -a sleep 10        perf stat -e probe:tcp_sendmsg -a sleep 10$ perf record -e probe:tcp_sendmsg -aR sleep 10# show current probed event$ perf probe -l# delete dynamic trace point$ perf probe -d tcp_sendmsg# show page faults, cache miss of whole system$ perf stat -B -e context-switches,cpu-migrations,cache-references,cache-misses,cycles,instructions,page-faults sleep 5Performance counter stats for &#x27;sleep 5&#x27;:             1      context-switches                                                         1      cpu-migrations                                                      18,143      cache-references                                                       957      cache-misses              #    5.275 % of all cache refs         1,598,875      cycles                                                             667,345      instructions              #    0.42  insn per cycle                    179      page-faults                                                    5.002159188 seconds time elapsed# default output for perf stat$ perf stat -B sleep 5Performance counter stats for &#x27;sleep 5&#x27;:      1.136437      task-clock (msec)         #    0.000 CPUs utilized                       1      context-switches          #    0.880 K/sec                               0      cpu-migrations            #    0.000 K/sec                             179      page-faults               #    0.158 M/sec                       1,612,448      cycles                    #    1.419 GHz                           671,118      instructions              #    0.42  insn per cycle                129,284      branches                  #  113.763 M/sec                           8,996      branch-misses             #    6.96% of all branches           5.002229617 seconds time elapsed# check which function triggered cache-miss or page fault event on system$ perf record -e cache-misses sleep 5$ perf record -e page-faults sleep 5$ perf report# check which function triggered cache-miss or page fault event of given process$ perf record -e cache-misses -p  $pid# perf stat with multiple events$ perf stat -e intel_cqm/local_bytes/,intel_cqm/total_bytes/ -a -I 1000

FAQperf kvm stat vs perf statYou can use the perf command with the kvm option to collect guest operating system statistics from the host, 
In order to use perf kvm record in the host, you must have access to the /proc/modules and /proc/kallsyms files from the guest, otherwise, perf report can NOT show symbol correctly. but these two files is not needed for perf kvm stat as it does not use symbol table
# On host################## perf kvm stat ##########################$ ps -ef | grep 84857root      84857      1 60 10:07 ?        01:02:09 /usr/libexec/qemu-kvm -daemonize -D /tmp/qemu_vm.log -machine pc,accel=kvm,usb=off,dump-guest-core=off -object memory-backend-file,id=ram-node0,prealloc=yes,mem-path=/mnt/huge_1GB,share=no,size=34359738368,host-nodes=0,policy=bind -numa node,nodeid=0,cpus=0-15,memdev=ram-node0 -m 32G -cpu host,host-cache-info=on,l3-cache=off -smp 16,maxcpus=16,sockets=1,cores=16,threads=1 -realtime mlock=off -no-user-config -drive file=./centos7.qcow2 -qmp unix:/var/run/qmp.sock,server,nowait -nodefaults -no-hpet -vga std -vnc 0.0.0.0:0 -serial pty -netdev tap,id=net1,ifname=vnet10,script=./qemu-ifup,downscript=./qemu-ifdown -device virtio-net,netdev=net1,mac=52:54:00:12:34:56 -device qemu-xhci -device usb-tablet# check splitlock from qemu-kvm$ perf kvm stat -e sq_misc.split_lock -p 84857 -a -I 1000 #           time             counts unit events     1.000627884            373,366      sq_misc.split_lock                                            (100.00%)################## perf kvm record ##########################$ scp root@GuestMachine:/tmp/kallsyms guest-kallsyms$ scp root@GuestMachine:/tmp/modules guest-modules$ perf kvm --host --guest --guestkallsyms=guest-kallsyms \--guestmodules=guest-modules record -a -o perf.data$ perf kvm --host --guest --guestmodules=guest-modules report -i perf.data.kvm \--force &gt; analyze

Ref
detect split lock

]]></content>
      <categories>
        <category>performance</category>
        <category>system</category>
      </categories>
      <tags>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-performance-systemtap</title>
    <url>/2022/01/04/linux-performance-systemtap/</url>
    <content><![CDATA[IntroductionSystemTap provides the infrastructure to monitor the running Linux kernel and application for detailed analysis. This can assist administrators and developers in identifying the underlying cause of a bug or performance problem. SystemTap is designed to eliminate this and allows users to gather the kernel information by running user-written SystemTap scripts., you do NOT need to write kernel module, compile it and load it by yourself, you just write systemtap script, then systemtap framework does all other things for you(which actually use kprobe)
For short， add hooks at point event(function enter, function return etc) for running application or kernel, in hooks print or check something.
How it works
First, SystemTap checks the script against the existing tapset library (normally in &#x2F;usr&#x2F;share&#x2F;systemtap&#x2F;tapset&#x2F; for any tapsets used. SystemTap will then substitute any located tapsets with their corresponding definitions in the tapset library.
SystemTap then translates the script to C, running the system C compiler to create a kernel module from it. The tools(stap) that perform this step are contained in the systemtap package
SystemTap loads the module, then enables all the probes (events and handlers) in the script. The staprun in the systemtap-runtime package  provides this functionality.
As the events occur, their corresponding handlers are executed.
Once the SystemTap session is terminated, the probes are disabled, and the kernel module is unloaded.



Setup to use systemtap$ yum install systemtap systemtap-runtime# install kernel debug info, refer to https://sourceware.org/systemtap/SystemTap_Beginners_Guide/using-systemtap.html$ stap-prep# check if systemtap works or not(script from command line)$ stap -v -e &#x27;probe vfs.read &#123;printf(&quot;read performed\n&quot;); exit()&#125;&#x27;Pass 1: parsed user script and 487 library scripts using 299532virt/96108res/3516shr/93332data kb, in 940usr/150sys/1278real ms.Pass 2: analyzed script: 1 probe, 1 function, 7 embeds, 0 globals using 468708virt/260648res/4900shr/262508data kb, in 2750usr/1330sys/4835real ms.Pass 3: translated to C into &quot;/tmp/stapYzHqgZ/stap_cde75c591f0c1e1bb6070ad11276b42b_2771_src.c&quot; using 468708virt/260908res/5160shr/262508data kb, in 10usr/50sys/69real ms. ----&gt; convert to kernel module(c)Pass 4: compiled C into &quot;stap_cde75c591f0c1e1bb6070ad11276b42b_2771.ko&quot; in 9250usr/2530sys/11991real ms.--------------------compile this custom modePass 5: starting run.----------------------&gt;load this module# command to run systemtap# monitor events for any process(script from a file)$ stap -vv -F xx.stp -o output.log# OR# monitor event for this process only # Sets the SystemTap handler function target() to the specified process ID$ stap -vv -F xx.stp -o output.log -x process_id

ProbeProbe &#x3D; event + its handler
The essential idea behind a SystemTap script is to name events, and to give them handlers. When SystemTap runs the script, SystemTap monitors for the event; once the event occurs, the Linux kernel then runs the handler as a quick sub-routine, then resumes.
There are several kind of events: entering/exiting a function, timer expiration, session termination, etc. A handler is a series of script language statements that specify the work to be done whenever the event occurs. This work normally includes extracting data from the event context, storing them into internal variables, and printing results. 
FORMATprobe PROBEPOINT [, PROBEPOINT] &#123; [STMT ...] &#125;
PROBEPOINT supports wildcard match
aliasNew probe points may be defined using aliases. A probe point alias looks similar to probe definitions, but instead of activating a probe at the given point, it defines a new probe point name as an alias to an existing one. New probe aliases may refer to one or more existing probe aliases. Multiple aliases may share the same underlying probe points
FORMATprobe &lt;alias&gt; = &lt;probepoint&gt; &#123; &lt;prologue_stmts&gt; &#125;
probe socket.sendmsg = kernel.function (&quot;sock_sendmsg&quot;) &#123; ... &#125;probe socket.sendmsg &#123;...&#125;probe socket.sendmsg.return &#123;...&#125;
&#x3D;&#x3D;&#x3D;  
probe kernel.function(&quot;sock_sendmsg&quot;) &#123;...&#125;probe kernel.function(&quot;sock_sendmsg&quot;).return &#123;...&#125;


functionSystemTap scripts may define subroutines to factor out common work. Functions may take any number of scalar arguments, and must return a single scalar value. Scalars in this context are integers or strings. For more information on scalars.
FORMAT
function &lt;name&gt;[:&lt;type&gt;] ( &lt;arg1&gt;[:&lt;type&gt;], ... ) &#123; &lt;stmts&gt; &#125;function thisfn (arg1, arg2) &#123;    return arg1 + arg2&#125;function thatfn:string(arg1:long, arg2) &#123;    return sprintf(&quot;%d%s&quot;, arg1, arg2)&#125;
Kernel ProbeProbePoint
# these two are actually alias!!syscall.system_callvfs.file_operationkernel.function(&quot;foo&quot;)kernel.statement(&quot;func@file:linenumber&quot;)kernel.function(&quot;foo&quot;).returnmodule&#123;&quot;ext3&quot;&#125;.function(&quot;ext3_*&quot;)module(&quot;modname&quot;).statement(&quot;func@file:linenumber&quot;)kernel.function(&quot;*@net/socket.c&quot;)timer.ms(5000)

Probe examples
probe kernel.function(&quot;sys_mkdir&quot;).call &#123; log (&quot;enter&quot;) &#125;probe kernel.function(&quot;sys_mkdir&quot;).return &#123; log (&quot;exit&quot;) &#125;

NOTEKernel has prebuilt probe markers This family of probe points connects to static probe markers inserted into the kernel or a module. These markers are special macro calls in the kernel that make probing faster and more reliable than with DWARF-based probes. DWARF debugging information is not required to use probe markers.
User ProbeProbePoint
# PATH can be binary or library!!!process.beginprocess(&quot;PATH&quot;).beginprocess.thread.beginprocess(&quot;PATH&quot;).thread.beginprocess.endprocess(&quot;PATH&quot;).endprocess.thread.endprocess(&quot;PATH&quot;).thread.endprocess.syscallprocess(&quot;PATH&quot;).syscallprocess.syscall.returnprocess(&quot;PATH&quot;).syscall.returnprocess(&quot;PATH&quot;).mark(&quot;LABEL&quot;)process(&quot;PATH&quot;).function(&quot;NAME&quot;)process(&quot;PATH&quot;).statement(&quot;*@FILE.c:123&quot;)process(&quot;PATH&quot;).function(&quot;*&quot;).return

Probe examples
probe process(&quot;/usr/lib64/libvirt/connection-driver/libvirt_driver_qemu.so&quot;).function(&quot;qemuMonitorSend&quot;) &#123;&#125;probe process(&quot;/usr/sbin/libvirtd&quot;).function(&quot;main&quot;) &#123;&#125;

static probingYou can probe symbolic static instrumentation compiled into programs and shared libraries with the following syntax:process(&quot;PATH&quot;).mark(&quot;LABEL&quot;)
Use STAP_PROBE1 for probes with one argument. Use STAP_PROBE2 for probes with 2 arguments, and so on. The arguments of the probe are available in the context variables $arg1, $arg2, and so on.
As an alternative to the STAP_PROBE macros, you can use the dtrace script to create custom macros. The sdt.h file also provides dtrace compatible markers through DTRACE_PROBE and an associated python dtrace script.
NOTE: static probing can probe at any point as it&#39;s programed by user, hence you can pass any args to it, like local var etc, but dynamic probling can only probe and enter and exit of a function
tapsetTapset scripts are libraries of probe aliases and auxiliary functions, refer to tapset function
location: /usr/share/systemtap/tapset/
Frequently used
tid()	The id of the current thread.pid()	The process (task group) id of the current thread.uid()	The id of the current user.execname()	The name of the current process.cpu()	The current cpu number.gettimeofday_s()	Number of seconds since epoch.get_cycles()	Snapshot of hardware cycle counter.pp()	A string describing the probe point being currently handled.ppfunc()	If known, the the function name in which this probe was placed.$$vars	If available, a pretty-printed listing of all local variables in scope.print_backtrace()	If possible, print a kernel backtrace.print_ubacktrace()	If possible, print a user-space backtrace. 


Example of User application with systemtapsystemtap script has similarity with C language, refer to script syntax, flow control, array
if you want to use systemtap for user application, make sure application is built with symbol(-g) or install symbol separately when dtrace is not compiled in
dtrace disableYou must have the symbol table to find the function used in stp.
test.c  
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void hello(char *name) &#123;    printf(&quot;hello %s\n&quot;, name);&#125;int main() &#123;    hello(&quot;Tom&quot;);    return 0;&#125;// gcc -o test test.c -g

test.stp
probe process(&quot;./test&quot;).begin &#123;    printf(&quot;start to probe\n &quot;)&#125;probe process(&quot;./test&quot;).function(&quot;hello&quot;) &#123;    printf(&quot;%s\n&quot;, user_string($name))&#125;probe process(&quot;./test&quot;).end &#123;    printf(&quot;probe end\n&quot;)&#125;

$ stap test.stp# on another terminal$ ./test

dtrace enabledNo need to build with debug info if only use marker as dtrace stores the address of marker when compiling the code, but if you want to use function name, symbol table is required as well.
test.c with markder(dtrace probe)
#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/sdt.h&gt;void hello(char *name) &#123;    printf(&quot;hello %s\n&quot;, name);&#125;int main() &#123;    int age = 12;    char *name = &quot;Tom&quot;;    DTRACE_PROBE2(test, hello_marker, age, name);    hello(name);    return 0;&#125;// gcc -o test test.c

test.stp
probe process(&quot;./test&quot;).begin &#123;    printf(&quot;start to probe\n&quot;)&#125;probe process(&quot;./test&quot;).mark(&quot;hello_marker&quot;) &#123;    // arg1 is int, not pointer, can access it directly    printf(&quot;age: %d name: %s\n&quot;, $arg1, user_string($arg2))&#125;probe process(&quot;./test&quot;).end &#123;    printf(&quot;probe end\n&quot;)&#125;

$ stap test.stp$ ./test


REF
systemtap docs
systemtap tutorial
systemtap beginner guide
systemtap tapset
systemtap docs

]]></content>
      <categories>
        <category>linux</category>
        <category>systemtap</category>
      </categories>
      <tags>
        <tag>systemtap</tag>
        <tag>probe</tag>
      </tags>
  </entry>
  <entry>
    <title>performance tools</title>
    <url>/2019/09/26/linux-performance-tools/</url>
    <content><![CDATA[Performance knowledgeMemory Usage MetricShow process memory usage by top  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND    1 root      20   0   38116   6136   3984 S   0.0  0.0   0:05.71 systemd    VIRT(VSS): The total amount of virtual memory used by the task.               It includes all code, data and shared libraries plus pages that have been swapped out(it not real physical memory current used)    RES(RSS):  The non-swapped physical memory a task has used, CODE+DATA    SHR:       The amount of shared memory used by a task.               It simply reflects memory that could be potentially shared with other processes.    %MEM:      Memory usage (RES)Show process memory usage by smem (metric used by smem and ps)    Swap: Swap size used by each process    VSS(virtual set size)         VSS (reported as VSZ from ps)is the total accessible address space of a process         (all allocated virtual addresses like malloc, stack, map(shared library))         This size also includes memory that may not be resident in RAM like mallocs that have been allocated but not written to.         VSS is of very little use for determining real memory usage of a process.    RSS(resident set size)        RSS is the total memory actually held in RAM for a process. RSS can be misleading,        because it reports the total all of the shared libraries that the process uses,        even though a shared library is only loaded into memory once regardless of how many processes use it.        RSS is not an accurate representation of the memory usage for a single process.    PSS(Proportional set size)        PSS differs from RSS in that it reports the proportional size of its shared libraries,        i.e.if three processes all use a shared library that has 30 pages,        that library will only contribute 10 pages to the PSS that is reported for each of the three processes.        PSS is a very useful number because when the PSS for all processes in the system are summed together,        that is a good representation for the total memory usage in the system.        When a process is killed, the shared libraries that contributed to its PSS will be proportionally distributed to        the PSS totals for the remaining processes still using that library.        In this way PSS can be slightly misleading, because when a process is killed, PSS does not accurately represent the memory returned to the overall system.    USS(unique set size)        USS is the total private memory for a process, i.e. that memory that is completely unique to that process.        USS is an extremely useful number because it indicates the true incremental cost of running a particular process.        When a process is killed, the USS is the total memory that is actually returned to the system.        USS is the best number to watch when initially suspicious of memory leaks in a process.    For example, there are two processes share a library which takes 2M physical memory                VSS     RSS     PSS     USS    process A   20M     18M     17M     16M    process B   20M     19M     18M     17M    (RSS=USS+shared_library_memory, PSS=USS+shared_library_memory/shared_process_count)

load averageThe load average is the average system load on a Linux server for a defined period of time. In other words, it is the CPU demand of a server that includes sum of the running and the waiting threads. on linux, it not only tracks running tasks, but also tasks in uninterruptible sleep (usually waiting for IO)
Measuring the load average is critical to understanding how your servers are performing; if overloaded, you need to kill or optimize the processes consuming high amounts of resources, or provide more resources to balance the workload.
For simple, let&#39;s assume a server with a single processor, if the load is less than 1, that means on average, every process that needed the CPU could use it immediately without being blocked. Conversely, if the load is greater than 1, that means on average, there were processes ready to run, but could not due to CPUs being unavailable.
For a single processor, ideal load average is 1.00, and anything above that is an action call to troubleshoot? Well, although it’s a safe bet, a more proactive approach is leaving some extra headroom to manage unexpected loads, many people tend to aim for a load number of about 0.7 to cater for the spikes
overloaded or not depends on how may cpus(not core) you have
You probably have a system with multiple CPUs. The load average numbers work a bit differently on such a system. For example, if you have a load average of 2 on a single-CPU system, this means your system was overloaded by 100 percent — the entire period of time, one process was using the CPU while one other process was waiting. On a system with two CPUs, this would be complete usage — two different processes were using two different CPUs the entire time. On a system with four CPUs, this would be half usage — two processes were using two CPUs, while two CPUs were sitting idle.


check load average
(py3.9) [root@dev ~]# uptime 14:41:58 up 11 days, 23:10,  3 users,  load average: 1.68, 0.55, 5.91 # These numbers are the averages of the system load over a period of one, five, and 15 minutes


The first value is 1.68. This is the value of CPU load during the last minute. this is a measure of how many programs(process in ready state) were using CPU time during the last minute. So, during the last minute on this machine, there were an average of 1.68 programs either using CPU processing time or waiting for CPU processing time. If this is a single-threaded CPU, that means the computer is overloaded. Users are waiting for their programs to run on the CPU, and experiencing degraded performance. If, instead, this is a dual-core computer or a quad-core, users are able to get CPU time just as quickly as they needed it, during the last minute.

The second value is 0.55. This is the measurement over the last 5 minutes. As we previously discussed, a measurement below 1 means that the CPU spent some of the time in that window completely idle. In this case, the CPU was idle for almost half the time. If we’re optimizing our CPU to be constantly doing something, that’s not a good sign.

The final number, 5.91, is a measurement of the last 15 minutes. If you’re using an eight-core CPU, then this number isn’t particularly shocking. If you’re using a dual-core CPU, then a number like 5.91 means your CPU is very overloaded. Users are regularly waiting for CPU time, and are probably experiencing significantly degraded performance.


troubleshooting high load average# show current load average$ uptime########################################################## cpu usage per cpu====================================# show per-cpu usage peridically$ mpstat -P ALL 1########################################################## cpu usage per cpu====================================########################################################## cpu usage per process====================================# show all active process cpu usage(like top)$pidstat 1# show cpu usage of given process peridically(2 second)# $pidstat -p 823471  2$pidstat -p 823471  1Linux 3.10.0-693.21.7.el7.x86_64 (A06-R08-I132-181-815KSRH.JCLOUD.COM)  07/20/2022      _x86_64_        (32 CPU)04:42:46 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command04:42:47 PM     0    823471   33.00    5.00    0.00   38.00    10  node_monitor04:42:48 PM     0    823471   54.00    7.00    0.00   61.00    10  node_monitor04:42:49 PM     0    823471   46.00    4.00    0.00   50.00    10  node_monitor04:42:50 PM     0    823471   31.00    7.00    0.00   38.00    10  node_monitor# show cpu usage of given process(thread displayed)peridically$pidstat -p 823471 -t 1########################################################## cpu usage per process====================================########################################################## cpu schedule latency per process=========================# CPU run queue latency, schedule latency for each process# monitor 10 seconds$ perf sched record -- sleep 10$ perf sched latency -----------------------------------------------------------------------------------------------------------------  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at       | -----------------------------------------------------------------------------------------------------------------  :632308:632308        |      0.478 ms |        2 | avg:   47.545 ms | max:   95.088 ms | max at: 1397496.686810 s  :632316:632316        |     13.820 ms |        8 | avg:   23.729 ms | max:   94.070 ms | max at: 1397496.685531 s  ovs-vsctl:(3)         |    107.932 ms |       31 | avg:   14.581 ms | max:   95.035 ms | max at: 1397496.485498 s  sh:(2)                |      6.058 ms |       11 | avg:    8.146 ms | max:   87.049 ms | max at: 1397495.885529 s  sleep:(26)            |     17.129 ms |       75 | avg:    6.211 ms | max:   94.541 ms | max at: 1397496.386032 s  :632320:632320        |      3.305 ms |       18 | avg:    5.288 ms | max:   94.205 ms | max at: 1397495.785685 s  node_monitor:(76)     |    342.577 ms |     2342 | avg:    4.063 ms | max:  192.182 ms | max at: 1397496.385540 s  perf:(183)            |    569.297 ms |     1597 | avg:    3.975 ms | max:  196.652 ms | max at: 1397496.386045 s  kworker/25:2:461486   |      0.009 ms |        1 | avg:    0.677 ms | max:    0.677 ms | max at: 1397496.187200 s  kworker/9:1:30855     |      0.012 ms |        1 | avg:    0.379 ms | max:    0.379 ms | max at: 1397496.186873 s  kworker/1:6:615135    |      0.022 ms |        2 | avg:    0.358 ms | max:    0.711 ms | max at: 1397496.189183 s  kworker/17:1:715010   |      0.009 ms |        1 | avg:    0.357 ms | max:    0.357 ms | max at: 1397496.186876 s  kworker/31:1:303204   |      0.011 ms |        1 | avg:    0.314 ms | max:    0.314 ms | max at: 1397496.186854 s  :632321:632321        |      3.882 ms |        7 | avg:    0.313 ms | max:    1.140 ms | max at: 1397495.887512 s  :632319:632319        |      0.815 ms |        6 | avg:    0.258 ms | max:    1.277 ms | max at: 1397495.792539 s  kworker/3:1:86183     |      0.010 ms |        1 | avg:    0.226 ms | max:    0.226 ms | max at: 1397496.186715 s  :632317:632317        |      0.866 ms |        7 | avg:    0.216 ms | max:    1.099 ms | max at: 1397495.794319 s  kworker/14:1:197420   |      0.010 ms |        1 | avg:    0.215 ms | max:    0.215 ms | max at: 1397496.186723 s  :632307:632307        |      0.872 ms |        8 | avg:    0.205 ms | max:    1.420 ms | max at: 1397496.188524 s  kworker/26:0:308384   |      0.011 ms |        1 | avg:    0.193 ms | max:    0.193 ms | max at: 1397496.186720 s  :632301:632301        |      0.275 ms |        4 | avg:    0.165 ms | max:    0.658 ms | max at: 1397496.591534 s########################################################## cpu schedule latency per process=========================

CPU usageCPU usage is a measurement, in a percentage, of how much time the CPU spends actively computing something. For instance, if you had a program that required uninterrupted processing power for 54 out of the last 60 seconds, your CPU usage on one core would be 90%. Instead, if the program only required six seconds processing time on one core, the usage would be 10%.
Most companies seek to keep the CPU usage of their servers as close to 100% as possible. Most servers are sold by overall computing power, and if your server is only sitting at 30% CPU usage, you’re paying for too much processor power. You could downgrade your processor to a lower tier, save money, and see no reduction in the quality of your server’s performance.
cpu usage vs load averageCPU usage: There ratio (usually expressed as a percentage)of time that the CPU is busy doing stuff. This measure only makes sense if you know over which period the percentage is being calculated.
Load: Average queue length for the CPU - including the process currently executing. For this to make sense, you need to know the period over which this is being measured.
They are related, but one does not necessarily correlate to the other.
Imagine this scenario - with slightly contrived numbers: An ideal world with a single CPU. No scheduling overhead, no I&#x2F;O overhead. Just keeping things simple.

You have 100 processes waiting for something.
When that “something” happens, each process will need 0.05 seconds of CPU time to do stuff in response.
When “something” does not happen, you have 0% CPU utilisation, and a queue length of 0. Basically stuff is just waiting. Life is good, and you’re merely wasting electrons and heating up the planet.
“something” happens. All 100 processes wake up. Your queue length jumps to 100, and your CPU is busy.
0.05 seconds later, your queue length is 99 as the first process has finished doing “stuff”. CPU is still busy.
After 0.1 seconds, your queue length is 98 as the 2nd process has finished doing “stuff”. CPU is still busy.
Every 0.05 your queue length drops by 1 as a process finishes. CPU remains busy.
After 5 seconds, all the processes have finished; CPU becomes idle again and your queue length is back to zero.
Your CPU utilisation over the last 60 seconds is now: 5&#x2F;60 &#x3D; 8.33%. But your average queue length (&#x3D;load average) over the last 60 seconds will be about 4.2.len:  100  99     98    97    ... 1      0     ...  0time: 0    0.05   0.1   0.15      4.95   5          59average = (0+0+...+1+2+3...+100)/(59/0.05+1) = 4.27

Looking at the 1-minute CPU utilisation alone (8.33%), you look good. But the 1-minute load average (4.2) shows that you have a performance bottleneck during that minute. Whether this is “bad” or not depends on whether you want it to be faster - do you need to respond to “something” happening more frequently than every 5 seconds?
NOTE

Load average is always high for periburst load(many processes are ready to run at a time) which runs shortly while cpu uage is not so high.
Too many D process as a process in state D is in uninterruptible, it’ counted by load average as well.

useful commands and performance toolsTo debug performance issue, there are lots of tools that we can use to help us identify the issue, but some of them are old tools, some of new, so here we only introduce new tools that’s are used today.

Old tool

grpof
Oprofile

new tool

gperftoolsgperftools is newer since 2007 developed by Google, it’s simpler, only from process view, stack of process
perfperf is already in kernel source tree(upstream) since since 2009, it’s complex, can showmore information from system-wide view it uses hardware counters to profile the application.The result of this profiler are really precise and because it is not doing instrumentation of the code, it is really fast.

perf can check a process(stack from kernel to process stack) or check system(without given process id)
useful commandsshow cpu load    Each running process either using or waiting for CPU resources adds 1 to the load average. So, if your system has a load of 5, five processes are either using or waiting for the CPU, the load number doesn’t mean too much. A computer might have a load of 0 one split-second, and a load of 5 the next split-second as several processes use the CPU. Even if you could see the load at any given time, that number would be basically meaningless. That’s why Unix-like systems don’t display the current load. They display the load average — an average of the computer’s load over several periods of time. This allows you to see how much work your computer has been performing.    # uptime    10:11:01 up 18:57,  4 users,  load average: 0.50, 2.13, 1.85    From left to right, these numbers show you the average load over the last one minute, the last five minutes, and the last fifteen minutesshow how much time process runs in sys, user    Real time is wall clock time. (what we could measure with a stopwatch)    User time is the amount of time spend in user-mode within the process    Sys is the CPU time spend in the kernel within the process.    NOTE: real can be less than user if, it&#x27;s app is multi-thread or multi-process!!!    The rule of thumb is:    real &lt; user: The process is CPU bound and takes advantage of parallel execution on multiple cores/CPUs.    real ≈ user: The process is CPU bound and takes no advantage of parallel exeuction.    real &gt; user: The process is I/O bound. Execution on multiple cores would be of little to no advantage.    #time ls         share  windows         real    0m0.002s         user    0m0.001s         sys 0m0.001sshow latency of RT linux kernel    #cyclictest    (git://git.kernel.org/pub/scm/utils/rt-tests/rt-tests.git)show slab info    #cat /proc/slabinfo    #slabtop        Active / Total Objects (% used)    : 133629 / 147300 (90.7%)        Active / Total Slabs (% used)      : 11492 / 11493 (100.0%)        Active / Total Caches (% used)     : 77 / 121 (63.6%)        Active / Total Size (% used)       : 41739.83K / 44081.89K (94.7%)        Minimum / Average / Maximum Object : 0.01K / 0.30K / 128.00K         OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME        44814  43159  96%    0.62K   7469        6     29876K ext3_inode_cache        36900  34614  93%    0.05K    492       75      1968K buffer_head        35213  33124  94%    0.16K   1531       23      6124K dentry_cache         7364   6463  87%    0.27K    526       14      2104K radix_tree_node         1280   1015  79%    0.25K     40       32       320K kmalloc-256  ---&gt; two pages for one slab    (Note, the management memory is not calculated!!!, but it&#x27;s small)    Each cache may have many slabs(empty, partial, full), each slab is one or multiple PAGE SIZE    (usually 4K for a PAGE SIZE)!    USE        = (ACTIVE/OBJS)*100/100    OBJS       = SLABS*(OBJ/SLAB)    OBJ/SLAB   = (4K*n)/OBJ_SIZE    CACHE SIZE = SLABS * (4K*n)show swap size used by each process    #smem      (RSS 656 mean 656K?)      PID User     Command                         Swap      USS      PSS      RSS     2516 rabbitmq sh -c /usr/lib/rabbitmq/bin        0       96      116      656     1451 lightdm  /bin/sh /usr/lib/lightdm/li        0      100      121      700     1130 root     /bin/sh -e /proc/self/fd/9         0      100      122      680     1157 root     /sbin/getty -8 38400 tty3          0      156      174      964    Show basic process information  smem    Show library-oriented view  smem -m    Show user-oriented view     smem -u    Show system view    smem -R 4G -K /path/to/vmlinux -w    Show totals and percentages     smem -t -p    Show different columns  smem -c &quot;name user pss&quot;    Sort by reverse RSS     smem -s rss -r    Show processes filtered by mapping  smem -M libxml    Show mappings filtered by process   smem -m -P [e]volution    Read data from capture tarball  smem --source capture.tar.gz    Show a bar chart labeled by pid     smem --bar pid -c &quot;pss uss&quot;    Show a pie chart of RSS labeled by name     smem --pie name -s rssShow memory usage by &#x27;free&#x27; command    $ free                   total        used        free      shared  buff/cache   available    Mem:       24687560    11825536     8579812      258488     4282212    12299492    Swap:      16774140           0    16774140    total== 11825536 + 8579812 + 4282212 == 24687560    available = 8579812 + part of(buff/cache which is not used by OS)    total: Your total (physical) RAM (excluding a small bit that the kernel permanently reserves for itself at startup);    used: memory in use by the OS(calculate apps, buffers, caches)    free: memory not in use.    total = used + free + buff/cache    shared /buff/cache: This shows memory usage for specific purposes    (write data to disk, buffer is used, which cache is used for storing data read from disk in memory)    The last line (Swap:) gives information about swap space usage (i.e. memory contents that have been temporarily moved to disk).    To actually understand what the numbers mean, you need a bit of background about the virtual memory (VM) subsystem in Linux.    Just a short version: Linux (like most modern OS) will always try to use free RAM for caching stuff, so Mem: free will almost always be very low.    caches will be freed automatically if memory gets scarce, so they do not really matter.Inside exec()    In computing, exec is a functionality of an operating system that runs an executable file in the context of an already existing process,    replacing the previous executable. This act is also referred to as an overlay. It is especially important in Unix-like systems, although exists elsewhere.   As a new process is not created, the process identifier (PID) does not change,   but the machine code, data, heap, and stack of the process are replaced by those of the new program.====================================================SAR===================================================================================sar(System Activity Report)： Show system activity information, its gives more details about cpu, memory, interrupt, io, power, network etcBut you can also check other commands for specific resource from below section    # -B is more general including(swap process memory + disk io)    # sar -B 5     Linux 3.10.0-1160.el7.x86_64 (dev) 	10/12/2022 	_x86_64_	(16 CPU)    05:14:19 PM  pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff    05:14:24 PM      0.00  31948.80     10.00      0.00    136.20      0.00      0.00      0.00      0.00    05:14:29 PM      0.00 236544.00     10.80      0.00     57.40      0.00      0.00      0.00      0.00    # -W is about swap of process memory(swap process page to disk when there is not engouh memory)    #sar -W 5    Linux 3.10.0-1160.el7.x86_64 (dev) 	10/12/2022 	_x86_64_	(16 CPU)        05:14:43 PM  pswpin/s pswpout/s    05:14:48 PM      0.00      0.00    05:14:53 PM      0.00      0.00    Report I/O and transfer rate statistics    # sar -b 5     Linux 3.10.0-1160.el7.x86_64 (dev)      10/22/2021      _x86_64_        (8 CPU)    05:34:02 PM       tps      rtps      wtps   bread/s   bwrtn/s    05:34:07 PM      0.00      0.00      0.00      0.00      0.00    05:34:12 PM      0.00      0.00      0.00      0.00      0.00    Report activity for each block device    # sar -d 5     Linux 3.10.0-1160.el7.x86_64 (dev)      10/22/2021      _x86_64_        (8 CPU)    05:34:20 PM       DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util    05:34:25 PM    dev8-0      0.20      0.00      6.40     32.00      0.00      1.00      1.00      0.02    05:34:25 PM  dev253-0      0.20      0.00      6.40     32.00      0.00      1.00      1.00      0.02    05:34:25 PM  dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:34:25 PM  dev253-2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    show interrupt per 5s    # sar -I ALL 5    Linux 3.10.0-1160.el7.x86_64 (dev)      10/22/2021      _x86_64_        (8 CPU)    05:37:36 PM      INTR    intr/s    05:37:41 PM         0      0.00    05:37:41 PM         1      0.00    05:37:41 PM         2      0.00    05:37:41 PM         3      0.00    05:37:41 PM         4      0.00    05:37:41 PM         5      0.00    05:37:41 PM         6      0.00    05:37:41 PM         7      0.00    05:37:41 PM         8      0.00    05:37:41 PM         9      0.00    05:37:41 PM        10      0.00    05:37:41 PM        11      0.00    05:37:41 PM        12      0.00    05:37:41 PM        13      0.00    05:37:41 PM        14      0.80    05:37:41 PM        15      0.00    show power management    $ ar -m ALL 10    Linux 3.10.0-327.36.4.el7.x86_64 (A04-R08-I138-47-91TYB72.JCLOUD.COM)   10/22/2021      _x86_64_        (32 CPU)    05:40:01 PM     CPU       MHz    05:40:11 PM     all   1258.82    05:40:01 PM    TEMP      degC     %temp               DEVICE    05:40:11 PM       1     43.00     55.84    coretemp-isa-0000    05:40:11 PM       2     38.00     49.35    coretemp-isa-0000    05:40:11 PM       3     37.00     48.05    coretemp-isa-0000    05:40:11 PM       4     34.00     44.16    coretemp-isa-0000    05:40:11 PM       5     38.00     49.35    coretemp-isa-0000    05:40:11 PM       6     33.00     42.86    coretemp-isa-0000    05:40:11 PM       7     34.00     44.16    coretemp-isa-0000    05:40:11 PM       8     37.00     48.05    coretemp-isa-0000    05:40:11 PM       9     35.00     45.45    coretemp-isa-0000    05:40:11 PM      10     44.00     57.14    coretemp-isa-0001    05:40:11 PM      11     36.00     46.75    coretemp-isa-0001    05:40:11 PM      12     36.00     46.75    coretemp-isa-0001    05:40:11 PM      13     37.00     48.05    coretemp-isa-0001    05:40:11 PM      14     37.00     48.05    coretemp-isa-0001    05:40:11 PM      15     34.00     44.16    coretemp-isa-0001    05:40:11 PM      16     36.00     46.75    coretemp-isa-0001    05:40:11 PM      17     34.00     44.16    coretemp-isa-0001    05:40:11 PM      18     34.00     44.16    coretemp-isa-0001    05:40:01 PM     BUS  idvendor    idprod  maxpower                manufact                                         product    05:40:11 PM       1      8087      800a         0                                                                            05:40:11 PM       2      8087      8002         0                                                                            05:40:11 PM       1      413c      a001       200         no manufacturer                                  Gadget USB HUB    show network stats, lots of fields, only list some    # sar -n ALL 10    Linux 3.10.0-327.36.4.el7.x86_64 (A04-R08-I138-47-91TYB72.JCLOUD.COM)   10/22/2021      _x86_64_        (32 CPU)    05:43:15 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s    05:43:25 PM tap_metadata      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM vxlan_sys_4789      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       br0      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM tap_proxy_ns      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM ovs-system      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM tap_proxy      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM      eth0      2.10      2.00      0.23      0.19      0.00      0.00      0.00    05:43:25 PM        lo      2.00      2.00      0.79      0.79      0.00      0.00      0.00    05:43:25 PM       em2      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       em4      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       em3      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:15 PM     IFACE   rxerr/s   txerr/s    coll/s  rxdrop/s  txdrop/s  txcarr/s  rxfram/s  rxfifo/s  txfifo/s    05:43:25 PM tap_metadata      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM vxlan_sys_4789      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       br0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM tap_proxy_ns      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM ovs-system      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM tap_proxy      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM      eth0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       em2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       em4      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM       em3      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    05:43:25 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00    Report cpu queue length and load averages    # sar -P ALL -q 10    Linux 3.10.0-327.36.4.el7.x86_64 (A04-R08-I138-47-91TYB72.JCLOUD.COM)   10/22/2021      _x86_64_        (32 CPU)    05:48:32 PM   runq-sz  plist-sz   ldavg-1   ldavg-5  ldavg-15   blocked    05:48:42 PM         0      1069      0.43      0.46      0.49         0        Report memory utilization statistics    # sar -r 10    Linux 3.10.0-327.36.4.el7.x86_64 (A04-R08-I138-47-91TYB72.JCLOUD.COM)   10/22/2021      _x86_64_        (32 CPU)    05:46:25 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty    05:46:35 PM 119478308  12269620      9.31      1868   9411160   5926572      3.99   4968572   5430432      1560    Report CPU utilization    # sar -P ALL -u 10    Linux 3.10.0-327.36.4.el7.x86_64 (A04-R08-I138-47-91TYB72.JCLOUD.COM)   10/22/2021      _x86_64_        (32 CPU)    05:47:32 PM     CPU     %user     %nice   %system   %iowait    %steal     %idle    05:47:42 PM     all      1.39      0.00      0.40      0.00      0.00     98.20    05:47:42 PM       0     40.49      0.00      0.00      0.00      0.00     59.51    05:47:42 PM       1      0.00      0.00      0.30      0.00      0.00     99.70    05:47:42 PM       2      0.41      0.00      1.32      0.00      0.00     98.28    05:47:42 PM       3      0.10      0.00      0.20      0.00      0.00     99.70    05:47:42 PM       4      0.30      0.00      0.80      0.00      0.00     98.89    05:47:42 PM       5      0.20      0.00      0.40      0.00      0.00     99.40    05:47:42 PM       6      0.20      0.00      0.40      0.00      0.00     99.40    05:47:42 PM       7      0.00      0.00      0.20      0.00      0.00     99.80    05:47:42 PM       8      0.70      0.00      0.90      0.00      0.00     98.39    Report task creation and system switching activity    # sar -w 10    Linux 3.10.0-327.36.4.el7.x86_64 (A04-R08-I138-47-91TYB72.JCLOUD.COM)   10/22/2021      _x86_64_        (32 CPU)    05:48:56 PM    proc/s   cswch/s    05:49:06 PM      2.30  12558.70====================================================SAR===================================================================================Show CPUS stats    CPU utilization stats runs on user, sys, virtual processor(vm)    # mpstat -P ALL -u    04:38:08 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle    04:38:08 PM  all    7.03    0.06    2.89    0.01    0.00    0.00    0.00    8.28    0.00   81.73    04:38:08 PM    0    0.88    0.08    3.26    0.01    0.00    0.19    0.00   10.02    0.00   85.56    04:38:08 PM    1    0.89    0.04    3.30    0.01    0.00    0.02    0.00    9.47    0.00   86.28    04:38:08 PM    2    0.83    0.08    3.15    0.01    0.00    0.01    0.00   10.15    0.00   85.78    04:38:08 PM    3    0.82    0.04    3.15    0.01    0.00    0.00    0.00    9.61    0.00   86.39    04:38:08 PM    4    1.03    0.07    4.59    0.01    0.00    0.01    0.00   12.51    0.00   81.78    04:38:08 PM    5    0.92    0.04    3.22    0.01    0.00    0.00    0.00    9.58    0.00   86.23    04:38:08 PM    6    1.10    0.07    4.63    0.01    0.00    0.00    0.00   12.52    0.00   81.66    04:38:08 PM    7    0.83    0.05    3.42    0.01    0.00    0.00    0.00   10.44    0.00   85.25    04:38:08 PM    8   99.75    0.00    0.25    0.00    0.00    0.00    0.00    0.00    0.00    0.00    CPU soft irq    # mpstat -I SCPU    Linux 3.10.0-693.21.4.el7.x86_64 (A01-R15-I124-40-CCK4HP2.JCLOUD.COM)   10/22/2021      _x86_64_        (64 CPU)    04:39:57 PM  CPU       HI/s    TIMER/s   NET_TX/s   NET_RX/s    BLOCK/s BLOCK_IOPOLL/s  TASKLET/s    SCHED/s  HRTIMER/s      RCU/s    04:39:57 PM    0       0.00      54.90       0.20       2.42       0.00       0.00       0.05      12.35       0.00      10.65    04:39:57 PM    1       0.00      41.11       0.00       0.48       0.04       0.00       7.10      43.85       0.00       7.03    04:39:57 PM    2       0.00      60.01       0.01      14.90       0.00       0.00       0.57      59.44       0.00      10.80    04:39:57 PM    3       0.00      33.81       0.00       0.50       0.04       0.00       0.00      52.79       0.00       3.72    04:39:57 PM    4       0.00      40.35       0.01      17.83       0.00       0.00       0.75       6.86       0.00      23.19    04:39:57 PM    5       0.00      44.60       0.00       0.51       0.04       0.00       0.00      53.62       0.00       7.76    04:39:57 PM    6       0.00      44.92       0.01      12.48       0.00       0.00       0.51       7.00       0.00      24.59    04:39:57 PM    7       0.00      58.52       0.00       0.46       0.04       0.00       0.00      57.85       0.00      12.73    04:39:57 PM    8       0.00      33.03       0.00       0.00       0.00       0.00       0.00       0.00       0.00      58.50Show CPU live stats    # top    top - 16:45:28 up 771 days,  3:16,  1 user,  load average: 6.66, 7.24, 6.54    Tasks: 670 total,   9 running, 661 sleeping,   0 stopped,   0 zombie    %Cpu(s):  9.7 us,  1.6 sy,  0.1 ni, 88.6 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st    KiB Mem : 26379142+total,  3811248 free, 23686681+used, 23113348 buff/cache    KiB Swap: 16777212 total, 16691312 free,    85900 used. 25565136 avail Mem    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND    127796 root      10 -10  0.105t 573032  20444 S 403.6  0.2  42365,18 vswitchd    131315 root      20   0 9188560  87436   5940 S 108.6  0.0 668887:05 qemu-kvm    113257 root      20   0 9220320  77628   5916 S  78.8  0.0 300804:39 qemu-kvm    69578 root      20   0 9043128  66088   3640 S  18.2  0.0   2199:03 qemu-system-x86    123753 root      20   0 9039996  63892   3604 S  15.9  0.0 764:21.77 qemu-system-x86    113084 root      20   0 9074688  68036   1916 S  12.3  0.0 170215:57 qemu-system-x86    99040 root      20   0 16.647g  65140   1900 S   9.6  0.0 158111:53 qemu-system-x86    133933 root      20   0 4836392  64328   3648 S   8.6  0.0 381:15.29 qemu-system-x86    92403 root      20   0 4825240  62916   3308 S   7.3  0.0  21040:53 qemu-system-x86    100018 root      20   0 3471696   5216   2616 S   7.0  0.0   8384:57 logd    # htopshow live virtual memory usage    show stats per 2s, actuall, it also shows io, system, cpu as well    $ vmstat  -n 2    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----    r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st    5  4  85900 4185664   4396 22772168    0    0     1    91    0    0 15  3 82  0  0    6  0  85900 4181612   4396 22772580    0    0    64   479 55060 82145  8  1 91  0  0    8  0  85900 4184968   4396 22772636    0    0    96    98 56364 87759  8  1 91  0  0    8  0  85900 4183828   4396 22772936    0    0    96   152 58835 88482  9  1 90  0  0    6  0  85900 4180524   4396 22772920    0    0     0   320 58749 94072  9  1 90  0  0    5  0  85900 4184580   4396 22773588    0    0     0   234 67631 111630  9  2 89  0  0show io statistics, most used for which disk has high io await.    the io wait of the whole system(96.0%wa)    # top    top - 14:31:20 up 35 min, 4 users, load average: 2.25, 1.74, 1.68    Tasks: 71 total, 1 running, 70 sleeping, 0 stopped, 0 zombie    Cpu(s): 2.3%us, 1.7%sy, 0.0%ni, 0.0%id, 96.0%wa, 0.0%hi, 0.0%si, 0.0%st    Mem: 245440k total, 241004k used, 4436k free, 496k buffers    Swap: 409596k total, 5436k used, 404160k free, 182812k cached    show iostat per 10s of each block device(check which block device has high io wait)    # sar -d 5    # iostat -txz 10    Linux 3.10.0-693.21.4.el7.x86_64 (A01-R15-I124-40-CCK4HP2.JCLOUD.COM)   10/22/2021      _x86_64_        (64 CPU)    10/22/2021 05:14:10 PM    avg-cpu:  %user   %nice %system %iowait  %steal   %idle            15.31    0.06    2.89    0.01    0.00   81.73    Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util    nvme0n1           0.00     0.06    1.06   47.46    43.69  4897.74   203.70     0.01    0.25    0.26    0.25   0.06   0.29    sda               0.00     0.03    0.00    0.73     0.07     7.51    20.56     0.01   19.20    6.80   19.27   3.83   0.28    nb100             0.00     0.01    0.29    7.33    36.40   545.36   152.62     0.02    3.25    9.77    2.99   1.07   0.82    nb101             0.00     0.00    0.00    1.76     0.03    14.69    16.69     0.00    0.94    1.29    0.94   0.29   0.05    nb102             0.00     0.00    0.00    6.72     0.01   132.59    39.46     0.01    1.18    0.70    1.18   0.28   0.19    nb103             0.00     0.00    0.00    0.55     0.02     7.14    25.97     0.00    0.86    0.54    0.86   0.46   0.03    nb104             0.00     0.00    0.00    0.31     0.01    10.68    68.74     0.00    1.45    0.50    1.45   0.43   0.01    nb105             0.00     0.00    0.00    1.29     0.02    78.13   121.00     0.00    3.56    0.62    3.56   0.50   0.06    nb106             0.00     0.00    0.01    0.71     0.83    40.81   116.51     0.00    1.19    0.57    1.19   0.69   0.05    nb107             0.00     0.00    0.00    0.17     0.01     1.37    16.55     0.00    1.21    8.04    1.20   0.39   0.01    nb108             0.00     0.00    0.00    0.17     0.00     1.29    15.10     0.00    0.90    0.53    0.90   0.44   0.01    nb109             0.00     0.00    0.00    0.00     0.00     0.04    60.20     0.00   42.15    0.42   43.57   0.46   0.00    show io per process, which process is writing high io    #iotop    Total DISK READ :       0.00 B/s | Total DISK WRITE :       0.00 B/s    Actual DISK READ:       0.00 B/s | Actual DISK WRITE:       0.00 B/s    TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND                                                                                                                                              17391 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.02 % [kworker/6:0]    16896 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % python -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0fe83e4c-ccd4-49f4-ae7e-4b07fabb2dc3.json        1 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % systemd --switched-root --system --deserialize 22        2 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kthreadd]        4 be/0 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kworker/0:0H]        6 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/0]        7 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/0]        8 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [rcu_bh]        9 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [rcu_sched]show interface statistics    show stats of ifaces    # ifstat    #kernel    Interface        RX Pkts/Rate    TX Pkts/Rate    RX Data/Rate    TX Data/Rate                      RX Errs/Drop    TX Errs/Drop    RX Over/Rate    TX Coll/Rate      lo                     0 0             0 0             0 0             0 0                              0 0             0 0             0 0             0 0          enp0s3                 8 0             6 0           560 0          1424 0                              0 0             0 0             0 0             0 0          docker0                0 0             0 0             0 0             0 0                              0 0             0 0             0 0             0 0          vethbedf2bf            0 0             0 0             0 0             0 0                              0 0             0 0             0 0             0 0        # live stats on each interface                        #iftop    # live stats on each process which has network io    # nethogs    NetHogs version 0.8.5        PID USER     PROGRAM             DEV        SENT      RECEIVED           13337 root     sshd: root@pts/2    enp0s3      0.218       0.186 KB/sec          ?     root     unknown TCP                     0.000       0.000 KB/sec    TOTAL                                          0.218       0.186 KB/sec    show details about interface like config, stats etc    # ethtool -h    ethtool -g|--show-ring DEVNAME	Query RX/TX ring parameters    ethtool -k|--show-features|--show-offload DEVNAME	Get state of protocol offload and other features    ethtool -i|--driver DEVNAME	Show driver information    ethtool -S|--statistics DEVNAME	Show adapter statistics    ethtool -n|-u|--show-nfc|--show-ntuple DEVNAME	Show Rx network flow classification options or rules    ethtool -x|--show-rxfh-indir|--show-rxfh DEVNAME	Show Rx flow hash indirection and/or hash keyshow power management    show power used by each process live    # powertop    PowerTOP v2.9     Overview   Idle stats   Frequency stats   Device stats   Tunables                                         Summary: 72.3 wakeups/second,  0.0 GPU ops/seconds, 0.0 VFS ops/sec and 0.3% CPU use                    Usage       Events/s    Category       Description                122.3 µs/s      20.0        Process        [PID 460] [xfsaild/dm-0]                72.8 µs/s       9.5        Timer          tick_sched_timer                116.2 µs/s       6.7        Timer          hrtimer_wakeup                93.0 µs/s       5.7        Process        [PID 1084] /usr/bin/containerd                63.7 µs/s       5.7        Process        [PID 9] [rcu_sched]                632.7 µs/s       4.8        Process        [PID 1049] /home/data/Anaconda3/bin/python /home/data/Anaconda3/bin/jupyter-notebook -y --no-browser --allow-root --ip=10.0.2.1                61.6 µs/s       4.8        Process        [PID 1082] /usr/bin/containerd                34.9 µs/s       2.9        Interrupt      [3] net_rx(softirq)                183.8 µs/s       1.9        Interrupt      [7] sched(softirq)                239.2 µs/s       1.0        kWork          e1000_watchdogBenchmark tools    for operation function    #apt-get install lmbench    Layer 4 Throughput using NetPerf and iPerf, two open source network performance benchmark tools that support both UDP and TCP protocols. Each tool provides in addition other information:    NetPerf for example provides tests for end-to-end latency (round-trip times or RTT) and is a good replacement for Ping    iPerf provides packet loss and delay jitter, useful to troubleshoot network performance.    for network, test the network between client(netperf) and server(netserver)    server side    #netserver    client side with testing 300s, or never stop(-l 0)    #netperf -H $server -l 300 -t TCP_STREAM    server side    #iperf3 --server --interval 30    client side    #iperf3 --client $server --time 300 --interval 30

check bottleneck, call graph++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Don&#x27;t use gprof which is old since 198xOprofile is old still release one version each year since 2002 and it uses the same backendas &#x27;perf&#x27; does, so can give almost the same output with &#x27;perf&#x27; but the Community recommend&#x27;perf&#x27; and intend to replace itgperftools is newer since 2007 developed by Google, it&#x27;s simpler, only from process viewperf is already in kernel source tree(upstream) since since 2009, it&#x27;s complex, can showmore information from system-wide view!!!!!!!!!!!!!!! it uses hardware counters to profile the application.The result of this profiler are really precise andbecause it is not doing instrumentation of the code, it is really fast.gperftools and perf are two good choices nowadays!!!!!!!!!!!++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

gperftools(great perf tools, originally from google performance tool, is package name)gperftools is a collection of a high-performance multi-threaded malloc() implementation, plus some pretty nifty performance analysis tools.
Ubuntu18#apt-get install google-perftools graphviz libgoogle-perftools-devCentos7$ yum install -y pprof gperftools-develUsage    CPUPROFILE:(check which function or line take much time) NO track forked child!!!        As gperftools provides tcmalloc, heap checker, heap profiler and cpu profiler        (heap checker, heap profiler are in &#x27;-ltcmalloc&#x27;         cpu profiler &#x27;-lprofiler&#x27;         google-pprof is used to analysis the profile file)        There are two ways to use gperftools, one is to compile it within your program        the other is to used PRELOAD and set env(use the first way always!)        Generate profile file            a. Compile it within your program                #include &lt;gperftools/profiler.h&gt;                #include &lt;stdio.h&gt;                #include &lt;stdlib.h&gt;                void func1() &#123;                    int i = 0;                    while (i &lt; 100000) &#123;                        ++i;                    &#125;                &#125;                void func2() &#123;                    int i = 0;                    while (i &lt; 200000) &#123;                        ++i;                    &#125;                &#125;                void func3() &#123;                    int i = 0;                    for (i = 0; i &lt; 1000; ++i) &#123;                        func1();                        func2();                    &#125;                &#125;                int main()&#123;                    ProfilerStart(&quot;my.prof&quot;);                    func3();                    ProfilerStop();                    return 0;                &#125;               # gcc -o test  test.c -g -Wall -lprofiler               This way(set CPUPROFILE) will do CPU profile definitely!!!, no switch               #CPUPROFILE_FREQUENCY=100 ./test               (100 samples per second, default value)            b. Use PRELOAD (not recommended!!!!) without recompiling!!!!!                #export LD_PRELOAD=/usr/lib64/libprofiler.so                /*turn on cpu profile during whole life*/                #env CPUPROFILE=my.prof ./test                -------------------------------------------------------------------------            | For a daemon process, run it forground!!! not a daemon for profiling. |            -------------------------------------------------------------------------        ---------------------------------------------------------------------------        Analyze the profile file( take care of the first three columns)        to see which function or lines consume more CPU time!!!!!!!!!!        ---------------------------------------------------------------------------            See which function takes much time            root@ubuntu:~# google-pprof --text ./test my.prof            OR            root@centos:~# pprof --text ./test my.prof            Using local file ./test. (test is the program)            Using local file my.prof.(my.prof is the data collected before)            Removing killpg from all stack traces.            Total: 71 samples                  53                74.6%            74.6%          53  74.6% func2                  18                25.4%           100.0%          18  25.4% func1                   0                 0.0%           100.0%          71 100.0% __libc_start_main                   0                 0.0%           100.0%          71 100.0% _start                   0                 0.0%           100.0%          71 100.0% func3                   0                 0.0%           100.0%          71 100.0% main            column meanings            1. Number of profiling samples in this function            2. Percentage of profiling samples in this function            3. Percentage of profiling samples in the functions printed so far            4. Number of profiling samples in this function and its callees            5. Percentage of profiling samples in this function and its callees            6. Function name             if you perf to run, from system view, you will get            $perf record ./test            $perf report            65.73%  test     test              [.] func2            33.68%  test     test              [.] func1            0.16%  test     [kernel.vmlinux]  [k] native_write_msr_safe            0.06%  test     [kernel.vmlinux]  [k] x86_pmu_enable            0.05%  test     [kernel.vmlinux]  [k] __intel_pmu_disable_all            0.05%  test     libc-2.17.so      [.] __GI___dl_iterate_phdr            0.00%  test     [kernel.vmlinux]  [k] __do_page_fault            0.00%  test     libc-2.17.so      [.] __memset_sse2            0.00%  test     [kernel.vmlinux]  [k] lapic_next_deadline            See which line takes much time, you have to build test with -g            root@ubuntu:~#google-pprof --lines --text ./test my.prof            OR            root@centos:~#pprof --lines --text ./test my.prof            Using local file ./test.            Using local file my.prof.            Removing killpg from all stack traces.            Total: 71 samples                  37  52.1%  52.1%       37  52.1% func2 /root/test.c:12 (discriminator 1)                  22  31.0%  83.1%       22  31.0% func1 /root/test.c:6 (discriminator 1)                  11  15.5%  98.6%       13  18.3% func2 /root/test.c:13                   1   1.4% 100.0%        1   1.4% func1 /root/test.c:7                   0   0.0% 100.0%       71 100.0% __libc_start_main /build/eglibc-3GlaMS/eglibc-2.19/csu/libc-start.c:287                   0   0.0% 100.0%       71 100.0% _start ??:?                   0   0.0% 100.0%        1   1.4% func1 /root/test.c:6                   0   0.0% 100.0%       11  15.5% func2 /root/test.c:12                   0   0.0% 100.0%       23  32.4% func3 /root/test.c:19 (discriminator 2)                   0   0.0% 100.0%       48  67.6% func3 /root/test.c:20 (discriminator 2)                   0   0.0% 100.0%       71 100.0% main /root/test.c:25            (--text, --pdf, --web, --dot, --gif, --gv etc )            ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    Below seems not working for nginx don&#x27;t know why        TCMALLOC(thread cache malloc), you don&#x27;t need to memory management for your own        tcmalloc does it for you, so that you don&#x27;t need to care memory managment!!!!            tcmalloc actually implements a cache or pool, so that you can get memory from cache or pool            fast, but as the presure grows, tcmalloc takes more memory from system, while when presure decrease            tcmalloc should return the memory back to system TCMALLOC_RELEASE_RATE            Usage            +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++                #gcc -o test test.c -ltcmalloc_minimal                (in your program, use malloc, free as you did before)            +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++        HEAP CHECKER(check memory leak, not working well, don&#x27;t know why)            #gcc -o test test.c -ltcmalloc            #HEAPCHECK=normal  ./test        HEAP Profile(check where/who alloc memory)            #gcc -o test test.c -ltcmalloc            dump heap profile when allocate 1M memory (only through malloc method)            #HEAPPROFILE=heap.prof HEAP_PROFILE_ALLOCATION_INTERVAL=1024*1024 ./test            also dump sbrk, mmap method as well            #HEAPPROFILE=heap.prof  HEAP_PROFILE_MMAP=true HEAP_PROFILE_ALLOCATION_INTERVAL=1024*1024 ./test            root@ubuntu:~#google-pprof --gv test test.0004.heap            root@centos:~#pprof --gv test test.0004.heap

Ref
official load average
load average
cpu load vs cpu usage
gperftools

]]></content>
      <categories>
        <category>performance</category>
        <category>application</category>
      </categories>
      <tags>
        <tag>performance</tag>
        <tag>perf</tag>
        <tag>gperftools</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-selinux</title>
    <url>/2022/08/04/linux-selinux/</url>
    <content><![CDATA[Ref
selinux tool
Selinux Overview

]]></content>
  </entry>
  <entry>
    <title>linux-signal</title>
    <url>/2021/10/08/linux-signal/</url>
    <content><![CDATA[IntroductionSignals are software interrupts sent to a program to indicate that an important event has occurred. The events can vary from user requests to illegal memory access errors. Some signals, such as the interrupt signal, indicate that a user has asked the program to do something that is not in the usual flow of control.
Every signal has a default action associated with it. The default action for a signal is the action that a script or program performs when it receives a signal.Some of the possible default actions are

Terminate the process.
Ignore the signal.
Dump core. This creates a file called core containing the memory image of the process when it received the signal.
Stop the process.(can run later on)
Continue a stopped process



sending signal to processThere are several methods of delivering signals to a program or script. 

One of the most common is for a user to type CONTROL-xxx while a script is executing.
The other common method for delivering signals is to use the kill command, the syntax of which is as follows
$kill -sig pid



signal table(x86)Default Action for signal!!!

Term   Default action is to terminate the process.
Ign    Default action is to ignore the signal.
Core   Default action is to terminate the process and dump core
Stop   Default action is to stop the process.
Cont   Default action is to continue the process if it is currently stopped.




NO
Short
Keyboard
Action
Comment



2
SIGINT
ctrl + c
Term
The process was “interrupted”.


3
SIGQUIT
ctrl + \
Core
Quit from keyboard


5
SIGTRAP

CORE
Trace&#x2F;breakpoint trap


18
SIGCON

Cont
Continue if stopped


19
SIGSTOP

STOP
Stop process


20
SIGTSTP
ctrl + z
STOP
Essentially the same as SIGSTOP




ctrl +d

it is not a signal, it’s EOF (End-Of-File). It closes the stdin pipe.


After ctrl + Z, current process is stopped, then you can call $fg to run it again
important signal  

A SIGTRAP signal is a type of signal that is sent to a process when it encounters a trap instruction, this signal is used mainly from within debuggers and program tracers

SIGSTOP vs SIGTSTPSIGSTOP and SIGTSTP are both signals used in Unix-like operating systems to suspend processes.

Nature and Origin
SIGSTOP is a signal that can only be sent programmatically (e.g., using kill -STOP pid)
SIGTSTP can be generated both programmatically and by user input, typically by pressing Ctrl+Z in a terminal.


Handling by Processes
SIGSTOP cannot be caught, blocked, or ignored by the receiving process.
SIGTSTP can be caught, ignored, or handled differently by the process.


Default behavior
Both signals typically suspend the process until a SIGCONT signal is received to resume execution



Ref
signal man
signal summary

]]></content>
      <categories>
        <category>linux</category>
        <category>signal</category>
      </categories>
      <tags>
        <tag>singal</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-tproxy</title>
    <url>/2020/11/17/linux-tproxy/</url>
    <content><![CDATA[Transparent ProxyOverviewTransparent proxy allows to bind a non-local address(address belong to none interface), so that packet can be sent out with non-local address, meanwhile, with proper iptables and ip rule, incoming packet for transparent socket(non-local dst) can be received properly.


ImplementationIn order to support this, changes needed as below

Change kernel to allow bind non-local address.
Add a netfilter plugin to match transparent socket based on five tuples(src, sport, dst, dport, proto), tag the matched packet to make it go local by ip rule.

kernel commit for tproxy
# allow bind non-local addressf5715aea: ipv4: Implement IP_TRANSPARENT socket optionb9fb1506: ipv4: Allow binding to non-local addresses if IP_TRANSPARENT is set# with socket match(transparent socket), redirect packet to local with ip rule# hence packet goes to local, then received by tcp/udp136cdc71: netfilter: iptables socket match

Example to usemake sure your kernel is built with such config

CONFIG_NETFILTER_XT_MATCH_SOCKET
CONFIG_NF_TPROXY_IPV4
CONFIG_NETFILTER_XT_TARGET_TPROXY

output, allow bind non-local
fd = socket(AF_INET, SOCK_STREAM, 0);int value = 1;setsockopt(fd, SOL_IP, IP_TRANSPARENT, &amp;value, sizeof(value));name.sin_family = AF_INET;name.sin_port = htons(0xCAFE);name.sin_addr.s_addr = htonl(0xDEADBEEF);//bind non-local address after setting IP_TRANSPARENT for this socketbind(fd, &amp;name, sizeof(name));

input packet
# create a new chain$ iptables -t mangle -N DIVERT# if packet matches tcp socket(non-local or not) of this host, go to that chain in PREROUTING$ iptables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT # -j jump to DIVERT chain# set mark with 1 for the matched packet(non-local or not)# this mark used for ip rule.$ iptables -t mangle -A DIVERT -j MARK --set-mark 1$ iptables -t mangle -A DIVERT -j ACCEPT# later on packet goes to routing# skb with mark 1, lookup route table 100$ ip rule add fwmark 1 lookup 100 # policy routing# all skb goes to local, up to Lay4 even it&#x27;s local dst or not.# add a route at table 100, for all packets(as 0.0.0.0/0 matches all) sent to local(passed to upper layer), skb-&gt;dev with loopback.$ ip route add local 0.0.0.0/0 dev lo table 100
REF
Kernel TProxy Doc

]]></content>
      <categories>
        <category>linux</category>
        <category>tproxy</category>
      </categories>
      <tags>
        <tag>tproxy</tag>
      </tags>
  </entry>
  <entry>
    <title>load-balancer-markert-solution</title>
    <url>/2021/04/13/load-balancer-markert-solution/</url>
    <content><![CDATA[OverviewThe load balancer can be splitted into different part based on different typesBy Component

Hardware
Software

By Type

Local Load Balancers
Global Load Balancers

By Deployment

On-premise
Cloud

Key Market Players

A1o Networks, Inc.
Amazon Web Services, Inc.
Citrix Systems, Inc.
F5 Networks, Inc.
Google LLC
IBM Corporation
Microsoft Corporation


Top vendor of load balancer, please refer to top lb vendor.
]]></content>
      <categories>
        <category>load balancer</category>
        <category>market</category>
      </categories>
      <tags>
        <tag>load balancer</tag>
      </tags>
  </entry>
  <entry>
    <title>linux-tips-daily</title>
    <url>/2019/11/25/linux-tips-daily/</url>
    <content><![CDATA[TipsHere are some tips that are useful in our daily life.


remove ‘du du’ sound when tap keyboard$ sudo rmmod pcspkr

linux mount filemount files from windows by samba
# Centos7$ yum install -y cifs-utils$ sudo mount -t cifs //serverip/share /media/share -o user,sync,user=xxx,password=pxx,uid=1000,gid=1000,file_mode=0600,dir_mode=0700,noperm# error cases    # case: wrong password (special characters that can not pass through command line)    # case: wrong source directory(caused by wrong spell or format is not correct)    # case: access control issue.    # case: ubuntu iptables    # case:  proxy(router)firewall    # case: china character show incorrectly. export LC_ALL=zh_CN.utf8    #       before you set, make sure to run $ locale -a to see all language supported.
mount iso file
$ mount -o loop disk.iso  /mnt/disk$ umount /mnt/disk

mount a raw file
# check disk type raw or qcow2$ qemu-img info disk.img...file format: raw...# get file system type$ blkid disk.img# mount it$ mount -t xfs disk.img /mnt/disk$ umount /mnt/disk


mount a qcow2 file
# use a free nbd device$ qemu-nbd -c /dev/nbd100 disk.qcow2# mount the first partition or second ..$ fdisk /dev/nbd100 -lDisk label type: dosDisk identifier: 0x000eb26e     Device Boot      Start         End      Blocks   Id  System/dev/nbd100p1   *        2048      616447      307200   83  Linux/dev/nbd100p2          616448  1011460095   505421824   8e  Linux LVM$ mount /dev/nbd100p1 /mnt/disk# =====fix the partition table=========================# if you see below errormount: special device /dev/nbd100p1 does not exist# if kpartx is not working, try partx -a /dev/nbd100$ kpartx -a /dev/nbd100# =====fix the partition table=========================$ lsblk NAME                MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT...nbd100               43:100  0   500G  0 disk ├─nbd100p1          253:0    0   300M  0 part └─nbd100p2          253:1    0   482G  0 part   ├─centos_dev-root 253:2    0   100G  0 lvm    ├─centos_dev-home 253:3    0   300G  0 lvm    ├─centos_dev-swap 253:4    0    32G  0 lvm    └─centos_dev-var  253:5    0    50G  0 lvm# mount this$ mount /dev/centos_dev/root /mnt/disk$ umount /mnt/disk$ qemu-nbd -d /dev/nbd100# =========================another way: guestmount===================# NOTE: this will start a micro vm(qemu-kvm)$ guestmount -a disk.qcow2 -i --ro /mnt/disk$ guestumount /mnt/disk# =========================another way: guestmount===================

lsattr and chattra: append only; 系统只允许在这个文件之后追加数据，不允许任何进程覆盖或截断这个文件。如果目录具有这个属性，系统将只允许在这个目录下建立和修改文件，而不允许删除任何文件。c: compressed; 系统以透明的方式压缩这个文件。从这个文件读取时，返回的是解压之后的数据；而向这个文件中写入数据时，数据首先被压缩之后才写入磁盘。d: no dump; 在进行文件系统备份时，dump程序将忽略这个文件。i: immutable; 系统不允许对这个文件进行任何的修改。如果目录具有这个属性，那么任何的进程只能修改目录之下的文件，不允许建立和删除文件。j: data journalizing; 如果一个文件设置了该属性，那么它所有的数据在写入文件本身之前，写入到ext3文件系统日志中，如果该文件系统挂载的时候使用了”data=ordered” 或”data=writeback”选项。当文件系统采用”data=journal”选项挂载时，所有文件数据已经记录日志，因此这个属性不起作用。仅仅超级用户或者拥有CAP_SYS_RESOURCE能力的进程可以设置和删除该属性。s: secure deletion; 让系统在删除这个文件时，使用0填充文件所在的区域。t: no tail-merging; 和其他文件合并时，该文件的末尾不会有部分块碎片(为支持尾部合并的文件系统使用)。u: undeletable; 当一个应用程序请求删除这个文件，系统会保留其数据块以便以后能够恢复删除这个文件。A: no atime updates; 告诉系统不要修改对这个文件的最后访问时间D: synchronous directory updates; 任何改变将同步到磁盘；这等价于mount命令中的dirsync选项：S: synchronous updates; 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘。T: top of directory hierarchy; 如果一个目录设置了该属性，它将被视为目录结构的顶极目录

config nameserverThere are two ways to configure it, one shot or permanently
# one shotedit  /etc/resolv.conf directly# Note: this file is dynamically created, if networking service restart or host reboots, its content will be overwritten# you can also change the file &#x27;attr&#x27; make it readonly to prevent rewriting$ chattr +i /etc/resolve.conf$ lsattr /etc/resolve.conf

config static address# Centos7# edit /etc/sysconfig/network-scripts/ifcfg-eth0HWADDR=52:54:00:12:34:56TYPE=EthernetBOOTPROTO=noneIPADDR=172.17.0.2NETMASK=255.255.255.0GATEWAY=172.17.0.1DNS1=172.17.0.1DNS2=8.8.8.8DEFROUTE=yesIPV6INIT=noNAME=eth0DEVICE=eth0ONBOOT=yesUUID=fd2fe799-fd63-4de9-b8db-773eecf9f8e0

grep&#x2F;egrep&#x2F;rgrep&#x2F;zgrep options# egrep: support basic regular pattern and extended pattern like + etc# grep : grep -E to use extended pattern and grep -v to search recursively# rgrep: recursively search# zgrep: like grep but support search .gz without extract content-a             treat binary as text-C | A | B num show NUM lines before/after/center the matching line-n             show line number-l             list file name that contains the content-H             list file name and line number for matching-v             list lines tht NOT matches-r             recursively search subdir as well without follow symbolic links-R             recursively search subdir as well with following symbolic links-o             Print only the matched (non-empty) parts of a matching line, with each such part on a separate output line, print matched part not the whole line!-E             extended pattern# better to add alias to your bashrc# alias grep=&#x27;grep -anH&#x27;$ grep -anH -C &quot;home&quot; text.txt$ grep -anH -C &quot;home&quot; *.txt# use pattern, must quote &#x27;ho+&#x27; to prevent the shell from erroneously replacing that# pattern with a list of filenames that match the pattern$ grep -anHE -C &quot;ho*&quot; *.txt$ grep &quot;home&quot; *.txt # only *.txt file at current level$ grep -r &quot;home&quot; *.txt # all .txt files and subdir$ grep -v &quot;Home&quot; text.txt # list unmatched lines!!!# grep chinese character$ grep -P &#x27;[\p&#123;Han&#125;]&#x27; *.txt# show lines with ipv4 address$ grep -E &#x27;[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;&#x27; ip.txt# show matched ipv4 address only$ grep -o -E &#x27;[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;&#x27; ip.txt# grep for multiple patterns$ grep -E &#x27;pattern1|pattern2&#x27; *.py$ egrep &#x27;pattern1|pattern2&#x27; *.py

show current runlevel# show current runlevel$ runlevel2

manage serviceservice management
# lists the state of services controlled by System V and upstart$ service --status-alloutput format:+   running-   stopped?   unknown(managed by upstart)

write a system V service/etc/init.d/dhcp----service name ==dhcp# run it at init by create symbol link to it on each runlevel# like /etc/rc3.d/S20dhcp (will boot dhcp when starts on runlevel 3)# dhcp file#!/bin/sh# dhcp#   Start/Stop dhcp#### BEGIN INIT INFO# Provides:          dhcp# Required-Start:    $network $local_fs# Required-Stop:     $network $local_fs# Default-Start:     2 3 4 5# Default-Stop:      0 1 6# Short-Description: dhcp Server### END INIT INFOstart() &#123;...&#125;stop() &#123;...&#125;## main#case &quot;$1&quot; in  start)        start        ;;  stop)        stop        ;;  status)        if [ -n &quot;$(pidof -xs &quot;$&#123;PROG&#125;&quot;)&quot; ] ; then            echo &quot;$&#123;PROG_TAG&#125; is running&quot;            exit 0        else            echo &quot;$&#123;PROG_TAG&#125; is not running&quot;            # Returning 3 like everyone else.            exit 3        fi        ;;  restart)        stop        start        ;;  clear)        stop        clear        ;;  *)        echo $&quot;Usage: $0 &#123;start|stop|restart|status&#125;&quot;        exit 3esac

write a upstart service(ubuntu18)/etc/init/dhcp.conf---service name==dhcp# !!!plain txt, but with upstart format!!!# dhcp.conf file# UpStart service config# details in http://upstart.ubuntu.com/cookbook/description &quot;dhcp server&quot;# Runlevelstart on runlevel [345]stop on runlevel [!345]# Umaskumask 0007# Core limitlimit core unlimited unlimited# Expect the process executed to call fork# fork: exactly once.# daemon: exactly twice.expect fork# Respawn the job up to 3 times within a 5 second period.# If the job exceeds these values, it will be stopped and# marked as failed.respawnrespawn limit 3 5# Redirect console to /var/log/upstart/*.logconsole log# Hookspre-start script    echo &quot;# Starting at $(date -u +&#x27;%Y-%m-%dT%H:%M:%S.%NZ&#x27;)&quot;    # Create run time file directory    mkdir -m 0755 -p /var/run/dhcpend scriptpost-start script    echo &quot;# Started at $(date -u +&#x27;%Y-%m-%dT%H:%M:%S.%NZ&#x27;)&quot;end scriptpre-stop script    echo &quot;# Pre-Stopping at $(date -u +&#x27;%Y-%m-%dT%H:%M:%S.%NZ&#x27;)&quot;end scriptpost-stop script    echo &quot;# Stopped at $(date -u +&#x27;%Y-%m-%dT%H:%M:%S.%NZ&#x27;)&quot;end script# Start the processexec start-stop-daemon --start --oknodo --chuid dhcp --group dhcp --umask 0007 --exec /opt/bin/dhcp-s -f /config/dhcp.cfg# comment the above line, then run it from shell, run it in foreground!!!$/opt/bin/dhcp-s -f /config/dhcp.cfg

write systemd service&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system is the path for service from rpm, pkg etc, so it’s common, if you want to add proxy due to your company, you should put specific conf for that service at &#x2F;etc&#x2F;systemd&#x2F;system.

systemd service from pkg: &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system
systemd service conf and local service not from pkg: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;

keywords

After&#x3D;: only checks if the unit is activated already, and does not explicitly activate the specified units.
Requires&#x3D;: The units listed in Requires are activated together with the unit. If any of the required units fail to start, the unit is not activated.
Wants&#x3D;: This directive is similar to Requires&#x3D; , but less strict. Systemd will attempt to start any units listed here when this unit is activated. If these units are not found or fail to start, the current unit will continue to function. This is the recommended way to configure most dependency relationships.

service daemon mode or notAs systemd must get the exit code to determine if the service starts or not, hence if service can NOT run in daemon mode, it must runs in background!!!, otherwise systemd can NOT know if it starts or not and always restart it again and again.
[Unit]Description=TestAfter=network-online.targetWants=network-online.target[Service]Type=simple# runs in backgroundExecStart=web.py &amp;Restart=on-failure[Install]WantedBy=default.target

[Unit]Description=TestAfter=network-online.targetWants=network-online.target[Service]Type=simple# run in daemon modeExecStart=app.py -SRestart=on-failure[Install]WantedBy=default.target
systemd parameters detail
simple systemd service named: onedrive.service
[Unit]Description=OneDrive Free ClientDocumentation=https://github.com/abraunegg/onedrive# After multiple service# After=network-online.target abc.serviceRequires=xyz.serviceWants=network-online.target[Service]Environment=BAR=default_bar_valueEnvironmentFile=/etc/conf.d/fooserviceEnvironment=&quot;HTTPS_PROXY=http://ip.address:port&quot;ExecStart=/usr/local/bin/onedrive --monitor# restart on failureRestart=on-failure# sleep time before restart# RestartSec=3[Install]WantedBy=default.target

# show all active systemd service$ systemctl --type=service# show all active/inactive systemd service$ systemctl --type=service --all# Reload the service files to include the new service.$ systemctl daemon-reload# Start your service$ systemctl start your-service.service# To check the status of your service$ systemctl status example.service# To enable your service on every reboot$ systemctl enable example.service# To disable your service on every reboot$ systemctl disable example.service

Set ENV for systemd service by two ways, inline or from file
[Service]Environment=FOO=default_foo_valueEnvironment=BAR=default_bar_valueEnvironmentFile=/etc/conf.d/fooserviceExecStart=/usr/bin/fooservice
later on to check the env of a given process by /proc/19571/environ
systemd restartsystemd services have start rate limiting enabled by default. If service is started more than StartLimitBurst times in StartLimitIntervalSec seconds is it not permitted to start any more. This parameters are inherited from DefaultStartLimitIntervalSec(default 10s) and DefaultStartLimitBurst(default 5) in /etc/systemd/system.conf, but you can disable start rate like this.
[Service]Restart=always# time to sleep before restarting a serviceRestartSec=1[Unit]# StartLimitIntervalSec in recent systemd versionsStartLimitInterval=0# StartLimitIntervalSec=0
systemd service parameterIn some case, service file can use variable that’s defined outside, like this/usr/lib/systemd/system/crond.service
[Unit]Description=Command SchedulerAfter=auditd.service systemd-user-sessions.service time-sync.target[Service]EnvironmentFile=/etc/sysconfig/crondExecStart=/usr/sbin/crond -n $CRONDARGSExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failureRestartSec=30s[Install]WantedBy=multi-user.target

like $CRONDARGS and $MAINPID where are they defined, actually, they defined at/etc/sysconfig/crond must has the same name(crond) without service as suffix, systemd daemon will read this file and pass variable to service.
# Settings for the CRON daemon.# CRONDARGS= :  any extra command-line startup arguments for crondCRONDARGS=

debug systemd servicecheck stdout stderr log for systemd service  
# -b mean since recent boots$ sudo journalctl --no-pager -b -u docker.service# -p: Filter output by message priorities#   &quot;emerg&quot; (0), &quot;alert&quot; (1), &quot;crit&quot; (2),#   &quot;err&quot; (3), &quot;warning&quot; (4), &quot;notice&quot; (5), #   &quot;info&quot; (6), &quot;debug&quot; (7). # If a single log level is specified, all messages with this log level or a lower (hence more important) log level are shown.$ sudo journalctl --no-pager -b -u docker.service -p 4# with time range(relative and absolute)# last two hours ago$ sudo journalctl --no-pager -u docker.service -p 4 -S -2h# last 15 minutes ago$ sudo journalctl --no-pager -u docker.service -p 4 -S -15m$ TM=$(date &quot;+%H:%M:%S&quot;) # wait a while$ sudo journalctl -k -p err --no-pager --since=$TM# since 9am$ journalctl --since 09:00# time range$ journalctl --since &quot;2015-01-10&quot; --until &quot;2015-01-11 03:00&quot;# show journal disk usage$ sudo journalctl --disk-usage

change systemd log level to debug&#x2F;etc&#x2F;systemd&#x2F;system.conf
[Manager]LogLevel=debug

check boot order of all systemd services
$ systemd-analyze plot &gt; something.svg

auto mount# Centos7$ sudo yum install -y autofs# edit  /etc/auto.master, make sure, create /nfs/nfs  /etc/auto.nfs# create /etc/auto.nfs with below content for linux servershared -fstype=nfs4,rw   serverIP:/home/shared#no password, default root user# for windows servershared -fstype=cifs,rw,uid=jason,gid=jason,username=xxx,password=xxx ://$server/shared# mounted by jason:jason with user/password# restart the service$ sudo service autofs restart# debug autofs$ service autofs stop$ automount -d -f -v

configure samba serverLinux export dir(run samba server), then Windows connects it with samba client.Centos7$ yum install samba samba-client$ systemctl start smb.service$ systemctl start nmb.service$ systemctl enable smb.service$ systemctl enable nmb.serviceadd samba user$ sudo smbpasswd -a user1 (user1 must be a system user as well!!!)edit /etc/samba/smb.conf with below content# A publicly accessible directory, but read only, except for people in# the &quot;staff&quot; group[global]log level = 5[share]comment = public sharepath = /home/sharepublic = yeswritable = noprintable = novalid users = user1 user2sudo service smb restart# check exported dir by samba serversmbclient -L 192.168.56.101Enter SAMBA\root&#x27;s password:         Sharename       Type      Comment        ---------       ----      -------        onedrive        Disk      one drive        IPC$            IPC       IPC Service (Samba 4.10.16)Reconnecting with SMB1 for workgroup listing.        Server               Comment        ---------            -------        Workgroup            Master        ---------            -------        SAMBA                DEV# dump the samba configtestparm  -v# run in forground with level 3smbd -i -d3# check samba log:   /var/log/samba# check db cache at: /var/lib/samba/
share file between linux and linux by nfsthrough mount command,you can mount remote directory(linux) to the local machine over network. make sure the kernel supports nfs(server) before you use &#x27;mount&#x27; command. by default Ubuntu disables nfs.On server sidesteps 1: install nfs if it is not installed         Ubuntu18         $sudo apt-get install nfs-kernel-server         Centos7         $yum install -y nfs-utilssteps 2: server exports a directory, edit /etc/exports file with something like this:        /home/share/  192.168.1.20(rw,insecure,sync,all_squash)        /home/share/  192.168.1.20(rw,insecure,sync,all_squash,anonuid=100,anongid=101)        /home/share/  192.168.1.20(rw,insecure,sync,no_root_squash)        /home/share/  192.168.1.20(rw,insecure,sync,no_root_squash,anonuid=100,anongid=101)        (override the attributes for clinet&lt;--&gt;192.168.1.20 )[   nfsd bases its access control to files on the server machine on the uid and gid provided in each NFS RPC request. The normal behavior a user would expect is that she can access her files on the server just as she would on a   normal file system. This requires that the same uids and gids are used on the client and the server machine. This is not always true, nor is it always desirable.   Very often, it is not desirable that the root user on a client machine is also treated as root when accessing files on the NFS server. To this end, uid 0 is normally mapped to a different  id:  the  so-called  anonymous  or   nobody uid. This mode of operation (called `root squashing&#x27;) is the default, and can be turned off with no_root_squash.   By default, exportfs chooses a uid and gid of 65534 for squashed access. These values can also be overridden by the anonuid and anongid options.  Finally, you can map all user requests to the anonymous uid by specifying the   all_squash option.   Here&#x27;s the complete list of mapping options:   root_squash          Map requests from uid/gid 0 to the anonymous uid/gid. Note that this does not apply to any other uids or gids that might be equally sensitive, such as user bin or group staff.   no_root_squash          Turn off root squashing. This option is mainly useful for diskless clients.   all_squash          Map all uids and gids to the anonymous user. Useful for NFS-exported public FTP directories, news spool directories, etc. The opposite option is no_all_squash, which is the default setting.   anonuid and anongid          These options explicitly set the uid and gid of the anonymous account.  This option is primarily useful for PC/NFS clients, where you might want all requests appear to be from one user. As an  example,  consider  the          export entry for /home/joe in the example section below, which maps all requests to uid 150 (which is supposedly that of user joe).steps 3: export  the list         #sudo exportfs -asteps 4: restart service         Ubuntu18         #sudo /etc/init.d/portmap restart         #sudo /etc/init.d/nfs-kernel-server restart         Centos7         $ systemctl start nfs-server         $ systemctl enable nfs-serverOn client sidecheck the export list         #showmount -e $server_ipsteps 5: client mounts the dieretory         #sudo mount -t nfs serverip:/home/share /media/share(don&#x27;t support user and password, create a user belongs to other group,so the permission of this user is determinedby the configuration of /etc/exports ) 首先mount后查看mount后的用户和组,然后登陆到export机器上查看该文件对该用户的权限,并作相应的更改 保证mount后的文件具有写权限  1.export中允许挂载后可以写【必须】 2.该文件对other 用户本身具有写权限【必须】 3.设置挂载点的权限，保证执行挂载的用户可以对该目录具有执行权限. 4.在mount 命令中加入 -o rw 参数【如果需要】 Permission denied when write on nfs dir, even export with RW 1. check the mount user at client, if it&#x27;s root, make sure export with no_root_squash set. 2. check /etc/exports to see RW is set.  if you have anything wrong, please restart the serviceB: show what nfs server exports to client!!!    #showmount -e remote_nfs_server    #showmount -e    show what my self exported by nfsC: FAQ   &quot;mount.nfs: access denied by server while mounting&quot;   1 illegal port (client uses port larger than 1024)   cat /var/log/messages | grep mount   Jan 2 12:49:04 localhost mountd[1644]: refused mount request from 192.168.0.100 for /home/nfsshare/ (/home/nfsshare): illegal port 1689   FIX: add &#x27;insecure&#x27; at server and restart nfs server   2 client and server use different nfs version   Try use other version to connect with server  -o &quot;vers=3&quot;   mount -v -o &quot;vers=3&quot; -t nfs 10.117.7.201:/root/jason /media   3 client must have the right to local mount point like /media

run your task daily, weekly etcuse cron daemon to execute schedule job, there are two ways you can choose to run your taskone is user predefined period like daily, weakly, minutes etc, the other is defined own schedule whenever you want.
cron online tester
$ crontab filename [-u user]    用指定的文件替代(用户)目前的crontab。$ crontab -1[-u user]        列出(用户)目前的crontab.$ crontab -e[-u user]        编辑(用户)目前的crontab.$ crontab -ri[-u user]        删除(用户)目前的crontab.文件格式基本格式 :*　　*　　*　　*　　*　[username] command分　 时　 日　 月　 周　 命令# ┌───────────── minute (0 - 59)# │ ┌───────────── hour (0 - 23)# │ │ ┌───────────── day of the month (1 - 31)# │ │ │ ┌───────────── month (1 - 12)# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;# │ │ │ │ │                                   7 is also Sunday on some systems)# │ │ │ │ │# │ │ │ │ │# * * * * **	any value,	value list separator-	range of values/	step values第1列表示分钟1～59 每分钟用*或者 */1表示第2列表示小时1～23（0表示0点）第3列表示日期1～31第4列表示月份1～12第5列标识号星期0～6（0表示星期天）第6列要运行的命令crontab文件的一些例子：/*脚本的地址必须是绝对地址*/30 21 * * * /usr/local/etc/rc.d/lighttpd restart   &gt;&gt; /var/log/root_crontab.log 2&gt;&amp;1上面的例子表示每晚的21:30重启apache。45 4 3,8,13,18,23,28 * * /usr/local/etc/rc.d/lighttpd restart45 4 1,10,22 * * /usr/local/etc/rc.d/lighttpd restart上面的例子表示每月1、10、22日的4 : 45重启apache。10 1 * * 6,0 /usr/local/etc/rc.d/lighttpd restart上面的例子表示每周六、周日的1 : 10重启apache。0,30 18-23 * * * /usr/local/etc/rc.d/lighttpd restart上面的例子表示在每天18 : 00至23 : 00之间每隔30分钟重启apache。0 23 * * 6 /usr/local/etc/rc.d/lighttpd restart上面的例子表示每星期六的11 : 00 pm重启apache。* */1 * * * /usr/local/etc/rc.d/lighttpd restart每一小时重启apache* 23-7/1 * * * /usr/local/etc/rc.d/lighttpd restart晚上11点到早上7点之间，每隔一小时重启apache0 11 4 * mon-wed /usr/local/etc/rc.d/lighttpd restart每月的4号与每周一到周三的11点重启apache0 4 1 jan * /usr/local/etc/rc.d/lighttpd restart一月一号的4点重启apacheNote:路径必须是绝对路径！！Another way is use cron daemon cron is a daemon, which execute jobs under /etc hourly/weekly/monthly etc$ ps -ef | grep cron (run as root)$ ls /etc/cron*cron.d/       cron.daily/   cron.hourly/  cron.monthly/ crontab       cron.weekly/$ service cron restart

write cron file under one of them, restart cron service to reload it
Note:

file under cron.d&#x2F; and crontab must use the above format and [username] is a must for these two
file under cron.xx just write with command, no time as xx means the time, no username is needed as it runs as root
for daily&#x2F;weekly&#x2F;monthly, the exact time is determined by /etc/anacrontab refer to anacrontab format.

See anacron(8) and anacrontab(5) for details.SHELL&#x3D;&#x2F;bin&#x2F;shPATH&#x3D;&#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;binMAILTO&#x3D;root
the maximal random delay added to the base delay of the jobsRANDOM_DELAY&#x3D;45the jobs will be started during the following hours onlySTART_HOURS_RANGE&#x3D;3-22
#period in days   delay in minutes   job-identifier   command1	5	cron.daily		nice run-parts &#x2F;etc&#x2F;cron.daily7	25	cron.weekly		nice run-parts &#x2F;etc&#x2F;cron.weekly@monthly 45	cron.monthly		nice run-parts &#x2F;etc&#x2F;cron.monthly&#96;

debug cron  

some checks

username is needed for jobs in cron.d and &#x2F;etc&#x2F;crontab are system jobs.
when username is used, should load user env from profile and set PATH for python, otherwise, cron has nothing with env(like $PATH set by profile not seen by cron)
30 21 * * *  user1 $HOME/.profile; /usr/local/etc/rc.d/lighttpd restart


Redirect console log, it’s usually for error. 30 21 * * * /usr/local/etc/rc.d/lighttpd restart   &gt;&gt; /var/log/root_crontab.log 2&gt;&amp;1

Check log $grep CRON /var/log/syslog OR $journalctl --no-pager -b -u crond.service

Make sure cron is running $ps -ef | grep cron

Check that the filename should NOT have an extension!!!

Ensure the file has execute permissions

Tell the system what to use when executing your script (eg. put #!&#x2F;bin&#x2F;sh at top)

PATH is restricted to &#x2F;bin:&#x2F;usr&#x2F;bin, if cmd not there, use absolute path


configure your own dns server by bind application1 steps of configuring bind9    1.1 edit /etc/bind/name.conf.local with the content below                zone &quot;example.com&quot; &#123;                        type master;                        file &quot;/var/cache/bind/db.example.com&quot;;                        allow-update &#123;any;&#125;;                        allow-transfer &#123;any;&#125;;                        allow-query &#123;any;&#125;;                &#125;;                zone &quot;0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.2.ip6.arpa&quot; &#123;                        type master;                        file &quot;/var/cache/bind/db.rev.2000&quot;;                        allow-update &#123;any;&#125;;                        allow-transfer &#123;any;&#125;;                        allow-query &#123;any;&#125;;                &#125;;    1.2 #sudo mkdir /var/cache/bind        #sudo chown bind:bind /var/cache/bind        #edit /var/cache/bind/db.example.com with content below            $ORIGIN .            $TTL 604800     ; 1 week            example.com             IN SOA  ns1.example.com. root.example.com. (                                            2006020222 ; serial                                            604800     ; refresh (1 week)                                            86400      ; retry (1 day)                                            2419200    ; expire (4 weeks)                                            604800     ; minimum (1 week)                                            )                                    NS      ns1.example.com.            $ORIGIN example.com.            ns1                     AAAA    2000::1        #edit /var/cache/bind/db.rev.2000 with content below            $ORIGIN .            $TTL 10800      ; 3 hours            0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.2.ip6.arpa IN SOA ns1.example.com.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.2.ip6.arpa. root.example.com. (                                            2002111312 ; serial                                            28800      ; refresh (8 hours)                                            14400      ; retry (4 hours)                                            3600000    ; expire (5 weeks 6 days 16 hours)                                            86400      ; minimum (1 day)                                            )                                    NS      ns1.example.com.            $ORIGIN 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.2.ip6.arpa.2 update AAAA using nsupdate    #nsupdate    &gt;server 2000::1(specify the dns server)    &gt;update add worf.example.com 7200 IN AAAA 2000::58(zone example.com must be configured at dns server)    &gt;send (send the update)3 query AAAA record    #dig @2000::1(dns server) worf.example.com AAAA

put file in memoryFor faster run, you can put files in memory, create a mount point with tmpfs, then put files there, but files are lost when it’s umounted or system boots, actually, you do not need to create this ram disk by yourself, system already creates somes for you, it’s &#x2F;run, &#x2F;dev&#x2F;shm
(base) [root@dev]# df -hFilesystem               Size  Used Avail Use% Mounted ondevtmpfs                 3.9G     0  3.9G   0% /devtmpfs                    3.9G     0  3.9G   0% /dev/shmtmpfs                    3.9G  9.0M  3.9G   1% /run

run script at current shell not subshellroot@dev:/tmp# cat test.sh#! /usr/bin/env bashexport  http=1.0root@dev:/tmp# source test.shroot@dev:/tmp# echo $http1.0# likes you type export http=1.0 from the terminal!

dos and unix formatwhen you work both on windows and unix(linux), move files between them, be careful! as the file content may be changed due to os
linux和windows，Mac的行结束符是不同的，因此有些编辑器会无法正确的显示换行，这是编辑器 没有正确的处理行结束符导致的，但是有些强大的编辑器就能正确的显示不同文件类型， 无论是windows文件 还是unix，mac文件，当遇到编辑器无法正确显示的时候.

首先确定文件的类型，在linux下运行file命令
看编辑器是否支持该类型
根据需要通过editplus–&gt;document—&gt;file format选项转化为能正确显示的文件类型。

• Window (CR&#x2F;LF)• Unix (LF)• Mac (CR)
$ dos2unix dos_format_file

中文乱码的原因分析因此解决乱码的关键分三步1.明确文件的编码格式2.系统支持(安装）了这种编码格式3.为应用程序设置此编码格式乱码的原因就是编码格式的选择不对或者不兼容，使得解析错误！因此解决办法就是选择正确的编码格式，任何应用程序都是调用系统默认或者本程序设定的编码格式，最终的都是调用系统安装的编码格式,去解析文件,中文乱码的解决1 显示系统支持的所有编码格式    $ locale -a    $ sudo apt-get install language-pack-zh-hant language-pack-zh-hans    (如果没有zh_CN 下载安装)2 如果不支持中文编码格式，修改    sudo vi /var/lib/locales/supported.d/local    加入    zh_CN.GB18030 GB18030    zh_CN.GBK GBK    zh_CN.GB2312 GB2312    zh_HK.BIG5 BIG5    zh_TW.BIG5 BIG5    然后执行    #sudo locale-gen    check all encodings system support:    $ locale -a    zh_CN.gb18030    zh_CN.gb2312    zh_CN.gbk    zh_CN.utf8    zh_HK.big5    zh_HK.utf8    zh_SG.utf8    zh_TW.big5    zh_TW.utf83 为各种应用设置其编码格式gbk，或者gb2312    ****step 1 and step 2 are a must for 中文乱码***    3.1 但是通过远程终端登录后，打开文件还是显示乱码    这是由于终端也有其编码格式，因此需要设置终端的编码格式为gbk或者gb2312，或者其他的编码格式，使其与文件内容的编码格式对应起来，具体设置方法参考终端的设置选项。    以上是常见的中文乱码的情况。    3.2 当远程登录后，英文乱码，出现？？等情况，这是由于终端的编码格式和系统的编码格式不兼容导致的    因此通过locale查看系统的当前编码格式，然后设置终端的编码格式与其兼容就可以解决问题了。    3.3 mount(from windows)的中文文件名字显示 ?? 设置mount的iocharset 为utf-8    set iocharset to utf8 for mount option    -fstype=cifs,rw,iocharset=utf8,uid=ubuntu,gid=ubuntu,

vncserver设置vncserver的分辨率，一种是修改配置文件，一种是启动vncserver的时候 通过参数传递，现在第二种方式更灵活，因此这里只介绍第二种方法：#vncserver -geometry 1680x1050 -alwaysshared :1这个命令包含了3个参数，第一个参数geometry表示分辨率，你可以指定适合自己桌面的分辨率；第二个参数alwaysshared表示运行多个人同时连接到同一个桌面；第三个参数:1表示开启一个1号远程桌面(如果该编号已经被占用，请选择其他的标号）此时你可以使用ip:1来连接这个远程桌面。(注意：上面1280与800之间的连接符为英文字母x(xyz的x)，而不是中英文的乘号!)关闭一个远程桌面，可以使用如下命令：$ vncserver -kill :1 其中，&quot;:1&quot;表示的创建远程桌面时的桌面号
ssh relatedinstall sshd
Centos7$ yum install -y openssh-server# ssh client timeout after sometime# edit sshd config and restart sshd service# edit /etc/ssh/sshd_configClientAliveInterval 60$ service sshd restart# ssh terminal timeout due to “timed out waiting for input: auto-logout”# edit ~/.bashrcunset TMOUT


Reset by peer even iptable and firewall is disabled
$ grep sshd /var/log/syslogOct 16 08:59:45 openstack sshd[1214]: error: Could not load host key: /etc/ssh/ssh_host_rsa_keyOct 16 08:59:45 openstack sshd[1214]: error: Could not load host key: /etc/ssh/ssh_host_dsa_keyOct 16 08:59:45 openstack sshd[1214]: error: Could not load host key: /etc/ssh/ssh_host_ecdsa_key# fix it(Ubuntu18)$ sudo rm -r /etc/ssh/ssh*key$ sudo dpkg-reconfigure openssh-server

only allow specific user to login
Edit /etc/ssh/sshd_configadd the statementAllowUsers lzq

long delay when login with ssh
try:ssh -o GSSAPIAuthentication=no user@yourserverupdate /etc/ssh/sshd_configUseDNS no

allow root user login with password by ssh
open /etc/ssh/ssd_configChange    PermitRootLogin prohibit-password    PasswordAuthentication noTo    PermitRootLogin yes    PasswordAuthentication yes

specify user when connect with ssh server
$ ssh root@server# OR$ ssh server -l root

disable openssl popup window when use git
$ unset SSH_ASKPASS

ssh with password on command line
# Centos7$ yum install -y sshpass$ sshpass -p your_password ssh user@hostname# change user password without prompt$ echo -e -n &quot;oldpasswd\nnewpasswd\nnewpasswd&quot; | sudo passwd user# OR$ echo &#x27;userid:newpasswd&#x27; | sudo chpasswd

launch GUI at ssh server by terminal
ssh server sideenable X11 on sshd by edit /etc/ssh/sshd_configX11Forwarding yesssh clientWhile doing ssh use the option -X to enable X11 forwarding.$ ssh username@hostname -XOREnable trusted X11 forwarding, by using the -Y option,$ ssh username@hostname -Y

run scp&#x2F;ssh&#x2F;sftp used in scripts without password input
key point is add the client public key to server&#39;s authorized_keys
use ssh-copy-id
# On client$ ssh-keygen -t rsa  (just enter is a must)$ ssh-copy-id -f -i /home/lzq/.ssh/xx.pub root@serverip# run command on remote server by ssh$ ssh root@serverip &quot;ls;pwd&quot;# enter particular dir after ssh login$ ssh root@serverip &quot;bash --login&quot;$ ssh root@serverip &quot;cd xxx; bash --login&quot;
OR
# scp is used on client to copy data to server# on client$ mkdir -p /home/lzq/.ssh # create parent dir as well if not exist# create private and public key pairs$ ssh-keygen -t rsa  (just enter, no other input)# copy public key to ssh server$ scp /home/lzq/.ssh/id_rsa.pub  root@serverip:/root/.ssh/new.pub# on server$ cd /root/.sshif  authorized_keys exists  $ cat new.pub &gt;&gt;authorized_keys  $ rm new.pubelse   $ mv new.pub authorized_keys $ chmod 644 authorized_keys# Client$ ssh  root@serverip

user management# show all processes of a given user$ ps -u $user$ pstree $user -ap# show a process and its owner$ ps u $pid# show all processes of given group$ ps -g $user# show all groups from local and remote LDAP$ getent group# show all users from local and remote LDAP$ getent passwd# add a group with given id and name at local$ groupadd [-g 6002] school# create a system group$ groupadd -r $group_name# delete a group from local$ groupdel $group_name# show groups for current user$ groups# show groups for a given user$ groups jason# --- add a user to group does not effect for the given login# --- you need to login again or use $ newgrp $group# add/delete an existing user to a given group$ gpasswd -a $user $group$ gpasswd -d $user $group$ gpasswd --members $user1, $user2 $group# add user to given group$ sudo usermod -a -G root jason # add jason to root group# show id of current user$ id -ru# show current user$ id -un# show id of given user(local or remote)$ id $user1# add a new user or remove a user from local# adduser [--home DIR] [--shell SHELL] [--no-create-home] [--uid ID]  [--ingroup GROUP | --gid ID] [--disabled-password] [--disabled-login]   user# -r means deleting its home directory$ userdel -r $user# change passwd for current user$ passwd$ passwd $user
&#x2F;etc&#x2F;sudoers(Ubuntu18)it’s a file used to control what commands a user can run
$ sudo visudo

&#x2F;etc&#x2F;sudoers
# User privilege specification, root user can run all with any user on all hosts with any command!!!root       ALL  =    (ALL)                     ALL[user]     [host] =  [runas user]              commandsjorge ALL=(root) NOPASSWD: /usr/bin/find, /bin/rm# user  jorge can run as root user!!!# the command must have absolute path!!!, user jorge can run /bin/rm as root without password required.# Allow members of group sudo to execute any command after they have provided their password%sudo ALL=(ALL) ALL# Members of the admin group may gain root privileges# allow members of group admin/users to execute any command with any user on all host without password.%admin      ALL=    (ALL) NOPASSWD:          ALL%users      ALL=    (ALL) NOPASSWD:          ALL[group] [host] =    (runas user)             commands

&#x2F;etc&#x2F;sudoers example
lb 10.10.10.3 = (root) NOPASSWD: /usr/bin/find, /bin/rm# ORlb 10.10.10.1/24 = (root) NOPASSWD: /usr/bin/find, /bin/rm# lb user can run /bin/rm as root without password on jason_dev host

lb@jason_dev$sudo rm xxsudo unable to resolve host jason_dev(hostname of my machine)# take long time to show such warning# let&#x27;s see how this happens and fix it# step 1: check the user: lb, ok allowed to run /bin/rm# step 2: check host(as lb is only allowed to run on 10.10.10.3#         we must know what&#x27;s ip for current machine(jason_dev)#         check /etc/hosts, if not found#         DNSLookUp#         still not found, can&#x27;t find get the above error)# step 3: run rm with root and no password# to avoid dnslook, add entry at /etc/hosts with127.0.0.1 jason_dev

broadcast message to all users from terminal$ wall &#x27;hello everyone&#x27;# send msg to one pts$ whoroot     pts/8        2019-11-24 22:30 (10.20.6.117)root     pts/9        2019-11-25 23:59 (10.20.6.117)$ write root pts/8

copy file from local to remote ssh server# -r will follow symbol link$ scp -r username@hostB:/home/lzq/vim .$ scp -r vim username@hostB:/home/lzq/
backup file with progress bar locally
$ rsync -avP  /home/lzq/Document   /share/# skip following symbol linkrsync -a --no-links vim root@hostB:/root/# -a, --archive               archive mode; equals -rlptgoD (no -H,-A,-X)# -v, --verbose               increase verbosity# -P  --progressrsync -avP --no-links vim root@hostB:/root/
create soft link for all files under a dir$ ln -s /home/lzq/* .

cfg file for shellnon-interactive terminalbash/csh/sh  /etc/bashrc and $HOME/.bashrcinteractive terminal/etc/profile and $HOME/.profile/etc/profile vs /etc/environmentSystem-wide environment variables[/etc/environment is not part of POSIX, it belongs to PAM (Pluggable Authentication Module), and only programs compiled with PAM support are able to use it (primarily login systems, which subsequently start the shell or user environment). This means it isn&#x27;t even read by your shell.--------------------------------------------------------------------------------------------------------You can see the programs using /etc/environment with grep -l pam_env /etc/pam.d/*.So /etc/environment is used for setting variables for programs which are usually not started from a shell--------------------------------------------------------------------------------------------------------]A suitable file for environment variable settings that affect the system as a whole (rather than just a particular user) is /etc/environment. An alternative is to create a file for the purpose in the /etc/profile.d directory./etc/environmentThis file is specifically meant for system-wide environment variable settings. It is not a script file, but rather consists of assignment expressions, one per line.FOO=barNote: Variable expansion does not work in /etc/environment./etc/profile.d/*.shFiles with the .sh extension in the /etc/profile.d directory get executed whenever a bash login shell is entered (e.g. when logging in from the console or over ssh), as well as by the DisplayManager when the desktop session loads.You can for instance create the file /etc/profile.d/myenvvars.sh and set variables like this:export JAVA_HOME=/usr/lib/jvm/jdk1.7.0export PATH=$PATH:$JAVA_HOME/binOther filesWhile /etc/profile is often suggested for setting environment variables system-wide, it is a configuration file of the base-files package, so it&#x27;s not appropriate to edit that file directly. Use a file in /etc/profile.d instead as shown above. (Files in /etc/profile.d are sourced by /etc/profile.)/etc/default/locale is specifically meant for system-wide locale environment variable settings. It&#x27;s written to by the installer and when you use Language Support to set the language or regional formats system-wide. On a desktop system there is normally no reason to edit this file manually.The shell config file /etc/bash.bashrc is sometimes suggested for setting environment variables system-wide. While this may work on Bash shells for programs started from the shell, variables set in that file are not available by default to programs started from the graphical environment in a desktop session.

sys related# Centos$ /etc/centos-releaseCentOS Linux release 7.9.2009 (Core)# show cmdline of running kernel$ cat /proc/cmdline# change system time$ date --set=&quot;2017-04-11 13:21:00&quot;# change timezone from command line$ ln -s /usr/share/zoneinfo/US/Pacific /etc/localtime# change hostname permanently/etc/hostname (or /etc/HOSTNAME)# show system reboot and shutodwn event# NOTE: last read files from /var/log/wtmp or /var/log/btmp$ last -xF reboot | head -3reboot   system boot  4.14.15-1.el7.el Fri May 17 04:45:16 2024 - Thu May 23 06:24:04 2024 (6+01:38)   reboot   system boot  4.14.15-1.el7.el Fri May 10 06:37:05 2024 - Thu May 23 06:24:04 2024 (12+23:46)  reboot   system boot  4.14.15-1.el7.el Fri May 10 06:29:01 2024 - Fri May 10 06:35:46 2024  (00:06) $ last -xF shutdown# show the latest reboot time$ who -b         system boot  2024-05-17 04:45$ uptime -s2024-05-17 04:43:51# show the lastest reboot$ last -x reboot | head -1# show how lang system boots$ uptime# disable selinux$ setenforce 0 # temporary close selinuxORedit /etc/selinux/config

reset root passwordIn the event your Linux box experiences disk or file system issues you may receive a “Give root password for maintenance” prompt upon reboot. If you have your root password you can login but in the event your using ‘slide’ or ‘sudo’ for wheel access or you’ve just mis-placed your root password – you’ll need to reset it.To reset your root password:    1. When the GRUB loader shows during boot press the spare bar to pause boot.    2. Select your boot kernel.    3. Type ‘e’ to edit the default kernel line.    4. Type ‘e’ again on the line that starts with ‘kernel’.    5. Add ‘init=/bin/bash’ to the end of the ‘kernel’ line then press enter.    6. Type ‘b’ to boot the modified kernel parameters.    7. Once you’re at the /bin/bash prompt you will need to remount the root file system as read/write in order to edit the passwd file:    # mount -o remount,rw /    8. Change your root password:    # passwd root    9. Remount the filesystem back to read only:    # mount -o remount,ro /    10. Reboot your server using CTR-ALT-DELETE.    11. You will now be able to login with your new root password and carry out the maintenance.

disable boot from save grub entry for grub2Ubuntu18GRUB_DEFAULT=savedGRUB_SAVEDEFAULT=truetoGRUB_DEFAULT=0(always first entry)GRUB_SAVEDEFAULT=falsethen generate /boot/grub2/grub.cfg newly#grub2-mkconfig -o /boot/grub2/grub.cfg

force enter command-line mode when boots$sudo vi /etc/default/grubDo below changes:Comment the line     GRUB_CMDLINE_LINUX_DEFAULT=”quiet splash”, by adding # at the beginning, which will disable the Ubuntu purple screen.Change               GRUB_CMDLINE_LINUX=”” to GRUB_CMDLINE_LINUX=”text”, this makes Ubuntu boot directly into Text Mode.Uncomment this line #GRUB_TERMINAL=console, by removing the # at the beginning, this makes Grub Menu into real black &amp; white Text Mode (without background image)Ubuntu18$ sudo update-grubCentos7$ grub2-mkconfig

enter single-user mode when boots开机进入grub时，用上下键移到第二行的恢复模式，按e（注意不是回车）把ro single 改成rw single init=/bin/bash ， 然后按ctrl+x，就可以进入 单用户模式，完成后，按”ctrl+alt+delete“组合神键重启passwd: Authentication token manipulation error进入单用户模式后， filessystem may be mounted with readonly， so remount it againmount -rw -o remount /ORmount -o remount, rw /

&#x2F;etc&#x2F;fstab file formatWhat is fstab file?fstab is a configuration file that contains information of all the partitions and storage devices in your computer. The file is located under /etc, so the full path to this file is /etc/fstab./etc/fstab contains information of where your partitions and storage devices should be mounted and how. If you can&#x27;t access your Windows partition from Linux, aren&#x27;t able to mount your CD or write to your floppy as a normal user, or have problems with your CD-RW, you probably have a misconfigured /etc/fstab file. So, you can usually fix your mounting problems by editing your fstab file./etc/fstab is just a plain text file, so you can open and edit it with any text editor you&#x27;re familiar with. However, note that you must have the root privileges before editing fstab. So, in order to edit the file, you must either log in as root or use the su command to become root.Sample fstab file looks like this## /etc/fstab## &lt;device&gt; &lt;mountpoint&gt; &lt;filesystemtype&gt;&lt;options&gt; &lt;dump&gt; &lt;fsckorder&gt;/dev/hdb5         /                       ext2               defaults       1              1/dev/hdb2        /home                  ext2               defaults       1             2/dev/hdc         /mnt/cdrom            iso9660         noauto,ro,user     0        0/dev/hda1     /mnt/dos/c              msdos              defaults        0             0/dev/hdb1    /mnt/dos/d              msdos            defaults           0             0/dev/fd0     /mnt/floppy               ext2               noauto,user     0             0/dev/hdb4     none                     ignore             defaults          0              0none            /proc                       proc                defaults/dev/hdb3    none                        swap                swNote that this system has two IDE partitions, one which is used as /, and the other used as /home. It also has two DOS partitions which are mounted under /mnt. Note the user option provided for the cdrom, and the floppy drive. This is one of the many default parameters you can specify. In this case it means that any user can mount a cdrom, or floppy disk. Other options will be dealt with later.fstab file format explinationfstab consists of a number of lines (one for each filesystem) seperated into six fields. Each field is seperated from the next by whitespace (spaces/tabs).So from the example given previously:/dev/hdc /mnt/cdrom iso9660 noauto,ro,user 0 0 first field (/dev/hdc) is the physical device/remote filesystem which is to be described. second field (/mnt/cdrom) specifies the mount point where the filesystem will be mounted. third field (iso9660) is the type of filesystem on the device from the first field. fourth field (noauto,ro,user) is a (default) list of options which mount should use when mounting the filesystem. fifth field (0) is used by dump (a backup utility) to decide if a filesystem should be backed up. If zero then dump will ignore that filesystem. The sixth field (0) is used by fsck (the filesystem check utility) to determine the order in which filesystems should be checked.If zero then fsck won&#x27;t check the filesystem.(as the example line above is a cdrom there is very little point in doing a fsck on it, so the value is zero).File system mount optionsAs the filesystems in /etc/fstab will eventually be mounted using mount(8) it isn&#x27;t surprising that the options field simply contains a comma-seperated list of options which will be passed directly to mount when it tries to mount the filesystem.The options common to all filesystems are:sync / asyncAll I/O to the file system should be done (a)synchronously.autoThe filesystem can be mounted automatically (at bootup, or when mount is passed the -a option). This is really unnecessary as this is the default action of mount -a anyway.noautoThe filesystem will NOT be automatically mounted at startup, or when mount passed -a. You must explicitly mount the filesystem.dev / nodevPermit any user to mount the filesyste. This automatically implies noexec,exec / noexecPermit/Prevent the execution of binaries from the filesystem.suid / nosuidPermit/Block the operation of suid, and sgid bits.roMount read-only.rwMount read-write.userPermit any user to mount the filesystem. This automatically implies noexec, nosuid,nodev unless overridden.nouserOnly permit root to mount the filesystem. This is also a default setting.defaultsUse default settings. Equivalent to rw,suid,dev,exec,auto,nouser,async.There are numerous options for the specific filesystes supported by mount.However these are some of the more useful, for the full list check out the man page for `mount`.ext2check=&#123;none, normal, strict&#125;Sets the fsck checking level.debugprint debugging info on each remount.sb=nn is the block which should be used as the superblock for the fs.fatcheck=&#123;r[elaxed], n[ormal], s[trict]&#125;Not the same as ext2. Rather deals with allowed filenames. See mount man page.conv=&#123;b[inary], t[ext], a[uto]&#125;Performs DOS&lt;-&gt;UNIX text file conversions automatically. See mount man page.uid=n, gid=niso9660norockDisables Rock Ridge extensions.fstab file Supported file systems listaffs - I have know idea what this is, if anyone else does please enlighten me.coherentext - Don&#x27;t use this. ext has been superseded by ext2.ext2 - The standard Linux filesystem. (NB, this has nothing to do with extended partitions.)fat - DOS.hpfs - OS/2 High Performance File System.iso9660 - CD-ROM&#x27;s. Supports Rock Ridge extensions by default.minix - can be useful for floppy disks.msdos - Just fat with some addtional error checking.nfs - Network FileSystem. Dealt with later.proc - The process psudeo-filesystem now standard in Linux.smb - Another network filesystem. Compatable with WFW, and NT. See Samba.ufs - Unix FileSystem.unsdos - Unix filesystem on a FAT partition.vfat - MS&#x27;s kludge of FAT to provide long filenames.xenixxiafs

deep in remove file$ rm test.c# actually, kernel takes two steps to delete the file1. remove fentry(then you can NOT see test.c)2. free inode of this file if its reference count is 0Only these two steps executed, the disk space is freed# show all deleted files(not see from disk), but still opened by process$  lsof 2&gt;/dev/null | grep deletedovsdb-ser  1089          openvswitch    7u      REG              253,0        159   69471332 /tmp/tmpfVYsNnz (deleted)

what about app opens a large file, during it’s reading, someone rm that file?
Only fentry is deleted, inode is not freed, as when app opens that file,it increments the reference of that inodewhen app closes that file, at that time, it will be deleted from disk.
# check file inode$ls -i a.c b.cOR$stat a.c b.c$ mv a.c b.c# actually, fentry(b.c) points to inode of (a.c)# a.c(fentry is deleted), inode of (b.c) is freed if no use open it!!!

update file time# Update the access and modification times of each FILE to the current time$ touch  test.doc$ touch -d &quot;2010-05-31 08:10:30&quot; test.doc# or change only one$ touch -m -d &quot;2010-05-31 08:10:30&quot; test.doc$ touch -a -d &quot;2010-05-31 08:10:30&quot; test.doc
cut command# cut split string by tab by default, the first indiex if 1$ echo &quot;hello boy&quot; | cut -d &#x27; &#x27; -f 1hello

remove file named with special character$ ls#a.c  -a.c$ rm &#x27;#a.c&#x27;$ rm \#a.c$ rm ./-a.c# use eval$ eval &quot;rm =1.3.0,&quot;

display output to screen and also write it to file$ echo &quot;just a test&quot; | tee log.txt

command xargs# xargs will split parameters passed by tab or space, then call after command with splited parameter one by one$ echo &quot;hello boy&quot; | xargs rmrm: cannot remove &#x27;hello&#x27;: No such file or directoryrm: cannot remove &#x27;boy&#x27;: No such file or directory# if you see xargs: argument line too long$ xx | xargs -l1 rm # -l1 each time each parameter

find command(better use find file, not content of file)# -name  exactly match!!!# -iname case insensitive# -type  f | d | c | l(link) | b(block) | p(pipe)# -size  n[cwbkMG] # 1M, -1k# -maxdepth 1 (max depth)# -perm 755  (permission with 755)# -user jason$ find - type f -size +1k -name &quot;/*test.c&quot;# when use | for find better to use -print0$ find . -type f -print0 | xargs -0 rm -f# more exec on some file$ find . -type f -print0 -exec ls &#123;&#125; \; -exec rm -f &#123;&#125; \;$ find . -type f -print0 -exec rm -f &#123;&#125; \;# in some case it may meet error &#x27;too long parameter/argument&#x27; something like use, use find | xargs to sovle itprint0: print the full file name on the standard output, followed by a null character (instead of the new-line character that ‘-print’ uses).  This allows file names that contain  newlines  or  other  types  of white  space  to  be correctly interpreted by programs that process the find output.  This option corresponds to the ‘-0’ option of xargs.# find file with executable attribute$ find . -executable# negative$ find . ! -name &quot;*.txt&quot;# find with two or more conditions$ find . -iname class.* -o -name basic*

rename a batch of files with pattern$ rename &#x27;s/ //g&#x27; *.txt # remove space from filename$ find . -type f -exec rename &#x27;s/ //g&#x27; &#123;&#125; \; # remove space from file name only$ find . name &quot;* *&quot; -exec rename &#x27;s/ //g&#x27; &#123;&#125; \; # remove space from file name or dir name

show stats of a file$ stat a.c

copy file$ cp -r dir1 dir2 dir3/ # copy dir1, dir2(include hiden files) to dir3(under)$ cp -r dir1/* dir2/$ cp -r dir1/ dir2/ # copy all files(except hiden files) under dir1 to dir2
tar# ERROR: Exiting with failure status due to previous errors# 原来是待压缩的文件夹是root权限创建的，而执行tar的时候未加sudo# create tar and exclude some dirs for files$ tar czvf test.tar.gz test/ --exclude=&#x27;test/exclude/&#x27;# extract .xz with tar$ tar -xJf xx.tar.xz# extract .gz file$ gunzip *.gz
sleep and usleepsleep (sec);usleep (microsec)sec = 1000 milisecond 毫秒milisecond = 1000 microsecond 微秒
history command# show history command with time$ export HISTTIMEFORMAT=&quot;`whoami` : |  %F  | %T: | &quot;# run history command$ history1 cd2 ls$ !2 # equal ls# show latest N history commands$ history 10

run command with a specific user or group$ su -m jason -c &#x27;ls&#x27; # jason is a user$ sg - jason_group -c &#x27;ls&#x27; # sg group

list all alias$ alias -p

redirect shell output# redirect stdout(1) to /dev/null# and redirect stder(2) to stdout(1)$ echo &quot;hello&quot; &gt;/dev/null 2&gt;&amp;1

read one file at a time from a file# read is a keyword# line is varwhere read -r linedo    echo $linedone &lt; file

read input from standard input instead of a file$ cat &lt;&lt; ENDhello boyEND# read from std and write to a file$ cat &gt;new.txt &lt;&lt; ENDhello boyEND# END is a marker can be any

git status can’t display chinese characteredit ～/.gitconfig[core]quotepath = false

check md5 value for a file$ md5sum xx.iso

format man output and save it to a file$ man strace | col -b &gt; trace.txt



watch a command output by ‘n seconds’# by default, free unix is KB -m means MB$ watch -n 1 free -m

file encoding convertor iconv$ iconv -l  # show supported encoding$ file a.c # get file encoding$ iconv -f one_encoding -t another_encoding a.c &gt;b.c

error ‘sorry, you must have a tty to run sudo’There must be Defaults requiretty in the file you try to edit.
generate uuid$ uuidgen

formated json output from shell$ echo &#x27;&#123;&quot;hostname&quot;:&quot;test&quot;,&quot;domainname&quot;:&quot;example.com&quot;&#125;&#x27; | python -m json.tool
Add an environment variable to a command$ test=hello; echo $test
eval to run command from string$ eval &#x27;ls&#x27;

killall command not found# Centos7$ yum install -y psmisc

capture signal in shell#!/bin/bash# trap ctrl-c and call ctrl_c()trap ctrl_c INTfunction ctrl_c() &#123;        echo &quot;** Trapped CTRL-C&quot;&#125;for i in `seq 1 5`; do    sleep 1    echo -n &quot;.&quot;done

write message to &#x2F;var&#x2F;log&#x2F;syslog by shell#!/bin/bashlogger -t &quot;title&quot; &quot;message&quot;         /* use tcp socket server port 601 */logger -d &quot;title&quot; &quot;message&quot;         /* use udp socket server port 514 */logger -u /dev/log &quot;title&quot; &quot;message&quot;# [looger is a command that can write syslog]

rsyslog confIn /var/log/syslog you may see these kind of message&quot;Previous message repeated X times&quot;, where X is a numberit&#x27;s useful for supressing message, but for debugging, you can&#x27;t see it directly, turn it off by(ubuntu)edit /etc/rsyslog.conf$RepeatedMsgReduction off

wgetdownload particular files from particular website
$ wget -r -A .html http://cunit.sourceforge.net/doc/$ wget -r --no-parent -A &#x27;*.deb&#x27; http://www.xyun.com/amd64/# download all files except auto-generated index.html$ wget -r -np -R &quot;index.html*&quot; http://example.com/configs/.vim/

wget breakpoint and retry$ wget -c -t 10 $url# -c breakpoin# -t 0, number for retransmission, 0 always retry!

run a basic http server$ python -m http.server 80 #python3

scan host on local network# if you know the mac and you are on the local network# you can easy to know the ip of that host(mac)# will print IP, MAC$ sudo arp-scan --interface=eth0 --localnet# check if host(ip) opens a specific port# check RDP port(windows remote desktop port)$ sudo nmap -p 3389 $IP

resize disk(vm) after change its sizeAfer you change vm disk by client, the free space of disk can be added to part of disk to expand its size
$ sudo fdisk -l...GPT PMBR size mismatch (20971519 != 209715199) will be corrected by w(rite).-----new size 100G, old size 10G-----Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: 1C6DFAAE-5BA8-4141-A6CF-953F8D9E871F----sda1 still use 10G-----Device      Start      End  Sectors  Size Type/dev/sda1  227328 20971486 20744159  9.9G Linux filesystem/dev/sda14   2048    10239     8192    4M BIOS boot/dev/sda15  10240   227327   217088  106M EFI System$ sudo LANG=en_US.UTF-8 growpart /dev/sda 1CHANGED: partition=1 start=227328 old: size=20744159 end=20971487 new: size=209487839,end=209715167$ sudo fdisk -lDisk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: 1C6DFAAE-5BA8-4141-A6CF-953F8D9E871F----sda1 has 100G, but the new add part should format with fs---Device      Start       End   Sectors  Size Type/dev/sda1  227328 209715166 209487839 99.9G Linux filesystem/dev/sda14   2048     10239      8192    4M BIOS boot/dev/sda15  10240    227327    217088  106M EFI System$ sudo resize2fs /dev/sda1resize2fs 1.44.1 (24-Mar-2018)Filesystem at /dev/sda1 is mounted on /; on-line resizing requiredold_desc_blocks = 2, new_desc_blocks = 13The filesystem on /dev/sda1 is now 26185979 (4k) blocks long.
count line of codeThere are lots of tools for counting code, two fast and popular ones, scc and tokei
$ go get -u github.com/boyter/scc/$ scc [flags] [files or directories]$ scc .───────────────────────────────────────────────────────────────────────────────Language                 Files     Lines   Blanks  Comments     Code Complexity───────────────────────────────────────────────────────────────────────────────Go                         113     19031     2746      2181    14104       3360Markdown                    29      2701      655         0     2046          0TOML                         2        87       12        32       43          0Makefile                     1        41        6        17       18          0Python                       1       112       27         2       83         25Shell                        1        35        9         2       24          6Smarty Template              1        25        3         0       22          5Systemd                      1        19        2         0       17          0XML                          1         1        0         0        1          0gitignore                    1         5        0         2        3          0───────────────────────────────────────────────────────────────────────────────Total                      151     22057     3460      2236    16361       3396───────────────────────────────────────────────────────────────────────────────$ scc --by-file .$ scc --exclude-dir vendor .

setup tcp&#x2F;udp server with ncnc is command to setup tcp(tls)&#x2F;udp server or used as tcp&#x2F;udp client
# start tcp server$ nc -lvnc -lvNcat: Version 7.50 ( https://nmap.org/ncat )Ncat: Listening on :::31337Ncat: Listening on 0.0.0.0:31337# send tcp data with nc$ echo &quot;hello&quot; | nc localhost 31337

check virtual disk real size#  use -s option #  disk size set 10G, but real size is 133M$ ls -slh pod-23vmqfdjge-2021-09-08-21-00-07.raw133M -rw-r--r-- 1 root root 10G Sep  8 20:51 pod-23vmqfdjge-2021-09-08-21-00-07.raw
timezone and ntpdate# show current timezone$ ls -al /etc/localtimelrwxrwxrwx. 1 root root 35 Sep 14 09:33 /etc/localtime -&gt; ../usr/share/zoneinfo/Asia/Shanghai# OR use command line$ timedatectl status      Local time: Tue 2021-09-14 09:36:45 CST  Universal time: Tue 2021-09-14 01:36:45 UTC        RTC time: Tue 2021-09-14 01:36:51       Time zone: Asia/Shanghai (CST, +0800)     NTP enabled: yesNTP synchronized: no RTC in local TZ: no      DST active: n/a# show all available zones$ timedatectl list-timezones# set correct zone$ timedatectl set-timezone Asia/Shanghai# ntpdate will update time with ntpserver to the zone you&#x27;re using# if you set zone incorrect, the time you get may be not expected.$ ntpdate cn.pool.ntp.org14 Sep 09:34:01 ntpdate[1524]: adjust time server 124.108.20.1 offset 0.028670 sec# without ntp, set time by date directly, note for web GMT(UTC) is returned!!!# so must with &#x27;Z&#x27; suffix to convert to local time zone$ date -s &quot;$(curl -H&#x27;Cache-Control:no-cache&#x27; -sI baidu.com | grep &#x27;^Date:&#x27; | cut -d&#x27; &#x27; -f3-6)Z&quot;# use proxychains for net proxy$ date -s &quot;$(proxychains -q curl -H&#x27;Cache-Control:no-cache&#x27; -sI baidu.com | grep &#x27;^Date:&#x27; | cut -d&#x27; &#x27; -f3-6)Z&quot;# use proxychains for net proxy$ date -s &quot;$(proxychains -q curl -H&#x27;Cache-Control:no-cache&#x27; -sI google.com | grep &#x27;^Date:&#x27; | cut -d&#x27; &#x27; -f3-6)Z&quot;

ps aux$ ps -auxUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot         1  0.0  0.0 193888  7052 ?        Ss   08:12   0:01 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root         2  0.0  0.0      0     0 ?        S    08:12   0:00 [kthreadd]root         4  0.0  0.0      0     0 ?        S&lt;   08:12   0:00 [kworker/0:0H]root         6  0.0  0.0      0     0 ?        S    08:12   0:00 [ksoftirqd/0]root         7  0.0  0.0      0     0 ?        S    08:12   0:00 [migration/0]root         8  0.0  0.0      0     0 ?        S    08:12   0:00 [rcu_bh]root         9  0.0  0.0      0     0 ?        S    08:12   0:00 [rcu_sched]# state of process can be in below codeD 	Uninterruptible sleep (usually IO), in kerenl space, can not wake up by signal!!R 	Running or runnable (on run queue)S 	Interruptible sleep (waiting for an event to complete)T 	Stopped, either by a job control signal(kill -STOP $pid) or because it is being traced.Z 	Defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent. When a process finishes its task, it `releases the system resources it was using and cleans up its memory`. However,` its entry from the process table is not removed`, and its status is set as EXIT_ZOMBIE.For BSD formats and when the stat keyword is used, additionalcharacters may be displayed:&lt;    high-priority (not nice to other users)N    low-priority (nice to other users)L    has pages locked into memory (for real-time and custom IO), never swaped the memory out to disk!!!s    is a session leaderl    is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)+    is in the foreground process group# show more info(backtrace) about D process$ echo w &gt; /proc/sysrq-triggerThis command produces a report and a list of all processes in D state and a full kernel stack trace to /var/log/messages$ grep &#x27;state:D stack&#x27; /var/log/messages# show all processes id in D state$ ps axl | awk &#x27;$10 ~ /D/&#x27;# show all processes in Zombine state$ ps axl | awk &#x27;$10 ~ /Z/&#x27;# show pstree of given process, show tree from 1 to its child with args$ pstree -ansp 63044systemd,1 --switched-root --system --deserialize 21  └─docker-containe,62978 c946e9c0e6845d4496d808c317d20a54086203932d940b103f9705322f4e9308 /var/run/docker/libcontainerd/c946e9c0e6845d4496d808c317d20a54086203932d940b103f9705322f4e9308 /usr/libexec/docker/docker-runc-current      └─(entry.sh,63044)# pstree -ansp 115764, as you can seee under docker-containe,115764, there are many &#123;docker-containe&#125;,$id which is thread of &#123;docker.containe&#125; process!!!systemd,1 --switched-root --system --deserialize 21  └─docker-containe,115764 c946e9c0e6845d4496d808c317d20a54086203932d940b103f9705322f4e9308 /var/run/docker/libcontainerd/c946e9c0e6845d4496d808c317d20a54086203932d940b103f9705322f4e9308 /usr/libexec/docker/docker-runc-current      ├─&#123;docker-containe&#125;,115765      ├─&#123;docker-containe&#125;,115766      ├─&#123;docker-containe&#125;,115767      ├─&#123;docker-containe&#125;,115768      ├─&#123;docker-containe&#125;,115770      ├─&#123;docker-containe&#125;,115771      ├─&#123;docker-containe&#125;,115772      ├─&#123;docker-containe&#125;,115774      ├─&#123;docker-containe&#125;,115775      └─(tail,116336)# clear process in D stateReboot, Reboot!# clear process in Z(zombine) stateWe can’t really kill a zombie process since it’s already dead. However, there are a few workarounds we can use to clean up a zombie process1. Manually send the SIGCHLD signal to the parent of a zombie process, it works only when parent process handles such signal!!! kill -s SIGCHLD $parent_pid2. Kill parent process.but if the parent process is init(pid 1), so that you can NOT kill initThis pretty much leaves you with two options.- Manually modify the process table, eg. create a dummy process, link the [defunct process] as a child of the dummy, then kill them off. Quite dangerous, and you may have to manually clean up other process resources such as semaphores and file handles.- Reboot the system

copy excludes some dir# copy html dir to dev without sites/video# The the --exclude path is relative to the source directly. Even if you put full absolute path, it will not work# -v is for verbose, -a is for archive mode which means you want recursion and want to preserve almost everything$ rsync -av --exclude &#x27;sites/video&#x27; /var/www/html /var/www/dev

show start time of process$ ps -eo pid,lstart,cmd PID                  STARTED CMD    1 Mon Jan 24 11:21:04 2022 /usr/lib/systemd/systemd --system --deserialize 15    2 Mon Jan 24 11:21:04 2022 [kthreadd]    4 Mon Jan 24 11:21:04 2022 [kworker/0:0H]    6 Mon Jan 24 11:21:04 2022 [ksoftirqd/0]    7 Mon Jan 24 11:21:04 2022 [migration/0]    8 Mon Jan 24 11:21:04 2022 [rcu_bh]    9 Mon Jan 24 11:21:04 2022 [rcu_sched]   10 Mon Jan 24 11:21:04 2022 [lru-add-drain]   11 Mon Jan 24 11:21:04 2022 [watchdog/0]

ls output blinking ls &#x2F;proc&#x2F;pid&#x2F;fd$ ls -al /proc/9358/fdtotal 0dr-x------ 2 root root  0 Jan 28 14:11 .dr-xr-xr-x 9 root root  0 Jan 28 14:11 ..lrwx------ 1 root root 64 Jan 28 14:12 0 -&gt; /dev/pts/4lrwx------ 1 root root 64 Jan 28 14:12 1 -&gt; /dev/pts/4lrwx------ 1 root root 64 Jan 28 14:11 2 -&gt; /dev/pts/4lrwx------ 1 root root 64 Jan 28 14:12 3 -&gt; socket:[985807] // inode number# the last line is blinking as, it points to socket not a file on the disk$ lsof -p 9358COMMAND  PID USER   FD   TYPE             DEVICE SIZE/OFF     NODE NAMEs       9358 root  cwd    DIR              253,2     4096  2079103 /home/data/tmps       9358 root  rtd    DIR              253,0     4096       64 /s       9358 root  txt    REG              253,2    17240  4216816 /home/data/tmp/ss       9358 root  mem    REG              253,0  2156592 33841830 /usr/lib64/libc-2.17.sos       9358 root  mem    REG              253,0   163312 33841791 /usr/lib64/ld-2.17.sos       9358 root    0u   CHR              136,4      0t0        7 /dev/pts/4s       9358 root    1u   CHR              136,4      0t0        7 /dev/pts/4s       9358 root    2u   CHR              136,4      0t0        7 /dev/pts/4s       9358 root    3u  unix 0xffff8fa41596b740      0t0   985807 /tmp/fd-pass.socket

top outputTop Example
$ toptop - 17:20:52 up  8:15,  6 users,  load average: 0.02, 0.03, 0.05Tasks: 216 total,   1 running, 215 sleeping,   0 stopped,   0 zombie%Cpu(s):  0.0 us,  0.7 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem :  8172968 total,  3056856 free,  1933172 used,  3182940 buff/cacheKiB Swap:  8257532 total,  8257532 free,        0 used.  5903604 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                         1 root      20   0  191832   4776   2620 S   0.0  0.1   0:07.57 systemd                                                                                                                                         2 root      20   0       0      0      0 S   0.0  0.0   0:00.02 kthreadd                                                                                                                                        4 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H                                                                                                                                    6 root      20   0       0      0      0 S   0.0  0.0   0:01.06 ksoftirqd/0       us - user cpu time (or) % CPU time spent in user spacesy - system cpu time (or) % CPU time spent in kernel spaceni - user nice cpu time (or) % CPU time spent on low priority processesid - idle cpu time (or) % CPU time spent idlewa - io wait cpu time (or) % CPU time spent in wait (on disk)hi - hardware irq (or) % CPU time spent servicing/handling hardware interruptssi - software irq (or) % CPU time spent servicing/handling software interruptsst - steal time % CPU time in involuntary wait by virtual cpu while hypervisor is servicing another processor (or) % CPU time stolen from a virtual machineNOTE: it is aggregate value, if you want to check usage for each CPUpress  `top` then press 1.top - 17:22:32 up  8:17,  6 users,  load average: 0.00, 0.02, 0.05Tasks: 216 total,   1 running, 215 sleeping,   0 stopped,   0 zombie%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu5  :  0.3 us,  0.3 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem :  8172968 total,  3056468 free,  1933540 used,  3182960 buff/cacheKiB Swap:  8257532 total,  8257532 free,        0 used.  5903236 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                      1856 mongod    20   0 1604016 116512  11132 S   0.3  1.4   2:23.77 mongod                                                                                                                                       2148 root      20   0  164024   6480   4952 S   0.3  0.1   0:15.29 sshd              NOTE: By default, the top will refresh the output every 3 seconds.TIPS:press `c` to show full pathpress `o` to filter based on Field.  example(support partial match): COMMAND=libvirtpress `=` to clear filterpress `e` to switch memory unit(kb, mb,gb,tb) for mem colums by default is kb# sort by CPU or memory$ top -o %CPU$ top -o %MEM# OR$ topthen pressPM (RES not VIRT)# add more fields to seeTIP:1 `press f` to show all fields2 move cursor to the line3 `press space` to define this line4 `press right arrow` to select it then move the field to righ position

create iso file from command line$ mkisofs -o test.iso $dir

setup proxy serversquid proxy
swapchange swap size by map it to a swap file setup swap file
# check current swap size$free -h              total        used        free      shared  buff/cache   availableMem:            31G        5.5G         25G        8.7M        870M         25GSwap:           31G          0B         31G$swapon -vNAME      TYPE      SIZE USED PRIO/dev/dm-1 partition  32G   0B   -2# check trend that swap is used from global, each cgroup can change it within that group$cat /proc/sys/vm/swappiness# temporary disable/enable swap$swapon -a# when swap is off, all data in swap(disk) will be swap in to memory!!! if not enough memory is avaiable, swapoff will fail with &#x27;can $swapoff -a

set process prorityPriority value(PRI) — The priority value is the process’s actual priority which is used by the Linux kernel to schedule a task.In Linux system priorities are 0 to 139 in which 0 to 99 for real-time and 100 to 139 for users.
Nice value(NI) — Nice values are user-space values that we can use to control the priority of a process. The nice value range is -20 to +19 where -20 is highest, 0 default and +19 is lowest.
The relation between nice value and priority is as suchPriority_value = Nice_value + 20
# nice value(NI) can be range -20(highest) to 19(lowest)# start process with nice value, threads will inherited parent&#x27;s nice value$ nice -5 ./app arg1# change nice value after process starts(only change 1055, not it&#x27;s children)$ renice -n -10  -p 1055# renice process which has multi-threads$ renice -n -10 -p `ps --no-heading -Lo tid 1055`

split large file into smaller onesSome website does not allow to upload larger file, split it into smaller ones.
# -d use digital sufffix# -a one digital# centos. as prefix$ split -b 1G CentOS-7-x86_64-GenericCloud.qcow2 -d -a 1 centos.centos.0  centos.1# combine them to orignal one# NOTE: not &gt;&gt;$ cat centos.* &gt; CentOS-7-x86_64-GenericCloud.qcow2

setup ftp server (centos)The difference for modes is who is the initialator for data connection, for active mode, server always use 20 as the source port.


In passive mode, the client uses the control connection to send a PASV command to the server and then receives a server IP address and server port number from the server, which the client then uses to open a data connection from an arbitrary client port to the server IP address and server port number received. 

vsftpd
By default, vsftpd allows local users to login in and switch to its $HOME
$ yum install vsftpd$ service vsftpd start# edit /etc/vsftpd/vsftpd.conf# Use virtual HOME for local user not $HOME/$USER# https://www.ryadel.com/en/vsftpd-configure-different-home-folder-each-user-specific-directory/# test it$ yum install -y ftp# standard port 21 for ftp$ ftp localhost# open on different port$ ftpftp&gt;open localhost 6621


ps command advanced# fields of auxUSER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND# show top 5 cpu usage processes$ ps aux --sort -%cpu| head -n 6USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDopenvsw+  1353  0.6  0.1 1220748 51412 ?       S&lt;Lsl 17:44   0:28 ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfile:info --mlockall --user openvswitch:openvswitch --no-chdir --log-file=/var/log/openvswitch/ovs-vswitchd.log --pidfile=/var/run/openvswitch/ovs-vswitchd.pid --detachmysql     2086  0.2  1.1 1980300 389644 ?      Ssl  17:44   0:11 /usr/sbin/mysqldroot         1  0.0  0.0 191528  4520 ?        Ss   17:44   0:01 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root         2  0.0  0.0      0     0 ?        S    17:44   0:00 [kthreadd]root         4  0.0  0.0      0     0 ?        S&lt;   17:44   0:00 [kworker/0:0H]# show the first 5 memory usage processes, by default memory unit is KB!!!$ ps aux --sort -%mem | head -n 6USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDmysql     2086  0.2  1.1 1980300 389644 ?      Ssl  17:44   0:11 /usr/sbin/mysqldroot      1796  0.0  0.2 274540 76708 ?        S    17:44   0:02 /home/data/Anaconda3/envs/py3.9/bin/python3.9 /home/data/Anaconda3/envs/py3.9/bin/jupyter-notebook -y --no-browser --allow-root --ip=172.17.0.2 --port=8080 --notebook-dir=/home/data/jupyteropenvsw+  1353  0.6  0.1 1220748 51412 ?       S&lt;Lsl 17:44   0:28 ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfile:info --mlockall --user openvswitch:openvswitch --no-chdir --log-file=/var/log/openvswitch/ovs-vswitchd.log --pidfile=/var/run/openvswitch/ovs-vswitchd.pid --detachroot      1794  0.0  0.0 574288 17504 ?        Ssl  17:44   0:00 /usr/bin/python2 -Es /usr/sbin/tuned -l -Ppolkitd   1060  0.0  0.0 614316 11136 ?        Ssl  17:44   0:00 /usr/lib/polkit-1/polkitd --no-debug# show thread info of given process# -L Show threads, possibly with LWP and NLWP columns.$ ps -Lf 13430UID        PID  PPID   LWP  C NLWP STIME TTY      STAT   TIME CMDroot     13430 13429 13430  0   11 Jun20 ?        Ssl    0:32 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13431  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13432  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13433  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13434  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13435  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13436  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13479  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13480  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13481  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.jsroot     13430 13429 13482  0   11 Jun20 ?        Ssl    0:00 node --no-warnings /root/.vim/bundle/coc.nvim/build/index.js# show thread name of a process$ ps -L -o tid,pid,comm 19562  PID   TID COMMAND19562 19562 qemu-kvm19562 19565 qemu-kvm19562 19566 qemu-kvm19562 19567 IO iothread119562 19571 CPU 0/KVM19562 19572 CPU 1/KVM19562 19574 vnc_worker# show custom output for given process$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd -fp $pid# show custom output and more $ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmdUSER       PID  PPID %CPU %MEM    VSZ   RSS PRI  NI  STARTED     TIME CMDroot         1     0  0.0  0.0 191528  3216  19   0   Jan 11 00:00:09 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root         2     0  0.0  0.0      0     0  19   0   Jan 11 00:00:00 [kthreadd]root         4     2  0.0  0.0      0     0  39 -20   Jan 11 00:00:00 [kworker/0:0H]root         6     2  0.0  0.0      0     0  19   0   Jan 11 00:00:03 [ksoftirqd/0]root         7     2  0.0  0.0      0     0 139   -   Jan 11 00:00:00 [migration/0]root         8     2  0.0  0.0      0     0  19   0   Jan 11 00:00:00 [rcu_bh]# custom fields with sorting$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd --sort=%cpu$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd --sort=%mem# The &quot;+&quot; is optional since default direction is increasing numerical or lexicographic order$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd --sort=+rss$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd --sort=rss# top five memory usage process$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd --sort=-rss | head -6# show memory in MB unit(format field 7) only support format one field by numfmt!!$ ps -eo user,pid,ppid,%cpu,%mem,vsz,rss,pri,ni,start,time,cmd --sort=-rss  | numfmt --header --field 7 --to=iec --from-unit=1024# show rss field value(default unit KB) only$ps -q 1354 -o rss=51464# show given process only$ ps -fp $pid# there  are two process PID 1 and PID 2 which have no parent!# all user processes are child of PID 1# all kernel processes are child of PID 2# UID         PID   PPID  C STIME TTY          TIME CMD# root          1      0  0 Mar08 ?        00:00:59 /usr/lib/systemd/systemd --switched-root --system --deserialize 21# root          2      0  0 Mar08 ?        00:00:02 [kthreadd]# only show user application process not kernel thread$ ps -ef | awk &#x27;$3!=&quot;2&quot; &#123;print $0&#125;&#x27;# only show kernel thread$ ps -ef | awk &#x27;$3==&quot;2&quot; &#123;print $0&#125;&#x27;

sort command advanedrefer to sort example
run python service from virtual envEdit your serivce file like this
# /root/py3.8 is virtual env$ cat /etc/systemd/system/test.service...ExecStart=/root/py3.8/bin/python /root/me/auto-tools/tool/web.py &amp;...# reload service file$ systemctl daemon-reload$ service test restart

encrypt a file$ yum install -y gnupg# prompt to input password for encryption$ gpg  --cipher-algo AES256 -c file$ gpg -d file.gpg# with output file$ gpg -o encrypt --cipher-algo AES256 -c file$ gpg -o file -d encrypt

disable cpu without reboot system###########One way############################$ chcpu -d 1-10$ chcpu -d 1$ chcpu -e 1###########One way############################$ echo 0 &gt; /sys/devices/system/cpu/cpu1/online$ echo 1 &gt; /sys/devices/system/cpu/cpu1/online# show online processor$ grep processor /proc/cpuinfo

show hardware meta$ lshw    description: Rack Mount Chassis    product: PowerEdge R630 (SKU=NotProvided;ModelName=PowerEdge R630)    vendor: Dell    serial: 91TYB72    width: 64 bits    capabilities: smbios-2.8 dmi-2.8 vsyscall32    configuration: boot=normal chassis=rackmount sku=SKU=NotProvided;ModelName=PowerEdge R630 uuid=44454C4C-3100-1054-8059-B9C04F423732  *-core       description: Motherboard       product: 0CNCJW       vendor: Dell       physical id: 0       version: A08       serial: .91TYB72.CN747515840576.     *-firmware          description: BIOS          vendor: Dell          physical id: 0          version: 1.3.6          date: 06/03/2015          size: 64KiB          capacity: 15MiB          capabilities: isa pci pnp upgrade shadowing cdboot bootselect edd int13floppytoshiba int13floppy360 int13floppy1200 int13floppy720 int9keyboard int14serial int10video acpi usb biosbootspecification netboot uefi     *-cpu:0          description: CPU          product: Xeon          vendor: Intel Corp.          physical id: 400          bus info: cpu@0          version: Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz          slot: CPU1          size: 2600MHz          capacity: 4GHz          width: 64 bits          clock: 3705MHz... more$ lshw -class disk  *-disk:0                       description: SCSI Disk       product: PERC H730 Mini       vendor: DELL       physical id: 2.0.0       bus info: scsi@0:2.0.0       logical name: /dev/sda       version: 4.25       serial: 009bf0a4080e81572100a1d44820844a       size: 558GiB (599GB)       capabilities: partitioned partitioned:dos       configuration: ansiversion=5 logicalsectorsize=512 sectorsize=512 signature=000ddb8f

use &#x2F;dev&#x2F;urandomIt’s a file like device, so you can cat it or copy from it
# 4K of random bytes$ dd if=/dev/urandom of=~/urandom_test count=4 bs=1024# quit until ctrl + c$ cat /dev/urandom &gt; ~/urandom_test2 # 30 lines of random$ head -30 /dev/urandom &gt; ~/urandom_test3# 30 bytes of randomhead -c 30 /dev/urandom &gt; random.bytes

no space left on systemThere are two possible reasons that may cause this.

no enough space
no enough inode

# disk which disk is full$ df -hFilesystem      Size  Used Avail Use% Mounted ondevtmpfs        440M     0  440M   0% /devtmpfs           464M     0  464M   0% /dev/shmtmpfs           464M   13M  451M   3% /runtmpfs           464M     0  464M   0% /sys/fs/cgroup/dev/vda1       8.0G  1.1G  7.0G  14% /tmpfs           119M     0  119M   0% /run/user/0# inside that disk, check whih dir uses the much space$ cd /home/data$ du -h -d 17.0G    ./Anaconda30       ./jupyter23G     ./github3.3G    ./tmp1.6G    ./onedrive34G     .# inode$ df -iFilesystem                  Inodes IUsed IFree IUse% Mounted ondevtmpfs                      2.0M   451  2.0M    1% /devtmpfs                         2.0M     2  2.0M    1% /dev/shmtmpfs                         2.0M   721  2.0M    1% /runtmpfs                         2.0M    16  2.0M    1% /sys/fs/cgroup/dev/mapper/centos_dev-root    50M  363K   50M    1% //dev/mapper/centos_dev-home   150M  580K  150M    1% /home/dev/mapper/centos_dev-var     25M   90K   25M    1% /var/dev/sda1                     150K   370  150K    1% /boottmpfs                         2.0M   366  2.0M    1% /run/user/0overlay                        25M   90K   25M    1% /var/lib/docker/overlay2/bec9168000402c04316e112d231a51fa1ba120b7e09c5ab65014b80f3161b6b6/merged# to see inodes used for each subdir$ find . -xdev | cut -d &quot;/&quot; -f 2 | sort | uniq -c | sort -n -r | head -10285913 Anaconda3226484 github   760 onedrive    11 tmp    11 jupyter

systemd socket activationhow socket activation works and systemd for developer
lsmem and free show different total memorylsmem lists memory blocks and their state; these reflect physical memory and are counted in units of memory blocks, i.e. 128MiB on your system. To do this, lsmem reads information made available by the kernel in /sys/devices/system/memory. On your system, the kernel tracks 64 memory blocks for a total of 8GiB.
free lists memory that’s usable by the system; “total” is the amount of physical memory, minus memory reserved by the system (for the firmware’s purposes mostly) and the kernel’s executable code. free reads this information from &#x2F;proc&#x2F;meminfo.
change PS1 env# \u    ---&gt;user# \h    ---&gt;hostanme[root@A04-R08-I138-44-3G1ZBS2 ~] $ echo $PS1[\u@\h \W]\$[root@A04-R08-I138-44-3G1ZBS2 ~] $ export PS1=&quot;[\u@node-debug \W]$ &quot;[root@node-debug ~]$


proxy setting# global setting, this works only when application use such environment variable$ cat ~/.bashrcexport http_proxy=&#x27;http://10.226.198.162:3128&#x27;export https_proxy=&#x27;http://10.226.198.162:3128&#x27;# application provides its own way for proxy# git proxy$ git config --global https.proxy http://127.0.0.1:1080$ git config --global https.proxy https://127.0.0.1:1080$ git config --global http.proxy &#x27;socks5://127.0.0.1:1080&#x27;$ git config --global https.proxy &#x27;socks5://127.0.0.1:1080&#x27;$ git config --global --unset http.proxy/su$ git config --global --unset https.proxy# npm proxy$ npm config set proxy  http://127.0.0.1:1080# docker proxy$ mkdir -p /etc/systemd/system/docker.service.d$ touch  /etc/systemd/system/docker.service.d/http-proxy.conf[Service]Environment=&quot;HTTP_PROXY=http://user01:password@10.10.10.10:8080/&quot;Environment=&quot;HTTPS_PROXY=https://user01:password@10.10.10.10:8080/&quot;Environment=&quot;NO_PROXY= hostname.example.com,172.10.10.10&quot;$ systemctl show docker --property Environment# go proxy$ go envGOPROXY=&quot;https://goproxy.io,direct&quot;# yum proxy$ cat /etc/yum.conf[main]proxy=http://192.226.198.162:3128# pip proxy or set global HTTP_PROXY$ pip install --proxy http://user:password@proxyserver:port TwitterApi# proxychains to force any application to use proxy# NOTE: it supports TCP only (no UDP/ICMP etc).$ yum install proxychains-ng$ cat /etc/proxychains.confstrict_chainproxy_dnsremote_dns_subnet 224tcp_read_time_out 15000tcp_connect_time_out 8000[ProxyList]http 192.226.136.231 3128# then run app this way$ proxychains pip install ipvsd

process limits# global setting(default for process, but can be reset by process)$ ulimit -acore file size          (blocks, -c) 0data seg size           (kbytes, -d) unlimitedscheduling priority             (-e) 0file size               (blocks, -f) unlimitedpending signals                 (-i) 1030598max locked memory       (kbytes, -l) 64max memory size         (kbytes, -m) unlimitedopen files                      (-n) 102400pipe size            (512 bytes, -p) 8POSIX message queues     (bytes, -q) 819200real-time priority              (-r) 0stack size              (kbytes, -s) 10240cpu time               (seconds, -t) unlimitedmax user processes              (-u) 307200virtual memory          (kbytes, -v) unlimitedfile locks                      (-x) unlimited# real limits of each process$ cat /proc/116446/limitsLimit                     Soft Limit           Hard Limit           Units     Max cpu time              unlimited            unlimited            seconds   Max file size             unlimited            unlimited            bytes     Max data size             unlimited            unlimited            bytes     Max stack size            10485760             20971520             bytes     Max core file size        0                    unlimited            bytes     Max resident set          unlimited            unlimited            bytes     Max processes             307200               307200               processes Max open files            204800               204800               files     Max locked memory         65536                65536                bytes     Max address space         unlimited            unlimited            bytes     Max file locks            unlimited            unlimited            locks     Max pending signals       1030598              1030598              signals   Max msgqueue size         819200               819200               bytes     Max nice priority         0                    0                    Max realtime priority     0                    0                    Max realtime timeout      unlimited            unlimited            us      

random linesuse shuf to write a random permutation of the input lines to standard output.
# standoutput$ shuf new.txt# file as output$ shuf new.txt -o random.txt

pgrep vs pidofThe programs pgrep and pidof are not quite the same thing, but they are very similar. For example:
$ pidof &#x27;firefox&#x27;5696$ pgrep &#x27;[i]ref&#x27;5696$ pidof &#x27;[i]ref&#x27;$ printf &#x27;%s\n&#x27; &quot;$?&quot;1# get children process(not thread) of given pid$ pgrep -P 18241825# it shows process and threads as well# 1694, 1695 are threads not process$ pstree -p 1824dnsmasq(1824)───dnsmasq(1825)
As you can see, pidof failed to find a match for [i]ref. This is because pidof program returns a list of all process IDs associated with a program called program. On the other hand, pgrep re returns a list of all process IDs associated with a program whose name matches the regular expression re.
In their most basic forms, the equivalence is actually: $ pidof &#39;program&#39; &#x3D;&#x3D; $ pgrep &#39;^program$&#39;
remove cgroup from &#x2F;sys&#x2F;fsFirst you have to move all processes from this cgroup, then delete it, otherwise it, error shows cgroups can&#39;t be cleaned up: Device or resource busy
# use cpu cgroup as an example$ cat /sys/fs/cgroup/cpu/test/cgroup.procs8666# move 8666 to root group, also moved it from test group$ echo 8666 &gt;/sys/fs/cgroup/cpu/cgroup.procs$ cat /sys/fs/cgroup/cpu/test/cgroup.procs$ rmdir /sys/fs/cgroup/cpu/test

find all shell scripts$ find /xyz | xargs file | grep shell./david/query-docker.sh:                                                                                                Bourne-Again shell script, ASCII text executable./david/db/import-db.sh:                                                                                                POSIX shell script, ASCII text executable, with very long lines$ find /xyz | xargs file | grep shell | cut -d &quot;:&quot; -f1./david/query-docker.sh./david/db/import-db.sh

add time out for any shell command$ timeout 5s ping www.sd# capture output of command$ RET=$(timeout 5s ls asd 2&gt;&amp;1)$ echo $RET# sometimes it will hang when capture command output, to solve this add &#x27;--foreground&#x27; option$ RET=$(timeout --foreground 5s $CMD 2&gt;&amp;1)$ echo $RET
which numa memory by a process# list all memory used by a process# numa bind policy for huage page, default policy for others$ cat /proc/82389/numa_maps7f069ab9a000 default file=/usr/lib64/ld-2.17.so anon=1 dirty=1 N0=1 kernelpagesize_kB=47f0588000000 bind:0-1 file=/mnt/huge_2MB/libvirt/qemu/141-i-f2w3m8owht/qemu_back_mem._objects_ram-node0.2IyOfZ\040(deleted) huge dirty=2048 mapmax=4 N0=2048 kernelpagesize_kB=2048# the first column is memory address# the second column is numa node used by this memory#   default means all numa nodes#   bind:0-1 means only numa node 0 and node 1 are allowed for this memory, but we may only used one node.# the third column is file related# the last column is kernel page for this memory# the one before last colum is the page count of this memory on each node# N0=2048 means allocate 2048*kernelpagesize_kB on node 0 $ numastat -p 82389Per-node process memory usage (in MBs) for PID 82389 (qemu-kvm)                           Node 0          Node 1           Total                  --------------- --------------- ---------------Huge                      4096.00            0.00         4096.00Heap                        65.16            0.00           65.16Stack                        0.03            0.00            0.04Private                     32.34            1.21           33.55----------------  --------------- --------------- ---------------Total                     4193.53            1.21         4194.74# total numa stat$ numastat                            node0           node1numa_hit            271446988685    306962573062numa_miss                 144931     15014353674numa_foreign         15014353674          144931interleave_hit             35352           35604local_node          271445218315    306961782987other_node               1915301     15015143749# huage page on two numa nodes$numastat -p 1144Per-node process memory usage (in MBs) for PID 1144 (qemu-kvm)                           Node 0          Node 1           Total                  --------------- --------------- ---------------Huge                      1752.00         6440.00         8192.00Heap                         1.04           44.79           45.82Stack                        0.00            0.04            0.04Private                     16.66           11.42           28.08----------------  --------------- --------------- ---------------Total                     1769.70         6496.25         8265.95# as you can see N0(node 0) 876*2M and N1(node 1) 3220*2M# numa bind policy$ cat /proc/1144/numa_maps | grep 20487fbd58000000 bind:0-1 file=/mnt/huge_2MB/libvirt/qemu/38-i-y4s8f9zg2o/qemu_back_mem._objects_ram-node0.dlp9WP\040(deleted) huge dirty=4096 mapmax=4 N0=876 N1=3220 kernelpagesize_kB=2048# numa prefer policy for huge page# prefre node 1 but huage page is allocated at node0(N0) 12G and node1(N1) 20G$ cat /proc/140662/numa_maps | grep 1G7f5480000000 prefer:1 file=/mnt/huge_1GB/qemu_back_mem._objects_ram-node0.8Jxnog\040(deleted) huge anon=32 dirty=32 N0=12 N1=20 kernelpagesize_kB=1048576

check initrd.img# list content of initrd$ lsinitrd build/initrd.imgmage: build/nc-initrd.img: 31M========================================================================Version: Arguments: dracut modules:========================================================================drwxr-xr-x  11 root     root            0 Mar  8 11:30 .drwxr-xr-x   2 root     root            0 Mar  8 11:30 binlrwxrwxrwx   1 root     root           13 Mar  8 11:30 bin/hwclock -&gt; /sbin/busyboxlrwxrwxrwx   1 root     root           13 Mar  8 11:30 bin/sh -&gt; /sbin/busyboxlrwxrwxrwx   1 root     root           13 Mar  8 11:30 bin/tar -&gt; /sbin/busyboxdrwxr-xr-x   2 root     root            0 Mar  8 11:30 dev-rwxr-xr-x   1 root     root       568240 Mar  8 11:30 initdrwxr-xr-x   2 root     root            0 Mar  8 11:30 kdump_tools-rw-r--r--   1 root     root     10162077 Mar  8 11:30 kdump_tools/initramfs-4.14.121-jdcloudkdump.img-rw-r--r--   1 root     root      4964304 Mar  8 11:30 kdump_tools/vmlinuz-4.14.121-jdclouddrwxr-xr-x   3 root     root            0 Mar  8 11:30 libdrwxr-xr-x   2 root     root            0 Mar  8 11:30 lib64-rwxr-xr-x   1 root     root       163312 Mar  8 11:30 lib64/ld-linux-x86-64.so.2-rwxr-xr-x   1 root     root        13536 Mar  8 11:30 lib64/libfreebl3.so-rwxr-xr-x   1 root     root       553072 Mar  8 11:30 lib64/libfreeblpriv3.so-rwxr-xr-x   1 root     root       287312 Mar  8 11:30 lib64/libnspr4.so-rwxr-xr-x   1 root     root       737664 Mar  8 11:30 lib64/libnss3.so-rwxr-xr-x   1 root     root       585848 Mar  8 11:30 lib64/libnssckbi.so-rwxr-xr-x   1 root     root       152576 Mar  8 11:30 lib64/libnssdbm3.so-rwxr-xr-x   1 root     root       175464 Mar  8 11:30 lib64/libnsspem.so-rwxr-xr-x   1 root     root        13312 Mar  8 11:30 lib64/libnsssysinit.so-rwxr-xr-x   1 root     root       229152 Mar  8 11:30 lib64/libnssutil3.so-rwxr-xr-x   1 root     root        23664 Mar  8 11:30 lib64/libplc4.so...# unpack $ file gunzip initrd.img.gzinitrd.img: gzip compressed data, from Unix, last modified: Wed Mar  8 11:30:10 2023, max compression$ gunzip initrd.img.gz$ cpio -idv &lt; initrd.imgbin/  dev/  init  kdump_tools/  lib/  lib64/ proc/  sbin/  sys/  usr/

Check if Hard Drive is SSD or HDD on Linux# 1 for HDD, 0 for SSD################ way1=======================$ cat /sys/block/sda/queue/rotational1################ way2=======================$ lsblk -d -o name,rotaNAME    ROTAsda        1...sdm        0nvme0n1    0################ way3=======================$ smartctl -a /dev/sda | grep &#x27;Rotation Rate&#x27;Rotation Rate:    7200 rpm$ smartctl -a /dev/sdm | grep &#x27;Rotation Rate&#x27;Rotation Rate:    Solid State Device

disk related# show filesystem, uuid etc for block device$blkid/dev/sda1: UUID=&quot;5cf26576-d3d1-48d1-aab4-1326cdda3c77&quot; TYPE=&quot;xfs&quot; /dev/sda2: UUID=&quot;mFfzPH-yRAk-6ASm-6ASF-Nizr-9u3m-25wQgr&quot; TYPE=&quot;LVM2_member&quot; /dev/mapper/centos_dev-root: UUID=&quot;6b5ed230-86e5-4ab6-9e96-08caf90afca6&quot; TYPE=&quot;xfs&quot; /dev/mapper/centos_dev-swap: UUID=&quot;638cc56c-e2c9-425a-b13a-0c890082db1b&quot; TYPE=&quot;swap&quot; /dev/mapper/centos_dev-home: UUID=&quot;cab6c113-6e6d-4d9a-8654-eb304b9100c3&quot; TYPE=&quot;xfs&quot; /dev/mapper/centos_dev-var: UUID=&quot;7aafa462-85af-4587-a334-beeb477e25f6&quot; TYPE=&quot;xfs&quot;$lsblk -fNAME                FSTYPE      LABEL UUID                                   MOUNTPOINTsda                                                                          ├─sda1              xfs               5cf26576-d3d1-48d1-aab4-1326cdda3c77   /boot└─sda2              LVM2_member       mFfzPH-yRAk-6ASm-6ASF-Nizr-9u3m-25wQgr   ├─centos_dev-root xfs               6b5ed230-86e5-4ab6-9e96-08caf90afca6   /  ├─centos_dev-swap swap              638cc56c-e2c9-425a-b13a-0c890082db1b   [SWAP]  ├─centos_dev-home xfs               cab6c113-6e6d-4d9a-8654-eb304b9100c3   /home  └─centos_dev-var  xfs               7aafa462-85af-4587-a334-beeb477e25f6   /var$df -ThFilesystem                  Type      Size  Used Avail Use% Mounted ondevtmpfs                    devtmpfs   16G     0   16G   0% /devtmpfs                       tmpfs      16G     0   16G   0% /dev/shmtmpfs                       tmpfs      16G  888K   16G   1% /runtmpfs                       tmpfs      16G     0   16G   0% /sys/fs/cgroup/dev/mapper/centos_dev-root xfs       100G   45G   56G  45% //dev/sda1                   xfs       297M  191M  107M  65% /boot/dev/mapper/centos_dev-var  xfs        50G  9.4G   41G  19% /var/dev/mapper/centos_dev-home xfs       300G   58G  243G  20% /homeoverlay                     overlay    50G  9.4G   41G  19% /var/lib/docker/overlay2/5c1f012f94cc69c18a9fbcc84c5403232f103c6a68b9544ba2284f59ee99e379/mergedtmpfs                       tmpfs     3.2G  876K  3.2G   1% /run/user/0# disk error hence disk mounted as read-only fix it by$ fsck -aCOR$ e2fsck -f -y -v /dev/sda1then$ reboot$ df -lh      # show disk partition$ df -ih      # show inode free/used$ du -h -d 1 # show file size# format disk with ext2 or ext3 etc# mkfs.bfs      mkfs.cramfs   mkfs.ext2     mkfs.ext3     mkfs.ext4     mkfs.ext4dev  mkfs.minix    mkfs.msdos    mkfs.ntfs     mkfs.vfat$ fdisk -l$ mkfs.minix /dev/sdb1# increase /tmp size by remounting it with a large file$ dd if=/dev/zero of=/usr/temp-disk bs=2M count=1024$ mkfs.ext3  /usr/temp-disk# add /etc/fstab file with below content/usr/temp-disk /tmp ext3 rw,noexec,nosuid,loop 0 0

UUID vs PARTUUID

UUID is a filesystem-level UUID, which is retrieved from the filesystem metadata inside the partition. It can only be read if the filesystem type is known and readable.
PARTUUID is a partition-table-level UUID for the partition, a standard feature for all partitions on GPT-partitioned disks. Since it is retrieved from the partition table, it is accessible without making any assumptions at all about the actual contents of the partition.

$ls /dev/disk/by-uuid/38db244b-e0b7-45fb-aftg-a168b6fd00fd  433acdf3-70b7-4e93-abcd-04817a6aeae1$ls /dev/disk/by-partuuid/0260d409-b765-4de2-adfd-493e92087395  7d05dd0b-ab8a-48eb-12ad-2ef4e4e8b3af ...# uuid is return if this partition has a filesystem$blkid /dev/sdm1/dev/sdm1: UUID=&quot;38db244b-e0b7-34fb-adbd-a168b6fd00fd&quot; TYPE=&quot;xfs&quot;# partuuid is return if no filesytem on it$blkid /dev/sdl1/dev/sdl1: PARTLABEL=&quot;meta&quot; PARTUUID=&quot;87a0bfbf-2374-4be8-93c7-b5d49810ee9b&quot;

regular pattern greedy vs lazy match&#39;Greedy&#39; means match longest possible string. while &#39;Lazy&#39; or non-greedy means match shortest possible string. 



Greedy quantifier
Lazy quantifier
Description



*
*?
Star Quantifier: 0 or more


+
+?
Plus Quantifier: 1 or more


?
??
Optional Quantifier: 0 or 1


{n}
{n}?
Quantifier: exactly n


{n,}
{n,}?
Quantifier: n or more


{n,m}
{n,m}?
Quantifier: between n and m


# grep with lazy match# egrep does not support non-greedy matching. Using perl mode -P will help you:# -o only print the matched part!!!$grep -o -P  &#x27;https.*?mp3&#x27; example.txt | sort | uniq 
tar with progress bar$D=&quot;.//dir1 ./file1&quot;$tar pcf - $D | pv -s &quot;$(du -sk --total $D | tail -n1 | cut -f1)k&quot;| gzip &gt; target.tar.gz

control file for sudoThe /etc/sudoers file is a critical configuration file on Unix-like operating systems, including Linux, that controls the permissions for users to execute commands with superuser (root) privileges using the sudo command. Here are the key aspects of the &#x2F;etc&#x2F;sudoers file:

Access Control: The primary function of the &#x2F;etc&#x2F;sudoers file is to define which users or groups can execute specific commands as the root user or another user. This allows for fine-grained control over administrative privileges without giving users full root access.
Defaults and Aliases: The file can also contain default settings and aliases to simplify configuration. Aliases can be defined for users, hosts, commands, and run-as specifications to group multiple entries togethe

By default sudo inherits ENV and PATH from users, but it should be reset by &#x2F;etc&#x2F;sudoers to prevent the use of potentially harmful variables and allow only trusted directories are used for command execution, so that user should set Defaults env_reset and Defaults env_reset at /etc/sudoers file
$ sudo cat /etc/sudoers...Defaults    env_reset                                                           Defaults    env_keep =  &quot;COLORS DISPLAY HOSTNAME HISTSIZE KDEDIR LS_COLORS&quot;     Defaults    env_keep += &quot;MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE&quot;  Defaults    env_keep += &quot;LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES&quot;Defaults    env_keep += &quot;LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE&quot;  Defaults    env_keep += &quot;LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY&quot;Defaults	secure_path=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin&quot;...
what does dmesg dodmesg is used to examine or control the kernel ring buffer, so that after each reboot, it’s fresh!
$ dmesg[    0.000000] Linux version 4.14.15-1.el7.elrepo.x86_64 (mockbuild@Build64R7) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC)) #1 SMP Tue Jan 23 20:28:26 EST 2018[    0.000000] Command line: BOOT_IMAGE=/vmlinuz-4.14.15-1.el7.elrepo.x86_64 root=UUID=d94e304b-6b86-4a2b-8016-4029b94f92cc ro crashkernel=auto modprobe.blacklist=mlx5_core,virtio_pci hugepages=800 intel_iommu=on processor.max_cstate=1 idle=poll intel_pstate=disable pci=realloc[    0.000000] x86/fpu: Supporting XSAVE feature 0x001: &#x27;x87 floating point registers&#x27;[    0.000000] x86/fpu: Supporting XSAVE feature 0x002: &#x27;SSE registers&#x27;[    0.000000] x86/fpu: Supporting XSAVE feature 0x004: &#x27;AVX registers&#x27;[    0.000000] x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256[    0.000000] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using &#x27;standard&#x27; format.[    0.000000] e820: BIOS-provided physical RAM map:[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x0000000000099bff] usable# show timestamp and color import thing$ dmesg -TL[Wed Aug 21 19:49:14 2024] Linux version 4.14.15-1.el7.elrepo.x86_64 (mockbuild@Build64R7) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC)) #1 SMP Tue Jan 23 20:28:26 EST 2018[Wed Aug 21 19:49:14 2024] Command line: BOOT_IMAGE=/vmlinuz-4.14.15-1.el7.elrepo.x86_64 root=UUID=d94e304b-6b86-4a2b-8016-4029b94f92cc ro crashkernel=auto modprobe.blacklist=mlx5_core,virtio_pci hugepages=800 intel_iommu=on processor.max_cstate=1 idle=poll intel_pstate=disable pci=realloc[Wed Aug 21 19:49:14 2024] x86/fpu: Supporting XSAVE feature 0x001: &#x27;x87 floating point registers&#x27;[Wed Aug 21 19:49:14 2024] x86/fpu: Supporting XSAVE feature 0x002: &#x27;SSE registers&#x27;[Wed Aug 21 19:49:14 2024] x86/fpu: Supporting XSAVE feature 0x004: &#x27;AVX registers&#x27;[Wed Aug 21 19:49:14 2024] x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256[Wed Aug 21 19:49:14 2024] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using &#x27;standard&#x27; format.[Wed Aug 21 19:49:14 2024] e820: BIOS-provided physical RAM map:[Wed Aug 21 19:49:14 2024] BIOS-e820: [mem 0x0000000000000000-0x0000000000099bff] usable

temporary select next reboot kernelIf you want to temporarily boot a different entry just once, you can use the grub-reboot command before rebooting.
# ----------------- temporary only once ----------------------# menu index starts from 0$ sudo grub-reboot 2# OR$ sudo grub2-reboot 2$ sudo reboot# 1&gt;2: main menu index: 1, submenu index: 2$ sudo grub-reboot &quot;1&gt;2&quot;# OR$ sudo grub2-reboot &quot;1&gt;2&quot;$ sudo reboot# ----------------- permanently---------------------# show all boot indexes$ sudo grubby --info=ALL# Locate the GRUB_DEFAULT line and set it to the desired index number or menu entry name$ sudo vi /etc/default/grub# update grub$ sudo grub2-mkconfig -o /boot/grub2/grub.cfg$ reboot
show full command used by gccSometimes we want to know the complete parameters used by gcc when compiling a file. There’s no need to change the Makefile; just add V=1 to the make command.
$ make V=1

how can know if i am a vm# -------------VM-------------$ dmesg | grep &quot;Hypervisor detected&quot;[    0.000000] Hypervisor detected: Microsoft HyperV$ virt-whathyperv$ systemd-detect-virtmicrosoft$ sudo dmidecode -s system-manufacturerMicrosoft Corporation$ sudo dmidecode -s system-product-nameVirtual Machine# -------------Baremetal-------------$ systemd-detect-virtnone$ dmesg | grep &quot;Hypervisor detected&quot;$ virt-what$ sudo dmidecode -s system-manufacturerSupermicro$ sudo dmidecode -s system-product-nameSuper Server]]></content>
      <categories>
        <category>linux</category>
        <category>tips</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_advanced_structure</title>
    <url>/2020/11/26/nginx-advanced-structure/</url>
    <content><![CDATA[Overviewnginx provides advanced data structure like array, list, queue, rb tree which are specific to nginx, like array, it can grow dynamically, but not support free element back to array, list actually is list of several arrays.

arrayarray will increase dynamically if no more space for use. but it’s better to allocate enough element at initialization.as when increase array size, move original data from old array to new array, so that new array has whole data, can iterate it to get all elements. but the new address may be not equal old(a-&gt;elts), the original array is not freed at all, because user still points to old part and access the element that get from old array as user has that pointer, new element is created from the new array.
+-----+     +------+               +--------+    +---------+|     |     |      |               |        |    |         ||  e1 |     | e2   |               |  e3    |    |  e4     |+--+--+     +--+---+               +--------++   +------+--+   |           |                             |          |   |           |   +------+        +-------+ |          |   |           |   |      |        |       | |          |   |           +--&gt;+  12  |        |  12   | |          |   |               |      |        |       | |          |   |               +------+        +-------+ |          |   |               |      |        |       | |          |   +-------------&gt; |  13  |        |  13   | |          |                   +------+        |       | |          |                                   +-------+ |          |                                   |       | |          |                                   |  14   +&lt;+          |                                   |       |            |                                   +-------+            |                                   |       |            |                                   |  15   +&lt;-----------+                                   |       |                                   +-------+e1 and e2 still point to old array after double arraye3 and e4 point to new array, but the first two elements still have the same info with old array

limitation

NOT support decrease array size
NOT support free element back to array
Only support free the whole array

data structure
typedef struct &#123;    void        *elts;      /* address of nalloc * size */    ngx_uint_t   nelts;     /* in use counter */    size_t       size;      /* size of the element */    ngx_uint_t   nalloc;    /* total elements can be used */    ngx_pool_t  *pool;&#125; ngx_array_t;

API
/* example to use: */typedef struct &#123;    ngx_int_t id;&#125; my_element_t;/* initialization size: 4 my_element_t */ngx_array_t *my_array = ngx_array_create(pool, 4, sizeof(my_element_t));my_node = ngx_array_push(my_array);my_node-&gt;id = 1;

listnginx list is a list of array(fixed size, nalloc * size) each list part is an array(fixed size), list stores the header part and last part.
when request an element from the list, it checks if the last array is used up or not, if not, gets it, otherwise creates a new array, get one element from it.
Note

list is good for dynamic element size, but not good for free ops
The array in the list is used once only when it’s last  later on even you free elements back to the array, if it’s not last anymore, never use it anymore.
Never delete element of list as it’s dangerous as it related data movement

data structure
struct ngx_list_part_s &#123;    void             *elts;     /* header of the continuos memory */    ngx_uint_t        nelts;    /* used counter of this part */    ngx_list_part_t  *next;     /* next part of the list */&#125;;typedef struct &#123;    ngx_list_part_t  *last;     /* the last part */    ngx_list_part_t   part;     /* the first part */    size_t            size;     /* size of each element */    ngx_uint_t        nalloc;   /* a list can have several parts, each part can take nalloc continuous memory */    ngx_pool_t       *pool;&#125; ngx_list_t;

API
typedef struct &#123;    ngx_int_t id;&#125; my_element_t;/* 4 elements for each array in the list */ngx_list_t *mylist = ngx_list_create(pool, 4, sizeof(my_element_t));my_element_t *node = ngx_list_push(mylist);node-&gt;id = 2;/* the iteration through the list: */part = &amp;mylist.part;data = part-&gt;elts;for (i = 0 ;; i++) &#123;    if (i &gt;= part-&gt;nelts) &#123;        if (part-&gt;next == NULL) &#123;            break;        &#125;        part = part-&gt;next;        data = part-&gt;elts;        i = 0;    &#125;    ...data[i] ...&#125;

queuengx_queue_t is just for linking, data area must be allocated &#x2F;freed by user, ngx_queue is bidirectional list.
it supports:

insert node at head&#x2F;tail
get head&#x2F;tail node
empty check
remove a node
get prev&#x2F;next node of the given one
sort
join&#x2F;split list

data structure
struct ngx_queue_s &#123;    ngx_queue_t  *prev;    ngx_queue_t  *next;&#125;;

API
/* example to use: */typedef struct my_node &#123;    ngx_queue_t q_node;    xxx;    yyy;&#125; my_node_t;ngx_queue_t head;my_node_t node;ngx_queue_init(&amp;head)ngx_queue_insert_head(&amp;head, &amp;node.q_node)ngx_queue_empty(&amp;head);ngx_queue_head(&amp;head);ngx_queue_last(&amp;head);ngx_queue_remove(&amp;node);

rb treered&#x2F;back tree is balanced binary search tree.

balance:

when delete&#x2F;insert a node, the tree may be rotated to make the depth small,hence quick search

binary:

each node have at most two child left&#x2F;right

search:

the left node is smaller than its parent
the right node is large than it’s parent
as it’s sorted, so each node must have a key(integer)

NoteBy default, key must be unique, if insert a node with same key the previous one will be replaced, BUT you can define custom ‘insert’ method to overwrite the default, hence allow two nodes can have the same key in the tree.
data structure
struct ngx_rbtree_s &#123;    /* root can be change after insert or delete as it may rotate    ngx_rbtree_node_t     *root;    /* sentinel is a node with black color, if node has no left/right child, set     * its left/right child with sentinel to meet rb tree required     */    ngx_rbtree_node_t     *sentinel;    /* &#x27;insert&#x27; method, how to insert a node in the tree     * you can define your own to allow same key     * actually nginx provides such way to us     */    ngx_rbtree_insert_pt   insert;&#125;;typedef ngx_uint_t  ngx_rbtree_key_t;typedef ngx_int_t   ngx_rbtree_key_int_t;struct ngx_rbtree_node_s &#123;    ngx_rbtree_key_t       key;    ngx_rbtree_node_t     *left;    ngx_rbtree_node_t     *right;    ngx_rbtree_node_t     *parent;    u_char                 color;    u_char                 data;&#125;;

API
/* example to use */typedef struct my_node &#123;  xxx  ngx_rbtree_node_t rb_node;&#125; my_node_t;my_node_t node;ngx_rbtree_t rbtree;ngx_rbtree_node_t sentinel;ngx_rbtree_init(&amp;rbtree, &amp;sentinel, ngx_rbtree_insert_value);node.rb_node.key = 1;ngx_rbtree_insert(&amp;rbtree, &amp;node.rb_node);
hashHash is for quick searching, elements are distributed at different buckets based on hash_value % bucket_count, inside each bucket, it’s an array, compared one by one, so for hash, the important are hash function and bucket counter to distribute element evenly to reduce conflict. nginx has built-in hash function and bucket count calculated by nginx itself.

data structure
typedef struct &#123;    /* each element is &lt;key, value&gt; pair     * key: name     * value: any value     */    void             *value;    /* as you can see each element is not fixed!!     * each element size = sizeof(void*) + sizeof(u_short) + 1 + len     */    u_short           len;    u_char            name[1];&#125; ngx_hash_elt_t;typedef struct &#123;    /* bucket address */    ngx_hash_elt_t  **buckets;    /* the bucket counts */    ngx_uint_t        size;&#125; ngx_hash_t;typedef struct &#123;    /* passed in parameter --&gt;converted to ngx_hash_elt_t */    ngx_str_t         key;    ngx_uint_t        key_hash;    void             *value;&#125; ngx_hash_key_t;/* Above are three must structures for hash */typedef struct &#123;    ngx_hash_t        hash;    void             *value;&#125; ngx_hash_wildcard_t;/* ngx_hash_combined_t is not a must for hash, but a helper for nginx to use for some case * to create different hashes for different types, like this * server_name abc.com *.example.com test.com.*; */typedef struct &#123;    ngx_hash_t            hash;    /* abc.com */    ngx_hash_wildcard_t  *wc_head; /* *.example.com */    ngx_hash_wildcard_t  *wc_tail; /* test.com.* */&#125; ngx_hash_combined_t;typedef struct &#123;    ngx_hash_t       *hash;     /* passed in if NULL, created inside init function */    ngx_hash_key_pt   key;    ngx_uint_t        max_size; /* max bucket count, how many buckets */                                /* the real bucket count is calculated based on hash keys */    ngx_uint_t        bucket_size; /* how many bytes used by each bucket */    char             *name;    ngx_pool_t       *pool;    ngx_pool_t       *temp_pool; /* used for wildcard hash_init to allocate memory for checking conflict */&#125; ngx_hash_init_t;/* helper structure to add keys to the keys_arrays * it supports key conflict checking * then pass each key array to hash_init */typedef struct &#123;    ngx_uint_t        hsize; /* bucket number */    ngx_pool_t       *pool;    ngx_pool_t       *temp_pool;    /* key array of exact match like abc.com*/    ngx_array_t       keys; /* key array, each of ngx_hash_key_t */    ngx_array_t      *keys_hash; /* bucket array, each bucket is array of ngx_str_t temporary used for checking conflict when creating ngx_hash_key_t */       /* key array of wc_head like *.test.com */    ngx_array_t       dns_wc_head;    ngx_array_t      *dns_wc_head_hash;    /* key array of wc_tail like abc.test.* */    ngx_array_t       dns_wc_tail;    ngx_array_t      *dns_wc_tail_hash;&#125; ngx_hash_keys_arrays_t;

API
/* without helper function for hash keys */ngx_hash_init_t  hash;ngx_array_t      keys_array;ngx_hash_key_t  *hk;/* add temporary hash key, no key conflicting check */ngx_array_init(&amp;keys_array, cf-&gt;temp_pool, 32, sizeof(ngx_hash_key_t));hk = ngx_array_push(&amp;keys_array);hk-&gt;key = &quot;test&quot;;hk-&gt;key_hash = ngx_hash_key_lc(&quot;test&quot;, 4);hk-&gt;value = XX; //anything you wanthash.hash = NULL;hash.key = ngx_hash_key_lc;hash.max_size = 2048;hash.bucket_size = 64;hash.name = &quot;test_hash&quot;;hash.pool = cf-&gt;pool;hash.temp_pool = NULL;/* add hash keys to hash buckets */ngx_hash_init(&amp;hash, keys_array.elts, keys_array.nelts);/* finding */ngx_uint_t hash_value;hash_value = ngx_hash_key_lc(&quot;test&quot;, 4);ngx_hash_find(hash.hash, hash_value, &quot;test&quot;, 4);

// with helper functionngx_hash_init_t  hash;ngx_hash_keys_arrays_t keys_array;// few keysngx_hash_keys_array_init(&amp;keys_array, NGX_HASH_SMALL);//add keys to helper array(wildard support and conflict checking!!!)ngx_hash_add_key(&amp;keys_array, &quot;test&quot;, XXX, NGX_HASH_WILDCARD_KEY);ngx_hash_add_key(&amp;keys_array, &quot;*.test&quot;, YYY, NGX_HASH_WILDCARD_KEY);ngx_hash_add_key(&amp;keys_array, &quot;test.*&quot;, ZZZ, NGX_HASH_WILDCARD_KEY);hash.hash = NULL;hash.key = ngx_hash_key_lc;hash.max_size = 2048;hash.bucket_size = 64;hash.name = &quot;test_hash_helper&quot;;hash.pool = cf-&gt;pool;if (keys_array.keys.nelts) &#123;    ngx_hash_init(&amp;hash, keys_array.keys.elts, keys_array.keys.nelts);    hash.temp_pool = NULL;    ngx_hash_t *hash_normal = hash.hash; // save hash&#125;if (keys_array.dns_wc_head.nelts) &#123;    ngx_qsort(keys_array.dns_wc_head.elts,              (size_t) keys_array.dns_wc_head.nelts,              sizeof(ngx_hash_key_t), xx_compare_helper);    hash.temp_pool = XXX; //must set    ngx_hash_wildcard_init(&amp;hash, keys_array.dns_wc_head.elts, keys_array.dns_wc_head.nelts);    ngx_hash_t *hash_head = hash.hash; //save&#125;if (keys_array.dns_wc_tail.nelts) &#123;    ngx_qsort(keys_array.dns_wc_tail.elts,              (size_t) keys_array.dns_wc_tail.nelts,              sizeof(ngx_hash_key_t), xx_compare_helper);    hash.temp_pool = XXX; // must set    ngx_hash_wildcard_init(&amp;hash, keys_array.dns_wc_tail.elts, keys_array.dns_wc_tail.nelts);    ngx_hash_t *hash_tail = hash.hash; //save&#125;]]></content>
      <categories>
        <category>nginx</category>
        <category>data structure</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx-balancer-algorithm</title>
    <url>/2021/04/12/nginx-balancer-algorithm/</url>
    <content><![CDATA[AlgorithmNginx now supports following load-balancing disciplines

Round-robin and weighted round-robin
Least-connected and weighted least-connected
Source-ip hash-based, and a weighted one
Generic hash
Consistent hash

Rule to choose a algorithm for your website, we’ll consider pros and cons of each method to help you narrow the range of choices to consider.

Running Tests to Compare Methods
Whichever subset of load‑balancing methods you consider, we encourage you to test them to see which works best for your traffic. “Best” usually means shortest time to deliver responses to clients, but you might have different criteria. Testing is most straightforward if all servers have the same capacity. If not, you need to set server weights so that machines with more capacity receive more requests



Some metrics to check during testing are:  

CPU and memory load – Look at the percentage of total capacity used, for both CPU and memory. If all servers aren’t equally loaded, traffic is not being distributed efficiently.

Server response time – If the time is consistently higher for some servers than others, somehow “heavier” requests (requiring more computation or calls to a database or other services) are getting directed to them in an unbalanced way. Try adjusting the weights, because the imbalance might be caused by incorrect weights rather than by a problem with the load‑balancing technique.

Total time to respond to the client – Again, consistently higher times for some servers suggest they’re getting a disproportionate share of time‑consuming requests. And again, you can try adjusting weights to see if that eliminates the issue.

Errors and failed requests – You need to make sure that the number of failed requests and other errors during the tests is not larger than is usual for your site. Otherwise you’re basing your decision on error conditions instead of realistic traffic. For some errors, the server can send its response more quickly than when the request succeeds. For HTTP response code 404 (File Not Found), for example, the server probably returns the error much more quickly than it could deliver the actual file if it existed. With the Least Connections and Least Time load‑balancing algorithms, this can lead the load balancer to favor a server that is actually not working well.


Pros, Cons, and Use CasesHash and IP HashThe Hash and IP Hash load‑balancing techniques create a fixed association between a given type of client request (captured in the hash value) and a certain server. You might recognize this as session persistence  – all requests with a given hash value always go to the same server.
The biggest drawback of these methods is that they are not guaranteed to distribute requests in equal numbers across servers, let alone balance load evenly. The hashing algorithm evenly divides the set of all possible hash values into “buckets”, one for each server in the upstream group, but there’s no way to predict whether the requests that actually occur will have hashes that are evenly distributed. 
So it makes sense to use Hash or IP Hash when the benefit of maintaining sessions outweighs the possibly bad effects of unbalanced load. They are the only form of session persistence available in NGINX.
There are a couple cases where IP Hash – and Hash when the client IP address is in the key – don’t work:

When the client’s IP address can change during the session, for example when a mobile client switches from a WiFi network to a cellular one.
When the requests from a large number of clients are passing through a forward proxy, because the proxy’s IP address is used for all of them.

Changing the set of upstream servers usually forces recalculation of at least some of the mappings, breaking session persistence. You can reduce the number of recalculated mappings somewhat:

For the Hash method, include the consistent parameter to the hash directive
For the IP Hash method, before removing a server from the upstream group temporarily, add the down parameter to its server directive, as for web2 in the following example. The mappings are not recalculated, on the assumption that the server will soon return to service  upstream backend &#123;    ip_hash;    server web1;    server web2 down;    server web3;&#125;

Round RobinThe general consensus is that Round Robin works best when the characteristics of the servers and requests are unlikely to cause some servers to become overloaded relative to others. Some of the conditions are:

All the servers have about the same capacity. This requirement is less important if differences between servers are accurately represented by server weights.
All the servers host the same content.
Requests are pretty similar in the amount of time or processing power they require. If there’s a wide variation in request weight, a server can become overloaded because the load balancer happens to send it a lot of heavyweight requests in quick succession.
Traffic volume is not heavy enough to push servers to near full capacity very often. If servers are already heavily loaded, it’s more likely that Round Robin’s rote distribution of requests will push some servers “over the edge” into overload as described in the previous bullet.

Least ConnectionsLeast Connections is the most suitable load‑balancing technique for the widest range of use cases, and particularly for production traffic.
Least Connections also effectively distributes workload across servers according to their capacity. A more powerful server fulfills requests more quickly, so at any given moment it’s likely to have a smaller number of connections still being processed (or even waiting for processing to start) than a server with less capacity. Least Connections sends each request to the server with the smallest number of current connections, and so is more likely to send requests to powerful servers.
(weighted)Least ConnectionIt is designed to distribute the load evenly among upstream servers, by selecting the one with the fewest number of active connections. If the upstream servers do not all have the same processing power, this can be indicated using the weight parameter to the server directive. The algorithm will take into account the different weighted servers when calculating the number of least connections.
upstream bakend &#123;     least_conn;       server 192.168.1.1:8080 weight=1;        server 192.168.1.2:8080 weight=2;    &#125; 
(Weighted)Round RobinThe load balancer runs through the list of upstream servers in sequence, assigning the next connection request to each one in turn. while weight is used to configure the weight. The default value is 1. The higher the weight, the more requests will be allocated to this server.The weight should be set according to the actual processing capacity of the server.
upstream bakend &#123;        server 192.168.1.1:8080 weight=1;        server 192.168.1.2:8080 weight=2;    &#125; 
(weighted)Generic HashWith the Hash method, for each request the load balancer calculates a hash that is based on the combination of text and NGINX variables you specify, and associates the hash with one of the servers. It sends all requests with that hash to that server, so this method establishes a basic kind of session persistence.
upstream bakend &#123;     hash $scheme$request_uri;    server 192.168.1.1:8080 weight=1;        server 192.168.1.2:8080 weight=2;    &#125; 
(weighted)IP HashThe certain IP addresses should always be mapped to the same upstream server. Nginx does this by using the first three octets of an IPv4 address or the entire IPv6 address, as a hashing key. The same pool of IP addresses are therefore always mapped to the same upstream server. So, this mechanism isn’t designed to ensure a fair distribution, but rather a consistent mapping between the client and upstream server. It is very useful for sessions, such as php store sessions on files which located on every web servers, in which case, it will be difficult to synchronize.
Why use first three octets of an IPV4, it is designed to optimize for ISP clients that are assigned IP addresses dynamically from a subnetwork (/24) range. In case of reboot or reconnection, the client’s address often changes to a different one in the /24 network range, but the connection still represents the same client, so there’s no reason to change the mapping to the server.
If, however, the majority of the traffic to your site is coming from clients on the same /24 network, IP Hash doesn’t make sense because it maps all clients to the same server. In that case (or if you want to hash on all four octets for another reason), instead use the Hash method with the $remote_addr variable.hash $remote_addr;
upstream bakend &#123;    ip_hash;        server 192.168.1.1:8080 weight=1;        server 192.168.1.2:8080 weight=2;    &#125; 
(weighted)Consistent HashConsistent hash algorithm: consistent_ Key is dynamically specified.
upstream bakend &#123;     hash $consistent_key consistent;       server 192.168.1.1:8080 weight=1;        server 192.168.1.2:8080 weight=2;    &#125; ]]></content>
      <categories>
        <category>nginx</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_conf_layout</title>
    <url>/2020/12/01/nginx-conf-layout/</url>
    <content><![CDATA[Overviewhttp &#123;    server &#123;        location /sdf &#123;        &#125;    &#125;&#125;

each block has a context, child block may share part of context with its parent, like server block shares main_conf with its parent http block, location block shares main_conf and srv_conf with its parent server block, except the sharing part, each context preserve a slot for all HTTP modules, each HTTP module has it own create_main&#x2F;create_srv&#x2F;create_loc if it wants to save parameters from cmd at different scopes(MAIN_CONF&#x2F;SER_CONF&#x2F;LOC_CONF), let’s say an example to explain it.

if a HTTP module has a command can be used only at MAIN_CONF scope, it only needs to provide create_main function which only called at http block {}
if a HTTP module has a command can be used only at SRV_CONF scope,  it has to provide create_srv function called both by http block &#123;&#125; and server block &#123;&#125;, but http block {} created part is not used actually because command is not allowed to use at http block, no merge needed.
if a module has a command can be used at MAIN_CONF/SRV_CONF scope, it has to provide create_srv function called both by http block &#123;&#125; and server block &#123;&#125;, then merge the value set at http block to server block {} at last.

conf layout
http
For each HTTP module that provides create_srv, it’s called once at http level, then for each server {}, it’s called once at server level as well, that means we merge http {} same srv_conf with different srv_conf at server {}.
stream
]]></content>
      <categories>
        <category>nginx</category>
        <category>conf</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_filter_framework</title>
    <url>/2020/12/01/nginx-filter-framework/</url>
    <content><![CDATA[Overviewnginx supports two kinds of filter when sending response to client, one is header filter, the other is body filter, header filter is called before sending response header to client, body filter is called before sending body to client, you can create your own filter module to do anything(mostly related to header manipulation or body manipulation) without break any nginx source code.


filter framework
As you can see, filter is like chain, the last configured one is called first, because the last one is the one who sets ngx_http_top_body_filter and ngx_http_top_header_filter which are called by ngx_http_output_filter() and ngx_http_send_header().
]]></content>
      <categories>
        <category>nginx</category>
        <category>filter</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_http_core_phase_event_handler</title>
    <url>/2020/12/01/nginx-http-core-phase-event-handler/</url>
    <content><![CDATA[Overviewnginx provides several phases for developer, developer can register its own handler to some phases, do its own work without breaking nginx source code, it’s flexible, here are phases with order(runs from top to bottom).

NGX_HTTP_SERVER_REWRITE_PHASE
NGX_HTTP_FIND_CONFIG_PHASE
NGX_HTTP_REWRITE_PHASE
NGX_HTTP_POST_REWRITE_PHASE
NGX_HTTP_PREACCESS_PHASE
NGX_HTTP_ACCESS_PHASE
NGX_HTTP_POST_ACCESS_PHASE
NGX_HTTP_PRECONTENT_PHASE
NGX_HTTP_CONTENT_PHASE
NGX_HTTP_LOG_PHASE

each phase has a checker, nginx phase engine runs the checker with fixed order, inside each checker there could be several handlers, each checker runs handlers of that phase.
Note: phase is for client request(filter is for response), engine runs after parsing all request headers.

HTTPphase
Data Structure
typedef struct ngx_http_phase_handler_s  ngx_http_phase_handler_t;typedef ngx_int_t (*ngx_http_phase_handler_pt)(ngx_http_request_t *r, ngx_http_phase_handler_t *ph);typedef ngx_int_t (*ngx_http_handler_pt)(ngx_http_request_t *r);/* each handler has such instance */struct ngx_http_phase_handler_s &#123;    /* each phase has a checker which calls handler     * and does extra work after handler returns     */    ngx_http_phase_handler_pt  checker;    ngx_http_handler_pt        handler;    /* next phase to run, if handler does not run sequently, quick jump in some case,     * next is not always used, but used in some rare case     *     * most phase handler     * next: point to the subsequent phase     *     * post rewrite phase handler     * next: points to find_conf_phase, used when uri change(rewrite change uri)     * in some phase, next is not used actually     */    ngx_uint_t                 next;&#125;;typedef struct &#123;    /* all ordered handlers from all phases used for runtime */    ngx_http_phase_handler_t  *handlers;    /* server_rewrite_index:   NGX_HTTP_SERVER_REWRITE_PHASE(first handler index)     * location_rewrite_index: NGX_HTTP_REWRITE_PHASE(first handler index)     * for quick jump in some case     */    ngx_uint_t                 server_rewrite_index;    ngx_uint_t                 location_rewrite_index;&#125; ngx_http_phase_engine_t;typedef struct &#123;    /* phase_engine is used runtime which copies handlers from each phase     * and save them in a large array phase_engine-&gt;handlers     * each element is ngx_http_phase_handler_s which has the copied handler     * and other info like phase checker and next phase to run     */    ngx_http_phase_engine_t    phase_engine;    /* stores all handlers for each phase, after initialized each module     * as a module can register handler at different phases     * we save that handler to the given phase     *     * each phase is an array which stores all handlers     * for that phase, each handler takes one slot     * ngx_http_handler_pt     *----------------------------------------------------     * it&#x27;s temporary use, NOT used runtime     *----------------------------------------------------     */    ngx_http_phase_t           phases[NGX_HTTP_LOG_PHASE + 1];&#125; ngx_http_core_main_conf_t;
API
// construct runtime phase enginestatic ngx_int_t ngx_http_init_phase_handlers(ngx_conf_t *cf, ngx_http_core_main_conf_t *cmcf)void ngx_http_core_run_phases(ngx_http_request_t *r)&#123;    ngx_int_t                   rc;    ngx_http_phase_handler_t   *ph;    ngx_http_core_main_conf_t  *cmcf;    cmcf = ngx_http_get_module_main_conf(r, ngx_http_core_module);    /* all handlers */    ph = cmcf-&gt;phase_engine.handlers;    /* checker converts handler return value     *     * handler return      checker return     *     * NGX_DECLINE         NGX_AGAIN: next handler     * NGX_DONE            NGX_OK:    run phase done     * NGX_OK              different based on checker     * NGX_XX(error)       finalize_request, NGX_OK, run phase done     * NGX_YYY(not error)  based on checker     *     * more detail refer to each checker definition     */    while (ph[r-&gt;phase_handler].checker) &#123;        rc = ph[r-&gt;phase_handler].checker(r, &amp;ph[r-&gt;phase_handler]);        if (rc == NGX_OK) &#123;            /* return only when checker returns NGX_OK */            return;        &#125;    &#125;&#125;

event handlerFor http request, the event handler changes during what it’s processing, process header or process body, here is diagram shows how they are changing during processing request.

]]></content>
      <categories>
        <category>nginx</category>
        <category>phase</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_event_framework</title>
    <url>/2020/12/01/nginx-event-framework/</url>
    <content><![CDATA[OverviewEach connection(preallocated for fast) has two events, one for read, the other for write, after nginx adds connection(fd) EPOLLIN and EPOLLOUT to epoll instance(each worker has its own), when event is ready, call handler saved at proper event in epoll directly, or adds the event in post queue, after epoll quits, at last step call event-&gt;handler.

Event framework
Data Structure
struct ngx_event_s &#123;    /* points to ngx_connection_s */    void            *data;    unsigned         write:1;    /* listen connection&#x27;s read event set it */    unsigned         accept:1;    /* active means add to epoll */    unsigned         active:1;    /* data is ready to read or has buffer to write */    unsigned         ready:1;    /* handler for event */    ngx_event_handler_pt  handler;&#125;struct ngx_connection_s &#123;    /* next connection if in free connection list */    void               *data;    ngx_event_t        *read;    ngx_event_t        *write;    ngx_socket_t        fd;&#125;

Timer eventwhen struct ngx_event_s is used as timer event, most of its fields are not used, only few are useful for timer event. when user adds a timer event, nginx adds the event to timer RB tree, the key is absolute time(expire time), each epoll_wait() call, use the minimal value of timer tree as timeout parameter, after epoll_wait() returns, it could be data ready or timeout, then check the timer event in RB tree, to see if some of them expire or not.
if there is no timer, epoll_wait will block for ever, wake up only when there is event on socket like data ready, EOF, error, interrupt etc.
struct ngx_event_t &#123;  unsigned         timedout:1;         /* timer expires or not */  unsigned         timer_set:1;        /* timer in rb tree or not */  ngx_event_handler_pt  handler;       /* handler for this timer event */  ngx_rbtree_node_t   timer;           /* timer event key(absolute time) */&#125;]]></content>
      <categories>
        <category>nginx</category>
        <category>event</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_http_request_upstream_close</title>
    <url>/2020/12/01/nginx-http-request-upstream-close/</url>
    <content><![CDATA[OverviewAfter a request is served, we need to free resource used by that request, nginx provides two main API(high level) to free downstream and upstream side.

ngx_http_finalize_request: free request and client connection etc
ngx_http_upstream_finalize_request: free upstream peer resource etc

Note: not always both APIs called for each request, it depends on config and phase it’s in,  call proper API(some time lower level API) to free resource.

How request is finalized
Most normal case: Step1—&gt;Step2—&gt;Step3—&gt;Step4case1no upstream is configured(no proxy_pass): only ngx_http_finalize_request() is needed if request is created
case2upstream is configured, but error happens before connect with upstream, only ngx_http_finalize_request() is needed.
case3upstream is configured, connected with upstream,  everything is ok, after send body to client, ngx_http_upstream_finalize_request ()is called(OK) firstly which invokes ngx_http_finalize_request() as well. this is the most normal case
case4upstream is configured, connected with upstream,error happens at upstream side before request is served, ngx_http_upstream_finalize_request() is called(ERROR) which invokes ngx_http_finalize_request() as well.
case5upstream is configured, connected with upstream, error happens at downside side before request is served, ngx_http_finalize_request() is called firstly which invokes ngx_http_upstream_finalize_request() as it’s cleanup chain.
In a short word, we should free both side if created, no matter error or ok, if no error, ngx_http_upstream_finalize_request is called firstly, otherwise, error side calls firstly
Note: some times low level API are called depends on what phase it’s in and what resource is created
]]></content>
      <categories>
        <category>nginx</category>
        <category>request_close</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx-log</title>
    <url>/2021/01/07/nginx-log/</url>
    <content><![CDATA[Loggingnginx provides two kinds of logs, ‘error’ log and debug log, you can use several APIs(macro) to log info to console or file, let’s explain how to use these APIs in detail.
support formats
/* src/core/ngx_string.c * supported formats: *    %[0][width][x][X]O        off_t *    %[0][width]T              time_t *    %[0][width][u][x|X]z      ssize_t/size_t *    %[0][width][u][x|X]d      int/u_int *    %[0][width][u][x|X]l      long *    %[0][width|m][u][x|X]i    ngx_int_t/ngx_uint_t *    %[0][width][u][x|X]D      int32_t/uint32_t *    %[0][width][u][x|X]L      int64_t/uint64_t *    %[0][width|m][u][x|X]A    ngx_atomic_int_t/ngx_atomic_uint_t *    %[0][width][.width]f      double, max valid number fits to %18.15f *    %P                        ngx_pid_t *    %M                        ngx_msec_t *    %r                        rlim_t *    %p                        void * *    %V                        ngx_str_t * *    %v                        ngx_variable_value_t * *    %s                        null-terminated string *    %*s                       length and string *    %Z                        &#x27;\0&#x27; *    %N                        &#x27;\n&#x27; *    %c                        char *    %%                        % * *  reserved: *    %t                        ptrdiff_t *    %S                        null-terminated wchar string *    %C                        wchar */

usageuser can configure log level like this
error /tmp/log debug;

level can be:

emerg
alert
crit
error
warn
notice
info
debug     -&gt; debug all

specific debug component  

debug_core
debug_alloc
debug_mutex
debug_event
debug_http
debug_mail
debug_stream

error logthere are only two high level APIs user should use

ngx_conf_log_error(level, ): log error during conf
ngx_log_error(level,): log error during runtime

level can be

#define NGX_LOG_EMERG             1
#define NGX_LOG_ALERT             2
#define NGX_LOG_CRIT              3
#define NGX_LOG_ERR               4
#define NGX_LOG_WARN              5
#define NGX_LOG_NOTICE            6
#define NGX_LOG_INFO              7
#define NGX_LOG_DEBUG             8

ngx_conf_log_error(NGX_LOG_EMERG, cf, 0,                   &quot;directive \&quot;%s\&quot; is not terminated by \&quot;;\&quot;&quot;,                   name-&gt;data);ngx_log_error(NGX_LOG_CRIT, cycle-&gt;log, ngx_socket_errno,              &quot;the inherited socket #%d has &quot;              &quot;an unsupported protocol family&quot;, ls[i].fd);

error_log /tmp/log info;
Note: ngx_log_error is always complied into source code, logging or not depends on what level user configured
debug logthere are several high level APIs(macros) user can use

ngx_log_debug0
ngx_log_debug1
ngx_log_debug2
ngx_log_debug3
ngx_log_debug4
ngx_log_debug5
ngx_log_debug6
ngx_log_debug7
ngx_log_debug8

level only limited to these below, any others like above will not be printed(like NGX_LOG_DEBUG, not printed if use ngx_log_debugx API)

#define NGX_LOG_DEBUG_CORE        0x010
#define NGX_LOG_DEBUG_ALLOC       0x020
#define NGX_LOG_DEBUG_MUTEX       0x040
#define NGX_LOG_DEBUG_EVENT       0x080
#define NGX_LOG_DEBUG_HTTP        0x100
#define NGX_LOG_DEBUG_MAIL        0x200
#define NGX_LOG_DEBUG_STREAM      0x400

ngx_log_debug1(NGX_LOG_DEBUG_CORE, cycle-&gt;log,...);// wrong use!!! never print it out// #define NGX_LOG_DEBUG             8// error_log set debug log-&gt;log_level = NGX_LOG_DEBUG_ALL(0x7ffffff0)// (log)-&gt;log_level &amp; level == 0, so never print it outngx_log_debug1(NGX_LOG_DEBUG, cycle-&gt;log,...);
Note:

when call ngx_log_debugxxx, should never pass non-debug level to these APIs, even it’s permitted.
ngx_log_debugxxx is complied into source code, only when --with-debug is configured
logging or not depends on what level user configured

error_log /tmp/log debug;

inside these logging APIShigh level APIS call low level APIs with some checks, but user should never call these low level APIs
// log_level is user setting// log_error check &gt;=#define ngx_log_error(level, log, ...)                                        \    if ((log)-&gt;log_level &gt;= level) ngx_log_error_core(level, log, __VA_ARGS__)// log_debug check &amp;, ngx_log_debugx calls ngx_log_debug#define ngx_log_debug(level, log, ...)                                        \    if ((log)-&gt;log_level &amp; level)                                             \        ngx_log_error_core(NGX_LOG_DEBUG, log, __VA_ARGS__)

For debug log(NGX_LOG_DEBUG_HTTP), it’s logged only when

–with-debug is configured by user
user set log level(log-&gt;log_level) which has that bit

Note: when error_log set debug log-&gt;log_level &#x3D; NGX_LOG_DEBUG_ALL(0x7ffffff0)
]]></content>
      <categories>
        <category>nginx</category>
        <category>logging</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_http_upstream_conf</title>
    <url>/2020/12/01/nginx-http-upstream-conf/</url>
    <content><![CDATA[Overviewupstream holds the backend information, how to select backend and free the peer connection based on balance algorithm.
explicit upstream defined in this way.
upstream up_1 &#123;    ip_hash;    server 1.1.1.1:443 weight=1;    server 1.1.1.1:443 weight=2;&#125;location /abc &#123;    proxy_pass: https://up_1;&#125;
There is also another way to define an upstream called implicit upstream, for implicit upstream, there is no way to define balance algorithm(round robin is used) and set parameters for it.
location /abc &#123;    proxy_pass: https://www.google.com/;&#125;


Http upstreamlocation may refer to an upstream by proxy_pass directive, that means at conf phase, we link the location to the upstream, later on when request matches that location, we select a server from the linked upstream, create a connection for the selected server, send request to it.

how server is selectedserver selection depends on balance algorithm, nginx supports several algorithms

round robin
least connection
ip hash

each algorithm must provide peer.get and peer.free

peer.get(): select a server, update counter, its return value:
NGX_DONE: get a connected connection
NGX_OK: get a server, need to create new connection
NGX_BUSY: no server is available


peer.free(): free resource(free counter etc) used by peer.get.

what port is used when connecting upstream serverit depends how upstream is defined and used.
explicit upstreamwe use port defined by server in upstream itself, if server has no port configured, for proxy_pass http:// use 80, for proxy_pass https:// use 443, never use port from url itself
http &#123;    upstream ups &#123;        server 1.1.1.1;        server 2.2.2.2:8000;    &#125;    server &#123;        listen 10.10.10.10;        location /a &#123;            # 8080 is ignored            proxy_pass http://ups:8080;        &#125;    &#125;    server &#123;        listen 10.10.10.10 ssl;        location /b &#123;            # 8080 is ignored            proxy_pass https://ups:8080;        &#125;    &#125;&#125;

implicit upstreamwe always use port from url itself, if no port in url, use 80 for http, 443 for https when connection with backend.
http &#123;    server &#123;        listen 10.10.10.10 ssl;        location / &#123;            # proxy_pass https://www.google.com;            proxy_pass https://www.google.com:8443;        &#125;    &#125;&#125;

uri sent to upstream serveruri sent to upstream server depends on how proxy_pass is used.
case: proxy_pass without uri  
# the request URI is passed to the server in the same form as by a client when the original request is processedlocation /abc &#123;    proxy_pass http://127.0.0.1;&#125;$curl http://test.com/abcd/cool# uri sent to upstream: /abcd/cool

case: proxy_pass has uri which uses variable  
# we use uri from variable directly to replace original uriproxy_pass http://127.0.0.1$request_uri;

case: proxy_pass with uri which is not variable  
# the part of a normalized request URI matching the location is replaced by a URI specified in the directivelocation /abc &#123;    proxy_pass http://127.0.0.1/hot;&#125;$curl http://test.com/abcd/cool# uri sent to upstream: /hot/d/cool
details refer to ngx_http_proxy_create_request().
]]></content>
      <categories>
        <category>nginx</category>
        <category>upstream conf</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_process_headers</title>
    <url>/2020/12/07/nginx-process-headers/</url>
    <content><![CDATA[Overviewnginx provides flexible way to manipulate headers, you can do this by directive or by lua, here we only say directive, headers can be request header or response header, that means nginx gives you the way to manipulate request header before sending to upstream and response header before sending to client.nginx supports

add a header
update existing header
delete a header

request header related directive before sending to upstream

proxy_set_header

response header related directives before sending to client

add_header
proxy_ignore_headers
proxy_hide_header
proxy_pass_header


Header buildingFor proxy header sent to upstream, it depends on three parts.

request sent(header in request itself) lowest priority
default behavior for some proxy headers
user setting by directive(proxy_set_header) highest priority

First check user setting, if has, use that value, otherwise, use default behavior for some proxy headers, if not header has no default behavior, just pass it to upstream.
For response header sent to client, it depends on two parts.

ignore&#x2F;hide
user setting by directive(add_header) highest priority

First check user setting, if has, use that value, otherwise not send to client if ignored or hidden.
Data Structureproxy header(most headers from client) sent to upstream
typedef struct &#123;    /* as proxy headers value can have variable, so need to compile it, then saved at values(handler)     * hash is the hash table for proxy header name     */    ngx_array_t                   *flushes;    ngx_array_t                   *lengths;    ngx_array_t                   *values;    ngx_hash_t                     hash;&#125; ngx_http_proxy_headers_t;typedef struct &#123;    ngx_str_t   key;    ngx_str_t   value;&#125; ngx_keyval_t;typedef struct &#123;    ngx_http_upstream_conf_t       upstream;   /* headers(used runtime sent to backend) are merged result from two parts     * default proxy header                         : ngx_http_proxy_headers     * header set by proxy_set_header directive     : headers_source(high priority)     */    ngx_http_proxy_headers_t       headers;    /* headers set by proxy_set_header directive, user defines     * proxy_set_header supports using variable as value like this:     * proxy_set_header Host       $http_host;     */    ngx_array_t                   *headers_source; // ngx_keyval_t array&#125;;/* * default proxy headers(overwrite such header if client sends it as well) * these headers will be sent/removed when sending request to backend * * like &#123; ngx_string(&quot;TE&quot;), ngx_string(&quot;&quot;)&#125; means remove such header * when sending request to backend, even client sends it. * * these can be overwritten by proxy_set_header * because we merge these two parts during conf * but proxy_set_header has higher priority */static ngx_keyval_t  ngx_http_proxy_headers[] = &#123;    &#123; ngx_string(&quot;Host&quot;), ngx_string(&quot;$proxy_host&quot;) &#125;,    /* if no set default Connection header is set with Connection: close when send request to upstream */    &#123; ngx_string(&quot;Connection&quot;), ngx_string(&quot;close&quot;) &#125;,    &#123; ngx_string(&quot;Content-Length&quot;), ngx_string(&quot;$proxy_internal_body_length&quot;) &#125;,    &#123; ngx_string(&quot;Transfer-Encoding&quot;), ngx_string(&quot;$proxy_internal_chunked&quot;) &#125;,    &#123; ngx_string(&quot;TE&quot;), ngx_string(&quot;&quot;) &#125;,    &#123; ngx_string(&quot;Keep-Alive&quot;), ngx_string(&quot;&quot;) &#125;,    &#123; ngx_string(&quot;Expect&quot;), ngx_string(&quot;&quot;) &#125;,    &#123; ngx_string(&quot;Upgrade&quot;), ngx_string(&quot;&quot;) &#125;,    &#123; ngx_null_string, ngx_null_string &#125;&#125;;

response header sent to client(hide or ignore some headers)
// response header sent to client(hide or ignore some headers)typedef struct &#123;    /* ignore header bit for supported ignore header, user can only ignore these headers     * you can ignore processing these response headers(limit scope) from backend     * no set var etc     * ngx_conf_bitmask_t  ngx_http_upstream_ignore_headers_masks[] = &#123;     *     &#123; ngx_string(&quot;X-Accel-Redirect&quot;), NGX_HTTP_UPSTREAM_IGN_XA_REDIRECT &#125;,     *     &#123; ngx_string(&quot;X-Accel-Expires&quot;), NGX_HTTP_UPSTREAM_IGN_XA_EXPIRES &#125;,     *     &#123; ngx_string(&quot;X-Accel-Limit-Rate&quot;), NGX_HTTP_UPSTREAM_IGN_XA_LIMIT_RATE &#125;,     *     &#123; ngx_string(&quot;X-Accel-Buffering&quot;), NGX_HTTP_UPSTREAM_IGN_XA_BUFFERING &#125;,     *     &#123; ngx_string(&quot;X-Accel-Charset&quot;), NGX_HTTP_UPSTREAM_IGN_XA_CHARSET &#125;,     *     &#123; ngx_string(&quot;Expires&quot;), NGX_HTTP_UPSTREAM_IGN_EXPIRES &#125;,     *     &#123; ngx_string(&quot;Cache-Control&quot;), NGX_HTTP_UPSTREAM_IGN_CACHE_CONTROL &#125;,     *     &#123; ngx_string(&quot;Set-Cookie&quot;), NGX_HTTP_UPSTREAM_IGN_SET_COOKIE &#125;,     *     &#123; ngx_string(&quot;Vary&quot;), NGX_HTTP_UPSTREAM_IGN_VARY &#125;,     *     &#123; ngx_null_string, 0 &#125;     * &#125;;     * by default, no header is ignored     */    ngx_uint_t                       ignore_headers;    /* hide_headers for quick searching */    ngx_hash_t                       hide_headers_hash;    /* by default nginx will hide some headers when send response to client     * these headers like Server, you can also add more hide header by proxy_hide_header     *     * if you don&#x27;t hide some default headers you can use     * proxy_pass_header to exclude them from hide_headers     */    /* both ignore and hiding headers are not processed, so you can&#x27;t get that header value by ngx.header.xx     * but ignore header has limit group, only support some headers     * hide header can hide any header, that&#x27;s the difference     /    ngx_array_t                     *hide_headers;    ngx_array_t                     *pass_headers;&#125; ngx_http_upstream_conf_t;static ngx_str_t  ngx_http_proxy_hide_headers[] = &#123;    /* by default, we did NOT send these to client, but you can use proxy_pass_header to exclude a header     * let nginx sends it to client     */    ngx_string(&quot;Date&quot;), // exception: Date means NOT use Date header from backend, but use nginx cache time, still send Date to client!!!    ngx_string(&quot;Server&quot;),    ngx_string(&quot;X-Pad&quot;),    ngx_string(&quot;X-Accel-Expires&quot;),    ngx_string(&quot;X-Accel-Redirect&quot;),    ngx_string(&quot;X-Accel-Limit-Rate&quot;),    ngx_string(&quot;X-Accel-Buffering&quot;),    ngx_string(&quot;X-Accel-Charset&quot;),    ngx_null_string&#125;;typedef struct &#123;    // set by add_header directive(user setting response header)    ngx_array_t               *headers;&#125; ngx_http_headers_conf_t;

// each request has such instancestruct ngx_http_upstream_s &#123;    /* all parsed headers from backend */    ngx_http_upstream_headers_in_t   headers_in;&#125;;struct ngx_http_request_s &#123;    ngx_http_upstream_t        *upstream;    ngx_http_headers_in_t             headers_in; // request headers from client    ngx_http_headers_out_t            headers_out; // response header sent to client&#125;typedef struct &#123;    ngx_list_t                        headers;    ngx_list_t                        trailers;    ngx_uint_t                        status;    ngx_str_t                         status_line;    ngx_table_elt_t                  *server;    ngx_table_elt_t                  *date;    ngx_table_elt_t                  *content_length;    ngx_table_elt_t                  *content_encoding;    ngx_table_elt_t                  *location;    ngx_table_elt_t                  *refresh;    ngx_table_elt_t                  *last_modified;    ngx_table_elt_t                  *content_range;    ngx_table_elt_t                  *accept_ranges;    ngx_table_elt_t                  *www_authenticate;    ngx_table_elt_t                  *expires;    ngx_table_elt_t                  *etag;    ngx_str_t                        *override_charset;    size_t                            content_type_len;    ngx_str_t                         content_type;    ngx_str_t                         charset;    u_char                           *content_type_lowcase;    ngx_uint_t                        content_type_hash;    ngx_array_t                       cache_control;    ngx_array_t                       link;    off_t                             content_length_n;    off_t                             content_offset;    time_t                            date_time;    time_t                            last_modified_time;&#125; ngx_http_headers_out_t;typedef struct &#123;    /* all headers parsed, each is a ngx_table_elt_t     * below header pointer points to the element in     * the list     */    ngx_list_t                        headers;    /* shortcut for HOST header in headers array     * the ngx_table_elt_t-&gt;value points to r-&gt;buf     * NOT copy from r-&gt;buf!!!     */    ngx_table_elt_t                  *host;    ngx_table_elt_t                  *connection;    ngx_table_elt_t                  *if_modified_since;    ngx_table_elt_t                  *if_unmodified_since;    ngx_table_elt_t                  *if_match;    ngx_table_elt_t                  *if_none_match;    ngx_table_elt_t                  *user_agent;    ngx_table_elt_t                  *referer;    ngx_table_elt_t                  *content_length;    ngx_table_elt_t                  *content_range;    ngx_table_elt_t                  *content_type;    ngx_table_elt_t                  *range;    ngx_table_elt_t                  *if_range;    ngx_table_elt_t                  *transfer_encoding;    ngx_table_elt_t                  *te;    ngx_table_elt_t                  *expect;    ngx_table_elt_t                  *upgrade;#if (NGX_HTTP_GZIP || NGX_HTTP_HEADERS)    ngx_table_elt_t                  *accept_encoding;    ngx_table_elt_t                  *via;#endif    ngx_table_elt_t                  *authorization;    ngx_table_elt_t                  *keep_alive;#if (NGX_HTTP_X_FORWARDED_FOR)    ngx_array_t                       x_forwarded_for;#endif#if (NGX_HTTP_REALIP)    ngx_table_elt_t                  *x_real_ip;#endif#if (NGX_HTTP_HEADERS)    ngx_table_elt_t                  *accept;    ngx_table_elt_t                  *accept_language;#endif#if (NGX_HTTP_DAV)    ngx_table_elt_t                  *depth;    ngx_table_elt_t                  *destination;    ngx_table_elt_t                  *overwrite;    ngx_table_elt_t                  *date;#endif&#125; ngx_http_headers_in_t;

API// build r-&gt;headers_in from request headerstatic void ngx_http_process_request_headers(ngx_event_t *rev);//build header sent to upstream(backend), create a temp buffer to hold all header copied from r-&gt;headers_instatic ngx_int_t ngx_http_proxy_create_request(ngx_http_request_t *r);// build header sent to client(set r-&gt;headers_out) when parsed all response headersstatic ngx_int_t ngx_http_upstream_process_headers(ngx_http_request_t *r, ngx_http_upstream_t *u);// header filter to add/update/remove(manipulate r-&gt;headers_out) response headerstatic ngx_int_t ngx_http_headers_filter(ngx_http_request_t *r);// last header filter based on r-&gt;headers_out create a temp buffer to hold all header copied from r-&gt;headers_out, then send it outstatic ngx_int_t ngx_http_header_filter(ngx_http_request_t *r)]]></content>
  </entry>
  <entry>
    <title>nginx_http_log</title>
    <url>/2020/12/09/nginx-http-log/</url>
    <content><![CDATA[Overviewnginx supports two kinds of log, one is error log, the other is access log, error log can happen at any time when process request, but access log only happens when a request is finalized, once a time.

log typeerror logTo write an error log, you need to enable it firstly by logging to a file or to syslog
# write error log to a particular fileerror_log /var/log/error.log info;
later on in your source code, call ngx_log_error() with info that you want to write, more details LOG API
add connection and http meta when writing error logIn most case, we’re processing http request, we want to add request metadata when writing log, nginx provides log handler for this, when log instance is used, handler is called automatically to add extra info, save your time to write that repeated info.
such handler is called only for non DEBUG log, most used when error happens(ngx_log_error())
client connection
// when call ngx_log_error(log_level, c-&gt;log, ...); ngx_http_log_error is called by ngx_log_error_core if log_level is NOT DEBUGc-&gt;log-&gt;handler = ngx_http_log_error;r-&gt;log_handler = ngx_http_log_error_handler;/* ngx_http_log_error: add client and server info * ngx_http_log_error_handler: add request_line, Host header, upstream info */
access logFor each request, there is only one access log which is written at last when request is finalized, by default access_log is enabled with specific format, but you can change to any format you want.
http  &#123;    # default access log format    # &quot;$remote_addr - $remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent&quot;    log_format access_log_format &#x27;Client.Ip: &quot;$remote_addr&quot;, Client.Port: &quot;$remote_port&quot;&#x27;;    # use predefined format access_log_format from above    access_log /var/log/nginx-access.log access_log_format;&#125;
insideaccess log is an NGX_HTTP_LOG_PHASE handler, which is called when request is freed ngx_http_free_request()-&gt;ngx_http_log_request(), ngx_http_log_request() calls handlers of NGX_HTTP_LOG_PHASE, access log handler ngx_http_log_handler() is one which writes access log to file.
]]></content>
      <categories>
        <category>nginx</category>
        <category>http log</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_process_request_and_response_body</title>
    <url>/2020/12/17/nginx-process-request-and-response-body/</url>
    <content><![CDATA[Overviewnginx provides flexible way when processing header and body to meet different network env to achieve better performance, say nginx can send the request immediately after get the whole request headers, or send the request to upstream after get the entire body, when cache request body, we can cache it in memory if memory is large enough or cache it to a file.Also we can cache the response body to a file or send it immediately to client, all are configurable!!!

Body processingrequestrequest header buffer
Before talking about request body, let’s say process header buffer first, here are directives related to request header
Syntax:	client_header_buffer_size size;Default: client_header_buffer_size 1k;Context: http, server
Sets buffer size for reading client request header. For most requests, a buffer of 1K bytes is enough. However, if a request includes long cookies, or comes from a WAP client, it may not fit into 1K. If a request line or a request header field does not fit into this buffer then larger buffers, configured by the large_client_header_buffers directive, are allocated.
Syntax:	large_client_header_buffers number size;Default: large_client_header_buffers 4 8k;Context: http, server
Sets the maximum number and size of buffers used for reading large client request header. A request line cannot exceed the size of one buffer, or the 414 (Request-URI Too Large) error is returned to the client. A request header field(one header) cannot exceed the size of one buffer as well, or the 400 (Bad Request) error is returned to the client. Buffers are allocated only on demand that means it&#39;s allocated only when 1K is not enough to hold request headers or request line.
request body
# It&#x27;s clear to call client_request_buffering_on to align with others.Syntax:	proxy_request_buffering on | off;Default: proxy_request_buffering on;Context: http, server, location
When buffering is enabled, the entire request body is read from the client before sending the request to a proxied server, send request(headers, then body) happens only when we receive the entire request body.
When buffering is disabled, the request body is sent to the proxied server immediately as it is received(request headers). In this case, the request cannot be passed to the next server if nginx already started sending the request body.
As mentioned above when buffering is enabled, in some case request body is large, how can we cache it?  nginx first uses memory, then disk for body caching, but if CPS is high and with large request body, it’s not a good way to buffer them all as it could use up disk or memory.
Syntax:	client_body_buffer_size size;Default: client_body_buffer_size 8k|16k;Context: http, server, location
Sets buffer size for reading client request body. In case the request body is larger than the buffer(part of unused header buffer + this buffer), the whole body or only its part is written to a temporary file(each request has only one temporary file), By default, buffer size is equal to two memory pages. This is 8K on x86, other 32-bit platforms, and x86-64. It is usually 16K on other 64-bit platforms.
Note: temporary file is unlink() after nginx open it, that means you CAN NOT see it even worker is writing data to it, temporary file is deleted after get response header from upstream that means we send out entire request body
# show fd of temporary file$lsof -p worker_pid | grep &#x27;deleted&#x27;# from error.log file2020/12/17 15:06:40 [warn] 3856#0: *1 a client request body is buffered to a temporary file /usr/local/nginx/client_body_temp/0000000001# temporary is removed automatically from disk get response header from upstream.

Syntax:	client_body_temp_path path [level1 [level2 [level3]]];Default: client_body_temp_path client_body_temp;Context: http, server, location
Defines a directory for storing temporary files holding client request bodies. Up to three-level subdirectory hierarchy can be used under the specified directory. For example, in the following configuration
client_body_temp_path /spool/nginx/client_temp 1 2;
a path to a temporary file might look like this:
/spool/nginx/client_temp/7/45/00000123457

Syntax:	client_body_in_file_only on | clean | off;Default: client_body_in_file_only off;Context: http, server, location
Determines whether nginx should save the entire client request body(no matter what size it is) into a file, it&#39;s same file with proxy_request_buffering directive. This directive can be used during debugging, or when using the $request_body_file variable.
When set to the value on, temporary files are not removed after request processing.
The value clean will cause the temporary files left after request processing to be removed.
no buffering request body conf
proxy_request_buffering off;proxy_pass_request_body on;proxy_http_version 1.1; # reason for this http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_request_buffering

Syntax:	client_max_body_size size;Default: client_max_body_size 1m;Context: http, server, location
Sets the maximum allowed size of the client request body, specified in the “Content-Length” request header field. If the size in a request exceeds the configured value, the 413 (Request Entity Too Large) error is returned to the client. Please be aware that browsers cannot correctly display this error. Setting size to 0 disables checking of client request body size.
responseThere are several directives used for control response, should we buffer it when necessary.
# write to temporary file only when client is slow than upstreamSyntax:	proxy_buffering on | off;Default: proxy_buffering on;Context: http, server, location
When buffering is enabled, nginx receives a response from the proxied server as soon as possible, saving it into the buffers set by the proxy_buffer_size and proxy_buffers directives. If the whole response does not fit into memory, a part of it can be saved to a temporary file on the disk.  when buffering is enabled, nginx tries to receive enough body as much as possible, but if client is ready to write, we still send body to it during receiving, that means temporary file may be not used if client is faster than upstream.  buffering NOT mean sending to client only when receive entire response, it’s different with buffering for request.
When buffering is disabled, the response is passed to a client synchronously, immediately as it is received. nginx will not try to read the whole response from the proxied server.The maximum size of the data that nginx can receive from the server at a time is set by the proxy_buffer_size directive.
Syntax:	proxy_buffers number size;Default: proxy_buffers 8 4k|8k;Context: http, server, location
Sets the number and size of the buffers used for reading a response from the proxied server, for a single connection. By default, the buffer size is equal to one memory page. This is either 4K or 8K, depending on a platform.
Syntax:	proxy_max_temp_file_size size;Default: proxy_max_temp_file_size 1024m;Context: http, server, location
When buffering of responses from the proxied server is enabled, and the whole response does not fit into the buffers set by the proxy_buffer_size and proxy_buffers directives, a part of the response can be saved to a temporary file. This directive sets the maximum size of the temporary file. the temporary file is deleted automatically when request is finalize in ngx_http_upstream_finalize_request().
The zero value disables buffering of responses to temporary files.
Syntax:	proxy_temp_path path [level1 [level2 [level3]]];Default: proxy_temp_path proxy_temp;Context: http, server, location
Defines a directory for storing temporary files with data received from proxied servers. Up to three-level subdirectory hierarchy can be used underneath the specified directory. For example, in the following configuration
proxy_temp_path /spool/nginx/proxy_temp 1 2;
a temporary file might look like this:
/spool/nginx/proxy_temp/7/45/00000123457

non buffering response
proxy_buffering off# receive buffer used for header and body at a time recv(fd, size)proxy_buffer_size size;

buffering response
proxy_buffering on;# below two used for saving response in memoryproxy_buffer_size size;proxy_buffers number size;# if memory is used up, save it to diskproxy_temp_path path;proxy_max_temp_file_size size;

API relatedHeaderrequest header
// build r-&gt;headers_in from request headerstatic void ngx_http_process_request_headers(ngx_event_t *rev);//build header sent to upstream(backend), create a temp buffer to hold all header copied from r-&gt;headers_instatic ngx_int_t ngx_http_proxy_create_request(ngx_http_request_t *r);

response header
// build header sent to client(set r-&gt;header_out) when parsed all response headersstatic ngx_int_t ngx_http_upstream_process_headers(ngx_http_request_t *r, ngx_http_upstream_t *u);// send response header to client and cleanup request body temp file(it will call header filter later on)static void ngx_http_upstream_send_response(ngx_http_request_t *r, ngx_http_upstream_t *u);// header filter to add/update/remove(manipulate r-&gt;header_out) response headerstatic ngx_int_t ngx_http_headers_filter(ngx_http_request_t *r);// last header filter based on r-&gt;header_out create a temp buffer to hold all header copied from r-&gt;header_out, then send it outstatic ngx_int_t ngx_http_header_filter(ngx_http_request_t *r)
Bodyrequest body
// client is ready to read, read request and send it to upstreamstatic void ngx_http_read_client_request_body_handler(ngx_http_request_t *r);// upstream is ready to write, read request body and send it to upstreamstatic void ngx_http_upstream_send_request_handler(ngx_http_request_t *r, ngx_http_upstream_t *u);

response Body
// downstream is ready to writestatic void ngx_http_upstream_process_downstream(ngx_http_request_t *r);// upstream is ready to readstatic void ngx_http_upstream_process_upstream(ngx_http_request_t *r, ngx_http_upstream_t *u);// sending response(non buffered mode)// downstream is ready to write(read response and send it to client)static void ngx_http_upstream_process_non_buffered_downstream(ngx_http_request_t *r);// upstream is ready to read(read response and send it to client)static void ngx_http_upstream_process_non_buffered_upstream(ngx_http_request_t *r, ngx_http_upstream_t *u);]]></content>
      <categories>
        <category>nginx</category>
        <category>body</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_request_conf_finding</title>
    <url>/2020/12/01/nginx-request-conf-finding/</url>
    <content><![CDATA[Overvieweach request is served by a virtual server, then based on uri, select the right location, hence each request has related main_conf, srv_conf, loc_conf that hold config, based on config select proper parameters to process the request.

request conf ctx
Data Structure
struct ngx_http_request_s &#123;    ngx_connection_t                 *connection;    void                            **main_conf;    void                            **srv_conf;    void                            **loc_conf;    ...&#125;typedef struct &#123;    void        **main_conf;    void        **srv_conf;    void        **loc_conf;&#125; ngx_http_conf_ctx_t;typedef struct &#123;    /* conf ctx like main/srv/loc conf, comes from default virtual server */    ngx_http_conf_ctx_t              *conf_ctx;&#125; ngx_http_connection_t;typedef struct &#123;    /* server ctx */    ngx_http_conf_ctx_t        *ctx;    ...&#125; ngx_http_core_srv_conf_t;]]></content>
      <categories>
        <category>nginx</category>
        <category>request</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_shared_memory_layout</title>
    <url>/2020/11/24/nginx-shared-memory-layout/</url>
    <content><![CDATA[OverviewShared memory is memory that can be accessed by different processes, as each process’s page table maps to the same physical page, nginx tracks all used shared memory as a list in each cycle. for each shared memory, mmap() is called by master processed after parse conf if same shared memory(same name, name is the key) can’t be reused, otherwise, just copy shared memory address from old cycle to new cycle, save that shared memory address, reused shared memory is not freed during reconfigure, later on fork worker process.

Shared memoryData structure and APIstructure
struct ngx_shm_zone_s &#123;    void                     *data; /* data context of the share memory                                     * most are conf related that will use                                     * the shared memory to save something                                     */    ngx_shm_t                 shm;  /* meta of the shared memory, like name, size, addr etc*/    ngx_shm_zone_init_pt      init; /* init callback after memory mapped */    void                     *tag;  /* module address used as part of key(name, tag) */    ngx_uint_t                noreuse;  /* unsigned  noreuse:1; */&#125;;typedef struct &#123;    u_char      *addr; /* mmapped address, it&#x27;s managed as slab pool, each worker only shares this part */    size_t       size; /* size of shared memory, user should make it as pages units */    ngx_str_t    name; /* key for shared memory */&#125; ngx_shm_t;struct ngx_cycle_s &#123;    /* each cycle has its shared memory list     * each element is ngx_shm_zone_t which is created during reconfigure based on nginx.conf     * the address of ngx_shm_zone_t is different for cycles     * but if old cycle and new cycle reuse same shared memory     * the shared memory address is copied from old cycle to new cycle     * saved at: ngx_shm_zone_t.shm.addr     */    ngx_list_t                shared_memory;&#125;
API
/* add a ngx_shm_zone_t to the cycle shared_memory list, shared memory is not mapped now */ngx_shm_zone_t * ngx_shared_memory_add(ngx_conf_t *cf, ngx_str_t *name, size_t size, void *tag);/* call mmap() to allocate shared memory */ngx_int_t ngx_shm_alloc(ngx_shm_t *shm);
organize all shared memory
This picture shows how shared memory is tracked by each cycle, reused old one(same name, size, tag) or create new one if cant’ be reused.
]]></content>
      <categories>
        <category>nginx</category>
        <category>shared memory</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>shared memory</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_shared_memory_lock</title>
    <url>/2020/11/24/nginx-shared-memory-lock/</url>
    <content><![CDATA[OverviewAs for shared memory, different processes may allocate memory from it at the same time, so it needs a lock when allocate and free shared memory, nginx provides ngx_shmtx_t which is a combination of spin lock and semaphore, in that lock, first try spin lock, if can’t get the lock, use semaphore to block process.


shared memory lockData structure and API/* This part always from shared memory, it is on shared memory */typedef struct &#123;    ngx_atomic_t   lock;#if (NGX_HAVE_POSIX_SEM)    ngx_atomic_t   wait;#endif&#125; ngx_shmtx_sh_t;/* shmtx is the lock for shared memory *     within atomic(spin lock) if not get the lock, process spins a while, then yield the CPU by way(semaphore) *     blocked until resource is available, when wake up, check resource again to see if can get it */typedef struct &#123;    ngx_atomic_t  *lock;    /* save the process id who holds the lock(can access the shared memory) */#if (NGX_HAVE_POSIX_SEM)    ngx_atomic_t  *wait;    /* count how many process blocked on this lock */    /* you can see lock and wait are pointers, because they always point to ngx_shmtx_sh_t     * which is allocated from the shared memory, hence every process can see lock and wait     */    ngx_uint_t     semaphore; /* flag that indicates semaphore is enabled or not */    sem_t          sem;       /* posix semaphore used by sem_init/sem_wait/sem_post */#endif    /* spin count if we can&#x27;t get the lock     * as it&#x27;s unit hence spin = -1 means spin for ever if call ngx_shmtx_lock()     * mostly used for accessing critical area within short period that another process only spin very shortly!!!     *     * Actually: nginx does not spin for ever even if spin = -1, as with spin = -1, nginx does NOT call ngx_shmtx_lock     * but ngx_shmtx_trylock()!!!     */    ngx_uint_t     spin;&#125; ngx_shmtx_t;typedef struct &#123;    ngx_shmtx_sh_t    lock; /* spin lock used by ngx_shmtx_t */    ngx_shmtx_t       mutex;&#125; ngx_slab_pool_t;

API
void ngx_shmtx_lock(ngx_shmtx_t *mtx);void ngx_shmtx_unlock(ngx_shmtx_t *mtx);

inside ngx_shmtx_lockvoidngx_shmtx_lock(ngx_shmtx_t *mtx)&#123;    ngx_uint_t         i, n;    for ( ;; ) &#123;        if (*mtx-&gt;lock == 0 &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, 0, ngx_pid)) &#123;            /* get the lock and save the process id */            return;        &#125;        if (ngx_ncpu &gt; 1) &#123;            /* only need to spin if multicore             * if there is only one core, no need to spin, spin here, the lock can&#x27;t free at all which is hold by other process, spin waste time             * because during the spin, you still can&#x27;t the lock as the hold process is not running!!!             */            for (n = 1; n &lt; mtx-&gt;spin; n &lt;&lt;= 1) &#123;                for (i = 0; i &lt; n; i++) &#123;                    /* use assemble directive to pause cpu */                    ngx_cpu_pause();                &#125;                /* spin means cpu pauses a while then check if i can get the lock                 * if NO, pause a while again, then check...repeated                 */                if (*mtx-&gt;lock == 0                    &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, 0, ngx_pid))                &#123;                    return;                &#125;            &#125;        &#125;#if (NGX_HAVE_POSIX_SEM)        /* goes here means after spin a while, we still can&#x27;t get the lock         * so block my self by semaphore         */        if (mtx-&gt;semaphore) &#123;            /* increase process counter who are waiting on the semaphore */            (void) ngx_atomic_fetch_add(mtx-&gt;wait, 1);            /* before call sem_wait to block the process, here let&#x27;s try one more time             * to see if the lock is freed by another process             */            if (*mtx-&gt;lock == 0 &amp;&amp; ngx_atomic_cmp_set(mtx-&gt;lock, 0, ngx_pid)) &#123;                (void) ngx_atomic_fetch_add(mtx-&gt;wait, -1);                return;            &#125;            while (sem_wait(&amp;mtx-&gt;sem) == -1) &#123;                /* block here */                ngx_err_t  err;                err = ngx_errno;                if (err != NGX_EINTR) &#123;                    ngx_log_error(NGX_LOG_ALERT, ngx_cycle-&gt;log, err,                                  &quot;sem_wait() failed while waiting on shmtx&quot;);                    break;                &#125;            &#125;            /* as I&#x27;m awoke, decrease the count of waiting processes?             * Yes! but not here as we already did this in wake process when call ngx_shmtx_wakeup             */            ngx_log_debug0(NGX_LOG_DEBUG_CORE, ngx_cycle-&gt;log, 0,                           &quot;shmtx awoke&quot;);            /* after wake up, start next loop(try to get lock) */            continue;        &#125;#endif        /* it goes here only when semaphore is unavailable         * yield just call usleep(1) after one spin loop is over to block the process         */        ngx_sched_yield();    &#125;&#125;]]></content>
      <categories>
        <category>nginx</category>
        <category>shared memory</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>shared memory</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_thundering_herd_accept</title>
    <url>/2020/04/14/nginx-thundering-herd-accept/</url>
    <content><![CDATA[Summary
old issue: All threads&#x2F;processes who called accept() are awoken when a connection comes: already fixed after kernel 2.6

select&#x2F;epoll have similar issue like accept(): already fixed at some kernel version

epoll_create before fork, same issue as accept() already fixed at some kernel version, but has other concern, never do this, see below

epoll_create after fork, still has thundering herd issue, see how nginx solves it


epoll_create before fork(never use it)if epoll_create is called before fork, all child processes share the same epoll instance in the kernel, if a connection comes, kernel selects just one process[**WQ_FLAG_EXCLUSIVE**](other is not awoken), wake it up, no thundering issue
The big issue for this case is that if one process creates fd and adds it to epoll, all processes may be awoken when event happens, process CAN NOT have private fd added to epoll, only for itself.
epoll_create after forkThis is what nginx uses, as each worker needs private fd, say established connection added to epoll should be only processed by the creator, not other worker process. so that different workers have their own epoll instances.
As each worker adds listening fd(created by master mostly for VIP) to its own epoll instance, so for new connection setup, all workers could be awoken if without any solution.
how nginx solve such issue:

option1:  accept mutex lock(deprecated)nginx uses accept mutex lock, for each loop of epoll_wait(), worker first gets the accept mutex lock, if gets it, adds the listening fd by epoll_ctl, otherwise, removes the listening fd, so that at any time there is only one worker has the listening fd, note the accept mutex is shared by all workers.

option2: SO_REUSEPORT(best one, balanced from kernel)SO_REUSEPORT(kernel 3.9) with this option set, kernel selects a process based on hash(src, port, dst, port), only wakes up that process it’s hash balancing from kernel, no lock from user space, nginx always uses this if available. master will create different fd(same ip+port) for each worker(fd for same ip+port(clone_listening)) before fork, after fork, each worker adds its fd to its epoll instance which is created after fork. that means for each listing_op_t, nginx creates several sockets(same with worker number). each worker has its own epoll instance and different fd.

option3 new epoll flag EPOLLEXCLUSIVE(since kernel 4.5)(better one, no balance)EPOLLEXCLUSIVE is thus useful for avoiding thundering herd problems in certain scenarios, only wake up one process If the same file descriptor is in multiple epoll instances. if use it, nginx master does not create different fd(VIP) for each worker, but all workers use the same(fd number), still epoll is created after fork, add the same fd to each worker’s epoll instance. when new connection comes, only the first on the waiting queue is wake up.nginx uses it if SO_REUSPORT is not available.


nginx chooses these options in below order

SO_REUSEPORT(need kernel support and configure at listen 1.1.1.1:80 reuseport, both needs)
EPOLLEXCLUSIVE(need kernel support)
accept mutex(needs user configure at events {accept_mutex on;})
None

]]></content>
      <categories>
        <category>nginx</category>
        <category>accept</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>thundering herd</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_shared_memory_slab</title>
    <url>/2020/11/24/nginx-shared-memory-slab/</url>
    <content><![CDATA[OverviewIn previous post shared memory tracking, it shows how shared memory is tracked and allocated, but it does not show how to manage allocated shared memory, then provide API for user to use memory from it, say you have a big shared memory, later on you may want to allocate blocks from the shared memory, how does nginx manage the shared memory and allocate block to user quickly? this is the post aims to tell you.


SlabActually nginx uses slab to manage the shared memory, same as kernel does for its slab. it provides several size of slabs like 8, 16, 2^x, that means you can only get 2^x bytes from the shared memory, for each slab, there is a slab header, each entry in the slab list is the slab page control part which has the metadata for that page(like which block is used etc), when user requests size bytes, round(size) to 2^x, then check the proper slab, find a free page in that slab, allocate block from that page, mark that block as used.
Data structure and APIData Structure
/* slab page control part OR slab header which no real slab page related*/struct ngx_slab_page_s &#123;    uintptr_t         slab;    ngx_slab_page_t  *next;    uintptr_t         prev;&#125;;typedef struct &#123;    ngx_uint_t        total; /* all available chunks(used and unused) except bitmap used if has */    ngx_uint_t        used;  /* used chunks count, NOT count bitmap used */    ngx_uint_t        reqs;    ngx_uint_t        fails;&#125; ngx_slab_stat_t;typedef struct &#123;    size_t            min_size; /* 8 bytes */    size_t            min_shift; /* 3, 2^3 = 8 */    /* as for each slab(2^x), first we need a page     * below fields are used for manges shared memory as pages.     * inside that page, use bitmap to track small block (2^x size)     */    ngx_slab_page_t  *pages; /* page control, each page has one ngx_slab_page_t */    ngx_slab_page_t  *last;  /* last control page structure */    ngx_slab_page_t   free;  /* header of free page structure */    ngx_slab_stat_t  *stats; /* stats for each slot */    ngx_uint_t        pfree; /* free pages */    u_char           *start; /* page data area(aligned) */    u_char           *end;   /* original addr + size */    void             *data; /* context of shared memory */    void             *addr; /* original shared memory addr */&#125; ngx_slab_pool_t;

API
static ngx_int_t ngx_init_zone_pool(ngx_cycle_t *cycle, ngx_shm_zone_t *zn);void ngx_slab_init(ngx_slab_pool_t *pool);void * ngx_slab_calloc(ngx_slab_pool_t *pool, size_t size);void * ngx_slab_alloc(ngx_slab_pool_t *pool, size_t size);

Slab layout
As you can see slab management sits at the beginning of the shared memory, like slab pool, slab header, stats header, ngx_slab_page_t(for each page), that means the available memory for user is less than the size passed to mmap().
Page movement during free and allocate
At first, all pages are linked to free list, but after allocation and free, the control part ngx_slab_page_t may be at different slots.
Bitmap for block(inside one page) managementBitmap is used to track the block status(used or not) in a page, each bit in the map represents a block in the page, it also needs memory for the bitmap, as different blocks need different bitmap size, nginx saves bitmap in two different places, if block size &gt;&#x3D;64 bytes, bitmap saves at ngx_slab_page_t-&gt;slab, otherwise, bitmap saves at page in place(beginning of the page).


How to get the chunk index from chunk addresspage offset are least 12 bits for each bytes in the page, then we group bytes as chunk, the chunk shift are offset inside each chunk, the least chunk_shift bit is offset inside each chunk, the high bit is same for a chunk（which takes 2^chunk_offset bytes).
high---------bit 0  1  0  0  0  0 0 0 0 0 0  1|--page index----|&lt;---page offset-----------&gt;|                 |chunk index  | chunk offset|
nginx first needs to know which page the block belongs, then the page control part, knows the chunk offset, then gets chunk index(set bitmap or clear it when free the block).
]]></content>
      <categories>
        <category>nginx</category>
        <category>shared memory</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>slab</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx_upstream_multiplexing_keepalive</title>
    <url>/2020/12/07/nginx-upstream-multiplexing-keepalive/</url>
    <content><![CDATA[OverviewHttp Keep alive(multiplexing) means after a TCP connection serves a request, it’s not closed by server, so that subsequent request can reuse the same connection to avoid create new. hence can improve http performance.enable it from client side(make sure sever does not disable it)

Http 1.0: send request(explicit enable) Connection: keep-alive
Http 1.1 default is keep-alive(explicit disable Connection: close)

Note: even connection is keep-alive, it&#39;s not kept for ever if it&#39;s idle(most server has a timeout for idle connection).


Upstream keep-aliveHere we say how to enable upstream keep-alive, as by default nginx uses http1.0 for upstream, keep-alive is disabled, to enable upstream keep-alive and make it work, it needs

nginx(now as client) to cache the idle connection(keep it)
nginx(now as client) not use http1.0 as by default http1.0 upstream server will close the connection after served.
server(upstream) must not disable keep-alive, out of nginx scope.

http &#123;    upstream ups &#123;        server 1.1.1.1;        server 2.2.2.2;        # cache 6 idle connection for this upstream, all servers shares the 6 free connection        keepalive 6;        keepalive_requests 100;        keepalive_timeout 60s    &#125;    server &#123;        listen 10.10.10.10:80;        location / &#123;            # use http 1.1 to tell server not close the connection, hence we can cache it for a while            # clear Connection header to avoid miss send Connection: close            proxy_http_version 1.1;            proxy_set_header Connection &quot;&quot;;            proxy_pass: http://ups;        &#125;    &#125;&#125;

As mentioned, for idle connection, it’s not kept for ever, it may be closed by client(nginx) Or server due to config

client(nginx) closes it due to timed out(on client side) or serve a number of request
server(upstream) closes it due to timed out on server side

keep-alive cacheHere is how nginx cache the idle connection to make sure we only cache a number of idle connection.

Data Structure
/* each upstream has one */typedef struct &#123;    /* max kept idle connections for the whole pool(upstream) not each peer */    ngx_uint_t                         max_cached;    /* close the idle keep-alive connection if it serves number of requests or timed out */    ngx_uint_t                         requests;    ngx_msec_t                         timeout;    /* cache keep-alive idle connection(tcp established) */    ngx_queue_t                        cache;    /* free cache entry that can be used to keep idle connection, queue size is max_cached at initialization */    ngx_queue_t                        free;&#125; ngx_http_upstream_keepalive_srv_conf_t;typedef struct &#123;    /* for each keep-alive idle connection we have a such entry insert to conf-&gt;cache at head */    ngx_http_upstream_keepalive_srv_conf_t  *conf;    ngx_queue_t                        queue;    /* idle peer connection(tcp established, not in use for request) */    ngx_connection_t                  *connection;    /* peer info */    socklen_t                          socklen;    ngx_sockaddr_t                     sockaddr;&#125; ngx_http_upstream_keepalive_cache_t;

API
/* check idle cache, if found, use existing connection, otherwise create a new by event framework */static ngx_int_t ngx_http_upstream_get_keepalive_peer(ngx_peer_connection_t *pc, void *data);/* save idle connection to cache */static void ngx_http_upstream_free_keepalive_peer(ngx_peer_connection_t *pc, void *data, ngx_uint_t state);/* close peer connection if timed out or closed by upstream */static void ngx_http_upstream_keepalive_close_handler(ngx_event_t *ev);]]></content>
      <categories>
        <category>nginx</category>
        <category>keepalive</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx-usage-sample</title>
    <url>/2020/12/16/nginx-usage-sample/</url>
    <content><![CDATA[nginxnginx can be used as light web server(serve request directly), reverse proxy(http) and proxy server(stream) for Pop3 and IMAP.

nginx as a light static web serverput your website at &#x2F;var&#x2F;www&#x2F;html
http &#123;    server &#123;        root /var/www/html;        index index.html;        location / &#123;            index index.htm index.html;        &#125;    &#125;&#125;
nginx as static content server(file server)location /images/ &#123;    autoindex on;&#125;


root directive  The root directive specifies the root directory that will be used to search for a file. To obtain the path of a requested file, NGINX appends the request URI to the path specified by the root directive. The directive can be placed on any level within the http {}, server {}, or location {} contexts. In the example below, the root directive is defined for a virtual server. It applies to all location {} blocks where the root directive is not included to explicitly redefine the root:
  http &#123;    server &#123;        listen 1.1.1.1;        # only for this server        root /www/data;        location / &#123;        &#125;        location /images/ &#123;        &#125;        location ~ \.(mp3|mp4) &#123;            root /www/media;        &#125;&#125;
  Here, NGINX searches for a URI that starts with &#x2F;images&#x2F; in the &#x2F;www&#x2F;data&#x2F;images&#x2F; directory in the file system. But if the URI ends with the .mp3 or .mp4 extension, NGINX instead searches for the file in the &#x2F;www&#x2F;media&#x2F; directory because it is defined in the matching location block.
  If a request ends with a slash, NGINX treats it as a request for a directory and tries to find an index file in the directory. 

index directive  The index directive defines the index file’s name (the default value is index.html). To continue with the example, if the request URI is &#x2F;images&#x2F;some&#x2F;path&#x2F;, NGINX delivers the file &#x2F;www&#x2F;data&#x2F;images&#x2F;some&#x2F;path&#x2F;index.html if it exists. If it does not, NGINX returns HTTP code 404 (Not Found) by default. To configure NGINX to return an automatically generated directory listing instead, include the on parameter to the autoindex directive
  # you can also put these at server level or http level# so that it serve as a whole file server not the /images part onlylocation /images/ &#123;    autoindex on;    autoindex_exact_size on;    autoindex_localtime on;&#125;
  You can list more than one filename in the index directive. NGINX searches for files in the specified order and returns the first one it finds.
  location / &#123;    index index.$geo.html index.htm index.html;&#125;

try directive  The try_files directive can be used to check whether the specified file or directory exists; NGINX makes an internal redirect if it does, or returns a specified status code if it doesn’t. For example, to check the existence of a file corresponding to the request URI, use the try_files directive and the $uri variable as follows:
  server &#123;    root /www/data;    location /images/ &#123;        try_files $uri /images/default.gif;    &#125;&#125;
  The file is specified in the form of the URI, which is processed using the root or alias directives set in the context of the current location or virtual server. In this case, if the file corresponding to the original URI doesn’t exist, NGINX makes an internal redirect to the URI specified by the last parameter, returning &#x2F;www&#x2F;data&#x2F;images&#x2F;default.gif.
  The last parameter can also be a status code (directly preceded by the equals sign) or the name of a location. In the following example, a 404 error is returned if none of the parameters to the try_files directive resolve to an existing file or directory.
  location / &#123;    try_files $uri $uri/ $uri.html =404;&#125;

  In the next example, if neither the original URI nor the URI with the appended trailing slash resolve into an existing file or directory, the request is redirected to the named location which passes it to a backend server.
  location / &#123;    try_files $uri $uri/ @backend;&#125;location @backend &#123;    proxy_pass http://backend.example.com;&#125;

Optimizing Performance for Serving Content
location /mp3 &#123;    sendfile           on;    sendfile_max_chunk 1m;    tcp_nopush         on;    tcp_nodelay        on;    #...&#125;

setup static content server
nginx as a proxy(for same app)http &#123;    # one backend server    upstream zp_server1&#123;        server 127.0.0.1:8089;    &#125;    server &#123;        listen 80;        # everything(original uri, args)will be passed to backend server        location / &#123;            proxy_pass http://zp_server1;        &#125;    &#125;&#125;
nginx as a load balancer(for same app)load balancer means for the same application, there are several deployed backend servers!, choose one for serving
http &#123;    #two backend servers    upstream zp_server1 &#123;        server 192.168.1.1:80 weight=5;        server 192.168.1.2:80 weight=8;    &#125;    server &#123;        listen 80;        location / &#123;            proxy_pass http://zp_server1;        &#125;    &#125;&#125;

nginx as https serverserver &#123;    # ssl is needed    listen       443 ssl;    server_name  www.helloworld.com;    # cert is a must    ssl_certificate      cert.pem;    ssl_certificate_key  cert.key;    ssl_session_cache    shared:SSL:1m;    ssl_session_timeout  5m;    ssl_ciphers  HIGH:!aNULL:!MD5;    ssl_prefer_server_ciphers  on;    location / &#123;        root   /var/www;        # try index.html, if no, try index.htm        index  index.html index.htm;    &#125;&#125;

nginx as proxy for different appsdifferent websites uri

www.helloworld.com/finance/
www.helloworld.com/product/
www.helloworld.com/admin/

http &#123;    upstream product_server&#123;        # product website server        server www.helloworld.com:8081;    &#125;    upstream admin_server&#123;        server www.helloworld.com:8082;    &#125;    upstream finance_server&#123;        server www.helloworld.com:8083;    &#125;    server &#123;        listen 80;        index index.html;        root /var/www/html;        location / &#123;            proxy_pass http://product_server;        &#125;        #================================================        # matching is search location in url        # if location is /production/        # url /product, doesn&#x27;t match        #================================================        # if location is /product        # url is /product match, /product/ also match!!        location /product/ &#123;            proxy_pass http://product_server;        &#125;        location /admin/ &#123;            proxy_pass http://admin_server;        &#125;        location /finance/ &#123;            proxy_pass http://finance_server;        &#125;    &#125;&#125;
proxy_pass&#x2F;fastcgi&#x2F;scgi&#x2F;uwsgiproxy_pass is for http(s) between backend, nginx also supports other transport(high level above layer 4) protocol with backend like fastcgi, scgi, uwsgi.
server &#123;    location / &#123;        fastcgi_pass  localhost:9000;        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;        fastcgi_param QUERY_STRING    $query_string;    &#125;    location ~ \.(gif|jpg|png)$ &#123;        root /data/images;    &#125;&#125;]]></content>
      <categories>
        <category>nginx</category>
        <category>usage</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>oauth-basic</title>
    <url>/2020/10/09/oauth-basic/</url>
    <content><![CDATA[OAuthOAuth 2 is an authorization framework that enables applications to obtain limited access to user accounts on an HTTP service, such as Facebook, GitHub, and DigitalOcean. It works by delegating user authentication to the service that hosts the user account, and authorizing third-party applications to access the user account. OAuth 2 provides authorization flows for web and desktop applications, and mobile devices.


OAuth RolesIn our case, the application we are building would be the third party application. Facebook would be our HTTP service and the end-user the resource owner. These three fall under the following OAuth roles:

Resource Owner&#x2F;End user(provide user&#x2F;password) - This is the user authorizing a third-party application to access certain protected resources from a resource server(facebook).
Client - This is the third party application making protected resource requests to a resource server on behalf of the resource owner(end user).
Resource Server - Hosts the protected resources e.g user profile(provided by facebook)
Authorization Server - Responsible for authenticating the resource owner and providing access token to clients.(provided by facebook as well)


In order for successful user authentication to happen, a series of steps need to be followed:

The client, (which is our application in this case), requests authorization(ask end-user input user&#x2F;password) from the end-user.
Once the end-user authorizes the client, an application grant is issued.
Our client then requests an access token from the authorization server using the authorization grant.
The authorization server validates the grant and authenticates the client. If the two processes are successful an access token is granted to the client, client only has the access token.
Our client then uses the access token to request the protected resource.
The resources server then validates the access token and if successful, the requested protected resources are shared with the client.

Application RegistrationBefore using OAuth with your application, you must register your application with the service(facebook, github etc). This is done through a registration form in the “developer” or “API” portion of the service’s website, where you will provide the following information (and probably details about your application):

Application Name
Application Website
Redirect URI or Callback URL

The redirect URI is where the service will redirect the user after they authorize (or deny) your application, and therefore the part of your application that will handle authorization codes or access tokens.Client ID and Client Secret
Once your application is registered, the service will issue “client credentials” in the form of a client identifier and a client secret. The Client ID is a publicly exposed string that is used by the service API to identify the application, and is also used to build authorization URLs that are presented to users. The Client Secret is used to authenticate the identity of the application to the service API when the application requests to access a user’s account, and must be kept private between the application and the API.
OAuth scopesScopes let you specify exactly what type of access need before third-part application gets the access token. Scopes limit access for OAuth tokens, When facebook is responding to your OAuth request, the requested scopes will be displayed to them when they are asked to approve your request, you can know what scopes that third-part application tries to get.
OAuth does not define any particular values for scopes, since it is highly dependent on the service’s internal architecture and needs. 

github account scope format
google account scope format

Ref
introduction to auth 2

]]></content>
      <categories>
        <category>Auth</category>
        <category>OAuth2</category>
      </categories>
      <tags>
        <tag>OAuth</tag>
      </tags>
  </entry>
  <entry>
    <title>pip-deep</title>
    <url>/2019/11/21/pip-deep/</url>
    <content><![CDATA[pippip commandsHere are frequently used commands with pip(pip3 for python3)
search available pkg online goto: official repo pypi
$ pip config debugenv_var:env:global:  /etc/xdg/pip/pip.conf, exists: False  /etc/pip.conf, exists: Falsesite:  /usr/pip.conf, exists: Falseuser:  /root/.pip/pip.conf, exists: False  /root/.config/pip/pip.conf, exists: False# use pip mirror for speeding up$ pip3 install -U pip# OR$ pip install -U pip$ pip config set global.index-url http://mirrors.aliyun.com/pypi/simple/# OR manually edit /root/.config/pip/pip.conf$ cat /root/.config/pip/pip.conf[global]index-url = http://mirrors.aliyun.com/pypi/simple/# or change index-url temporary for each install$ pip install --index-url http://mirrors.aliyun.com/pypi/simple/ ansible-core==2.11.12$ pip cache dir/home/ubuntu/.cache/pip$ pip cache listNothing cached.# global install, all users see it!!!$ sudo pip install package_name# install/uninstall for current user$ pip install package_name # ~/.local/lib/python# list all version of given package$ pip install ansible==not_exist# install a package of given version$ pip install package_name==version$ pip uninstall package_name# pip list --format=freeze&gt;req.txt$ pip install -r req.txt# list installed python packages using pip$ pip list -vPackage                            Version             Location                                           Installer---------------------------------- ------------------- -------------------------------------------------- ---------alabaster                          0.7.12              /home/ubuntu/anaconda3/lib/python3.8/site-packages condaanaconda-client                    1.7.2               /home/ubuntu/anaconda3/lib/python3.8/site-packages condaanaconda-navigator                 1.10.0              /home/ubuntu/anaconda3/lib/python3.8/site-packagesanaconda-project                   0.8.3               /home/ubuntu/anaconda3/lib/python3.8/site-packages condaargh                               0.26.2              /home/ubuntu/anaconda3/lib/python3.8/site-packagesargon2-cffi                        20.1.0              /home/ubuntu/anaconda3/lib/python3.8/site-packages condaasn1crypto                         1.4.0               /home/ubuntu/anaconda3/lib/python3.8/site-packages condaastroid                            2.4.2               /home/ubuntu/anaconda3/lib/python3.8/site-packages condaastropy                            4.0.2               /home/ubuntu/anaconda3/lib/python3.8/site-packagesasync-generator                    1.10                /home/ubuntu/anaconda3/lib/python3.8/site-packages condaatomicwrites                       1.4.0               /home/ubuntu/anaconda3/lib/python3.8/site-packages condaattrs                              20.3.0              /home/ubuntu/anaconda3/lib/python3.8/site-packages condaautopep8                           1.5.4               /home/ubuntu/anaconda3/lib/python3.8/site-packages condaBabel                              2.8.1               /home/ubuntu/anaconda3/lib/python3.8/site-packages condabackcall                           0.2.0               /home/ubuntu/anaconda3/lib/python3.8/site-packages condabackports.functools-lru-cache      1.6.1               /home/ubuntu/anaconda3/lib/python3.8/site-packages condabackports.shutil-get-terminal-size 1.0.0               /home/ubuntu/anaconda3/lib/python3.8/site-packages condabackports.tempfile                 1.0                 /home/ubuntu/anaconda3/lib/python3.8/site-packages pip$ pip listPackage                            Version---------------------------------- -------------------alabaster                          0.7.12anaconda-client                    1.7.2anaconda-navigator                 1.10.0anaconda-project                   0.8.3argh                               0.26.2argon2-cffi                        20.1.0asn1crypto                         1.4.0astroid                            2.4.2astropy                            4.0.2$ pip list --format=freezealabaster==0.7.12anaconda-client==1.7.2anaconda-navigator==1.10.0anaconda-project==0.8.3argh==0.26.2argon2-cffi==20.1.0asn1crypto==1.4.0astroid==2.4.2astropy==4.0.2async-generator==1.10atomicwrites==1.4.0attrs==20.3.0autopep8==1.5.4Babel==2.8.1backcall==0.2.0$ pip list --outdated# !!!while dpkg -l only shows apt-get installed packages!!!# show info of package$ pip show self-tool-libName: self-tool-libVersion: 0.1Summary: self utilities for toolHome-page: Author: Jason LuoAuthor-email: jason_lkm@163.comLicense: UNKNOWNLocation: /home/data/Anaconda3/envs/py3.9/lib/python3.9/site-packagesRequires: Required-by: # show files of installed  package$ pip show -f self-tool-libName: self-tool-libVersion: 0.1Summary: self utilities for toolHome-page: Author: Jason LuoAuthor-email: jason_lkm@163.comLicense: UNKNOWNLocation: /home/data/Anaconda3/envs/py3.9/lib/python3.9/site-packagesRequires: Required-by: Files:  self_tool_lib-0.1.dist-info/INSTALLER  self_tool_lib-0.1.dist-info/LICENSE  self_tool_lib-0.1.dist-info/METADATA  self_tool_lib-0.1.dist-info/RECORD  self_tool_lib-0.1.dist-info/REQUESTED  self_tool_lib-0.1.dist-info/WHEEL  self_tool_lib-0.1.dist-info/direct_url.json  self_tool_lib-0.1.dist-info/top_level.txt# show files in wheel file(which can be installed by pip directly)# pip install dist/self_tool_lib-0.1-py3-none-any.whl$ python -m zipfile --list  dist/self_tool_lib-0.1-py3-none-any.whlFile Name                                             Modified             Sizeself_tool_lib-0.1.dist-info/LICENSE            2022-06-14 15:19:08         1074self_tool_lib-0.1.dist-info/METADATA           2022-06-14 15:19:08          403self_tool_lib-0.1.dist-info/WHEEL              2022-06-14 15:19:08           92self_tool_lib-0.1.dist-info/top_level.txt      2022-06-14 15:19:08            1self_tool_lib-0.1.dist-info/RECORD             2022-06-14 15:19:08          404

build pip package
apt-get vs pipNote: some python pkg may be only available by apt source or pypi source, not both
pkg source is differentapt-get can install any package from remote repo, not only python related but pip&#x2F;pip3 only for python package, for some python packages you can install it from apt-get or pip, the difference is apt-get use source.list library that&#39;s dedicated for particular ubuntu version, the package from there is always old, but pip uses its own library(PyPI https://pypi.org/pypi) which always have new version for the application, while for some python package, apt-get may NOT have. from apt-get you can’t specify the version to install while pip can have.
pypi usually has many versions of a single pkg while apt source only has one
install dest location is differentapt-get installs python modules in system-wide location. We cannot just install modules in our project virtualenv. pip solves this problem for us. If we are using pip after activating the virtualenv, it is intelligent enough to only install the modules in our project virtualenv. As mentioned in previous point, if there is a version of a particular python package already installed in system-wide location, and one of our project requires an older version of the same python package, in such situations we can use virtualenv and pip to install that older version of python package without any conflicts.
apt dest location
# show dest location$ dpkg -L python3-httplib2/usr/bin/usr/lib/

pip dest location
# show dest location$ pip show httplib2/user/local/bin/usr/local/lib~/.local/lib~/.local/bin

if you install one package by these two, there will be two copies(may be two version of this pkg), but apt-get always wins, as the sys.path always, /usr/bin/; /usr/local/bin, but you can change the PATH.
name is different$ apt-get install python3-jinja2$ yum install -y python3-jinja2# vs$ pip install jinjia2

condaconda(which has its own repo: https://anaconda.org/) is another way to install python pkg, as conda has a python env, activated after you reboot system after installation, hence pip&#x2F;pip3 comes from conda python env, all packages installed either by conda or pip, they see the others as well!!!.
$ conda info     active environment : base    active env location : /home/ubuntu/anaconda3            shell level : 1       user config file : /home/ubuntu/.condarc populated config files :           conda version : 4.9.2    conda-build version : 3.20.5         python version : 3.8.5.final.0       virtual packages : __glibc=2.27=0                          __unix=0=0                          __archspec=1=x86_64       base environment : /home/ubuntu/anaconda3  (read only)           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64                          https://repo.anaconda.com/pkgs/main/noarch                          https://repo.anaconda.com/pkgs/r/linux-64                          https://repo.anaconda.com/pkgs/r/noarch          package cache : /home/ubuntu/anaconda3/pkgs                          /home/ubuntu/.conda/pkgs       envs directories : /home/ubuntu/.conda/envs                          /home/ubuntu/anaconda3/envs               platform : linux-64             user-agent : conda/4.9.2 requests/2.24.0 CPython/3.8.5 Linux/4.15.0-136-generic ubuntu/18.04.5 glibc/2.27                UID:GID : 1000:1000             netrc file : None           offline mode : False# USE conda mirror$ cat /root/.condarc channels:  - defaultsshow_channel_urls: truedefault_channels:  - http://mirrors.aliyun.com/anaconda/pkgs/main  - http://mirrors.aliyun.com/anaconda/pkgs/r  - http://mirrors.aliyun.com/anaconda/pkgs/msys2custom_channels:  conda-forge: http://mirrors.aliyun.com/anaconda/cloud  msys2: http://mirrors.aliyun.com/anaconda/cloud  bioconda: http://mirrors.aliyun.com/anaconda/cloud  menpo: http://mirrors.aliyun.com/anaconda/cloud  pytorch: http://mirrors.aliyun.com/anaconda/cloud  simpleitk: http://mirrors.aliyun.com/anaconda/cloud# list installed pkgs$ conda list# packages in environment at /home/ubuntu/anaconda3:## Name                    Version                   Build  Channel_ipyw_jlab_nb_ext_conf    0.1.0                    py38_0  _libgcc_mutex             0.1                        main  alabaster                 0.7.12                     py_0  anaconda                  2020.11                  py38_0  anaconda-client           1.7.2                    py38_0  anaconda-navigator        1.10.0                   py38_0  anaconda-project          0.8.4                      py_0  argh                      0.26.2                   py38_0  argon2-cffi               20.1.0           py38h7b6447c_1  asn1crypto                1.4.0                      py_0  astroid                   2.4.2                    py38_0  astropy                   4.0.2            py38h7b6447c_0  async_generator           1.10                       py_0  atomicwrites              1.4.0                      py_0  attrs                     20.3.0             pyhd3eb1b0_0  autopep8                  1.5.4                      py_0  babel                     2.8.1              pyhd3eb1b0_0  backcall                  0.2.0                      py_0  backports                 1.0                        py_2  # search available pkgs from remote repo$ conda search &#x27;*scikit&#x27;$ conda search &#x27;numpy&gt;=1.12&#x27;# always use pkg from channel conda-forge which has high priority$ conda search pytest-covLoading channels: done# Name                       Version           Build  Channel             pytest-cov                     2.5.1          py27_0  pkgs/main           pytest-cov                     2.5.1  py27hc4181bb_0  pkgs/main           pytest-cov                     2.5.1          py35_0  pkgs/main           pytest-cov                     2.5.1  py35h9d7ae03_0  pkgs/main           pytest-cov                     2.5.1          py36_0  pkgs/main           pytest-cov                     2.5.1  py36hd4733f3_0  pkgs/main           pytest-cov                     2.5.1          py37_0  pkgs/main           pytest-cov                     2.6.0          py27_0  pkgs/main           pytest-cov                     2.6.0          py35_0  pkgs/main           pytest-cov                     2.6.0          py36_0  pkgs/main           pytest-cov                     2.6.0          py37_0  pkgs/main           pytest-cov                     2.6.1          py27_0  pkgs/main           pytest-cov                     2.6.1          py36_0  pkgs/main           pytest-cov                     2.6.1          py37_0  pkgs/main           pytest-cov                     2.7.1            py_0  pkgs/main           pytest-cov                     2.8.1            py_0  pkgs/main           pytest-cov                     2.9.0            py_0  pkgs/main           pytest-cov                    2.10.0            py_0  pkgs/main           pytest-cov                    2.10.1            py_0  pkgs/main           pytest-cov                    2.10.1    pyhd3eb1b0_0  pkgs/main           pytest-cov                    2.11.0    pyhd3eb1b0_0  pkgs/main           pytest-cov                    2.11.1    pyhd3eb1b0_0  pkgs/main$ conda install -c conda-forge pytest-cov $ conda info pytest-cov$ conda uninstall pytest-cov]]></content>
      <categories>
        <category>python</category>
        <category>pip</category>
      </categories>
  </entry>
  <entry>
    <title>nginx-virtual-server</title>
    <url>/2020/12/01/nginx-virtual-server/</url>
    <content><![CDATA[OverviewIn nginx, a virtual server is defined by a server block with listen directive inside it, something like this.
server &#123;    listen 1.1.1.1:8000;    server_name a.b.com;&#125;

Each virtual server has a unique context, for each request, we first need to know who should be right server that serves the request, this is determined by comparing Host header value or SNI with server_name configured with each virtual server, if none matches, use default virtual server, default virtual server is the one with default keyword in listen directive, if no default configured, the first one in the config(nginx.conf) for that ip:port.
As there could be multiple virtual servers(with different names) listen on same ip:port, nginx builds a hash for the server names for quick searching the matched one, input: Host(SNI): output: server block ctx, the next section I will show how virtual server is parsed and management in runtime.
As epoll supports metadata for fd, nginx stores combined context to VIP fd, hence when a new connection comes, we already know that context, from that context get the proper vip by comparing Host header or SNI or use default.
/* same ip:port only has one ngx_listening_t(combined context) */ngx_listening_t &#123;    /* virtual servers ctx listen array of ngx_http_in_addr_t, for example */    void               *servers;&#125;

virtual server organize
]]></content>
      <categories>
        <category>nginx</category>
        <category>virtual-server</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>virtual server</tag>
      </tags>
  </entry>
  <entry>
    <title>protocol_ip</title>
    <url>/2021/04/11/protocol-ip/</url>
    <content><![CDATA[checksumA check sum is basically a value that is computed from data packet to check its integrity. Through integrity, we mean a check on whether the data received is error free or not. This is because while traveling on network a data packet can become corrupt and there has to be a way at the receiving end to know that data is corrupted or not. This is the reason the checksum field is added to the header. At the source side, the checksum is calculated and set in header as a field. At the destination side, the checksum is again calculated and crosschecked with the existing checksum value in header to see if the data packet is OK or not.
This article aims to explain how checksum is calculated, the method is used to calculate checksum. both IP and TCP uses the same method to calculate its checksum, IP checksum is only for IP header, while TCP checksum is for Pseudo IP header + TCP header + TCP payload.


IP header checksummethod:we divide the IP header is 16 bit words and sum each of them up and then finally do a one’s compliment(补集) of the sum then the value generated out of this operation would be the checksum.
ExampleIP header as example. b1e6 is the checksum, let’s see how it’s calculated.4500 003c 1c46 4000 4006 b1e6 ac10 0a63 ac10 0a0c
4500003c

sum: 453c

1c46

sum: 6182

4000

sum: a182

 4006

sum: e188

 ac10

sum: 8d99  larger than 16 bits, overflow, ignore overflow and + 1

 0a63

97fc

 ac10

sum: 440d  larger than 16 bits, overflow, ignore overflow and + 1

 0a0c

sum: 4e19

checksum: b1e6 (b1e6 + 4e19 &#x3D;&#x3D; ffff)
]]></content>
      <categories>
        <category>protocol</category>
        <category>ip</category>
      </categories>
      <tags>
        <tag>protocol</tag>
        <tag>ip</tag>
      </tags>
  </entry>
  <entry>
    <title>authentication_encryption</title>
    <url>/2020/11/10/protocol-security-authentication-encryption/</url>
    <content><![CDATA[Overviewcryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography.

Data IntegrityIn real world, something is sent along with message, make sure it’s not modified by the third party.
MD5The MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value, this hash value sent out with message to protects a message’s data integrity, receiver uses MD5(algorithm) to calculate the hash value again, if it’s same with the one sent by sender, the data is not modified by someone else.
AuthenticationMAC(message authentication code)A message authentication code (often called MAC) is a block of a few bytes that is used to authenticate a message.
The MAC value protects a message’s data integrity,as well as its authenticity(because it uses private key as MAC input), by allowing verifiers (who also possess the secret key) to detect any changes to the message content.
here is workflow of MAC

The term message integrity code (MIC) is frequently substituted for the term MAC,especially in communications, to distinguish it from the use of MAC meaning MAC address (for media access control address).
MAC is the abstract part defined by RFC, In implementation, there are ways(algorithms) to calculate the code, that’s what you mostly see like HMAC, PMAC, OMAC, CMA, UMAC etc, HMAC is the most popular one.
HMACHMAC stands for hash-based message authentication code(the generated authentication code also called Digest). It is a specific type of MAC. It contains cryptographic hash functions and a secret cryptographic key. HMAC is capable of verifying data integrity and authentication of a message at the same time.
HAC(in implementation) has several algorithms to generate digest, here is a summary of that.
hash algorithm     digest length(bit)HmacMD5                 128HmacSHA1                160HmacSHA256              256HmacSHA384              384HmacSHA512              512

Python 3.5.2 (default, Sep 10 2016, 08:21:44) [GCC 5.4.0 20160609] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import hmac&gt;&gt;&gt; message = b&#x27;Hello, world!&#x27;&gt;&gt;&gt; key = b&#x27;secret&#x27;&gt;&gt;&gt; h = hmac.new(key, message, digestmod=&#x27;MD5&#x27;)&gt;&gt;&gt; h.hexdigest()&#x27;fa4ee7d173f2d97ee79022d1a7355bcf&#x27;  ------&gt;128 bits

Modern cryptographySymmetric-key cryptographySymmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key.

Implementation

Twofish
AES(Advanced Encryption Standard) original name: Rijndael
Blowfish
RC4
DES
3DES

Public-key cryptography(asymmetric key)A public key system is so constructed that calculation of one key (the ‘private key’) is computationally infeasible from the other (the ‘public key’), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair.


Diffie–Hellman key exchange protocol
DSS (Digital Signature Standard), which incorporates the Digital Signature Algorithm
RSA encryption algorithm
YAK authenticated key agreement protocol

REF
Cryptography wiki

]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>authentication</tag>
        <tag>encryption</tag>
        <tag>public-key</tag>
        <tag>symmetric-key</tag>
      </tags>
  </entry>
  <entry>
    <title>protocol-ssl</title>
    <url>/2021/04/16/protocol-ssl/</url>
    <content><![CDATA[OverviewHandshakeSSL&#x2F;TLS handshake is different between TLS1.2 and TLS1.3, but they have common concept.
The purpose of the SSL&#x2F;TLS handshake is to perform all the cryptographic work needed to have a secure connection. This includes authenticating the SSL certificate being used, and generating an encryption key.
The TLS handshake accomplishes 3 main things:

Exchanging cipher suites and parameters
Authenticating one or both parties
Creating&#x2F;Exchanging symmetric session keys



Negotiating Cipher SuitesThe first step of the TLS handshake requires the client and server to share their capabilities so they can find the cryptographic features they mutually support.
Once a client and server agree on the exact encryption methods they will use, this is called a cipher suite – the server sends the client its SSL certificate
Authentication SSL certificateUpon its receipt, the client checks to make sure the certificate is “authentic.” This is an extremely important step. To truly have a secure connection, you can’t just encrypt your data, you also need to know it’s being sent to the right website&#x2F;organization. SSL&#x2F;TLS certificates provide that authentication.
During the authentication portion of the TLS handshake, the client performs several cryptographically secure checks to make sure the certificate provided by the server is authentic. This includes checking the digital signature and making sure the certificate originates from a trusted CA.
With the most common public key cryptosystem, RSA, the client will encrypt random data(pre-master-key) with the public key that needs to be used to generate the session key. The server will only be able to decrypt and use that data if it has the private key, which provides proof of possession.
Key ExchangeThe last part of the TLS handshake involves creating the “session key,” which is the key that will actually be used for secure communication, both client and server compute the session key by them self based on (client random number, server random number, pre-master key)
Session keys are “symmetric,” meaning the same key is used for encryption and decryption. These keys can achieve strong encryption much more efficiently than asymmetric keys, making them appropriate for sending data back and forth in an HTTPS connection。
 The exact method for generating the key varies based on the cipher suite that was chosen, with the two most common schemes being RSA and Diffie-Hellman.
 To end the handshake, each party lets the other know they have done all the necessary work, then both run check-sums to make sure the handshake occurred without any malicious tampering or corruption.
TLS 1.2 Handshake Please note this is just the logical process, several steps combine together and may send in one TCP packet.


The first message is called the “ClientHello.” This message lists the client’s capabilities so that the server can pick the cipher suite that the two will use to communicate. It also includes a large, randomly picked prime number called a “client random.”

The server politely responds with a “SeverHello” message. In this message it tells the client what connection parameters it has selected from the provided list and returns its own randomly selected prime number called a “server random.” If the client and server do not share any capabilities in common, the connection terminates unsuccessfully.

In the “Certificate” message, the Server sends its SSL certificate chain (which includes its leaf certificate and intermediate certificates) to the client. To provide authentication to the connection an SSL certificate is signed by a CA, which allows the client to verify that the certificate is legitimate. Upon receipt, the client performs several checks to authenticate the certificate. This includes checking the certificate’s digital signature, verifying the certificate chain, and checking for any other potential problems with the certificate data (expired certificate, wrong domain name, etc). The client will also make sure the server has possession of the certificate’s private key. This is done during the key exchange&#x2F;generation process.

(OPTIONAL)[Server Key Exchange]This is an optional message, only needed for certain key exchange methods (Diffie-Hellman) that require the server provides additional data.

The “Server Hello Done” message tells the client that it has sent over all its messages.

[Client Key Exchange] The client then provides its contribution to the session key. The specifics of this step depend on the key exchange method that was decided on in the initial “Hello” messages. In this example we’re looking at RSA, so the client is going to generate a random string of bytes called a pre-master secret, then encrypt it with the server’s public key and transmit it.

The “Change Cipher Spec” message lets the other party know that it has generated the session key and is going to switch to encrypted communication.

The “Finished” message is then sent to indicate that the handshake is complete on the client side. The Finished message is encrypted, and is the first data protected by the session key. The message contains data (MAC) that allows each party to make sure the handshake was not tampered with.

Now it’s the server’s turn to do the same. It decrypts the pre-master secret and computes the session key. Then it sends its “Change Cipher Spec” message to indicate it is switching to encrypted communication.

The server sends its “Finished” message using the symmetric session key it just generated, it also performs the same check-sum to verify the integrity of the handshake.


After these steps the SSL handshake is complete. Both parties now have a session key and will begin to communicate with an encrypted and authenticated connection.2, 3, 4, 5 may in one TCP message, 6.7.8 may in one TCP message, 9, 10 may in one TCP message.
Another way to see SSL handshake  

TLS 1.3 HandshakeCipher SuiteA cipher suite is a collection of algorithms that determine the parameters of a secure connection.
At the start of any connection, the very first interaction, the Client Hello, is a list of cipher suites that are supported. The server looks for the best, most secure option that is mutually supported and responds with its choice. You can look at a cipher suite and figure out all of the parameters of the handshake and the connection
TLS1.2four distinct algorithms represented in TLS 1.2 cipher suites:

Key Exchange(how to generate pre-master key)
Authentication(how to valid certificate)
Bulk Cipher(data encryption)
Hashing Algorithm(data integrity check)

Example Format

TLS is the protocol
ECDHE is the key exchange algorithm
ECDSA is the authentication algorithm
AES 128 GCM is the symmetric encryption algorithm
SHA256 is the hashing algorithm.

In the example above, we’re using Elliptic Curve Diffie-Hellman Ephemeral(ECDHE) for key exchange and Elliptic Curve Digital Signature Algorithm(ECDSA) for authentication. DH can also be paired with RSA (functioning as a digital signature algorithm) to accomplish authentication.
TLS 1.2 has 37 available ciphers in total, though not all of them are advisable. Here’s a list of the most widely-supported cipher suites:

TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256

TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384

TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA

TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA

TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256

TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384

TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256

TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384

TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA

TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA

TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256

TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384

TLS_DHE_RSA_WITH_AES_128_GCM_SHA256

TLS_DHE_RSA_WITH_AES_256_GCM_SHA384

TLS_DHE_RSA_WITH_AES_128_CBC_SHA

TLS_DHE_RSA_WITH_AES_256_CBC_SHA

TLS_DHE_RSA_WITH_AES_128_CBC_SHA256

TLS_DHE_RSA_WITH_AES_256_CBC_SHA256


TLS 1.3TLS 1.3 has made countless improvements over its predecessors, which is good considering it was in development for about a decade. The IETF removed support for older outmoded algorithms and streamlined everything, shortening the entire handshake from two round trips to one and reducing the sizes of cipher suites from four negotiations&#x2F;algorithms to two.
The number of supported cipher suites has also dropped from 37 to five. Here’s an example of a TLS 1.3 cipher suite:

TLS is the protocol
AES 256 GCM is the Authenticated Encryption with Associated Data (AEAD) algorithm
SHA384 is the Hashed-Key Derivation Function (HKFD) algorithm

We(TLS 1.3) already know we’re going be using some version of Diffie-Hellman Ephemeral key exchange, we just don’t know the parameters, so that means that the first two algorithms in the TLS 1.2 cipher suite are no longer needed. Those functions are still occurring, they just no longer need to be negotiated during the handshake.
Here are the five supported TLS 1.3 cipher suites:

TLS_AES_256_GCM_SHA384
TLS_CHACHA20_POLY1305_SHA256
TLS_AES_128_GCM_SHA256
TLS_AES_128_CCM_8_SHA256
TLS_AES_128_CCM_SHA256

SNIwhy need SNILet’s begin from an example, Let’s say you have two HTTP websites and you want to run them on a single IP address and port, how to config nginx to support this.
http &#123;    server &#123;        listen  80;        server_name www.example.com;        ...    &#125;    server &#123;        listen  80;        server_name www.example.org;        ...    &#125;&#125;# Both server has the same IP and port!!!# The default_server parameter, if present, will cause the server to become the default server for the specified address:port pair. # If none of the directives have the default_server parameter # then the first server with the address:port pair will be the default server for this pair
when a user requests a particular site out of two, it uses a unique HTTP header(‘HOST’) that includes hostname.nginx will use this header to match the request server, hence use that server to serve the request.

but this won’t work in the case of HTTPS, that’s because the HTTPS protocol uses an SSL&#x2F;TLS handshake to enforce secure communications between the client (browser) and server. The HTTP header will only be sent once the handshake is over. during the handshake, we need to send server’s certificate to client, which server’s certificate should be sent is a problem because we do not have HOST header right now, before SNI, we must listen on different IPs to support different website, so that one IP&lt;—–&gt;one certificate, after SNI was introduced, no need to use different addresses.
what’s SNIhttp &#123;    server &#123;        listen 443 ssl;        server_name www.example.com;        ssl_certificate     www.example.com.crt;        ssl_certificate_key www.example.com.key;        ...    &#125;    server &#123;        listen 443 ssl;        server_name www.example.org;        ssl_certificate     www.example.org.crt;        ssl_certificate_key www.example.org.key;        ...    &#125;&#125;
as Server Name Indication (SNI), an extension to the SSL&#x2F;TLS protocol, allows multiple SSL certificates to be hosted on a single unique IP address. SNI does this by inserting the HTTP header (virtual domain) in the SSL&#x2F;TLS handshake. As the server is able to see the virtual domain at ssl handshake phase, hence send correct certificate and serves the client with the website he&#x2F;she requested.

SSL certificateCertificate is used for identification, like you’re a good man, I trust you, Or it’s good company, I can send my personal info to it!!
How to identify the certificate to know you’re good man?As certificate is issued by CA encrypted with its private key(popular CA’s public key is always installed at PC), if we can decrypt the certificate and valid the field in it, Ok, you’re good man!!!
certificate contains company’s info and its public key! and encrypted with CA’s private key!!!  

Actually, certificate contains two parts, one is CSR(public key+ basic info), the other is signature(CA signed use CA private key), then encode these two with BASE64. that means you can get the basic info and public key,without verify it, but you can’t trust them until you verify it(use CA public key)
Everyone should have a certificate if he&#x2F;she needs to be identified by others. First he&#x2F;she gives his&#x2F;her certificate(like an ID) to the one she&#x2F;he wants to talk, then the one ASKs CA(certificate authority ), if this guy is a good man(good company), did he register at CA? CA checks his db, ok, he is good man(good company), you can talk with he&#x2F;her, or trust he&#x2F;her. but be sure talk with encryption, so that others can NOT know what you’re saying!
Before SSL, what client&#x2F;server needs to do?client&#x2F;server need to get their certificates and install CA, for two direction identification!

Install CA  Always the computer(OS) pre installed commonly used CA, organized like a tree(CA tree)
  root(CA)|      ||      |   CA1    CA2
  Note: each CA entry contains CA’s public key! key point!!  (certificate is encrypted by CA’s private key)

Get certificate server(client) generates its own public&#x2F;private keys, then send a CSR(certificate signing request) which contains its public key&#x2F;company name, website etc to CA, CA encrypts these info with CA’s private key to issue a certificate! now server(client) has its certificate(encrypted with CA’s private key) which contains his public key, company info and signature.
 


Certificate formatX.509 - 这是一种证书标准,主要定义了证书中应该包含哪些内容.其详情可以参考RFC5280,SSL使用的就是这种证书标准.
目前有以下两种编码格式(实现方式).(PEM, DER编码格式)
PEMPrivacy Enhanced Mail,打开看文本格式,以”—–BEGIN…”开头, “—–END…”结尾,内容是BASE64编码.  
查看PEM格式证书的信息:$ openssl x509 -in certificate.pem -text -nooutApache和NGINX服务器偏向于使用这种编码格式.
PEM – Openssl使用 PEM(Privacy Enhanced Mail)格式来存放各种信息, 它是 **openssl 默认采用的信息存放方式**。Openssl 中的 PEM 文件一般包含如下信息:

内容类型:表明本文件存放的是什么信息内容,它的形式为“——-BEGIN XXXX ——”,与结尾的“——END XXXX——”对应。

头信息:表明数据是如果被处理后存放,openssl 中用的最多的是加密信息,比如加密算法以及初始化向量 iv。

信息体:为 BASE64 编码的数据。可以包括所有私钥（RSA 和 DSA）、公钥（RSA 和 DSA）和 (x509) 证书。它存储用 Base64 编码的 DER 格式数据，用 ascii 报头包围，因此适合系统之间的文本模式传输。


使用PEM格式存储的证书：  
—–BEGIN CERTIFICATE—–MIICJjCCAdCgAwIBAgIBITANBgkqhkiG9w0BAQQFADCBqTELMAkGA1UEBhMCVVMx………1p8h5vkHVbMu1frD1UgGnPlOO/K7Ig/KrsU=—–END CERTIFICATE—–

使用PEM格式存储的私钥：  
—–BEGIN RSA PRIVATE KEY—–MIICJjCCAdCgAwIBAgIBITANBgkqhkiG9w0BAQQFADCBqTELMAkGA1UEBhMCVVMx………1p8h5vkHVbMu1frD1UgGnPlOO/K7Ig/KrsU=—–END RSA PRIVATE KEY—–
使用PEM格式存储的证书请求文件：
—–BEGIN CERTIFICATE REQUEST—–MIICJjCCAdCgAwIBAgIBITANBgkqhkiG9w0BAQQFADCBqTELMAkGA1UEBhMCVVMx………1p8h5vkHVbMu1frD1UgGnPlOO/K7Ig/KrsU=—–END CERTIFICATE REQUEST—–

Here is a certificate(encode with base64)
-----BEGIN CERTIFICATE-----MIICPjCCAaegAwIBAgIBADANBgkqhkiG9w0BAQ0FADA8MQswCQYDVQQGEwJ1czELMAkGA1UECAwCYmoxCzAJBgNVBAoMAnd3MRMwEQYDVQQDDAp3d3cuYXMuY29tMB4XDTE5MTAyODE1MDUxNFoXDTIwMTAyNzE1MDUxNFowPDELMAkGA1UEBhMCdXMxCzAJBgNVBAgMAmJqMQswCQYDVQQKDAJ3dzETMBEGA1UEAwwKd3d3LmFzLmNvbTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA1P7qDXxtezldoIv5Id7pylE0k3JMqUa1nw3HLhITZfOs2kt/hvoeeVcsOZMsgXtHNTHZCJrA6KilnCUpJUM3qhKjmLLfRE9QwahEbd9zoCxvDNhBztcx8UFZKtX3oOtYBlmW2QHrzm9mHfRduYFSiRsLyMOzeU98BKfMwV+ZdZMCAwEAAaNQME4wHQYDVR0OBBYEFFzwMNE95TTwAifd8k2//byKQzycMB8GA1UdIwQYMBaAFFzwMNE95TTwAifd8k2//byKQzycMAwGA1UdEwQFMAMBAf8wDQYJKoZIhvcNAQENBQADgYEACxvLypbO4tugxpcgl6NFsYx9pGwp4eMteyPVa1ptFuRytVrTyV/AP397nazQ9Msm7vFxlmHjQoVyZ29a49QTN/uFAkV+OkZu0QqjY//I0t0uW2lD/+jdRP+MLt2aVga3qPuS9DaxN0b52DOoJTLCmjJfApdrqdqLC+QiOfxrj+Q=-----END CERTIFICATE-----

After decode with base64
Certificate:    Data:        Version: 3 (0x2)        Serial Number: 0 (0x0)    Signature Algorithm: sha512WithRSAEncryption        Issuer: C=us, ST=bj, O=ww, CN=www.as.com        Validity            Not Before: Oct 28 15:05:14 2019 GMT            Not After : Oct 27 15:05:14 2020 GMT        Subject: C=us, ST=bj, O=ww, CN=www.as.com        Subject Public Key Info:            Public Key Algorithm: rsaEncryption                Public-Key: (1024 bit)                Modulus:                    00:d4:fe:ea:0d:7c:6d:7b:39:5d:a0:8b:f9:21:de:                    e9:ca:51:34:93:72:4c:a9:46:b5:9f:0d:c7:2e:12:                    13:65:f3:ac:da:4b:7f:86:fa:1e:79:57:2c:39:93:                    2c:81:7b:47:35:31:d9:08:9a:c0:e8:a8:a5:9c:25:                    29:25:43:37:aa:12:a3:98:b2:df:44:4f:50:c1:a8:                    44:6d:df:73:a0:2c:6f:0c:d8:41:ce:d7:31:f1:41:                    59:2a:d5:f7:a0:eb:58:06:59:96:d9:01:eb:ce:6f:                    66:1d:f4:5d:b9:81:52:89:1b:0b:c8:c3:b3:79:4f:                    7c:04:a7:cc:c1:5f:99:75:93                Exponent: 65537 (0x10001)        X509v3 extensions:            X509v3 Subject Key Identifier:                 5C:F0:30:D1:3D:E5:34:F0:02:27:DD:F2:4D:BF:FD:BC:8A:43:3C:9C            X509v3 Authority Key Identifier:                 keyid:5C:F0:30:D1:3D:E5:34:F0:02:27:DD:F2:4D:BF:FD:BC:8A:43:3C:9C            X509v3 Basic Constraints:                 CA:TRUE    Signature Algorithm: sha512WithRSAEncryption         0b:1b:cb:ca:96:ce:e2:db:a0:c6:97:20:97:a3:45:b1:8c:7d:         a4:6c:29:e1:e3:2d:7b:23:d5:6b:5a:6d:16:e4:72:b5:5a:d3:         c9:5f:c0:3f:7f:7b:9d:ac:d0:f4:cb:26:ee:f1:71:96:61:e3:         42:85:72:67:6f:5a:e3:d4:13:37:fb:85:02:45:7e:3a:46:6e:         d1:0a:a3:63:ff:c8:d2:dd:2e:5b:69:43:ff:e8:dd:44:ff:8c:         2e:dd:9a:56:06:b7:a8:fb:92:f4:36:b1:37:46:f9:d8:33:a8:         25:32:c2:9a:32:5f:02:97:6b:a9:da:8b:0b:e4:22:39:fc:6b:         8f:e4
As you can see that it contains basic info and public key and signature(signed with CA private key)
Certificate revocation list(CRL)In cryptography, a certificate revocation list (or CRL) is “a list of digital certificates that have been revoked by the issuing certificate authority (CA) before their scheduled expiration date and should no longer be trusted.
Expiration dates are not a substitute for a CRL. While all expired certificates are considered invalid, not all unexpired certificates should be valid
Common Name(CN) vs Subject Alternative Name(SAN)CN vs SAN
DER – 辨别编码规则 (DER) 可包含所有私钥、公钥和证书**它是大多数浏览器的缺省格式，并按 ASN1 DER 格式存储。它是无报头的 － PEM 是用文本报头包围的 DER。
DER - Distinguished Encoding Rules,打开看是二进制格式,不可读.查看DER格式证书的信息:$ openssl x509 -in certificate.der -inform der -text -nooutJava和Windows服务器偏向于使用这种编码格式.
证书编码的转换PEM转为DER $ openssl x509 -in cert.crt -outform der -out cert.der
DER转为PEM $ openssl x509 -in cert.crt -inform der -outform pem -out cert.pem
(提示:要转换KEY文件也类似,只不过把x509换成rsa,要转CSR的话,把x509换成req…)
相关的文件扩展名这是比较误导人的地方,虽然我们已经知道有PEM和DER这两种编码格式, 但文件扩展名并不一定就叫”PEM”或者”DER”,常见的扩展名除了PEM和DER还有以下这些, 它们除了编码格式可能不同之外,内容也有差别,但大多数都能相互转换编码格式.

CRT - CRT应该是certificate,其实还是证书的意思,常见于 NIX系统,有可能是PEM编码,也有可能是DER编码, 大多数应该是PEM编码,相信你已经知道怎么辨别.

CER - 还是certificate,还是证书,常见于Windows系统,同样的,可能是PEM编码,也可能是DER编码,大多数应该是DER编码.证书中没有私钥，DER 编码二进制格式的证书文件

KEY - 通常用来存放一个公钥或者私钥,并非X.509证书, 编码同样的,可能是PEM,也可能是DER.  查看KEY的办法 PEM 格式: $ openssl rsa -in mykey.key -text -noout  DER格式: $ openssl rsa -in mykey.key -text -noout -inform der

CSR - Certificate Signing Request,即证书签名请求,这个并不是证书,而是向权威证书颁发机构获得签名证书的申请,其核心内容是一个公钥(当然还附带了一些别的信息),在生成这个申请的时候,同时也会生成一个私钥,私钥要自己保管好.做过iOS APP的朋友都应该知道是怎么向苹果申请开发者证书的吧.查看的办法:openssl req -noout -text -in my.csr (如果是DER格式的话照旧加上-inform der,这里不写了)

PFX&#x2F;P12 - predecessor of PKCS#12,包含公钥和私钥的二进制格式证书
  对nginx服务器来说,一般CRT和KEY是分开存放在不同文件中的,但Windows的IIS则将它们存在一个PFX文件中,(因此这个文件包含了证书及私钥)这样会不会不安全？应该不会,PFX通常会有一个”提取密码”,你想把里面的东西读取出来的话,它就要求你提供提取密码,PFX使用的时DER编码,如何把PFX转换为PEM编码？
  $ openssl pkcs12 -in for-iis.pfx -out for-iis.pem -nodes  这个时候会提示你输入提取代码. for-iis.pem就是可读的文本.  生成pfx的命令类似这样: $ openssl pkcs12 -export -in certificate.crt -inkey privateKey.key -out certificate.pfx
  其中CACert.crt是CA(权威证书颁发机构)的根证书,有的话也通过-certfile参数一起带进去.这么看来,PFX其实是个证书密钥库.

p7b - 以树状展示证书链(certificate chain)，同时也支持单个证书，不含私钥。

JKS - 即Java Key Storage,这是Java的专利,跟OpenSSL关系不大,利用Java的一个叫”keytool”的工具,可以将PFX转为JKS,当然了,keytool也能直接生成JKS,不过在此就不多表了.


Openssl command# check certificate with .pem format$ openssl x509 -text -noout -in cert.pem# create self-signed certificate with openssl# no extension$ cat san.cnf[ req ]default_bits       = 2048distinguished_name = req_distinguished_nameprompt = no[ req_distinguished_name ]countryName                 = CNstateOrProvinceName         = bjlocalityName               = bjorganizationName           = cyun commonName                 = cyun.com$ openssl req -config san.cnf -new -x509 -sha256 \   -newkey rsa:2048 -nodes -keyout private.key -days 365 -out cert.pem# with extension$ cat san.cnf[ req ]default_bits       = 2048distinguished_name = req_distinguished_namereq_extensions     = req_extprompt = no[ req_distinguished_name ]countryName                 = CNstateOrProvinceName         = bjlocalityName               = bjorganizationName           = cyun commonName                 = cyun.com[ req_ext ]subjectAltName = @alt_names[alt_names]DNS.1   = bestflare.comDNS.2   = usefulread.comDNS.3   = chandank.com$ openssl req -config san.cnf -new -x509 -sha256 \  -newkey rsa:2048 -nodes -keyout private.key -days 365 \  -out cert.pem -extensions req_ext

REF
SSL handshake

]]></content>
      <categories>
        <category>protocol</category>
        <category>ssl</category>
      </categories>
      <tags>
        <tag>ssl</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title>prototol_gre</title>
    <url>/2021/04/22/prototol-gre/</url>
    <content><![CDATA[Overview
passenger protocol(inner)  The type of protocol (IPv4, IPv6, or MPLS) used by the networks that are connected by a GRE tunnel. Packets that are encapsulated and routed across the transport network are payload packets.

encapsulation protocol(gre)  The type of network layer protocol (GRE) used to encapsulate passenger protocol packets so that the resulting GRE packets can be carried over the transport protocol network as the packet payload.

transport protocol(outer)  The type of protocol (IPv4) used by the network that routes passenger protocol packets through a GRE tunnel. The transport protocol is also called the delivery protocol.




Generic Routing Encapsulation (GRE) is one of the available tunneling mechanisms which uses IP as the transport protocol and can be used for carrying many different passenger protocols. The tunnels behave as virtual point-to-point links that have two endpoints identified by the tunnel source and tunnel destination addresses at each endpoint.
A GRE packet header structure is represented in the diagram below.

Five GRE flag bits indicate whether a particular GRE header includes any optional fields (Checksum, Offset, Key, Sequence Number, and Routing). Of the five optional fields, filter-based GRE IPv4 tunneling uses the Key field only.

The first two octets encode GRE flags, indicate whether a particular GRE header includes any optional fields (Checksum, Offset, Key, Sequence Number, and Routing)

The 2-octet Protocol Type field contains the value 0x0800 to specify the EtherType value for the IPv4 protocol.

The 4-octet Key field is included only if the Key Present bit is set to 1. The Key field carries the key value of the tunnel defined on the encapsulator. If the GRE tunnel definition specifies a key, the Packet Forwarding Engine for the encapsulating endpoint sets the Key Present bit and adds the Key to the GRE header.


How GRE worksTopopogy
R1 (GRE config only)interface s0/0/0ip address 63.1.27.2 255.255.255.0interface tunnel0ip address 10.0.0.1 255.255.255.0tunnel mode gre ip //this command can be ignoredtunnel source s0/0tunnel destination 85.5.24.10router ospf 1R1&gt;ip route 192.168.2.0 255.255.255.0 10.0.0.1

R2 (GRE config only)interface s0/0/0ip address 85.5.24.10 255.255.255.0interface tunnel1ip address 10.0.0.2 255.255.255.0tunnel source 85.5.24.10tunnel destination 63.1.27.2router ospf 1R2&gt;ip route 192.168.1.0 255.255.255.0 10.0.0.2
Sssigns the IP address for the tunnel interface: 10.0.0.1&#x2F;24.The IP addresses of two tunnel interfaces must be in the same subnet(10.0.0.1&#x2F;24 on R1 &amp; 10.0.0.2&#x2F;24 on R2 in this case).
This address is not used by Delivery header who uses tunnel source and tunnel destination(tunnel source and tunnel destination should reach each other!!!)
FlowWhen the sending router decides to send a packet into the GRE Tunnel, it will “wrap” the whole packet into another IP packet with two headers: one is the GRE header (4 bytes) which uses to manage the tunnel itself. The other is called “Delivery header” (20 bytes) which includes the new source and destination IP addresses of two virtual interfaces of the tunnel (called tunnel interfaces). This process is called encapsulation.

In the example above when R1 receives an IP packet, it wraps the whole packet with a GRE header and a delivery header. The delivery header includes new source IP address of 63.1.27.2 (the IP address of R1’s physical interface which is used to create tunnel) and new destination IP address of 85.5.24.10 (the IP address of R2’s physical interface which is used to create tunnel).
It is important to note that the GRE tunnel does not encrypt the packet, only encapsulate it. If we want to encrypt the packet inside GRE Tunnel we must use IPSec.
When the GRE packet arrives at the other end of the tunnel (R2 in this case), the receiving router R2 needs to remove the GRE header and delivery header to get the original packet.
Ref
Juniper GRE
GRE example

]]></content>
      <categories>
        <category>protocol</category>
        <category>gre</category>
      </categories>
      <tags>
        <tag>GRE</tag>
      </tags>
  </entry>
  <entry>
    <title>protocol_vxlan_nvgre_geneve</title>
    <url>/2021/04/22/protocol-vxlan-nvgre-geneve/</url>
    <content><![CDATA[OverviewTunneling is a mechanism that makes transfer of payloads feasible over an incompatible delivery network. It allows the network user to gain access to denied or insecure networks. Data encryption may be employed to transport the payload, ensuring that the encapsulated user network data appears as public even though it is private and can easily pass the conflicting network.
An information center hosting hundreds of thousands of customers located across many locations with virtualized customer workloads will most likely need many VLANs and at the same time the ability to share assets on each other networks. VXLAN&#x2F; NVGRE SUPPORT Software Defined Networking (SDN) was created partly to solve problems associated with multi-tenant environments. This wide resource sharing enables Microsoft to create NVGRE for Hyper-V, a native hypervisor that can create virtual machines on x86-64 systems starting with Windows 8 and VMware to create Virtual Extensible LAN (VXLAN), a tunneling protocol that recognizes network users to access or provide networking services to networks that does not support or provide directly.
NVGRE standard is proposed by Microsoft, Intel, HP and Dell.VXLAN specification was originally created by Cisco, VMware, and Arista Networks

NVGRENVGRE is a networking virtualization process that primarily seeks to ease the scalability problems related to large cloud computing deployments. The network virtualization process uses encapsulation to tunnel the data link layer (layer 2) packets over the network layer (layer 3) networks. The aim is to permit multi-tenant and load-level networks that could be shared across on-site and cloud-based environments.
In NVGRE, the packets of the virtual machine are encapsulated inside another packet resulting to this new NVGRE-formatted packet possessing the appropriate source and destination provider area (PA) IP addresses.
NVGRE uses the lower 24 bits of the GRE header as the TNI (tenant network identifier), which, like the VXLAN, can support 16 million virtual networks. In order to provide a flow-level granularity describing the bandwidth utilization, the transmission network needs to use the GRE header. But this results in NVGRE not being compatible with traditional load balancing, which is the main shortcoming of NVGRE and the biggest difference from NVGRE.

OUTER IP + GRE HEADER + (Inner Layer2 + Inner Layer3 + TCP&#x2F;UDP + payload) as IP payload
no extra UDP header as GRE is at same level as TCP or UDP.


VXLAN（Virtual extensible local area network)The purpose of VXLAN is to provide scalable network isolation. VXLAN is a Layer 2 overlay scheme on a Layer 3 network. It allows an overlay layer-2 network to spread across multiple underlay layer-3 network domains. Each overlay is termed a VXLAN segment. Only VMs within the same VXLAN segment can communicate.
The VXLAN packet header includes a 24-bit ID segment, which stands for 16 million unique virtual segments. This ID is usually generated by pseudo-random algorithm on UDP ports. This helps to keep load balancing based on 5-tuple and preserve the order of packets between VMs by mapping the MAC group within packets to a unique UDP port group. VXLAN encapsulation expands the packet size to 50 bytes, which is shown as below.

OUTER IP + UDP + (VXLAN HEADER + Inner Layer2 + Inner Layer3 + TCP&#x2F;UDP + payload)as UDP payload


GENEVE(Generic Network Virtualization Encapsulation)VXLAN (Virtual Extensible LAN), NVGRE (Network Virtualization using Generic Routing Encapsulation) and STT (Stateless Transport Tunneling). All three encapsulate application data in a new larger fixed header field. That header size is 24-bit for VXLAN and NVGRE, the latter being used mostly by Microsoft, while STT has a 64-bit header size. None of these encapsulation tunnelling methods require any change to hardware networking infrastructure, though some vendors offer hardware that can help assist in accelerating the efficiency of the solution. However, none of the solutions are compatible with each other.
Geneve is designed to recognize and accommodate changing capabilities and needs of different devices in network virtualization. It provides a framework for tunneling rather than being prescriptive about the entire system. Geneve defines the content of the metadata flexibly that is added during encapsulation and tries to adapt to various virtualization scenarios. It uses UDP as its transport protocol and is dynamic in size using extensible option headers. Geneve supports unicast, multicast, and broadcast.
GENEVE encapsulated packets are designed to be transmitted via standard networking equipment. Packets are sent from one tunnel endpoint to one or more tunnel endpoints using either unicast or multicast addressing.
GENEVE is being adopted as the default tunnelling protocol for OVN (Open Virtual Network) which in turn is being promoted as an implementation of OVS (OpenvSwitch) in future OpenStack releases
]]></content>
      <categories>
        <category>protocol</category>
        <category>overlay</category>
      </categories>
      <tags>
        <tag>overlay</tag>
      </tags>
  </entry>
  <entry>
    <title>python-advanced</title>
    <url>/2019/11/15/python-advance/</url>
    <content><![CDATA[Advanced featurelambdalambda is like a function(with return implicitly) with  one statement, but no name(anonymous function), if you have more logic, use function instead, create a lambda means create a function object, then call it later on.

lambda features

return implicitly
one statement
anonymous function

f = lambda arg1, arg2: expression the result of the expression is return value.


[expression for x in list condition]# save result of expression to a list if condition matches# expression can be simple or lambda!!!

Most use case of lambda is like this.&#96;

some API parameters need a function
the function is small
In this case, use lambda instead of a function.


map(), filter(), reduce() all return a new list after applying function for each element.
map()
Map applies a function to all the items in an input_list. Here is the blueprint:
map(function_to_apply_must_return_value_with_elm_as_input, list_of_inputs)
filter()
As the name suggests, filter creates a list of elements for which a function returns true.filter(filter_function_must_return_true_or_false_with_elm_as_input, list_of_inputs)
reduce()
Reduce is a really useful function for performing some computation on a list and returning the result. It applies a rolling computation to sequential pairs of values in a list, it passed the previous result as the second parameter!
reduce(fun_return_value_with_elm_and_previous_result_as_input, list_of_inputs)
def lambda_demo():    from functools import reduce    #function created by lambda is said to be anonymous because it was never given a name    f = lambda x: x**3    print(f(3))        items = [1, 2, 3, 4, 5]    print(&quot;org:&quot;, items)    squared = list(map(lambda x: x**2, items))    print(&quot;map():&quot;,squared)        number_list = range(-5, 5)    print(&quot;org:&quot;,  list(number_list))    less_than_zero = list(filter(lambda x: x &lt; 0, number_list))    print(&quot;filter():&quot;,less_than_zero)    list1 = [1, 2, 3, 4]    # y is the result of previous op, x is the element of list1.    print(reduce(lambda x, y: x * y, list1))    print(reduce(lambda x, y: x + y, list1))    lambda_demo()

27
org: [1, 2, 3, 4, 5]
map(): [1, 4, 9, 16, 25]
org: [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4]
filter(): [-5, -4, -3, -2, -1]
24
10

retrySometimes function needs to run again when it fails, that’s retry, you can use function from python retrying module, it provides a decorator that can do retrying, just add this decorator to your function that needs retrying.
# pip3 install retryingdef retry_demo():    from retrying import retry        # by default, retry happens when exception happens,but can be customized.    @retry(stop_max_attempt_number=2, wait_random_min=1000, wait_random_max=2000)    def run():        print(&#x27;you should see me twice&#x27;)        raise NameError(&quot;hi&quot;)            def retry_if_result_not_2(result):        return result != 2        # retry by checking result of run2    result = 0    @retry(retry_on_result=retry_if_result_not_2, wait_random_min=1000, wait_random_max=2000)    def run2():        nonlocal result        print(&#x27;you should see me twice:&#x27;, result)        if result != 2:            result = result + 1                return result            run2()retry_demo()

you should see me twice: 0
you should see me twice: 1

daemonIn some case, app needs to run as a daemon, python-daemon provides us such function, in order to use it, first install pip3 install python-daemon
main.py
# pip3 install python-daemon#! /usr/bin/env python3import daemonclass App:    def __init__(self):        pass    def start(self):        with daemon.DaemonContext():            # fork a new process            self.run()    def run(self):        time.sleep(10000)        # print is not to console as it&#x27;s a daemon!!!        # print(&quot;I runs as a daemon&quot;)app = App()app.start()

Run it this way
$./main.py$ ps -ef | grep main.py

Run shell commandsometimes we need to run shell command from python, there are several APIs you can use to run shell command.
These APIs can be grouped into two types

one is with shell output, 
the other is without shell output returned.

shell output returned as valueOld APIs

getoutput()
getstatusoutput()

parameter
these two methods implicitly are invoked with shell mode!, that means parameter is &#39;ls -al&#39; not [&#39;ls&#39;, &#39;-al&#39;]
result:
getoutput() returns a str of command output or error message like &#39;/bin/sh: 1: lh: not found&#39;
getstatusoutput() returns a tuple (returncode, output), return code 0 means good!
New API

check_output()–&gt;waits until all output is read. If ssh inherits the pipe then check_output() will wait until it exits (until it closes its inherited pipe ends).

return value: output in bytes(utf-8)
Raise exception if return code is non-zero otherwise output with bytes, it more like getoutput(), but getoutput() doesn&#39;t raise exception if fails.
no shell output returned but print it to consoleOld API

call()

New API

check_call()–&gt;returns as soon as /bin/sh process exits without waiting for descendant processes, fast

These two methods do not store the output, only print it out inside library(may be to console or file depends on current STD)!!! it only returns code with 0 if success to run, otherwise raises exception for check_xx version, catch the exception(CalledProcessError) to see the return code and output attribute in it
check_call() does the same thing as call(), but check_call() will throw exception when command fails to run, so that you can see where the problem is!!! so always use check_call().
paramter
these API can run with two modes, shell enabled or disabled, with shell enabled, the parameter is a str, otherwise it’s a list [] like this.  
when shell&#x3D;True, it uses /bin/sh to run command, /bin/sh does not support brace expansion($echo ‘hi ‘{jason,josh})
call([&#x27;ls&#x27;, &#x27;-al&#x27;])check_call(&#x27;ls -al&#x27;, shell=True)check_call(cmd, shell=True, executable=&#x27;/bin/bash&#x27;) # use bash


result of check_call() or check_output()  exception subprocess.CalledProcessError
  Subclass of SubprocessError, raised when a process run by check_call() or check_output() returns a non-zero exit status.
  returncode
  Exit status of the child process. If the process exited due to a signal, this will be the negative signal number.
  cmd
  Command that was used to spawn the child process.
  output
  Output of the child process if it was captured by run() or check_output(). Otherwise, None.
  stdout
  Alias for output, for symmetry with stderr.
  stderr
  Stderr output of the child process if it was captured by run(). Otherwise, None.
import subprocessprint(&quot;getoutput(&#x27;ls /bin/ls&#x27;) show below&quot;)print(subprocess.getoutput(&#x27;ls /bin/ls&#x27;))print(&quot;getstatus(&#x27;/bin/ls&#x27;) show below&quot;)print(subprocess.getstatusoutput(&#x27;/bin/ls&#x27;))print(&quot;check_output(&#x27;/bin/ls&#x27;, shell=True) show below&quot;)# need to decode return value by check_outputprint(subprocess.check_output(&#x27;/bin/ls&#x27;, shell=True).decode(&#x27;utf8&#x27;))try:    subprocess.check_output(&#x27;/bin/not_found&#x27;, shell=True)except subprocess.CalledProcessError:    print(&#x27;exception: check_output() not found command&#x27;)    print(&#x27;no exception getoutput()&#x27;, subprocess.getoutput(&#x27;/bin/not_found&#x27;)) # no exception

getoutput(&#39;ls /bin/ls&#39;) show below
/bin/ls
getstatus(&#39;/bin/ls&#39;) show below
(0, &#39;algorithm\nclass_advanced.ipynb\ndebug\nfunction_advanced.ipynb\nhello.txt\npython3_advanced.ipynb\npython3_basic.ipynb\npython3_lib.ipynb\nreadme_pip.ipynb\nsamples&#39;)
check_output(&#39;/bin/ls&#39;, shell=True) show below
algorithm
class_advanced.ipynb
debug
function_advanced.ipynb
hello.txt
python3_advanced.ipynb
python3_basic.ipynb
python3_lib.ipynb
readme_pip.ipynb
samples

exception: check_output() not found command
no exception getoutput() /bin/sh: /bin/not_found: No such file or directory


/bin/sh: /bin/not_found: No such file or directory

Deep copy and shallow copy of list or dictA shallow copy means constructing a new collection object and then populating it with references to the child objects found in the original. The copying process does not recurse and therefore will not create copies of the child objects themselves. In case of shallow copy, a reference of object is copied in other object, it means that any changes made to the copied object do reflect in the original object.
reference only works for object(like list, dict, class object) but for integer etc
Create shallow copy
# use copy libraryimport copy# shallow copy, no recurse for embedded object!!!newlt = copy.copy(list)newdt = copy.copy(dict)# shallow copy(another way)newlt = list1[:]newdt = dict.copy()

Create deep copy
import copy# recurse for embed object!!!copy.deepcopy(list)copy.deepcopy(dict)

shallow copydt = &#123;&#x27;a&#x27;: [1, 2]&#125;print(&#x27;before creating new dict, org dict: &#x27;, dt)# shadow copyndt = dt.copy()ndt[&#x27;a&#x27;].append(3)print(&#x27;new dict &#x27;, ndt)print(&#x27;org dict&#x27;, dt)lt = [1, [2, 3]]print(&#x27;before creating new list, org list: &#x27;, lt)# shadow copynlt = lt[:]nlt[1][0] = 5print(&#x27;new list: &#x27;, nlt)print(&#x27;org list: &#x27;, lt)

before creating new dict, org dict:  &#123;&#39;a&#39;: [1, 2]&#125;
new dict  &#123;&#39;a&#39;: [1, 2, 3]&#125;
org dict &#123;&#39;a&#39;: [1, 2, 3]&#125;
before creating new list, org list:  [1, [2, 3]]
new list:  [1, [5, 3]]
org list:  [1, [5, 3]]

Deep copyimport copydt = &#123;&#x27;a&#x27;: [1, 2]&#125;dtcc = copy.deepcopy(dt)  # deep copy.print(&quot;orignal dict: &quot;, dt)print(&quot;copied dict after deepcopy is: &quot;, dtcc)dt[&#x27;a&#x27;].append(3)  # dtcc doesn&#x27;t change!!!!print(&quot;added a item in orignal dict: &quot;, dt)print(&quot;copied dict now is: &quot;, dtcc)

orignal dict:  &#123;&#39;a&#39;: [1, 2]&#125;
copied dict after deepcopy is:  &#123;&#39;a&#39;: [1, 2]&#125;
added a item in orignal dict:  &#123;&#39;a&#39;: [1, 2, 3]&#125;
copied dict now is:  &#123;&#39;a&#39;: [1, 2]&#125;

any&#x2F;all functionall(iterable object) returns True if all values in the iterable match bool(elm)&#x3D;&#x3D;Truewhile any(iterable object) returns True if any value in the iterable matches bool(elm)&#x3D;&#x3D;True.

all(empty_iterable) &#x3D;&#x3D; True
any(empty_iterable) &#x3D;&#x3D; False

One use case for all() is to check if an iterable object includes another.
def any_all_demo():    print(&quot;any([])=&quot;, any([]))    print(&quot;any([0])=&quot;, any([0]))    print(&quot;any([1])=&quot;, any([1]))    print(&quot;all(())=&quot;, all(()))    print(&quot;all((1,))=&quot;, all((1,)))    print(&quot;all([0])=&quot;, all([0]))any_all_demo()def check_list_container_another():    list1 = [1, 2]    list2 = [1, 2, 3]    # for..in to loop    # in to test    result = [elm in list2 for elm in list1]    print(result)    if all(result):        print(&#x27;list2 contains list1&#x27;)    else:        print(&#x27;list2 doest not contain list1&#x27;)        check_list_container_another()

any([])= False
any([0])= False
any([1])= True
all(())= True
all((1,))= True
all([0])= False
[True, True]
list2 contains list1

CSV fileIf file type is csv, you can use import csv or import pandas to access it
use pandas always as it’s powerful!
csv file is text file but with fixed format, each row should have same columns and separated by a &#39;separator&#39;
import osimport pandas as pddef pandas_csv():    # 任意的多组列表    a = [1, 2, 3]    b = [4, 5, 6]        # 字典中的key值即为csv中列名    dataframe = pd.DataFrame(&#123;&#x27;a_name&#x27;: a, &#x27;b_name&#x27;: b&#125;)    # 将DataFrame存储为csv,index表示是否显示行名，default=True    # dataframe.to_csv(&quot;test.csv&quot;, index=False, sep=&#x27;,&#x27;, mode=&#x27;a&#x27;)   a--&gt;append  w--&gt;write    dataframe.to_csv(&quot;test.csv&quot;, index=False, sep=&#x27;,&#x27;, mode=&#x27;w&#x27;)    &#x27;&#x27;&#x27;    a_name,b_name    1,     4    2,     5    3,     6    &#x27;&#x27;&#x27;    data = pd.read_csv(&#x27;test.csv&#x27;)    print(&#x27;pandas read test.csv\n&#x27;, data)    print(&#x27;pandas read test.csv only values:\n&#x27;, data.values)    os.remove(&#x27;test.csv&#x27;)pandas_csv()

pandas read test.csv
    a_name  b_name
0       1       4
1       2       5
2       3       6
pandas read test.csv only values:
 [[1 4]
 [2 5]
 [3 6]]

serialization class, tuple, list, dictIn some case we want to save our data(object) into a file with binary(not json), as it’s small, then read it back into object that we use before, use cloudpickle which extends pickle, it supports serialization of many objects, class, tuple, list, dict etc.
def save_object_into_file():    import os    import cloudpickle            lt = [&#x27;a&#x27;, &#x27;b&#x27;]        # write data to file with binary mode    with open(&#x27;data_binary&#x27;, &#x27;wb&#x27;) as fd:        cloudpickle.dump(lt, fd) # cloudpickle.dumps(lt) returns result of bytes not writing to file    # read it back    print(&quot;write to file with data: &quot;, lt)    # must open with binary mode    with open(&#x27;data_binary&#x27;, &#x27;rb&#x27;) as fd:        lt = cloudpickle.load(fd) # cloudpickle.loads(bytes) loads from bytes        print(&#x27;read data back: &#x27;, lt)            os.remove(&#x27;data_binary&#x27;)    save_object_into_file()

write to file with data:  [&#39;a&#39;, &#39;b&#39;]
read data back:  [&#39;a&#39;, &#39;b&#39;]

show trace with try&#x2F;exceptWithout try/except, trace is printed when exception happens, but if we want to see trace even we catch the exception, use traceback(new traceback2).
def show_traceback_even_with_exception():    import sys    import traceback2        try:        a = 12 / 0    except Exception as error:        print(&#x27;------------traceback is printed&#x27;)        traceback2.print_exc(file=sys.stdout)        print(&#x27;------------traceback is printed&#x27;)        show_traceback_even_with_exception()

------------traceback is printed
Traceback (most recent call last):
  File &quot;/tmp/ipykernel_2840/2931145062.py&quot;, line 6, in show_traceback_even_with_exception
ZeroDivisionError: division by zero
------------traceback is printed

package and module
A package is dir with __init__.py under it
a module is a python file like request.py

The most use case is to group related modules within a package then use them, the struct may look like this.
`-- common    |-- __init__.py    `-- request.py

after import you may use xxx.yyy (yyy must be a symbol of xxx.py or symbol of xxx&#x2F;_init_.py)
import package_name# then use this way: package_name.module.functionfrom package_name import module_name# then use this way: module_name.functionimport module_namefrom module_name import function_xxfrom module_name import var_xx# then use this way: function_xx


what content of _init_.py?
Idealy, you can write anything in it, it&#39;s just a python file that is called when import the package, but mostly we write it with below content.
Examples of _init_.py

empty, just required by python

#__init__.py

# test.py to show how use module# must use module with package prefix or module prefix!!!from common import requestrequest.aaa

wrapper modules API

# __init__.pyfrom request import *   # import all from request module.from request import aaa # OR only import aaa for package
# test.py# then you can access module&#x27;s function by package name like below# use itimport common# as in common/__init__.py we import aaa from request# so here just use common.aaa, not request.aaa, common seems a wrapper for its module!!!common.aaa()


wrapper module API and define package level API as well

# __init__.pyfrom request import *   # import all for packagefrom request import aaa # OR only import aaa for package# package level API, it may call package api to wrapper them.def pkg_aaa():    print(&quot;pkg_aaa&quot;)
# test.pyimport commoncommon.aaa()common.pkg_aaa()

access module from parent directory`-- app    |-- app.py    |-- common    |   |-- __init__.py    |   `-- util.py    `-- tests        `-- test_app.py

Add module to sys.path
# file test_app.pyimport os                                                                       import sys                                                                                                                                                      currentdir = os.path.dirname(os.path.realpath(__file__))                        parentdir = os.path.dirname(currentdir)                                         sys.path.append(parentdir)                                                                                                                                      import app                                                                      from common import util                           

Use relative path
# file test_app.py                                                                                                                                                             import ..app                                                                      from ..common import util      import ..common.util

inside with statementclass-based support with()when “with” statement is executed, Python evaluates the expression, calls the __enter__ method on the resulting value (which is called a &quot;context guard&quot;) and assigns whatever __enter__ returns to the variable given by as. Python will then execute the code body and no matter what happens in that code, call the guard object’s exit method&#96;
This interface of enter() and exit() methods which provides the support of with statement in user defined objects is called Context Manager.
class ctl:    def __enter__(self):        #set things up        return self    def __exit__(self, type, value, traceback):        # tear things down        return True # True means continue goes run even exception happens!!!!        # most of time, False used    def divide_zero(self):        bar = 1/0  # the execption will pass to __exit__        # if no __exit__, program quit, otherwise, if __exit__ return False        # exception, True --contine rundef with_as_example():    # like th = ctl().__enter__()    with ctl() as th:        th.divide_zero()    print(&#x27;go here&#x27;)

function-based: contextlib module to support with()A class based context manager as shown above(__enter__, __exit__) is not the only way to support the with statement in user defined objects. The contextlib module provides a few more abstractions built upon the basic context manager interface.
from contextlib import contextmanager  class MessageWriter(object):    def __init__(self, filename):        self.file_name = filename      @contextmanager    def open_file(self):        try:            file = open(self.file_name, &#x27;w&#x27;)            yield file        finally:            file.close()  # usagemessage_writer = MessageWriter(&#x27;hello.txt&#x27;)with message_writer.open_file() as my_file:    my_file.write(&#x27;hello world&#x27;)

with for file and lockAs it reduces user work for progamming, hence some builtin object like file and lock, condition from threading support context manager, hence for file and lock always use with statement.
lock&#x2F;condition from threading
impor threadinglock = threading.Lock()# without withlock.acquire()try:    logging.debug(&#x27;Lock acquired directly&#x27;)finally:    lock.release()# with withwith lock:    logging.debug(&#x27;Lock acquired directly&#x27;)

open file
# without using with statementfile = open(&#x27;file_path&#x27;, &#x27;w&#x27;)try:    file.write(&#x27;hello world&#x27;)finally:    file.close()# with using with statementwith open(&#x27;file_path&#x27;, &#x27;w&#x27;) as file:    file.write(&#x27;hello world&#x27;)

schedule jobAPScheduler has four kinds of components:

triggers
job stores
executors
schedulers

Triggers contain the scheduling logic. Each job has its own trigger which determines when the job should be run next. Beyond their initial configuration, triggers are completely stateless.
Job stores house the scheduled jobs. The default job store simply keeps the jobs in memory, but others store them in various kinds of databases. A job’s data is serialized when it is saved to a persistent job store, and deserialized when it’s loaded back from it. Job stores (other than the default one) don’t keep the job data in memory, but act as middlemen for saving, loading, updating and searching jobs in the backend. Job stores must never be shared between schedulers.
Executors are what handle the running of the jobs. They do this typically by submitting the designated callable in a job to a thread or process pool. When the job is done, the executor notifies the scheduler which then emits an appropriate event.
Schedulers are what bind the rest together. You typically have only one scheduler running in your application. The application developer doesn’t normally deal with the job stores, executors or triggers directly. Instead, the scheduler provides the proper interface to handle all those. Configuring the job stores and executors is done through the scheduler, as is adding, modifying and removing jobs.

choose triggerWhen you schedule a job, you need to choose a trigger for it. The trigger determines the logic by which the dates&#x2F;times are calculated when the job will be run. APScheduler comes with three built-in trigger types:

date: use when you want to run the job just once at a certain point of time
interval: use when you want to run the job at fixed intervals of time
cron: use when you want to run the job periodically at certain time(s) of day


choose schedulerYour choice of scheduler depends mostly on your programming environment and what you’ll be using APScheduler for. Here’s a quick guide for choosing a scheduler:

BlockingScheduler: use when the scheduler is the only thing running in your process
BackgroundScheduler: use when you’re not using any of the frameworks below, and want the scheduler to run in the background inside your application
AsyncIOScheduler: use if your application uses the asyncio module
GeventScheduler: use if your application uses gevent

Usagestart schedulerStarting the scheduler is done by simply calling start() on the scheduler. For schedulers other than BlockingScheduler, this call will return immediately and you can continue the initialization process of your application, possibly adding jobs to the scheduler.
For BlockingScheduler, you will only want to call start() after you’re done with any initialization steps.
Add a jobThere are two ways to add jobs to a scheduler

by calling add_job()
by decorating a function with scheduled_job()

# args = [name]# kwargs= &#123;&#x27;n&#x27;: name&#125;def add_job(self, func, trigger=None, args=None, kwargs=None, id=None, name=None,            misfire_grace_time=undefined, coalesce=undefined, max_instances=undefined,            next_run_time=undefined, jobstore=&#x27;default&#x27;, executor=&#x27;default&#x27;,            replace_existing=False, **trigger_args): 

The first way is the most common way to do it. The second way is mostly a convenience to declare jobs that don’t change during the application’s run time. The add_job() method returns a apscheduler.job.Job instance that you can use to modify or remove the job later.
name = &#x27;jack&#x27;# add a job with args, na=name, position argsdef job(na):    print(na)    scheduler.add_job(job, trigger=&#x27;interval&#x27;, args=[name], seconds=3)# here must named n!!!def job(n):    print(n)   # named parameters    scheduler.add_job(job, trigger=&#x27;interval&#x27;, kwargs=&#123;&#x27;n&#x27;: name&#125;, seconds=3)

Remove a jobWhen you remove a job from the scheduler, it is removed from its associated job store and will not be executed anymore. There are two ways to make this happen:

by calling remove_job() with the job’s ID and job store alias
by calling remove() on the Job instance you got from add_job()

job = scheduler.add_job(myfunc, &#x27;interval&#x27;, minutes=2)job.remove()scheduler.add_job(myfunc, &#x27;interval&#x27;, minutes=2, id=&#x27;my_job_id&#x27;)scheduler.remove_job(&#x27;my_job_id&#x27;)

list all jobsGetting a list of scheduled jobs  
To get a machine processable list of the scheduled jobs, you can use the get_jobs() method. It will return a list of Job instances.
Cron job
Parameter for cron type
# Schedules job_function to be run on the third Friday# of June, July, August, November and December at 00:00, 01:00, 02:00 and 03:00sched.add_job(job_function, &#x27;cron&#x27;, month=&#x27;6-8,11-12&#x27;, day=&#x27;3rd fri&#x27;, hour=&#x27;0-3&#x27;)

year (int|str) – 4-digit year
month (int|str) – month (1-12)
day (int|str) – day of month (1-31)
week (int|str) – ISO week (1-53)
day_of_week (int|str) – number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun)
hour (int|str) – hour (0-23)
minute (int|str) – minute (0-59)
second (int|str) – second (0-59)
start_date (datetime|str) – earliest possible date&#x2F;time to trigger on (inclusive)
end_date (datetime|str) – latest possible date&#x2F;time to trigger on (inclusive)
timezone (datetime.tzinfo|str) – time zone to use for the date&#x2F;time calculations (defaults to scheduler timezone)
jitter (int|None) – delay the job execution by jitter seconds at most

document and schedule framework
use mongodb as default job storeBy default, apscheduler uses memory as job store, that means jobs are lost when it restarts, for persistent job store, now it supports sql, mongodb, redis etc.
from apscheduler.jobstores.mongodb import MongoDBJobStore                                                                                                         jobstores = &#123; &#x27;default&#x27;: MongoDBJobStore(database=&#x27;apscheduler&#x27;, collection=&#x27;jobs&#x27;, host=&#x27;localhost&#x27;, port=27017)&#125;                                                                               scheduler = BackgroundScheduler(timezone=&#x27;Asia/Shanghai&#x27;, jobstores=jobstores)  


#! /usr/bin/env python3from apscheduler.schedulers.blocking import BlockingSchedulerimport timescheduler = BlockingScheduler(timezone=&#x27;Asia/Shanghai&#x27;)def job():    print(&quot;%s: start job&quot; % time.asctime())# run every 1 hour and 1 minutes and 3 secondsscheduler.add_job(job, &#x27;interval&#x27;, hours=1, minutes=1,seconds=3)# run once at a given time# scheduler.add_job(job, trigger=&#x27;date&#x27;, run_date=&#x27;2022-01-04 09:04:01&#x27;)# cron job# scheduler.add_job(job, trigger=&#x27;cron&#x27;, minute=&#x27;*/5&#x27;)# start to run as a daemon#scheduler.start()




&lt;Job (id=91ed6da1d3ba41e6b6169d329243687b name=job)&gt;

typesPython has support for optional “type hints” from python3.6.
These “type hints” are a special syntax that allow declaring the type of a variable, function, parameters etc
By declaring types for your variables, editors and tools can give you better support.

types introduction
types cheat sheet
types example

from typing import List, Set, Dict, Tuple, Optional# For simple built-in types, just use the name of the typex: int = 1x: float = 1.0x: bool = Truex: str = &quot;test&quot;x: bytes = b&quot;test&quot;# For collections, the type of the collection item is in brackets# (Python 3.9+)x: list[int] = [1]x: set[int] = &#123;6, 7&#125;# In Python 3.8 and earlier, the name of the collection type is# capitalized, and the type is imported from the &#x27;typing&#x27; modulex: List[int] = [1]x: Set[int] = &#123;6, 7&#125;    # This is how you annotate a function definitiondef stringify(num: int) -&gt; str:    return str(num)# Use Union when something could be one of a few typesx: list[Union[int, str]] = [3, 5, &quot;test&quot;, &quot;fun&quot;]

CLI Framework

For good application, it’s better provide good CLI for user, so that user can know how to use it with help, python provides a good built-in parser argparse, but there are other frameworks as well which is better than argparse, python-fire is the best on to go which is developed by Google, while click is also another good choice which is suitable to understand and auto generate help!!
python-firePython Fire is a library for automatically generating command-line interfaces from absolutely any Python object. It can help debug Python code more easily from the command line, create CLI interfaces to existing code, allow you to interactively explore code in a REPL, and simplify transitioning between Python and Bash (or any other shell).
You can call Fire on any Python object:functions, classes, modules, objects, dictionaries, lists, tuples, etc. They all work!
click (use me)Recommended Way
Click is a Python package for creating beautiful command line interfaces in a composable way with as little code as necessary. It’s the “Command Line Interface Creation Kit”. It’s highly configurable but comes with sensible defaults out of the box.
It aims to make the process of writing command line tools quick and fun while also preventing any frustration caused by the inability to implement an intended CLI API.
Click in three points:

Arbitrary nesting of commands
Automatic help page generation
Supports lazy loading of subcommands at runtime
click options

Only one command
import click@click.command()@click.option(&quot;--count&quot;, default=1, help=&quot;Number of greetings.&quot;)@click.option(&quot;-name&quot;,              &quot;--name&quot;,              prompt=&quot;Your name&quot;,              help=&quot;The person to greet.&quot;)def cli(count, name):    # handler of command    &quot;&quot;&quot;Simple program that greets NAME for a total of COUNT times.&quot;&quot;&quot;    for _ in range(count):        click.echo(f&quot;Hello, &#123;name&#125;!&quot;)if __name__ == &#x27;__main__&#x27;:    cli()

$ ./cli.py --helpUsage: cli.py [OPTIONS]  Simple program that greets NAME for a total of COUNT times.Options:  --count INTEGER     Number of greetings.  -name, --name TEXT  The person to greet.  --help              Show this message and exit.

Multiple commands
import click@click.group()def cli():    pass# NOTE here is cli which is group function name!!! not click.command!!!# so it&#x27;s under cli group!!@cli.command()def initdb():    click.echo(&#x27;Initialized the database&#x27;)@cli.command()def dropdb():    click.echo(&#x27;Dropped the database&#x27;)if __name__ == &#x27;__main__&#x27;:    cli()

$./t.pyUsage: t.py [OPTIONS] COMMAND [ARGS]...Options:  --help  Show this message and exit.Commands:  dropdb  initdb

argparseThree steps to use this library

initialize the parser by argparse.ArgumentParser
add argument(keyword) by named argument: parser.add_argument(‘–port’) or positional argument: parser.add_argument(‘port’)
get the parsed args by parser.parse_args() then do more

Here is a basic example for it.
app.py
# parametrs for add_argument# name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.# action - The basic type of action to be taken when this argument is encountered at the command line.# nargs - The number of command-line arguments that should be consumed.# const - A constant value required by some action and nargs selections.# default - The value produced if the argument is absent from the command line.# type - The type to which the command-line argument should be converted.# default type is str!!! if no type specified, str is the type of that!!!# choices - A container of the allowable values for the argument.# required - Whether or not the command-line option may be omitted (optionals only).# help - A brief description of what the argument does.# metavar - A name for the argument in usage messages.# dest - The name of the attribute to be added to the object returned by parse_args(). [if not privided, use name as keyword to store]import sysimport argparsecli_commands = &#123;    &#x27;get&#x27;: &#123;        &#x27;usage&#x27;: [&quot;get &lt;type&gt;&quot;]    &#125;,    &#x27;put&#x27;: &#123;        &#x27;usage&#x27;: [&quot;put &lt;type&gt; value&quot;]    &#125;&#125;def contruct_help_commands():    help_commands = []    for command in cli_commands.keys():        help_commands.extend(            [&quot;- &quot; + x for x in cli_commands[command][&#x27;usage&#x27;]])    # here create a command list with &#x27;- &#x27; insert&#x27;    # like [&#x27;- get &lt;type&gt;&#x27;, &#x27;- put &lt;type&gt; value&#x27;]    return help_commandsdef create_cli():    commands = contruct_help_commands()    # initialize parser    parser = argparse.ArgumentParser(        description=&#x27;app cli&#x27;,        formatter_class=argparse.RawTextHelpFormatter)    parser.add_argument(        &#x27;--cmd&#x27;, &#x27;-c&#x27;, nargs=&#x27;+&#x27;,        help=&#x27;Runs a CLI command where CMD is one of:\n%s&#x27; %        &quot;\n&quot;.join(commands))    # &#x27;--cmd&#x27; is the keyword for user to use, as nargs is &#x27;+&#x27; means at least 1 parameter for it    # so parsed args is a list        # example    # ./app.py --cmd get LB     # ./app.py -c get LB    # after parsed    # args.cmd = [&#x27;get&#x27; &#x27;LB&#x27;]    # true or false parameter    parser.add_argument(        &#x27;--json&#x27;, action=&#x27;store_true&#x27;,        help=&#x27;Outputs in json format (default: %(default)s)&#x27;)        # example, if provided True, not provided false                                     # ./app.py --json                                                                   # args.json = True                                                                                                                                                  # int parameter                                                                     parser.add_argument(                                                                    &#x27;--count&#x27;, type=int, default=10,                                                    help=&#x27;Outputs count (default: %(default)s)&#x27;)                                    # example, if provided 20, not provided 10                                     # ./app.py --count 20                                                               # args.count = 20                  parser.add_argument(&#x27;host&#x27;, nargs=1, help=&#x27;host address&#x27;)     #positional, no need to proivde keyword, always the last one        args = parser.parse_args()    print(args)    # ./apppy --cmd get LB --json True www.test.com    # Namespace(client_id=&#x27;edge&#x27;, cmd=[&#x27;get&#x27;, &#x27;LB&#x27;], json_output=&#x27;True&#x27;, host=&#x27;wwww.test.com&#x27;)

Some tips zip() creates pairs from two sequences.
def step_two_sequences_pars():    countries = (&#x27;Japan&#x27;, &#x27;Korea&#x27;, &#x27;China&#x27;)    cities = (&#x27;Tokyo&#x27;, &#x27;Seoul&#x27;, &#x27;Beijing&#x27;)    for country, city in zip(countries, cities):        print(f&#x27;The capital of &#123;country&#125; is &#123;city&#125;&#x27;)step_two_sequences_pars()

The capital of Japan is Tokyo
The capital of Korea is Seoul
The capital of China is Beijing

better code snips# create a new list based on condition from a listimport sysarr = [1, 2, -1]positives = [val+10 for val in arr if val &gt;= 0]print(positives)# open a file always this way#with open(&#x27;filename.txt&#x27;, &#x27;w&#x27;) as fd:  #    fd.write(&#x27;Hello&#x27;)# compared with Nonevalue = Noneif value is None:      # some task    pass# Use join() to concatenate strings which is fasternst = &#x27;a &#x27; + &#x27;b&#x27;print(&#x27; &#x27;.join([&#x27;a&#x27;, &#x27;b&#x27;]))# Store unique values with Setsmy_list = [1, 2, 3, 4, 5, 6, 7, 7, 7]my_set = set(my_list) # removes duplicates#Save Memory With Generators when list is huge# list comprehensionmy_list = [i for i in range(10000)]print(sum(my_list)) # 49995000print(sys.getsizeof(my_list), &#x27;bytes&#x27;) # 87616 bytes# generator comprehensionmy_gen = (i for i in range(10000))print(sum(my_gen)) # 49995000print(sys.getsizeof(my_gen), &#x27;bytes&#x27;) # 112 bytes

two prints in the same lineimport timeprint(&quot;hello&quot;,end=&#x27;\r&#x27;)time.sleep(2)print(&quot;boy&quot;)

boylo

Ref
python intermediate
design pattern with python
context manager

]]></content>
      <categories>
        <category>python</category>
        <category>advanced</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>python-advanced</tag>
      </tags>
  </entry>
  <entry>
    <title>python-class</title>
    <url>/2019/11/15/python-class/</url>
    <content><![CDATA[ClassPython class is more like C++ class, it supports inheritance, function overridden (not like C++, if you create an instance of child class, you never see parent&#39;s method if child defines method with same name even different signature(argument list)), operator overload, static method etc.

Overloading vs OverridingOverloading occurs when two or more methods in one class have the same method name but different parameters.
Overriding occurs when two methods have the same method name and parameters. One of the methods is in the parent class, and the other is in the child class. Overriding allows a child class to provide the specific implementation of a method that is already present in its parent class



basic rules

&quot;_&quot; is used to indicate that it should be used in a private context, but still can be accessed outside of class
prefix __show two underscores for private user defined, NO accessable outside of class, exception happens when accesses it.
prefix __len__ for built-in attr, but accessable outside of class
no prefix for public attribute
def __init__(self) is not need if nothing in it
child must call super().__init__() only if parent needs input
self can be other name, but it has to be the first parameter of any function in the class
del instance


parameter can be class type or other built-in type as well


def fun_parm_types(fun_n, class_n):    fun_n()    t1 = class_n(&#x27;show test&#x27;, 12)  # create an instance of class_n    t1.show()  def factory(n):    return n(&quot;hello&quot;)print(factory(str)) # pass str type!!!


Always use composition not inheritance for python class

class CA:    def __init__(self, company, group):        self._company = company # plan to be private, but still acceable outside        self.__group = group    # private, not acceable outside            def _should_show(self):        print(self._company)            def __show(self):        print(self.__group)    # __xx__ is for built-in, internal function    def __say__(self):        print(&#x27;hello&#x27;)        c1 = CA(&quot;google&quot;, &#x27;CJ&#x27;)print(c1._company)c1.__say__()c1._should_show()try:    c1.__show()except AttributeError as e:    print(str(e))    try:    print(c1.__group)except AttributeError as e:    print(str(e))

google
hello
google
&#39;CA&#39; object has no attribute &#39;__show&#39;
&#39;CA&#39; object has no attribute &#39;__group&#39;

dynamic attributeThere is one special feature for python that is different from C++, you can add attribute dynamically that means you can add attribute anytime you want to a class or an instance, but it’s not a good habit as others do not know that attribute at all.
by default you can add any attribute to an instance, but you can restrict it by __slots__, but restriction only applies on instance not class.
class CA:    # attribute allowed to added for instance    __slots__ = (&#x27;new_attr_c&#x27;,&#x27;new_attr_b&#x27;)    def __init__(self):        passc1 = CA() CA.new_attr = 12 # you can see we can add attr for class even not in __slots__# this must before print, but can after creating an instance, after this, all instances now have new_attrprint(c1.new_attr)c1.new_attr_c = 13print(c1.new_attr_c)try:    c1.new_attr_d = 12except AttributeError:    print(&quot;add new attr not in __slots__, exception happened&quot;)    

12
13
add new attr not in __slots__, exception happened

function overloading&#x2F;overridingclass person:    &#x27;&#x27;&#x27;    this part will be person.__doc__    &#x27;&#x27;&#x27;    def __init__(self, name):        # constructor!!!        self.__name = name    def eat(self):        print(&quot;I eat from person&quot;)        self.after()        # if self is child class, self.after() will call child&#x27;s after(), if not found, call person.after()            def after(self):        print(&quot;after eat from person&quot;)        class student(person): # inheritance this way    def __init__(self, name, sid):        # child must call parent init        super().__init__(name)        self.__sid = sid        def after(self):# overriding        print(&quot;after eat from student&quot;)class teacher(person):    def __init__(self, name, tid):        super().__init__(name)  # call parent&#x27;s constructor        self.__tid = tid    def eat(self, msg): # overloading        print(&quot;i eat %s from teacher&quot; % msg)        s1 = student(&quot;s1&quot;, 1)t1 = teacher(&quot;t1&quot;, 1)s1.eat()# t1.eat() called with no parameter# error as parent function is hiden even with different signaturest1.eat(&quot;apple&quot;)

I eat from person
after eat from student
i eat apple from teacher

operator overloadingIn some case, you may want to access user defined class like other built-in type, like len(user_class_instance), user_class_instance[0] what you need to do is to implement special functions, here are a list for such special functions.
built-in function(descriptor)
- len           __len__- str           __str__- []            __getitem__- ins.attr      __getattr__- CLASS()       __call__- hash          __hash__- with/as       __enter__/__exit__
operator



p1 + p2
__add__



p1 - p2
__sub__


*,&#x2F;,&#x2F;&#x2F;
__xx__


p1 % p2
__mod__


&lt;&lt;,&gt;&gt;
__xx__


p1 &lt; p2
__lt__


&gt;, &lt;&#x3D;, &gt;, &gt;&#x3D;
__le__, __gt__, __ge__


class Phone:          def __str__(self):        return &#x27;Apple&#x27;        def __len__(self):        return len(&#x27;Apple&#x27;)


import sysph1 = Phone()str(ph1)




&#39;Apple&#39;

print(len(ph1))

5

setattr&#x2F;getattr&#x2F;hasattr&#x2F;delattr with classMost of time, class attribute(include function) is defined when defines a class, so we know the name of it, hence we can access it by class_instance.name or class_instance.get_name(), but there are times when you might not know the name of a attribute until runtime as some of them defined by setattr at runtime!!!, that’s where hasattr and getattr (as well as delattr and setattr) come into play.
# must quote attribute# getattr(class_name, &#x27;attribute&#x27;)# getattr(instance, &#x27;attribute&#x27;)class Apple:    pass# without default valueval = getattr(Apple, &#x27;name&#x27;)var = &#x27;name&#x27;# with default valueval = getattr(Apple, var, &#x27;default value&#x27;)


class Foo:    def __init__(self, name):        self.name = name    def get_name(self):        return self.namef1 = Foo(&#x27;jason&#x27;)print(&quot;all attributes(not include function) for instance are stored at instance._dict__=&quot;, f1.__dict__)if not hasattr(f1, &#x27;id&#x27;):    print(&quot;class instance doesn&#x27;t have id, use default value &quot;, getattr(f1, &#x27;id&#x27;, &#x27;123&#x27;))print(&quot;f1.get_name return &quot;, getattr(f1, &#x27;get_name&#x27;)())

all attributes(not include function) for instance are stored at instance._dict__= &#123;&#39;name&#39;: &#39;jason&#39;&#125;
class instance doesn&#39;t have id, use default value  123
f1.get_name return  jason

staticmethod and classmethodstaticmethod
staticmethod is used to group functions which have some logical connection within a class. it knows nothing about the class or instance it was called on, It just gets the arguments that were passed, no self argument, just group functions in a class. but it can be called by class or instance as well.
Class.staticmethod() # class directlyOr evenClass().staticmethod() # instance calls it

It is basically useless in Python you can just use a module function instead of a staticmethod
classmethod
A classmethod is a method that is bound to a class rather than its object. It doesn&#39;t require creation of a class instance, much like staticmethod.
The differences between a static method and a class method are:

Static method knows nothing about the class and just deals with the parameters
Class method works with the class since its first parameter is always the class type.

The class method can be called both by the class and its object, both pass class type as first parameter.
Class.classmethod() # class directlyOr evenClass().classmethod() # instance
But no matter what, the class method is always attached to a class with first argument as the class itself cls.
# cls is class type!!!def classMethod(cls, args...)

This is useful when you want a method to be a factory for the class
class Student:    COUNT = 0 # COUNT here means it blongs to CLASS, can access it by class name or instance    # can use COUNT as without class prefix as parameter    # but inside function, must add prefix Student.count inside function    def __init__(self, ct=COUNT):        self.count_inc_class()  # even COUNT is increase here,         @classmethod    def count_inc_class(cls): # pass CLASS, cls is CLASS type, not self!        cls.COUNT += 1 # access class COUNT by CLASS name    @staticmethod    def get_count_static():        return Student.COUNT # access class COUNT by instance    @classmethod    def get_count_class(cls): # pass CLASS as firt argument, not self        print(cls)        s3 = cls()        return cls.COUNTs1 = Student()s2 = Student()print(s1.get_count_static(), Student.get_count_static())print(s1.get_count_class(), Student.get_count_class())#s1.get_count_class passed CLASS implicitly not self to classmethodprint(s1)print(Student)

2 2
&lt;class &#39;__main__.Student&#39;&gt;
&lt;class &#39;__main__.Student&#39;&gt;
3 4
&lt;__main__.Student object at 0x7f5e5d055df0&gt;
&lt;class &#39;__main__.Student&#39;&gt;

setter&#x2F;gettersetter&#x2F;getter provide a way that you can access&#x2F;modify attribute with obj.name but the implementation behind it is a function, this is very useful in some case, let’s say your old application writing code with obj.name, but after several years, you want to &#x2F;need to return value depends on more logic, but without changing user code, getter comes into play.
class Student:    # two name functions with different decorators    @property    def name(self):        print(&#x27;getting name&#x27;)        return self._name    @name.setter #@name must be same with attribute    def name(self, value):        print(&#x27;setting name %s&#x27; % value)        self._name = value                st1 = Student()# behind it, a function is called!!!st1.name = &quot;jason&quot;print(st1.name)

setting name jason
getting name
jason

enum classenum is used to group literal together, so that you can use them from class, more specific for its purpose
from enum import Enum# Month is the ID of the enummth = Enum(&#x27;Month&#x27;, (&quot;Jan&quot;, &quot;Feb&quot;))print(mth.Jan) # the resut is a number but can use name like c enum

Month.Jan

]]></content>
      <categories>
        <category>python</category>
        <category>class</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>python-class</tag>
      </tags>
  </entry>
  <entry>
    <title>python-function</title>
    <url>/2019/11/15/python-function/</url>
    <content><![CDATA[Advanced function featureIn python, function is an object that means you can assign it to a variable or  pass it as a parameter! we can also return a function inside another function, cool!
Assign functions to a variable
def greet(name):    return &quot;hello &quot; + namegreet_someone = greetgreet_someone(&quot;John&quot;)
Function can return other function
def compose_greet_func(name):    def get_message():        return &quot;Hello &quot; + name    return get_messagegreet = compose_greet_func(&quot;jason&quot;)greet()
if nothing returns in function, None returned by default


descriptor(class method)In general, a descriptor is an object attribute with &quot;binding behavior&quot; whose access has been overridden by method in the descriptor protocol. Those methods are __get__(), __set__(), and __delete__(). If any of those methods is defined for an object, it is said to be a descriptor
Calling property() is a succinct way of building a data descriptor that triggers function call upon access to an attribute.
define a propertyproperty(fget=None, fset=None, fdel=None, doc=None) -&gt; property attribute we can also use decorator @property and @xx.setter to define a property, must define @property first!!!
class Person:    #descriptor    def __init__(self, name):        self._name = name            def get_name(self):        print(&quot;get_name() is called&quot;)        return self._name    def set_name(self, name):        print(&#x27;set_name() is called with name:&#x27;, name)        self._name = name            # declare name as a property, obj.name    # but inside class, it&#x27;s function.    # @property is getter which must define first!!!       @property    def name(self):        print(&#x27;name getter is called&#x27;)        return self._name        @name.setter    # name must be same between getter and setter.    def name(self, name):        print(&#x27;name setter is called&#x27;)        self._name = name    # bname: a property of person(fget() fset(), property can be different names)    # but if you use decorator to define a propterty, in that case fget, fset, property must be same name!!!    bname = property(get_name, set_name) # declare a property     p1 = Person(&quot;jason&quot;)print(p1.bname)p1.bname = &quot;kk&quot;print(p1.name)

get_name() is called
jason
set_name() is called with name: kk
name getter is called
kk

function closureif a function defines another function in its scope and returns it, it’s a closure
def welcome(name):    def wrapper():        print(&quot;wrapper is called with: &quot;, name)    return wrapper # here we return a functionwf = welcome(&quot;jason&quot;)wf() 

wrapper is called with:  jason

decoratorPython has built-in decorator like property, to apply a decorator to a function by placing it just before the function definition. like 
@xxxdef test():    pass# with parameter(s)@xxx(arg)def test()    pass

Decorator is needed when you want do something before a function run, place it anywhere that needs it.
Decorator can be a class or function, if it is class, must be callable, has __call__ function defined.
below are the ways to create a custom decorator
function as a decorator.decorator is applied when you define that function, later on when you call it, the wrapped function is called actually.
def log(text):    print(&#x27;we got parameter for decorator: &#x27;, text)    def myDecorator(f): # passed in user function.        def wrapper(*args, **argv): # parameters for user functioin            # *args for dynamic position parameters            # args is a list [&#x27;a&#x27;, &#x27;b&#x27;]            f(*args, **argv) # here *args, separate the list == f(&#x27;a&#x27;, &#x27;b&#x27;)        return wrapper    # return a function object, when it&#x27;s called, call wrapped f() actually!    return myDecorator@log(&quot;param1&quot;) # parameter is used by decorator, not wrapper function!!!def aFunction(name):    print (&quot;inside aFunction &quot;, name)aFunction(&quot;jason&quot;)

we got parameter for decorator:  param1
inside aFunction  jason

class as a decoratordecorator is applied when you define that function, later on when you call it, the wrapped function is called actually.
**Always use call as decorator, as it’s easy to understand.
class decorator without parameterclass cDecorator:    &#x27;doc of class decorator&#x27;    def __init__(self, f): # fixed format        # must passed function        print(&#x27;decorator is initialized&#x27;)        self.f = f    def __call__(self, *argc, **argv): # argc is a list, argv is a dict, fixed format        # do extra here        print(&#x27;decorator is called&#x27;)        self.f(*argc, **argv) # get args from wrap function, then pass it orginal.        # here is NOT @cDecorator()# this happend when function is declared!!!# wrapped = cDecorator(bFunction)---&gt;__init__()@cDecoratordef bFunction(msg, name=&quot;josh&quot;):    print (&quot;inside bFunction: &#123;0&#125; &#123;1&#125;&quot;.format(msg, name))# repalced with wrapped function# wrapped(&quot;hi&quot;, name=&quot;josh&quot;)---&gt;__call__()bFunction(&#x27;hi&#x27;)print(bFunction.__doc__)

decorator is initialized
decorator is called
inside bFunction: hi josh
doc of class decorator

class decorator with parameterclass cDecorator:    &quot;doc of class decorator&quot;    def __init__(self, msg):  # fixed format        # must passed function        print(&quot;decorator is initialized&quot;, msg)        self.msg = msg    def __call__(self, fn):  # argc is a list, argv is a dict, fixed format        print(&quot;wrapper parameter:&quot;, self.msg)        def wrapper(*args, **argv):            print(&quot;call original function&quot;)            fn(*args, **argv)        return wrapper# wrapped = cDecorator(&quot;cool&quot;)(bFunction) --&gt;__init__(), then __call__()@cDecorator(&quot;cool&quot;)def bFunction(msg, name=&quot;josh&quot;):    print(&quot;inside bFunction: &#123;0&#125; &#123;1&#125;&quot;.format(msg, name))# wrapped(&quot;hi&quot;)bFunction(&quot;hi&quot;)print(bFunction.__doc__) # see below function wraps 

decorator is initialized cool
wrapper parameter: cool
call original function
inside bFunction: hi josh
None

why use functools.wrapswhen a function is decorated, getting the docstring and function signature return “wrapper”, the inner function, not the original function, this is a problem, we can use “functool.wraps” decorator to decorate our inner function, By applying this “wraps” decorator to our inner function, we copy over func name, docstring, and signature to our inner function, avoiding the issues.
from functools import wrapsdef log(text):    def decorator(func):                @wraps(func) # assign func.__name__ to inner function        def wrapper(*args, **kw):            print(&#x27;%s call %s()&#x27; % (text, func.__name__))            return func(*args, **kw)                return wrapper        return decorator@log(&#x27;execute&#x27;)def f(msg, name=&#x27;josh&#x27;):    print(&#x27;&#123;0&#125; &#123;1&#125;&#x27;.format(msg, name))f(&#x27;hi&#x27;)print(f.__name__) # __name__ is not wrapper, but f great!

execute call f()
hi josh
f

Generator vs Iteratorgenerator function returns a generator object, generator function is function that yield(like return) value when next() call,it will execute until last yield point, most of time next() is not called explictly but implicitly like this for elm in list.
Iterator
An iterator is an object that implements the iterator protocol (don’t panic!). An iterator protocol is nothing but a specific class with __next__() in Python which further has the __next__() method.
While for iterable object, No need to __next__() but mostly have __iter__() and __getitem__(), __iter__() allows to convert the object into an iterator,  __getitem__() will be called in iterator’s __next__() function.
iter(object) will call object.__iter__() while next(object) will call object.__next__()
Generator

Iterator is an object has __next__() method
generator is an object which implements __next__(),  so that it’s an iterator!  
generator function is a function that uses yield as return, when you call this function, it returns a generator object!

more details about these two, refer to iterator-generator-descriptor.

 Most uses case, you create a generator function which returns a generator object and for elm in generator_object
def f():    yield &#x27;start&#x27;    yield &#x27;middle&#x27;    yield &#x27;end&#x27;gen = f()# return generator object, then call next()  to # yield object(which has __next__() method provided by yield)print(gen)print(dir(gen))print(&#x27;use next(generator_object)&#x27;)print(next(gen))print(next(gen))print(next(gen))# next(gen) exception happend!!!print(&quot;use for/in to iterate generator&quot;)for stage in f():    print(stage)

&lt;generator object f at 0x7f3e30235f90&gt;
[&#39;__class__&#39;, &#39;__del__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__name__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__next__&#39;, &#39;__qualname__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;close&#39;, &#39;gi_code&#39;, &#39;gi_frame&#39;, &#39;gi_running&#39;, &#39;gi_yieldfrom&#39;, &#39;send&#39;, &#39;throw&#39;]
use next(generator_object)
start
middle
end
use for/in to iterate generator
start
middle
end

# List is iterable, but it&#x27;s not an Iterator!# but can be converted to an Iteratorclass MyList:    def __init__(self, data):        self.__list = data[:]            def __iter__(self):        return MyIterator(self) # return Iterator            def __getitem__(self, i):        return self.__list[i]        def __len__(self):        return len(self.__list)    class MyIterator:    def __init__(self, obj):        self.__obj = obj        self.__index = 0 # key: the current index        self.__size = len(obj)        # must __next__ as it&#x27;s Iterator    def __next__(self):        if self.__index &gt;= self.__size:            # stop next by return StopIteration!!!            raise StopIteration        else:            value = self.__obj[self.__index] # call obj.__getitem__()            self.__index += 1            return value            lt = [1, 2]mlt = MyList(lt)it = iter(mlt)print(it)print(next(it))print(next(it))

&lt;__main__.MyIterator object at 0x7f3e30278d00&gt;
1
2

]]></content>
      <categories>
        <category>python</category>
        <category>function</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>python-basics</title>
    <url>/2019/11/15/python-basics/</url>
    <content><![CDATA[OverviewHere are some basics about python3 that you should know. if your code is written with python2, convert it to python3 with 2to3 tool. 

difference between python2 and python3




$ 2to3 python2_file$ 2to3 python2_dir

You can also run your app with virtual env, so that different python applications can use different python(python2 or python3) libraries


$sudo pip3 install virtualenv#(python version &gt;= 3.3)$python3 -m venv ~/my_app_env#(will copy python3 binary(default) and related libs(less) from system, my_app_env is a directory)# if you want to inherit all system sites packages$python3 -m venv ~/my_app_env --system-site-packages$source ~/my_app_env/bin/activate#(set python path as ~/my_app_env, run with virtual env)(my_new_app)$ pip3 install flask#(will install flask at ~/my_app_env (dir)!!! not system lib directory, so that each env can have separate flasks)(my_new_app)$deactivate


.py, pyc, pyo

.py:  this is normal input source code that you’ve written.
.pyc: this is the compiled bytecode. If you import a module(python file), python will build a *.pyc file that contains the bytecode for importing to make it easier (and faster). pycache&#x2F;lib.cpython-39.pyc
.pyo: this is a another form of *.pyc file that was created while optimizations (-O) was on.

When the Python interpreter is invoked with the -O flag, optimized code is generated and stored in ‘.pyo’ files.
.pyo is gone, never use it
A program doesn’t run faster when it is read from a ‘.pyc’ or ‘.pyo’ file than when it is read from a ‘.py’ file,the only thing that’s faster about &#39;.pyc&#39; or &#39;.pyo&#39; files is the speed with which they are loaded.
When a script is run by giving its name on the command line, the bytecode for the script is never written to a &#39;.pyc&#39; or &#39;.pyo&#39; file automatically. Thus, the startup time of a script may be reduced by moving most of its code to a module and having a small bootstrap script that imports that module.
#generate pyc manually(pyc is generated automatically for module when it&#x27;s imported by others)$ python3 -m compileall test.py$ ls __pycache__/test.cpython-3.9.pyc$ python3 __pycache__/test.cpython-3.9.pyc


generic methods for types
Each variable has a type, each type has some common methods and specific methods, here are generic rules you should know
methods for all types(str, list, tuple, range, dict, set)

dir(str) to get methods of type(str) for help, or detail for a specific one help(str.index).  

 item in s, item not in s, len(s), max&#x2F;min(except dict, str), for item in s(s must be iterable)

NOTE: len(str) DO NOT count &#39;\0&#39;



methods for sequence type(str, list, tuple, range)

 s[i], s[i:j](not include j), s[i:j:k], s + s1, s.count(x)
a, b, c = s (size of s must be 3)
call func(*s), pass each elem to func, like func(s[0], s[1], s[2])
the first index of sequence is 0

you can’t declare var with explicit type, the type of var depends on its value, can be changed at runtime!!!
naming conventionpython uses underscores to write multiword names like get_name, no capital letter. 
Files  

python follows a convention where source files are all lower case with underscore separating multiple words, client_log.py
Compound file names are separated with _

Functions and Methods  

private functions like this _do_work()
built-in function like this __init__()
If a name consists of multiple words, should write like this get_name().
function names are case-sensitive (car, Car and CAR are three different variables).

Variables

should not include the name of your type in the name of your variable’s name, tet_list
Generally, use relatively simple (short) name(lower case), (_ underscore for multiple worlds) long var.
user to u
user_id to uid
server_listener
lpcfg


If variable type is bool, its name should start with has, is, can or allow, etc.
Single letter represents index: i, j, k

Constants  

Constant should be capitalized all letters. WORLD

class  

type of class should be capitalized and camel case Class Person(object):
private function(_private_f()) and built-in(__eq__()) function can NOT be accessed!

import package  

package(is dir with init.py under it) name should be lowercase. like import xxx.testhello, xxx is package name

printingPrinting is an easy way to show result, there are several ways to print variables.

print one line

print(&quot;hello&quot;      &quot;jason&quot;)print(&quot;hello \       jason&quot;)


print multiple lines

print(&quot;hello \n jason&quot;)print(&#x27;&#x27;&#x27;hello      jason&#x27;&#x27;&#x27;)


show one parameter at last position

name = &quot;jason&quot;print(&quot;hello&quot;, name) # a space is added between after &#x27;hello&#x27; automaticallyprint(&quot;hello %s&quot; % name) # no auto added space (python2 format)


show multiple parameters at different positions

# position parametersmsg = &quot;welcome&quot;print(&quot;hello %s msg %s&quot; % (name, msg)) # python2 format# named parameters like &#123;0&#125; or &#123;n&#125;# format index start from 0print(&quot;hello &#123;0&#125;, msg &#123;1&#125;&quot;.format(name, msg))# no need to write index if used with orderprint(&quot;hello &#123;&#125;, msg &#123;&#125;&quot;.format(name, msg))print(&quot;hello &#123;1&#125;, msg &#123;0&#125;&quot;.format(name, msg))print(&quot;hello &#123;n&#125;, msg &#123;m&#125;&quot;.format(n=name, m=msg))


variable can be at any position

# print add space between different parts!!!print(name, &quot;hi&quot;) #has auto added space before &#x27;hi&#x27;print(name, &quot;hi&quot;, name) # has auto added space before and after &#x27;hi&#x27;print(name, &quot;hi %s&quot; % name) # has auto added space before &#x27;hi&#x27;, python2 format


print with string.format()

print(&#x27;&#123;&#123; &#123;0&#125;&#x27;.format(&#x27;hi&#x27;))# &#123;&#123; for escape

print with template which is less verbose than format() but need high python version

# use var directly in template with &#123;&#125;name=&#x27;jason&#x27;print(f&quot;hi&#123;name&#125;&quot;)# print in multiple linesprint(f&#x27;&#x27;&#x27;hi &#123;name&#125;boy&#x27;&#x27;&#x27;)# more advanced, use var and function directly!!!print(f&#x27;hi&#123;2+3&#125;&#x27;)print(f&#x27;hi&#123;fun1(name)&#125;&#x27;)

Suggestion for python3 printing

print(‘{}’.format(var))
print(f”hello {name}”)

format() details
name=&#x27;jason&#x27;def msg(name):    return &#x27;hi &#x27; + nameprint(f&#x27;hi &#123;name&#125;&#x27;)print(f&#x27;hi &#123;2+3&#125;&#x27;)print(f&#x27;&#123;msg(name)&#125;&#x27;)# print in one linesmsg=(    &quot;hello &quot;    &quot;world&quot;)print(msg)print(&quot;hello &quot;      &quot;world&quot;)# + is optionalprint(&quot;hello &quot; +      &quot;world&quot;)# write at different lines with \ or () but print in one lineprint(&quot;hello \world in one line&quot;)# print in multiple linesprint(&quot;hello \nworld&quot;)print(&#x27;&#x27;&#x27;helloworld&#x27;&#x27;&#x27;)

hi jason
hi 5
hi jason
hello world
hello world
hello world
hello world in one line
hello 
world
hello
world

built-in APISPython(without library) provides many methods to make it easy to use. here is a list of that.

Complexc1 = 4 + 3jc2 = complex(4, 3)c1*c2
Built-in functionpow(2, 3)divmod(8, 4) # result: (2, 0)abs(-12)sum([1,2]) sum([False, True])max([1, 2])min([1,2])round(3.23, 1) # return 3.2round(3.26, 1) # return 3.3
built-in modulesimport mathmath.pimath.sin()math.cos()math.pow()math.log2()math.ceil(2.1) == 3   # rounds up to close integermath.floor(2.1) == 2  # rounds down to close integer
asserta = 1assert a, &#x27;bad value&#x27;  # assert true, never raise exception.assert 0, &#x27;bad value&#x27;  # assert fail, exception with msg &#x27;bad value&#x27;
argvprint(sys.argv) # argv is list that includes the app name and parameter# ./app.py -f xx.text# sys.argv = [&#x27;./app.py&#x27;, &#x27;-f&#x27;, &#x27;xx.text&#x27;]

constants
Python doesn’t have built-in constant types.
By convention, Python uses a variable whose name consist of capital letters to define a constant.

NOTE: someone can go and change the value of constant, even it’s unlikely but possible.

MY_CONSTANT = &quot;Whatever&quot;# trick wayclass CONST(object):    FOO = 1234    def __setattr__(self, *_):        passCONST = CONST()print CONST.FOO    # 1234CONST.FOO = 4321   # as we do thing for setter as above.CONST.BAR = 5678print CONST.FOO    # Still 1234!print CONST.BAR    # Oops AttributeError# or this wayclass Const:  @property  def FOO(self):    return &quot;foo&quot;CONST = Const()  # You need an instanceif something == CONST.FOO:


variableA variable is a label or a name given to a certain location in memory. This location holds the value you want your program to remember for use later on. What’s great in Python is that you do not have to explicitly state what the type of variable you want to define is - it can be of any type (string, integer, float, list, dict etc.). To create a new variable in Python, you simply use the assignment operator (&#x3D;, a single equals sign) and assign the desired value to it, its type is determined by its value, but if you change the value, the type can be changed as well.

first_string_var = &quot;First String&quot;  first_int_var = 1total = 1 + 2 * 3total = &quot;hello&quot;

Local ScopeA variable created inside a function belongs to the local scope of that function, and can only be used inside that function.
def print_number():    first_num = 1    # Print statement 1    print(&quot;The first number defined is:&quot;, first_num)print_number()

The first number defined is: 1

Enclosing ScopeInner function can access var defined in outer function directly, while the reverse direction is not allowed.
def outer():    first_num = 1        def inner():        second_num = 2        # Print statement 1 - Scope: Inner        print(&quot;first_num from outer:&quot;, first_num)        # Print statement 2 - Scope: Inner        print(&quot;second_num from inner:&quot;, second_num)    inner()    # Print statement 3 - Scope: Outer        # this will cause error if uncommented    # print(&quot;second_num from inner: &quot;, second_num)outer()

first_num from outer: 1
second_num from inner: 2

nolocal keywordThe nonlocal keyword is useful in nested functions. It causes the variable to refer to the previously bound variable in the closest enclosing scope. In other words, it will prevent the variable from trying to bind locally first,
def outer():    first_num = 1        def inner():        # try without nonlocal        # without nonlocal, you can still access first_num, but if you modify it, a new local variable is defined        nonlocal first_num        first_num = 0        second_num = 1        print(&quot;inner - second_num is:&quot;, second_num)            inner()    print(&quot;outer - first_num is:&quot;, first_num)outer()

inner - second_num is: 1
outer - first_num is: 0

Global ScopeA variable created in the main body of the Python code is a global variable and belongs to the global scope.
Global variables are available from any scope, global and local.
If you need to create a global variable, but are stuck in the local scope, you can use the global keyword.
The global keyword makes the variable global which is defined in a function or refer to a predefined global variable

def gb(new_greeting):    # define a global variable from a function or refer to an existing one if defined somewhere!!!    global gt    gt = new_greeting    gb(&quot;cool&quot;)print(gt)


greeting = &quot;Hello&quot;def greeting_world():    world = &quot;World&quot;    print(greeting, world)def greeting_name(name):    print(greeting, name)greeting_world()greeting_name(&quot;Samuel&quot;)

Hello World
Hello Samuel

Global keywordTo change the value of a global variable inside a function, refer to the variable by using the global keyword, it’s similar like nolocal keyword.
greeting = &quot;Hello&quot;def change_greeting(new_greeting):    # try without it    global greeting    greeting = new_greetingdef greeting_world():    world = &quot;World&quot;    print(greeting, world)change_greeting(&quot;Hi&quot;)greeting_world()

Hi World

typesoperatorsArithmetic operators



Operator
Meaning
Example



+
Add two operands or unary plus
x + y+ 2


-
Subtract right operand from the left or unary minus
x - y- 2


*
Multiply two operands
x * y


&#x2F;
Divide left operand by the right one (always results into float)
x &#x2F; y


%
Modulus - remainder of the division of left operand by the right
x % y (remainder of x&#x2F;y)


&#x2F;&#x2F;
Floor division - division that results into whole number adjusted to the left in the number line
x &#x2F;&#x2F; y


**
Exponent - left operand raised to the power of right
x**y (x to the power y)



Comparison operators



Operator
Meaning
Example



&gt;
Greater than - True if left operand is greater than the right
x &gt; y


&lt;
Less than - True if left operand is less than the right
x &lt; y


&#x3D;&#x3D;
Equal to - True if both operands are equal
x &#x3D;&#x3D; y


!&#x3D;
Not equal to - True if operands are not equal
x !&#x3D; y


&gt;&#x3D;
Greater than or equal to - True if left operand is greater than or equal to the right
x &gt;&#x3D; y


&lt;&#x3D;
Less than or equal to - True if left operand is less than or equal to the right
x &lt;&#x3D; y



Logical operators



Operator
Meaning
Example



and
True if both the operands are true
x and y


or
True if either of the operands is true
x or y


not
True if operand is false (complements the operand)
not x



Bitwise operators



Operator
Meaning
Example



&amp;
Bitwise AND
x &amp; y &#x3D; 0 (0000 0000)




Bitwise OR


~
Bitwise NOT
~x &#x3D; -11 (1111 0101)


^
Bitwise XOR
x ^ y &#x3D; 14 (0000 1110)


&gt;&gt;
Bitwise right shift
x &gt;&gt; 2 &#x3D; 2 (0000 0010)


&lt;&lt;
Bitwise left shift
x &lt;&lt; 2 &#x3D; 40 (0010 1000)



Assignment operators



Operator
Example
Equivalent to



&#x3D;
x &#x3D; 5
x &#x3D; 5


+&#x3D;
x +&#x3D; 5
x &#x3D; x + 5


-&#x3D;
x -&#x3D; 5
x &#x3D; x - 5


*&#x3D;
x *&#x3D; 5
x &#x3D; x * 5


&#x2F;&#x3D;
x &#x2F;&#x3D; 5
x &#x3D; x &#x2F; 5


%&#x3D;
x %&#x3D; 5
x &#x3D; x % 5


&#x2F;&#x2F;&#x3D;
x &#x2F;&#x2F;&#x3D; 5
x &#x3D; x &#x2F;&#x2F; 5


**&#x3D;
x **&#x3D; 5
x &#x3D; x ** 5


&amp;&#x3D;
x &amp;&#x3D; 5
x &#x3D; x &amp; 5



&#x3D;
x


^&#x3D;
x ^&#x3D; 5
x &#x3D; x ^ 5


&gt;&gt;&#x3D;
x &gt;&gt;&#x3D; 5
x &#x3D; x &gt;&gt; 5


&lt;&lt;&#x3D;
x &lt;&lt;&#x3D; 5
x &#x3D; x &lt;&lt; 5



Membership operators



Operator
Meaning
Example



in
True if value&#x2F;variable is found in the sequence
5 in x


not in
True if value&#x2F;variable is not found in the sequence
5 not in x



NOTE: +, &#x3D;&#x3D;, !&#x3D; works for tuple, list, string (sequence type)as well.
IntegerInteger supports most operators like C language but with less difference, here are the operators it supports.

+, -, *, &#x2F;, &#x2F;&#x2F;
**, %
x or y (C ||), x and y (C &amp;&amp;), not x (C !)
&lt;, &lt;&#x3D;, &gt;, &gt;&#x3D;, &#x3D;&#x3D;, !&#x3D;
x | y, x &amp; y, x ^ y, x &gt;&gt; y, x &lt;&lt; y

def integer_demo():    print(&quot;convert str to int int(&#x27;12&#x27;) =&quot;, int(&#x27;12&#x27;))    print(&quot;convert int to string str(12) =&quot;, str(12))    print(&quot;// only remains interger 5//1.5=&quot;, 5//1.5)    # format float    print(&quot;/  result is float 1/2=%0.2f 1/2=%0.4f&quot;%(1/2, 1/2))    print(&quot;%0.2f&quot;%1.234)    print(format(1.234, &quot;.2f&quot;))    print(&quot;2**3 =&quot;, 2**3) # like pow(2,3)    print(&quot;8%4 =&quot;, 8 % 4) # result: 0 while divmod(8,4)=(2,0)


integer_demo()

convert str to int int(&#39;12&#39;) = 12
convert int to string str(12) = 12
// only remains interger 5//1.5= 3.0
/  result is float 1/2=0.50 1/2=0.5000
1.23
1.23
2**3 = 8
8%4 = 0

stringstring is a kind of sequence, like list, tuple, but its content can&#39;t be modified in place, in order to change its content, you must create a new one. 
no char type in python, that means ‘’ and “” has the same meaning

Create a string

s &#x3D; “hello”
s &#x3D; ‘hello’
s &#x3D; “hi” + “world”
s = r&#39;hello\nworld&#39; # raw literal string now \n is a normal char!!


Ops

index from 0, last index -1
len(str), no \0 is there but we count in C.
count(), index(), isdigit(), islower(), lower(), isupper(), upper(), isspace(), join(), replace(), find()(first matched one), split(), splitlines(), strip(), startwith(), endwith(), rfind()(last matched one)
join() each string element of iterable object, “-“.join([1,2]) will cause error as element is not string!!!
str.strip(&#39;hy&#39;) will strip h or y both from right and left not ‘hy’ as a whole
index() and find() return same if found(index), otherwise, exception ValueError for index, -1 returned for find()
slicing [2:5], [2:], [:5], [2:-1] all with default step 1 from left to right
slicing with steps, [2:5:2] ([start:end:step])
[::-1](reverse the string)
str.xxx() all such methods do NOT support regular pattern

def str_demo():    print(&quot;raw string r\&quot;a\\nb\&quot; = &quot;, r&quot;a\nb&quot;)    # very useful when use re as regular pattern may contain special char to python    # use r before a string to suppress(r means raw literal)    s1 = &quot;hello boy&quot;        print(&quot;str=&#x27;hello boy&#x27;&quot;)    print(&quot;reverse s1 by s1[::-1]=&quot;, s1[::-1])    print(&quot;count sub words, str.count(&#x27;o&#x27;) =&quot;, s1.count(&#x27;o&#x27;))    print(&quot;first element index 0, str[0] =&quot;, s1[0])    print(&quot;last element index: str[-1] =&quot;, s1[-1])    print(&quot;not include \\0 len(str) =&quot;, len(s1))    print(&quot;a:b means [a, b)  str[:-1] =&quot;, s1[:-1])    try:        s1[2] = &#x27;a&#x27;    except:        print(&quot;can&#x27;t change the content of a string by str[2] = &#x27;a&#x27;&quot;)    print(&quot;create a copy of str by str[:]!&quot;)    # newstr = s1[:]#a copy a str.    print(&quot;str is: &quot; + s1)    print(&#x27;s1.startswith(&quot;hello&quot;) =&#x27;, s1.startswith(&quot;hello&quot;))    print(&#x27;s1.endswith(&quot;bo&quot;) =&#x27;, s1.endswith(&quot;bo&quot;))    if s1.isdigit():        print(&quot;str is digital&quot;)    elif s1.islower():        print(&quot;str is lower&quot;)    elif s1.isupper():        print(&#x27;str is upper&#x27;)    elif s1.isspace():        print(&quot;str is space&quot;)    else:        print(&quot;str is&quot;, s1)    idx = s1.find(&quot;hello&quot;)    if idx != -1:        print(&quot;find hello at %d in : %s&quot; % (idx, s1))    print(&quot;replace boy with girl, new str is: &quot; + s1.replace(&quot;boy&quot;, &quot;girl&quot;))    print(&quot;splitlines() == split(&#x27;\\n&#x27;), split() will return a list, but s1 is unchanged s1 = %s s1.split(&#x27; &#x27;) = %s&quot; % (s1, s1.split(&#x27; &#x27;)))    print(&#x27;&#x27;&#x27;strip() removes characters(whitespace by default)             from both left and right based on the argument set of character             while rstrip() only from right&#x27;&#x27;&#x27;)    print(&quot;before strip str =&quot;, s1)    print(&quot;after strip(&#x27;hy&#x27;) str =&quot;, s1.strip(&#x27;hy&#x27;))    print(&quot;join() join each string elem in an iterable object with character provided, &#x27;-&#x27;&#x27;.join(&#x27;hello&#x27;) =&quot;, &#x27;-&#x27;.join(&quot;hello&quot;))    print(&quot;&#x27;-&#x27;.join([&#x27;hello&#x27;, &#x27;boy&#x27;] =&quot;, &#x27;-&#x27;.join([&quot;hello&quot;, &quot;boy&quot;]))


str_demo()

raw string r&quot;a\nb&quot; =  a\nb
str=&#39;hello boy&#39;
reverse s1 by s1[::-1]= yob olleh
count sub words, str.count(&#39;o&#39;) = 2
first element index 0, str[0] = h
last element index: str[-1] = y
not include \0 len(str) = 9
a:b means [a, b)  str[:-1] = hello bo
can&#39;t change the content of a string by str[2] = &#39;a&#39;
create a copy of str by str[:]!
str is: hello boy
s1.startswith(&quot;hello&quot;) = True
s1.endswith(&quot;bo&quot;) = False
str is lower
find hello at 0 in : hello boy
replace boy with girl, new str is: hello girl
splitlines() == split(&#39;\n&#39;), split() will return a list, but s1 is unchanged s1 = hello boy s1.split(&#39; &#39;) = [&#39;hello&#39;, &#39;boy&#39;]
strip() removes characters(whitespace by default)
             from both left and right based on the argument set of character
             while rstrip() only from right
before strip str = hello boy
after strip(&#39;hy&#39;) str = ello bo
join() join each string elem in an iterable object with character provided, &#39;-&#39;&#39;.join(&#39;hello&#39;) = h-e-l-l-o
&#39;-&#39;.join([&#39;hello&#39;, &#39;boy&#39;] = hello-boy

encoding and decodingPython3’s str type is meant to represent human-readable text and can contain any Unicode character.
The bytes type, conversely, represents binary data or sequences of raw bytes, that do not intrinsically have an encoding attached to it.



Python 3 is all-in on Unicode and UTF-8 specifically. Here is what that means:

Python 3 source code is assumed to be UTF-8 by default. This means that you don’t need # -- coding: UTF-8 -- at the top of .py files in Python 3.

All text (str) is Unicode by default. Encoded Unicode text is represented as binary data in memory (bytes). The str type can contain any literal Unicode character, such as “Δv &#x2F; Δt”, all of which will be stored as Unicode.

Python’s re module defaults to the re.UNICODE flag rather than re.ASCII. This means, for instance, that r”\w” matches Unicode word characters, not just ASCII letters.

The default encoding&#x2F;decoding in str.encode() and bytes.decode() is UTF-8.


# The length of a single Unicode character as a Python str will always be 1# no matter how many bytes it occupies.# The length of the same character encoded to bytes# will be anywhere between 1 and 4.def encode():    ibrow = &quot;🤨&quot;    print(&#x27;len of character:&#x27;, len(ibrow))    bs = ibrow.encode(&quot;utf-8&quot;)    print(&#x27;encoded with utf-8:&#x27;, bs)    print(&#x27;len of bytes:&#x27;, len(bs))    encode()

len of character: 1
encoded with utf-8: b&#39;\xf0\x9f\xa4\xa8&#39;
len of bytes: 4

listList is a sequence, element can be any type even for one list. but for sort() method it requires same type of all elements.

To create a list

lv &#x3D; list()
lv &#x3D; [], lv &#x3D;&#x3D; [] True, len(lv)&#x3D;&#x3D;0
lv &#x3D; [1, 2]
lv &#x3D; list(‘abc’) same as lv &#x3D; [‘a’, ‘b’, ‘c’]
lv &#x3D; list(dt.items()), dt&#x3D;{‘a’:1} same as  lv&#x3D;[(‘a’, 1)]
lv &#x3D; list(range(3)) same as  lv &#x3D; [0, 1, 2]
lv &#x3D; [1, ‘a’, False, [3, 4]]
lv &#x3D; [1] * 3 same as [1, 1, 1]
lv &#x3D; old[:] # shadow copy, only the top level is copied
lv = [val+10 for val in arr if val &gt;= 0]


Ops

append(value), count(value), extend(iterable), index(value, start&#x3D;None, stop&#x3D;None), insert(index, value), pop(index&#x3D;None)(default from last), remove(value), reverse(), sort(key&#x3D;None, reverse&#x3D;False)
sort() needs all elements have the same type.
for i in range(len(lt)):lt[i]
for elem in lt: print(elem)
for i in range(len(lt) - 1, -1, -1):lt[i] loop from last


Note

reverse() and sort() are in place methods.
if you pass a list to a function, modification in that function of the list will take place in original.
pop() from last by default ----&gt; pop(0) pop from head.

import copydef list_demo():    la = [&#x27;1&#x27;, 5, [3, 4], 2]    print(&quot;list element can be any type: &quot;, la)    la.append(6)    print(&quot;append only take one element, after list.append(6): &quot;, la)    la.extend([3, 4])    print(&quot;after list.extend([3,4]) = &quot;, la)    la.insert(1, 10)    print(&quot;after insert 10 at index 1, the previous index 1 move back, list.insert(1,10): &quot;, la)        # la.pop() remove the last    # la.pop(0) remove the first one(from from index 0)        la.pop(2) # same as del la[2] here 2 is index    print(&quot;after pop() value at index 2, list=: &quot;, la)    la.remove(2) # here 2 is value but index    print(&quot;after remove() element whose value is 2,\           list.remove(2) now list is: &quot;, la)    lt = [2, 1, 4]    # please note: to sort, make sure all elements has the same type!!!    lt.sort() # take in place!!!    print(&quot;[2, 1, 4] after sorted , now lt is %s&quot;,lt)    lt.reverse()    print(&quot;list.reverse() after reverse now list is: &quot;, lt)        # loop list with index    for index, value in enumerate(lt):        print(&quot;with index&quot;, (index, value))            # another way to loop list with index    for i in range(len(lt)):        print(&quot;with index:&quot;, i, lt[i])    for e in lt:        print(&quot;without index:&quot;,e)        print(&#x27;reverse loop: &#x27;)    # loop list from end to start    for i in range(len(lt) - 1, -1, -1):        print(i, lt[i])            # shadow copy, only the top level is copied    la = [&#x27;1&#x27;, 2, [3, 4], 6]    cla = la[:]    print(&quot;shadow copy la: %s, cla: %s&quot; %(la, cla))    cla[1] += 10    print(&quot;cla[1] += 10, now la:&quot;,la)    cla[2].append(5)    print(&quot;cla[2].append(5), la changes as well la:&quot;,la)        # deep copy    la = [&#x27;1&#x27;, 2, [3, 4], 6]    dla = copy.deepcopy(la)    print(&quot;*&quot;*60)    print(&quot;deep copy la: %s, dla: %s&quot; %(la, dla))    dla[1] += 10    print(&quot;dla[1] += 10, now la:&quot;,la)    dla[2].append(5)    print(&quot;dla[2].append(5), la changes as well la:&quot;,la)


list_demo()

list element can be any type:  [&#39;1&#39;, 5, [3, 4], 2]
append only take one element, after list.append(6):  [&#39;1&#39;, 5, [3, 4], 2, 6]
after list.extend([3,4]) =  [&#39;1&#39;, 5, [3, 4], 2, 6, 3, 4]
after insert 10 at index 1, the previous index 1 move back, list.insert(1,10):  [&#39;1&#39;, 10, 5, [3, 4], 2, 6, 3, 4]
after pop() value at index 2, list=:  [&#39;1&#39;, 10, [3, 4], 2, 6, 3, 4]
after remove() element whose value is 2,           list.remove(2) now list is:  [&#39;1&#39;, 10, [3, 4], 6, 3, 4]
[2, 1, 4] after sorted , now lt is %s [1, 2, 4]
list.reverse() after reverse now list is:  [4, 2, 1]
with index (0, 4)
with index (1, 2)
with index (2, 1)
with index: 0 4
with index: 1 2
with index: 2 1
without index: 4
without index: 2
without index: 1
reverse loop: 
2 1
1 2
0 4
shadow copy la: [&#39;1&#39;, 2, [3, 4], 6], cla: [&#39;1&#39;, 2, [3, 4], 6]
cla[1] += 10, now la: [&#39;1&#39;, 2, [3, 4], 6]
cla[2].append(5), la changes as well la: [&#39;1&#39;, 2, [3, 4, 5], 6]
************************************************************
deep copy la: [&#39;1&#39;, 2, [3, 4], 6], dla: [&#39;1&#39;, 2, [3, 4], 6]
dla[1] += 10, now la: [&#39;1&#39;, 2, [3, 4], 6]
dla[2].append(5), la changes as well la: [&#39;1&#39;, 2, [3, 4], 6]

pack&#x2F;unpack listunpack list to separate elements, while pack from separate elements to a list, this is mostly used for parameter passing.
def list_unpack_pack():    # unpack, must have the same elements number    a, b, c = [1, 2, 3]    a, *e = [1, 2, 3]            def test_unpack(a, b, c):        print(a, b, c)    # *list in call to unpack list    test_unpack(*[1, 2, 3]) # same element numbers        # *list in definition to pack separate ones, must be used as the last argument.    def test_pack(*lt):        print(lt)    test_pack(1, 2, 3)    list_unpack_pack()

1 2 3
(1, 2, 3)

dictdict is not sequence type, but we can convert dict to a list.

to create a dict

dt &#x3D; {}
dt &#x3D; dict()
dt &#x3D; {“a”:1, “b”:2}
lt &#x3D; [(‘a’, 1), (‘b’, 2)] dt &#x3D; dict(lt)

key without &#39;&#39; or &quot;&quot;, treated as variable if not literal must define it firstly
# both are valid, but key 2 and &#x27;2&#x27; are different keys!!!!dt = &#123;2: 1&#125;dt = &#123;&#x27;2&#x27;: 1&#125;# must define var a firstly!!!, otherwise error!!!dt = &#123;a: 1&#125;

Ops

dt[newkey] = 12(add a new element)

dt[key] (exception if no key), dt.key NOT support it s only valid for class

dt.get(key, default=None) # should use this always to avoid exception

dt.pop(key, default&#x3D;None) # get its value and pop it.

del dt[key] # only delete it.

dt.keys()

dt.values()

dt.items() # each element is a tuple

dt.update(another_dict) # extend a dict or update existing elem

dt.copy() # shallow copy!!!

dt.clear() # reset dict


def dict_demo():    dt = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;    # different with list as list is []    print(&quot;dict now is:&quot;, dt)        # add an element    dt[&#x27;c&#x27;] = 3    print(&quot;add dt[&#x27;c&#x27;] = 3 now dt is:&quot;, dt)    dt[&#x27;c&#x27;] += 1    dt[&#x27;d&#x27;] = [1, 2]    dt[&#x27;e&#x27;] = &#123;&quot;f&quot;: 6&#125;    print(&quot;&quot;&quot;after added and modifed           dt[&#x27;c&#x27;] += 1           dt[&#x27;d&#x27;] = [1, 2]           dt[&#x27;e&#x27;] = &#123;&quot;f&quot;:6&#125;           now dict:&quot;&quot;&quot;, dt)    # access element, you can NOT access elem by dt.key    print(&quot;access one element dt[&#x27;e&#x27;][&#x27;f&#x27;] =&quot;, dt[&#x27;e&#x27;][&#x27;f&#x27;])    print(&quot;get a its value with dt.get():&quot;, dt.get(&#x27;a&#x27;, &quot;doesn&#x27;t exist&quot;))    print(&quot;get f its vlaue with dt.get():&quot;, dt.get(&quot;f&quot;,  &quot;doesn&#x27;t exist&quot;))    print(&quot;pop f from dict with dt.pop():&quot;, dt.pop(&quot;f&quot;, &quot;doesn&#x27;t exist&quot;))    print(&quot;now dic is: &quot;, dt)        # delete an element    dt.pop(&#x27;a&#x27;) # delete key-value pair, can also be done: del dt[&#x27;a&#x27;]    print(&quot;pop a from dict, dt.pop(&#x27;a&#x27;) now dict:&quot;, dt)    print(&quot;all keys in dict is dt.keys() =&quot;, dt.keys())  # a list    print(&quot;all values in dict is dt.values() =&quot;, dt.values())  # a list    print(&quot;all items in dict is dt.items() =&quot;, dt.items()) # list, each element is a tuple        # join two dicts if has duplicated key, use the later one(update one)    dt.update(&#123;&quot;c&quot;: 100, &quot;g&quot;: 100&#125;)    print(&#x27;update dict with &#123;&quot;c&quot;: 100, &quot;g&quot;: 100&#125;, now dt =&#x27;, dt)    # iterate all elements with dict    for it in dt.keys():        print(it, &quot;==&quot;, dt[it])    dt.clear() # remove all elements    print(&quot;dt is %s after clear()&quot; % dt)


dict_demo()

dict now is: &#123;&#39;a&#39;: 1, &#39;b&#39;: 2&#125;
add dt[&#39;c&#39;] = 3 now dt is: &#123;&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3&#125;
after added and modifed
           dt[&#39;c&#39;] += 1
           dt[&#39;d&#39;] = [1, 2]
           dt[&#39;e&#39;] = &#123;&quot;f&quot;:6&#125;
           now dict: &#123;&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 4, &#39;d&#39;: [1, 2], &#39;e&#39;: &#123;&#39;f&#39;: 6&#125;&#125;
access one element dt[&#39;e&#39;][&#39;f&#39;] = 6
get a its value with dt.get(): 1
get f its vlaue with dt.get(): doesn&#39;t exist
pop f from dict with dt.pop(): doesn&#39;t exist
now dic is:  &#123;&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 4, &#39;d&#39;: [1, 2], &#39;e&#39;: &#123;&#39;f&#39;: 6&#125;&#125;
pop a from dict, dt.pop(&#39;a&#39;) now dict: &#123;&#39;b&#39;: 2, &#39;c&#39;: 4, &#39;d&#39;: [1, 2], &#39;e&#39;: &#123;&#39;f&#39;: 6&#125;&#125;
all keys in dict is dt.keys() = dict_keys([&#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;])
all values in dict is dt.values() = dict_values([2, 4, [1, 2], &#123;&#39;f&#39;: 6&#125;])
all items in dict is dt.items() = dict_items([(&#39;b&#39;, 2), (&#39;c&#39;, 4), (&#39;d&#39;, [1, 2]), (&#39;e&#39;, &#123;&#39;f&#39;: 6&#125;)])
update dict with &#123;&quot;c&quot;: 100, &quot;g&quot;: 100&#125;, now dt = &#123;&#39;b&#39;: 2, &#39;c&#39;: 100, &#39;d&#39;: [1, 2], &#39;e&#39;: &#123;&#39;f&#39;: 6&#125;, &#39;g&#39;: 100&#125;
b == 2
c == 100
d == [1, 2]
e == &#123;&#39;f&#39;: 6&#125;
g == 100
dt is &#123;&#125; after clear()

pack&#x2F;unpack dictunpack dict to separate elements, while pack from separate elements to a dict, this is only used for parameter passing, when unpack, the parameters must use the same ‘key’ as dict and same count of element.
dt = &#123;&quot;a&quot;:1, &quot;b&quot;:2&#125;def test(a, b):    print(a,b)    # unpack for call a fucntiontest(**dt)# pack argument must be the last argument with **argv for dict, *argv for listdef test1(**argv):    print(argv)    test1(a=1, b=2, c=3)

1 2
&#123;&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3&#125;

tupletuple is similar as list(element can be any type) except it can NOT be modified meaning its top level element can not be changed, but the embedded object can be modified, tuple has no special methods for itself. it’s less used actually.

Create a tuple

tp &#x3D; 1, 2
tp &#x3D; (1, 2)
tp &#x3D; tuple([1, 2])


Ops

no special method for itself, has sequence type generic methods

def tuple_demo():    tp = (1, 2, &#x27;d&#x27;, &#123;&#x27;a&#x27;: 1&#125;, [3, 4])    print(&quot;tuple is:&quot;, tp)    print(&quot;tp[:-1]:&quot;, tp[:-1])    print(&quot;tp + (6,):&quot;, tp + (6,))        # the top level can NOT change but the embeded type can be modified if possible    tp[3][&#x27;a&#x27;] = 2 # good    print(&quot;change the value it points tuple is:&quot;, tp)        # tp can NOT modified just means it can&#x27;t change top level.    # but the value it points can be modified


tuple_demo()

tuple is: (1, 2, &#39;d&#39;, &#123;&#39;a&#39;: 1&#125;, [3, 4])
tp[:-1]: (1, 2, &#39;d&#39;, &#123;&#39;a&#39;: 1&#125;)
tp + (6,): (1, 2, &#39;d&#39;, &#123;&#39;a&#39;: 1&#125;, [3, 4], 6)
change the value it points tuple is: (1, 2, &#39;d&#39;, &#123;&#39;a&#39;: 2&#125;, [3, 4])

setSet is like dict without value, only keys and it’s not sequence type but iterable and has its special methods
NO duplicate element in set.

Create a set

st &#x3D; set()  empty set, only one to create set not like list, tuple, dict
st &#x3D; set(“abc”) &#x3D;&#x3D; st&#x3D;{‘a’, ‘b’, ‘c’}
st &#x3D; {‘a’, ‘b’, ‘c’}
st &#x3D; set([‘a’, ‘b’, ‘c’]) # from list


Ops

s1.add()
s1.remove()
s1.update()   #extend a set with another one
s1 | s2  并集
s1 &amp; s2  交集
s1 - s2  差集
s1 ^ st2 对称差
s1.issubset(s2) s1 is subnet of s2
s2.issuperset(s1) s2 is subnet of s1

def set_demo():    # &#123;&#125; not []    st = &#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;    print(&quot;st =&quot;, st)    st.add(3)    print(&quot;after st.add(3) =&quot;, st)    st.remove(&#x27;a&#x27;)    print(&quot;after st.remove(&#x27;a&#x27;) =&quot;, st)    # set is iterable, but not sequence.    for elm in st:        print(elm)


set_demo()

st = &#123;&#39;c&#39;, &#39;b&#39;, &#39;a&#39;&#125;
after st.add(3) = &#123;3, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;&#125;
after st.remove(&#39;a&#39;) = &#123;3, &#39;c&#39;, &#39;b&#39;&#125;
3
c
b

rangeWhen you call range() API, it returns an object of range type, but most of time, we convert range to list or tuple implicitly.
def range_demo():    # range(stop)------&gt;[start, stop)    rg = range(5) #[0, 4]    print(&quot;range is:&quot;, rg)    lt = list(rg) # convert range to list explicitly    print(&quot;convert range to list:&quot;, lt)    tp = tuple(rg)    print(tp)    for i in range(5): #convert range to tuple implicitly!        print(i)            # range(start, stop, step)------&gt;[start, stop)    # a list from large to small    print(list(range(5, -1, -1)))


range_demo()

range is: range(0, 5)
convert range to list: [0, 1, 2, 3, 4]
(0, 1, 2, 3, 4)
0
1
2
3
4
[5, 4, 3, 2, 1, 0]

conversion between different types# string &lt;--&gt; integera = int(&#x27;12&#x27;)b = str(12)print(a, b)# string &lt;---&gt; listlt = list(&#x27;abc2&#x27;)s = &#x27;&#x27;.join(lt)print(lt, s)# string &lt;--&gt;setst = set(&#x27;abc&#x27;)s = &#x27;&#x27;.join(st)print(st, s)# list &lt;---&gt;dictlt = list(&#123;&quot;a&quot;:1, &quot;b&quot;:2&#125;.items())print(lt)dt = dict(lt)print(dt)

functionvariable scope is like C, variable is local by default, if want to change value of global variable, must use keyword global x in function, you can access global variable in function without global keyword, but only for accessing, not modifying.
variable defined in a function is also visible to the function embedded in it.
argument can have default value when define it
def show(x, y=0):    print(y)

when you call function with named parameter, the order can be any
show(1, 2)show(y=2, x=1) ### order can be different with function definition!!!show(1, y=23)

For dynamic args(the number of argument is not defined)

use *arg for list like parameters in function definition
**arg for dict like parameters in function definition

def test(*args):    for i in args:        print(&#x27;arg: &#x27;, i)test([1, 2])  # passed args [[1, 2]]test(1, 2)    # passed args [1, 2]

Note: &gt;&#x3D;python3.5, python supports define function with type
def function_demo_python3_5(name: str=&#x27;jason&#x27;) -&gt; bool:    return True


# no global keyword when defines it.g_y = 100def function_demo():    x = 12     def show_y():        print(&#x27;g_y: &#x27;, g_y)            def show_v():        x = 10  # modified here, define a local x, not impact x outside        global g_y  # no need this line if only access y, not modify it!!!!        g_y = 200        print(&quot;x in show_v is: &quot;, x)        def show_sub_v():            # x defined in show_v is visible in show_sub_v            print(&quot;x in show_sub_v is: &quot;, x)        show_sub_v()    show_v()    show_y()    print(&quot;out of show_v x is: &quot;, x)    print(&quot;out of show_v y is:&quot;, g_y)    # dynamic arguments    def show_arg(x, *argc, **argv):        print(&quot;x=%d, argc=%s argv=%s&quot; % (x, argc, argv))    show_arg(1, 2, 3, name=&#x27;jason&#x27;, id=12)


function_demo()

x in show_v is:  10
x in show_sub_v is:  10
g_y:  200
out of show_v x is:  12
out of show_v y is: 200
x=1, argc=(2, 3) argv=&#123;&#39;name&#39;: &#39;jason&#39;, &#39;id&#39;: 12&#125;

flow controlFlow control controls the logic of statement, like C, python supports many flow control directives like while/else, for/else, if/elif/else, pass, break/continue, try/except/finally, raise
NO goto, switch/case in python.
if&#x2F;elif&#x2F;elsedef if_demo():    con = 1    short_out = &#x27;con is 1&#x27; if con == 1 else &#x27;con is not 1&#x27; # shortcut for if/else at same line no &#x27;:&#x27; needed    print(short_out)        a = 12 if False else 23    print(&quot;like: a = 12 if False else 23&quot;)    print(&quot;a =&quot;, a)        if 1:        print(&quot;say hi&quot;)    else:        pass  # nothing        if 1:        print(&quot;say hi&quot;)  # simple        if_demo()

con is 1
like: a = 12 if False else 23
a = 23
say hi
say hi

while&#x2F;elsedef while_demo():    i = 10    while i:        i -= 1    else:        print(&quot;loop is out but not by break statement inside while&quot;)while_demo()

loop is out but not by break statement inside while

for&#x2F;elsedef for_demo():    lt = [1, 2]    if lt[0] == 1:        print(&quot;equal&quot;)    for i in [1, 2, 3, 4, 5, 6]:        if i == 4:            print(&quot;break from loop&quot;)            break    else:        print(&quot;loop for is out but not by break statement inside for&quot;)            lt1 = [1, 2, 3, 4]    lines = &#x27;-&#x27;.join([str(m) for m in lt1])    print(lines)    for_demo()

equal
break from loop
1-2-3-4

try&#x2F;exceptiontry:    fd = open(&#x27;/sdaf/b.c&#x27;, &#x27;wb&#x27;)except Exception as error:    print(error)

system envSometimes, you may want to get environment from system or set an environ for a process, use from os import environ
note: environ[‘PATH’] is different with os.path which is only used for search .py
def env_demo():    from os import environ    environ[&#x27;HOST&#x27;] = &quot;10.64.32.1&quot;    # set env    print(&#x27;get HOST env: &#x27;, environ.get(&#x27;HOST&#x27;, &#x27;HOST default value&#x27;))    for key, v in environ.items():        print(&quot;env-&gt; %s : %s&quot; % (key, v))            # environ[&#x27;PATH&#x27;] si different with os.path which is only used for seerch .py!!!


env_demo()

get HOST env:  10.64.32.1
env-&gt; http_proxy : http://10.226.136.231:3128
env-&gt; ftp_proxy : http://10.226.136.231:3128
env-&gt; PATH : /home/data/Anaconda3/envs/py3.9/bin:/opt/llvm/bin:/home/data/Anaconda3/envs/py3.9/bin:/home/data/Anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/home/go:/home/go/bin:/root/.yarn_pkg/bin:/usr/lib64:/usr/local/go/bin:/home/data/Anaconda3/envs/py3.9/libexec/git-core:/root/bin:/root/.yarn_pkg/bin:/home/go/bin:/home/go:/usr/local/go/bin
env-&gt; PWD : /
env-&gt; LANG : en_US.UTF-8
env-&gt; https_proxy : http://10.226.136.231:3128
env-&gt; SHLVL : 1
env-&gt; _ : /usr/bin/env
env-&gt; GOPROXY : https://goproxy.cn,direct
env-&gt; GO111MODULE : on
env-&gt; GOMODCACHE : /home/go/pkg/mod
env-&gt; GOCACHE : /root/.cache/go-build
env-&gt; GOPATH : /home/go
env-&gt; PYDEVD_USE_FRAME_EVAL : NO
env-&gt; JPY_PARENT_PID : 1811
env-&gt; TERM : xterm-color
env-&gt; CLICOLOR : 1
env-&gt; PAGER : cat
env-&gt; GIT_PAGER : cat
env-&gt; MPLBACKEND : module://matplotlib_inline.backend_inline
env-&gt; HOST : 10.64.32.1

small tipsId of objectIn python, each object has a unique identifier(memory address), to get it by id(var), a is b checks the id of them, for integer, string, if content is same, id is same as well, but this is not true for list, tuple, dict. for example:
a = 3b = 3id(a) == id(b) # Truea is b # Truea = [1]b = [1]a == b # Truea is b # False

while isinstance(var, str) to check if a var is an instance of a class
None () {} [] “”None () {} [] “” are false when used a condition.
print(&quot;&quot;&quot;None, (), &#123;&#125;, [], &quot;&quot;, &#x27;&#x27; all are false but they are different types&quot;&quot;&quot;)print(bool(None), bool(()), bool([]), bool(&#123;&#125;), bool(&quot;&quot;))msg = &#x27;None == []&#x27; if None == [] else &quot;None not equal []&quot;print(msg)

None, (), &#123;&#125;, [], &quot;&quot;, &#39;&#39; all are false but they are different types
False False False False False
None not equal []

exec eval
exec is used to execute python statements which are stored in a string or file
eval is used to evaluate valid python expression which are stored in a string.

# execute python code from stringexec(&#x27;print(&quot;hello&quot;)&#x27;)# eval python expression from stringprint(eval(&#x27;2*3&#x27;))

hello
6

try&#x2F;excepttry:    1.0 / 0except ZeroDivisionError:    print(&#x27;Error: Division by zero&#x27;)except TypeError:    print(&#x27;Error: Unsupported operation&#x27;)except Exception as error:  # catch all other exception    print(error) # general error, unknown

Error: Division by zero

ref
python basics in Chinese
python tutorial

]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>function</tag>
        <tag>class</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu-kvm-command-options</title>
    <url>/2022/04/14/qemu-kvm-command-options/</url>
    <content><![CDATA[Standard Options-boot [order&#x3D;drives][,once&#x3D;drives][,menu&#x3D;on|off][,splash&#x3D;sp_name][,splash-time&#x3D;sp_time][,reboot-timeout&#x3D;rb_timeout][,strict&#x3D;on|off]

Specify boot order drives as a string of drive letters. Valid drive letters depend on the target architecture. The x86 PC uses: a, b (floppy 1 and 2), c (first hard disk), d (first CD-ROM), n-p (Etherboot from network adapter 1-4), hard disk boot is the default. To apply a particular boot order only on the first startup, specify it via once. Note that the order or once parameter should not be used together with the bootindex property of devices, since the firmware implementations normally do not support both at the same time.



-machine [type&#x3D;]name[,prop&#x3D;value[,…]]

Select the emulated machine(chipset) by name. Use -machine help to list available machines.For architectures which aim to support live migration compatibility across releases, each release will introduce a new versioned machine type. For example, the 2.8.0 release introduced machine types “pc-i440fx-2.8” and “pc-q35-2.8” for the x86_64&#x2F;i686 architectures. -machine pc-i440fx-rhel7.6.0,accel=kvm,usb=off,dump-guest-core=off

-cpu model

Select CPU model (-cpu help for list and additional feature selection), -cpu help to check availabe cpu and flags, enable or disable or use default.-cpu IvyBridge,pcid=on,movbe=on,hypervisor=on,arat=on,tsc_adjust=on,avx2=on,rdseed=on,clflushopt=on,abm=on,3dnowprefetch=on,f16c=off,smep=off,erms=off,xsaveopt=off  without -cpu default is used that’s model name : QEMU Virtual CPU version 2.5+

-daemonize

Daemonize the QEMU process after initialization. QEMU will not detach from standard IO until it is ready to receive connections on any of its devices. This option is a useful way for external programs to launch QEMU without having to cope with initialization race conditions.

-no-shutdown

Don’t exit QEMU(qemu process is still there) on guest shutdown, but instead only stop the emulation. This allows for instance switching to monitor to commit changes to the disk image. but this is not working for libvirt as it will kill qemu process when shutdown from guest finished.

-nodefaults  

Don’t create default devices. Normally, QEMU sets the default devices like serial port, parallel port, virtual console, monitor device, VGA adapter, floppy and CD-ROM drive and others. The -nodefaults option will disable all those default devices if -nodefaults is used, you must set vga adapter explicitly(-vga std), otherwise VNC is not working!!!

-S

Do not start CPU at startup (you must type ‘c’ in the monitor), this is mostly used for migration, at dst host, start qemu with such option, then we can copy memory from source to dst

-object typename[,prop1&#x3D;value1,…]

Create a new object of type typename setting properties in the order they are specified. Note that the ‘id’ property must be set. These objects are placed in the ‘&#x2F;objects’ path

-no-user-config  

The -no-user-config option makes QEMU not load any of the user-provided config files on sysconfdir(&#x2F;etc&#x2F;qemu&#x2F;qemu.conf)

-global driver.prop&#x3D;value

In particular, you can use this to set driver properties for devices which are created automatically by the machine model, example -global kvm-pit.lost_tick_policy=delay

# get all properties of a given driver /usr/libexec/qemu-kvm -device kvm-pit,helpkvm-pit.iobase=uint32kvm-pit.lost_tick_policy=LostTickPolicy

-rtc [base&#x3D;utc|localtime|datetime][,clock&#x3D;host|rt|vm][,driftfix&#x3D;none|slew]

Specify base as utc or localtime to let the RTC start at the current UTC or local time, respectively. localtime is required for correct date in MS-DOS or Windows.


By default the RTC is driven by the host system time. This allows using of the RTC as accurate reference clock inside the guest, specifically if the host time is smoothly following an accurate external reference clock, e.g. via NTP. If you want to isolate the guest time from the host, you can set clock to rt instead, which provides a host monotonic clock if host support it. timezones abbre and rtc, pit, hpet


check guest clock source by cat  /sys/devices/system/clocksource/clocksource0/current_clock

-msg [timestamp[&#x3D;on|off]][,guest-name[&#x3D;on|off]]

Control error message format of qemu process

memoryFor each memory(main ram, device ram), Qemu will create memory backend object automatically, QEMU has alternative memory backends.

memory-backend-ram – The default backend
memory-backend-file – Memory backed by a file, typcially used for non volatile memories (to save the contents when powering off the machine)
memory-backend-memfd – Anonymous memory file backend

-m [size&#x3D;]megs[,slots&#x3D;n,maxmem&#x3D;size]

Sets guest startup RAM(main ram) size to megs megabytes. Optionally, a suffix of “M” or “G” can be used to signify a value in megabytes or gigabytes respectively. Optional pair slots, maxmem could be used to set amount of hotpluggable memory slots and maximum amount of memory. Note that maxmem must be aligned to the page size. 


For example, the following command-line sets the guest startup RAM size to 1GB, creates 3 slots to hotplug additional memory and sets the maximum memory the guest can reach to 4GB:qemu-system-x86_64 -m 1G,slots=3,maxmem=4G


If slots and maxmem are not specified, memory hotplug won’t be enabled and the guest startup RAM will never increase.


By default, Qemu will create a memory backend(memory-backend-ram) object(named pc.ram) for the main ram automatically if user does not set memory backend object(pc.ram) explicitly.

-object memory-backend-ram,id&#x3D;id,merge&#x3D;on|off,dump&#x3D;on|off,share&#x3D;on|off,prealloc&#x3D;on|off,size&#x3D;size,host-nodes&#x3D;host-nodes,policy&#x3D;default|preferred|bind|interleave

Creates a memory backend object, which can be used to back the guest RAM. Memory backend objects offer more control than the -m option that is traditionally used to define guest RAM, with host-nodes can specify where guest ram comes from(which numa node).bind: Strict policy means that the allocation will fail if the memory cannot be allocated on the target node.interleave: Memory pages are allocated across nodes specified by a nodeset, but are allocated in a round-robin fashion.preferred: Memory is allocated from a single preferred memory node. If sufficient memory is not available, memory can be allocated from other nodes.

-object memory-backend-file,id&#x3D;id,size&#x3D;size,mem-path&#x3D;dir,share&#x3D;on|off,discard-data&#x3D;on|off,merge&#x3D;on|off,dump&#x3D;on|off,prealloc&#x3D;on|off,host-nodes&#x3D;host-nodes,policy&#x3D;default|preferred|bind|interleave,align&#x3D;align,readonly&#x3D;on|off

Creates a memory file backend object(mem-path is required), which can be used to back the guest RAM with huge pages.


The id parameter is a unique ID that will be used to reference this memory region in other parameters, e.g. -numa, -device nvdimm, etc.


The size option provides the size of the memory region, and accepts common suffixes, e.g. 500M.


The mem-path provides the path to either a shared memory or huge page filesystem mount.

No Explicit memory backend# by default, memory is not prealloc, memory is grabed from host only when it&#x27;s accessed.-m 512M

Explicit memory backend ram more control# new qemu version, memory-backend as the key# guest ram bind to host numa node0, if no sufficient memory, it fails!!!-object memory-backend-ram,id=pc.ram,size=512M,dump=on,share=on,prealloc=on,host-nodes=0,policy=bind \-machine memory-backend=pc.ram \-m 512M# old qemu version, memdev as the key-object memory-backend-ram,id=pc.ram,size=512M,dump=on,share=on,prealloc=on \-numa node,nodeid=0,memdev=pc.ram \ -m 512M

Explicit memory backend file to use hugepageQemu will create this file automatically, but not delete it when qemu exits
# new qemu version-object memory-backend-file,id=pc.ram,size=512M,mem-path=/dev/hugepages/libvirt/qemu/centos,prealloc=on,share=on,host-nodes=0,policy=bind \-machine memory-backend=pc.ram \-m 512M# old qemu version-object memory-backend-file,id=pc.ram,size=512M,mem-path=/dev/hugepages/libvirt/qemu/centos,prealloc=on,share=on,host-nodes=0,policy=bind \-numa node,nodeid=0,memdev=pc.ram \ -m 512M

CPUS and Numa-smp [[cpus&#x3D;]n][,maxcpus&#x3D;maxcpus][,sockets&#x3D;sockets][,dies&#x3D;dies][,clusters&#x3D;clusters][,cores&#x3D;cores][,threads&#x3D;threads]

Simulate a SMP system with ‘n’ CPUs initially present on the machine type board. On boards supporting CPU hotplug, the optional ‘maxcpus’ parameter can be set to enable further CPUs to be added at runtime. When both parameters are omitted, the maximum number of CPUs will be calculated from the provided topology members and the initial CPU count will match the maximum number. When only one of them is given then the omitted one will be set to its counterpart’s value.


-smp 8,sockets=2,cores=2,threads=2,maxcpus=8


Either the initial CPU count, or at least one of the topology parameters must be specified. Values for any omitted parameters will be computed from those which are given.

-numa node[,mem&#x3D;size][,cpus&#x3D;firstcpu[-lastcpu]][,nodeid&#x3D;node][,initiator&#x3D;initiator]-numa node[,memdev&#x3D;id][,cpus&#x3D;firstcpu[-lastcpu]][,nodeid&#x3D;node][,initiator&#x3D;initiator]-numa dist,src&#x3D;source,dst&#x3D;destination,val&#x3D;distance-numa cpu,node-id&#x3D;node[,socket-id&#x3D;x][,core-id&#x3D;y][,thread-id&#x3D;z]-numa hmat-lb,initiator&#x3D;node,target&#x3D;node,hierarchy&#x3D;hierarchy,data-type&#x3D;tpye[,latency&#x3D;lat][,bandwidth&#x3D;bw]-numa hmat-cache,node-id&#x3D;node,size&#x3D;size,level&#x3D;level[,associativity&#x3D;str][,policy&#x3D;str][,line&#x3D;size]

Define a NUMA node and assign RAM and VCPUs to it. Set the NUMA distance from a source node to a destination node. Set the ACPI Heterogeneous Memory Attributes for the given nodes.

Legacy CPU assigmentLegacy VCPU assignment uses ‘cpus’ option where firstcpu and lastcpu are CPU indexes. Each ‘cpus’ option represent a contiguous range of CPU indexes (or a single VCPU if lastcpu is omitted). A non-contiguous set of VCPUs can be represented by providing multiple ‘cpus’ options. If ‘cpus’ is omitted on all nodes, VCPUs are automatically split between them.
New CPU assignment Way‘cpu’ option is a new alternative to ‘cpus’ option which uses ‘socket-id|core-id|thread-id’ properties to assign CPU objects to a node using topology layout properties of CPU.
memory assignment way(legacy and new)Legacy ‘mem’ assigns a given RAM amount to a node (not supported for 5.1 and newer machine types). ‘memdev’ assigns RAM from a given memory backend device to a node. If ‘mem’ and ‘memdev’ are omitted in all nodes, RAM is split equally between them.
New Way for cpu(cpu) and memory(mdev)
-M pc \-m 2G,slots=2,maxmem=4G \-object memory-backend-ram,size=1G,id=m0 \-object memory-backend-ram,size=1G,id=m1 \-numa node,nodeid=0,memdev=m0 \-numa node,nodeid=1,memdev=m1 \-smp 2,sockets=2,maxcpus=2  \-numa cpu,node-id=0,socket-id=0 \-numa cpu,node-id=1,socket-id=1

Legency CPU(cpus) and New memory way(mdev)
-M pc \-m 2G,slots=2,maxmem=4G \-object memory-backend-ram,size=1G,id=m0 \-object memory-backend-ram,size=1G,id=m1 \-smp 2,sockets=2,maxcpus=2  \-numa node,nodeid=0,memdev=m0,cpus=0 \-numa node,nodeid=1,memdev=m1,cpus=1

Legency CPU(cpus) and legency memory way(mem)
-M pc \-m 2G,slots=2,maxmem=4G \-smp 2,sockets=2,maxcpus=2  \-numa node,nodeid=0,mem=1G,cpus=0 \-numa node,nodeid=1,mem=1G,cpus=1

Note that the -numa option doesn’t allocate any of the specified resources, it just assigns existing resources to NUMA nodes. This means that one still has to use the -m, -smp options to allocate RAM and VCPUs respectively.
-sandbox arg[,obsolete&#x3D;string][,elevateprivileges&#x3D;string][,spawn&#x3D;string][,resourcecontrol&#x3D;string]

Enable Seccomp mode 2 system call filter. ‘on’ will enable syscall filtering and ‘off’ will disable it. The default is ‘off’.  It disables system calls that are not needed by QEMU, thereby reducing the host kernel attack surface.


obsolete&#x3D;string  

   Enable Obsolete system calls  



elevateprivileges&#x3D;string  

   Disable set*uid|gid system calls  



spawn&#x3D;string  

   Disable *fork and execve  



resourcecontrol&#x3D;string  

   Disable process affinity and schedular priority   


NOTE

qemu does not proivde parameter to bind host’s physical cpu to vcpu, vcpu affinity is set outside of qemu either by cgroup as we create cpu cgroup for each vm or cpuset_setaffinity() if cgroup is not available

migration-incoming tcp:[host]:port[,to&#x3D;maxport][,ipv4&#x3D;on|off][,ipv6&#x3D;on|off]-incoming rdma:host:port[,ipv4&#x3D;on|off][,ipv6&#x3D;on|off]  

Prepare for incoming migration, listen on a given tcp port.

-incoming unix:socketpath  

Prepare for incoming migration, listen on a given unix socket.

-incoming fd:fd  

Accept incoming migration from a given filedescriptor.

# -S -incoming tcp:0.0.0.0:10000 is used for migration at dst host# start qemu at dst host, now cpu is stopped, listn on 10000 for migration$/usr/libexec/qemu-kvm $args -S -incoming tcp:0.0.0.0:10000qmp_shell/&gt; query-status&#123;    &quot;running&quot;: false,    &quot;singlestep&quot;: false,    &quot;status&quot;: &quot;inmigrate&quot;&#125;# then at src host, you can start migration process# after migration completed, start vm at dst host

Device Emulationusb options-usb

Enable USB emulation on machine types with an on-board USB host controller (if not enabled by default). Note that on-board USB host controllers may not support USB 3.0. In this case -device qemu-xhci can be used instead on machines with PCI.

-usbdevice devname

Add the USB device devname, and enable an on-board USB controller if possible and necessary (just like it can be done via -machine usb&#x3D;on). Note that this option is mainly intended for the user’s convenience only. More fine-grained control can be achieved by selecting a USB host controller (if necessary) and the desired USB device via the -device option instead. For example, instead of using -usbdevice mouse it is possible to use -device qemu-xhci -device usb-mouse to connect the USB mouse to a USB 3.0 controller instead (at least on machines that support PCI and do not have an USB controller enabled by default yet).

# easy way# use on-board usb controller-usb \-usbdevice tablet \# more fine-grained control way(use usb3.0)-device qemu-xhci \-device usb-tablet \

-device usb-tablet  

Pointer device that uses absolute coordinates (like a touchscreen). This means QEMU is able to report the mouse position without having to grab the mouse. Also overrides the PS&#x2F;2 mouse emulation when activated. with this enabled, mouse cursor follows mous position right way.

block options-blockdev option[,option[,option[,…]]]Define a new block driver node(backend) only. Some of the options apply to all block drivers, other options are only accepted for a specific block driver.
Options that expect a reference to another node (e.g. file) can be given in two ways. Either you specify the node name of an already existing node (file&#x3D;node-name), or you define a new node inline, adding options for the referenced node after a dot (file.filename&#x3D;path,file.aio&#x3D;native).
A block driver node created with -blockdev can be used for a guest device by specifying its node name for the drive property in a -device argument that defines a block device.
Valid options for any block driver node:

driver

Specifies the block driver to use for the given node.



node-name

This defines the name of the block driver node by which it will be referenced later. The name must be unique, i.e. it must not match the name of a different block driver node, or (if you use -drive as well) the ID of a drive.




Driver-specific options for file

filename

The path to the image file in the local filesystem -blockdev driver=file,node-name=disk,filename=disk.img




Driver-specific options for raw

file

Reference to or definition of the data source block driver node (e.g. a file driver node), -blockdev driver=raw,node-name=disk,file=disk_file




Driver-specific options for qcow2

file

Reference to or definition of the data source block driver node (e.g. a file driver node) -blockdev driver=qcow2,node-name=hda,file=my_file,overlap-check=none,cache-size=16777216




-drive option[,option[,option[,…]]]

Define a new drive. This includes creating a block driver node (the backend) as well as a guest device, and is mostly a shortcut for defining the corresponding -blockdev and -device options. 


-drive accepts all options that are accepted by -blockdev. In addition, it knows the following options:


file&#x3D;file

This option defines which disk image to use with this drive



if&#x3D;interface

This option defines on which type on interface the drive is connected. Available types are: ide, scsi, sd, mtd, floppy, pflash, virtio, none



bus&#x3D;bus,unit&#x3D;unit

These options define where is connected the drive by defining the bus number and the unit id.



index&#x3D;index

This option defines where the drive is connected by using an index in the list of available connectors of a given interface type, cdrom and disk share the index!!!



media&#x3D;media

This option defines the type of the media: disk or cdrom



format&#x3D;format

Specify which disk format will be used rather than detecting the format. Can be used to specify format&#x3D;raw to avoid interpreting an untrusted format header.



iops&#x3D;i,iops_rd&#x3D;r,iops_wr&#x3D;w

Specify request rate limits in requests per second, either for all request types or for reads or writes only.




# cdromqemu-system-x86_64 -drive file=file,index=2,media=cdrom# fda, fdbqemu-system-x86_64 -drive file=file,index=0,if=floppyqemu-system-x86_64 -drive file=file,index=1,if=floppy# index0 hda, index1 hdb as if=ide by default.qemu-system-x86_64 -drive file=file,index=0,media=diskqemu-system-x86_64 -drive file=file,index=1,media=diskqemu-system-x86_64 -drive file=file,index=2,media=diskqemu-system-x86_64 -drive file=file,index=3,media=disk

FAQwhat’s chipset?A chipset is a set of electronic components in an integrated circuit known as a &quot;Data Flow Management System&quot; that manages the data flow between the processor, memory and peripherals. It is usually found on the motherboard. Chipsets are usually designed to work with a specific family of microprocessors. Because it controls communications between the processor and external devices, the chipset plays a crucial role in determining system performance.
add console to qemuWith this, we can access console from pty directly
# run qemu-kvm with daemon, then later connect to its console-D /tmp/qemu_vm.log \-daemonize \-device virtio-serial-pci,id=virtio-serial0 \-chardev pty,id=charconsole1 -device virtconsole,chardev=charconsole1,id=console1 \$ cat /tmp/qemu_vm.logqemu-kvm: -chardev pty,id=charconsole1: char device redirected to /dev/pts/1 (label charconsole1)$ screen /dev/pts/1# quit from console(terminate screen session)ctrl + a, then press \

REF
Qemu Options

]]></content>
      <categories>
        <category>qemu</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu-kvm-security</title>
    <url>/2022/04/15/qemu-kvm-security/</url>
    <content><![CDATA[OverviewWhy qemu need security?There are several cases where security is needed like The virtualization use cases rely on hardware virtualization extensions to execute guest code safely on the physical CPU at close-to-native speed.
The following entities are untrusted, meaning that they may be buggy or malicious:

Guest
User-facing interfaces (e.g. VNC, SPICE, WebSocket)
Network protocols (e.g. NBD, live migration)
User-supplied files (e.g. disk images, kernels, device trees)
Passthrough devices (e.g. PCI, USB)

Bugs affecting these entities are evaluated on whether they can cause damage in real-world use cases and treated as security bugs if this is the case.


Security Architecture in QemuDesign principlesBelow are design principles that ensure the security requirements are met.
Guest IsolationGuest isolation is the confinement of guest code to the virtual machine. When guest code gains control of execution on the host this is called escaping the virtual machine. Isolation also includes resource limits such as throttling of CPU, memory, disk, or network. Guests must be unable to exceed their resource limits.
QEMU presents an attack surface to the guest in the form of emulated devices. The guest must not be able to gain control of QEMU. Bugs in emulated devices could allow malicious guests to gain code execution in QEMU. At this point the guest has escaped the virtual machine and is able to act in the context of the QEMU process on the host.
Guests often interact with other guests and share resources with them. A malicious guest must not gain control of other guests or access their data. Disk image files and network traffic must be protected from other guests unless explicitly shared between them by the user
Principle of Least PrivilegeThe principle of least privilege states that each component only has access to the privileges necessary for its function. In the case of QEMU this means that each process only has access to resources belonging to the guest.
The QEMU process should not have access to any resources that are inaccessible to the guest. This way the guest does not gain anything by escaping into the QEMU process since it already has access to those same resources from within the guest
In reality certain resources are inaccessible to the guest but must be available to QEMU to perform its function. For example, host system calls are necessary for QEMU but are not exposed to guests. A guest that escapes into the QEMU process can then begin invoking host system calls
Isolation mechanismsSeveral isolation mechanisms are available to realize this architecture of guest isolation and the principle of least privilege. With the exception of Linux seccomp(dployed by qemu itself), these mechanisms are all deployed by management tools that launch QEMU, such as libvirt.
The fundamental isolation mechanism is that QEMU processes must run as unprivileged users. Sometimes it seems more convenient to launch QEMU as root to give it access to host devices (e.g. &#x2F;dev&#x2F;net&#x2F;tun) but this poses a huge security risk. File descriptor passing can be used to give an otherwise unprivileged QEMU process access to host devices without running QEMU as root. It is also possible to launch QEMU as a non-root user and configure UNIX groups for access to &#x2F;dev&#x2F;kvm, &#x2F;dev&#x2F;net&#x2F;tun, and other device nodes. Some Linux distros already ship with UNIX groups for these devices by default.

SELinux and AppArmor make it possible to confine processes beyond the traditional UNIX process and file permissions model. They restrict the QEMU process from accessing processes and files on the host system that are not needed by QEMU.
Resource limits and cgroup controllers provide throughput and utilization limits on key resources such as CPU time, memory, and I&#x2F;O bandwidth.
Linux namespaces can be used to make process, file system, and other system resources unavailable to QEMU. A namespaced QEMU process is restricted to only those resources that were granted to it.
Linux seccomp is available via the QEMU –sandbox option. It disables system calls that are not needed by QEMU, thereby reducing the host kernel attack surface.

Limiting syscalls with seccompSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel.
Seccomp filtering(Seccomp BPF) provides a means for a process to specify a filter for incoming system calls. The filter is expressed as a Berkeley Packet Filter (BPF) program, as with socket filters, except that the data operated on is related to the system call being made: system call number and the system call arguments. This allows for expressive filtering of system calls using a filter program language with a long history of being exposed to userland and a straightforward data set.
Ref
Qemu security

]]></content>
      <categories>
        <category>qemu</category>
        <category>security</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title>react_basic</title>
    <url>/2020/07/23/react-basic/</url>
    <content><![CDATA[BasicsJSXJSX is a syntax extension to JavaScript, it combines js with html, it only comes in when writes your js code like this, but not all browsers support such extension, if browser does not support it, you can convert it to native js code by Babel at server-side or at client side by include Babel.js for that website, we recommend do this in server-side as we can do some optimization. but Most mordent browsers support JSX now.
const element = &lt;h1 className=&quot;greating&quot;&gt; Hello &lt;/h1&gt;;// React doesn’t require using JSX, same as below if browser does not support JSXconst element = React.createElement(  &#x27;h1&#x27;,  &#123;className: &#x27;greeting&#x27;&#125;,  &#x27;Hello, world!&#x27;);//the final object.const element = &#123;  // type and props are two attributes of each React Element!!!!  type: &#x27;h1&#x27;,  props: &#123;    className: &#x27;greeting&#x27;,    children: &#x27;Hello, world!&#x27;  &#125;&#125;;
Embedding Expressions in JSXYou can put any valid JavaScript expression inside the curly braces in JSX.  For example, 2 + 2, user.firstName, or formatName(user) are all valid JavaScript expressions.
const name = &#x27;Josh Perez&#x27;;const element = &lt;h1&gt;Hello, &#123;name&#125;&lt;/h1&gt;;const element = &lt;h1&gt;Hello, &#123;2+2&#125;&lt;/h1&gt;;const element = &lt;h1&gt;Hello, &#123;fun(2)&#125;&lt;/h1&gt;;const element = &lt;h1&gt;Hello, &#123;false&#125;&lt;/h1&gt;;const element = &lt;img src=&#123;user.avatarUrl&#125;&gt;&lt;/img&gt;;
React DOM uses camelCase property naming convention instead of HTML attribute names

class becomes className in JSX, and tabindex becomes tabIndex
built-in tag attributes may be renamed if it has two words or one word that is reserved by JSX

There are several rules about JSX

&lt;tags&gt; are elements

JSX &lt;tags&gt; map to calls to React.createElement().
Use &lt;lowercase &#x2F;&gt; tags when you need a DOM elements, and  tags for component elements


JSX children become child elements

they become the element’s props.children


Attributes are props

Use “” quotes when your props are strings
Use {} braces when your props are literals or variables


{} interpolates children

When a pair of {} braces is encountered within a JSX element, it’s value will be interpolated in as a child.
let Hello &#x3D; (props) &#x3D;&gt; &lt;div&gt;Hello, {props.to}


Empty &lt;&gt; tags

A pair of empty &lt;&gt; and &lt;&#x2F;&gt; tags get’s turned into a React.Fragment element, i.e. an element that doesn’t map to DOM nodes


{…object} acts like Object.assign()

Passing {…object} as an attribute will add all of the properties of the object as separate attributes



Function ComponentFunction Component has no state, is cheap, always use it if no state needed or no life cycle hooks needed.Function Component must return a React element or null to prevent render there are several ways to achieve this.
Actually you can use any name to replace props but props seems have the right meaning. any object name would be fine.
function Welcome(props) &#123;    console.log(&quot;children:&quot;,props.children);    return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125;//All attributes will store at an object and passed to Constructor!!// &#123;name: &quot;jason&quot;&#125; passed as props= &#123;name: &quot;jason&quot;&#125;const elment_jason = &lt;Welcome name=&quot;jason&quot;/&gt;;//anything inside &lt;Welcome&gt;xxx&lt;/Welcom&gt; are called childrenconst elment_jason = &lt;Welcome name=&quot;jason&quot;&gt; children&lt;/Welcome&gt;//operator spreading in parameterfunction Hi(&#123;name:name1, ...other&#125;) &#123;    // other is also an object &#123;id: 12, city: &quot;shanghai&quot;&#125;    return &lt;h1&gt;Hello, &#123;name1&#125; &#123;other.id&#125; &#123;other.city&#125;&lt;/h1&gt;;&#125;//The other same wayfunction Hi(props) &#123;    const &#123;name: name1, ...other&#125; = props;    // other is also an object &#123;id: 12, city: &quot;shanghai&quot;&#125;    return &lt;h1&gt;Hello, &#123;name1&#125; &#123;other.id&#125; &#123;other.city&#125;&lt;/h1&gt;;&#125;const element_jason = &lt;Hi name=&quot;jason&quot; id=&#123;12&#125; city=&quot;shanghai&quot;/&gt;;//pass each attr in other object separately same as aboveconst other = &#123;id: 12, city: &quot;shanghai&quot;&#125;;const element_jason = &lt;Hi name=&quot;jason&quot; &#123;...other&#125; /&gt;// if there are several statements, use &#123;&#125; and return is a must//otherwise, one statement, must omit &#123;&#125; and return at same time, it&#x27;s required by arrow functionconst Greeting = (props) =&gt; &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;const Greeting = (props) =&gt; &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;const Greeting = (props) =&gt; &#123; return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125;;const Greeting = (props) =&gt; &#123;  if(props.warn) &#123;      return null;  &#125;  return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125;

props check and default valueWhen we declare an property of an React element, we can add some checks and default value for it.
import PropTypes from &#x27;prop-types&#x27;;function Welcome(props) &#123;    return &lt;h1&gt;Hello, &#123;props.name&#125;: &#123;props.id&#125;&lt;/h1&gt;;&#125;Welcome.defaultProps = &#123;  name: &quot;josh&quot;&#125;Welcome.propTypes = &#123;  id: PropTypes.element.isRequired&#125;const hiJason = &lt;Welcome id=&#123;12&#125; name=&quot;Jason&quot;/&gt;;const hiJosh = &lt;Welcome id=&#123;16&#125; /&gt;;
Note this way also works for Class Component as well!!!
More step of function component.As normally, Function Component is not stateful or no lifecycle hook like Class Component, but class Component is heavy, is there a way we can use state and lifecycle hook like Class does. that’s why Hooks comes into place which let you use state and other React features without writing a class(use them in Function.
Hooks are functions that let you “hook into” React state and lifecycle features from function components. Hooks don’t work inside classes.
State Hook
import React, &#123; useState &#125; from &#x27;react&#x27;;function Example() &#123;  // Declare a new state variable, which we&#x27;ll call &quot;count&quot;  // you can call useState(0) many times if you need more states  // const [number, setNumber] = useState(12);  12 is initial value  // const [data, setData] = useState(&#123;ob: 12&#125;); &#123;ob: 12&#125;;  //then update count in callback by call setCount(1)  const [count, setCount] = useState(0);  return (    &lt;div&gt;      &lt;p&gt;You clicked &#123;count&#125; times&lt;/p&gt;      &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt;        Click me      &lt;/button&gt;    &lt;/div&gt;  );&#125;

useState is a Hook. We call it inside a function component to add some local state to it. React will preserve this state between re-renders. useState returns a pair: the current state value and a function that lets you update it. You can call this function from an event handler or somewhere else. It’s similar to this.setState in a class, except it doesn’t merge the old and new state together
Effect HookThe Effect Hook, useEffect, adds the ability to perform side effects from a function component. It serves the same purpose as componentDidMount, componentDidUpdate, and componentWillUnmount in React classes, but unified into a single API.
import React, &#123; useState, useEffect &#125; from &#x27;react&#x27;;function Example() &#123;  const [count, setCount] = useState(0);  // Similar to componentDidMount and componentDidUpdate:  // also you can call useEffect many times.  useEffect(() =&gt; &#123;    // Update the document title using the browser API    // this is call when DidMount and DidUpdate    document.title = `You clicked $&#123;count&#125; times`;    //useEffect only takes on parameter, callback but if the callback return a function    //that returned function is called when DidUnmount!  &#125;);  return (    &lt;div&gt;      &lt;p&gt;You clicked &#123;count&#125; times&lt;/p&gt;      &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt;        Click me      &lt;/button&gt;    &lt;/div&gt;  );&#125;
Effects are declared inside the component so they have access to its props and state. By default, React runs the effects after every render — including the first render.
Rules for Hooks

Only call Hooks at the top level. Don’t call Hooks inside loops, conditions, or nested functions.
Only call Hooks from React function components. Don’t call Hooks from regular JavaScript functions.Class ComponentClass Component has a built-in state attribute, an object stores all states of such component, it must
inherit from React.Component
call super(props) if has custom constructor, you can also access this.props in its method!!!
have a render() method that returns an React element

class Clock extends React.Component &#123;  constructor(props) &#123;	super(props);	//init the state object, state is a object with many key:value pairs	this.state = &#123;date: new Date()&#125;;  &#125;  render() &#123;    const now = this.state.date.toLocaleTimeString();    return &lt;div&gt;&lt;h2&gt;It is &#123;now&#125;&lt;/h2&gt;&lt;/div&gt;;  &#125;  xxx() &#123;    //add another state Id, setState is built-in method    this.setState(&#123;Id: 2&#125;);    //another way to call setState with callback which must return &#123;k: v&#125; object used to update state store    //setState((prestate, preprops)=&gt;&#123; return &#123;id: prestate.id + 1&#125;;&#125;);  &#125;&#125;

React element life cycle hooksWhen we mount a React element to a Dom node(mount point), other remove it from that, several event happens, React already added some hooks to each event, you just need to implement that hook if you want to do thing.
class Clock extends React.Component &#123;  constructor(props) &#123;    super(props);    this.state = &#123;        counter: 12    &#125;  &#125;  componentDidMount() &#123;    //setup thing  &#125;  componentWillUnmount() &#123;    // do cleanup  &#125;  render() &#123;    // pass state to child as props    return &lt;Sub counter=&#123;this.state.counter&#125;/&gt;;  &#125;

Data(state) Flow Downstate is often called local or encapsulated, always set it and access it in the component who owns it, if child wants to access it, pass it as props to the child.
Lift state upTwo child elements wants to modify same state(shared same one), this is impossible if each child has a state as it’s local so lift state up, after Lift state up, parent elements owns this state, that means parent is only person who can call setState to update it, but two children want to modify it and see same, so parent passes two props to child, one is the value, the other is callback defined by parent, each child calls the callback which setState() to update the shared state which owns by parent. child update parent’s state by parent’s callback.
class AButton extends React.Component &#123;  constructor(props) &#123;    super(props);    this.update = this.update.bind(this);  &#125;  update() &#123;    //must bind otherwise, you can not see this here!!!    this.props.add(1);  &#125;  render() &#123;    return &lt;button onClick=&#123;this.update&#125;&gt;A:&#123;this.props.num&#125;&lt;/button&gt;;  &#125;&#125;class BButton extends React.Component &#123;  constructor(props) &#123;    super(props);    this.update = this.update.bind(this);  &#125;  update() &#123;    this.props.add(2);  &#125;  render() &#123;    return &lt;button onClick=&#123;this.update&#125;&gt;B:&#123;this.props.num&#125;&lt;/button&gt;;  &#125;&#125;class GButton extends React.Component &#123;  constructor(props) &#123;    super(props);    this.state = &#123;      counter: 0    &#125;;    this.add = this.add.bind(this);  &#125;  add(n) &#123;    this.setState((prestate) =&gt; &#123;return &#123;counter: prestate.counter + n&#125;&#125;);  &#125;  render() &#123;    return &lt;div&gt; &lt;AButton num=&#123;this.state.counter&#125; add=&#123;this.add&#125;/&gt; &lt;BButton num=&#123;this.state.counter&#125; add=&#123;this.add&#125;/&gt;&lt;/div&gt;;  &#125;&#125;ReactDOM.render(&lt;GButton/&gt;, document.getElementById(&#x27;root&#x27;))
Composition vs InheritanceWe recommend using composition instead of inheritance to reuse code between components, here are some good example to use composition
function FancyBorder(props) &#123;  return (    //put children directly here    &lt;div className=&#123;&#x27;FancyBorder FancyBorder-&#x27; + props.color&#125;&gt;      &#123;props.children&#125;    &lt;/div&gt;  );&#125;function WelcomeDialog() &#123;  return (    //&lt;FancyBorder &gt;xxx&lt;/FancyBorder&gt; xxx are children    &lt;FancyBorder color=&quot;blue&quot;&gt;      &lt;h1 className=&quot;Dialog-title&quot;&gt;        Welcome      &lt;/h1&gt;      &lt;p className=&quot;Dialog-message&quot;&gt;        Thank you for visiting our spacecraft!      &lt;/p&gt;    &lt;/FancyBorder&gt;  );&#125;//instead of use children as a whole, we use specific property set by caller with proper &#x27;reuse&#x27; elementfunction SplitPane(props) &#123;  return (    &lt;div className=&quot;SplitPane&quot;&gt;      &lt;div className=&quot;SplitPane-left&quot;&gt;        &#123;props.left&#125;      &lt;/div&gt;      &lt;div className=&quot;SplitPane-right&quot;&gt;        &#123;props.right&#125;      &lt;/div&gt;    &lt;/div&gt;  );&#125;function App() &#123;  //React Element as in expression  return &lt;SplitPane left=&#123;&lt;Contacts /&gt;&#125; right=&#123; &lt;Chat /&gt;&#125;/&gt;;&#125;//combine attribute and childrenfunction Dialog(props) &#123;  return (    &lt;FancyBorder color=&quot;blue&quot;&gt;      &lt;h1 className=&quot;Dialog-title&quot;&gt;        &#123;props.title&#125;      &lt;/h1&gt;      &lt;p className=&quot;Dialog-message&quot;&gt;        &#123;props.message&#125;      &lt;/p&gt;      &#123;props.children&#125;    &lt;/FancyBorder&gt;  );&#125;class SignUpDialog extends React.Component &#123;  constructor(props) &#123;    super(props);    this.handleChange = this.handleChange.bind(this);    this.handleSignUp = this.handleSignUp.bind(this);    this.state = &#123;login: &#x27;&#x27;&#125;;  &#125;  render() &#123;    return (      &lt;Dialog title=&quot;Mars Exploration Program&quot;              message=&quot;How should we refer to you?&quot;&gt;        &lt;input value=&#123;this.state.login&#125; onChange=&#123;this.handleChange&#125; /&gt;        &lt;button onClick=&#123;this.handleSignUp&#125;&gt; Sign Me up! &lt;/button&gt;      &lt;/Dialog&gt;    );  &#125;  handleChange(e) &#123;    this.setState(&#123;login: e.target.value&#125;);  &#125;  handleSignUp() &#123;    alert(`Welcome aboard, $&#123;this.state.login&#125;!`);  &#125;&#125;

Controlled ComponentIn HTML, form elements such as &lt;input&gt;, &lt;textarea&gt;, and &lt;select&gt; typically maintain their own state and update it based on user input. In React, mutable state is typically kept in the state property of components, and only updated with setState().
We can combine the two by making the React state be the “single source of truth”. Then the React component that renders a form also controls what happens in that form on subsequent user input. An input form element whose value is controlled by React in this way is called a “controlled component”.  React does not want HTML dom element maintain their own state, React wants state be ‘single source of truth’, that why React wants to control the value(state property).
class NameForm extends React.Component &#123;  constructor(props) &#123;    super(props);    this.state = &#123;value: &#x27;&#x27;&#125;;    this.handleChange = this.handleChange.bind(this);    this.handleSubmit = this.handleSubmit.bind(this);  &#125;  handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125;  handleSubmit(event) &#123;    alert(&#x27;A name was submitted: &#x27; + this.state.value);    event.preventDefault();  &#125;  // value attribute comes from state which is controlled by React.  render() &#123;    return (      &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt;        &lt;label&gt;          Name: &lt;input type=&quot;text&quot; value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125; /&gt;        &lt;/label&gt;        &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;      &lt;/form&gt;    );  &#125;&#125;

Uncontrolled component
class NameForm extends React.Component &#123;  constructor(props) &#123;    super(props);    this.handleSubmit = this.handleSubmit.bind(this);    //create Ref    this.input = React.createRef();  &#125;  handleSubmit(event) &#123;    //ref.current points to linked element    alert(&#x27;A name was submitted: &#x27; + this.input.current.value);    event.preventDefault();  &#125;  render() &#123;    //link the Ref to &lt;input&gt; element    return (      &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt;        &lt;label&gt; Name: &lt;input type=&quot;text&quot; ref=&#123;this.input&#125; /&gt;&lt;/label&gt;        &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;      &lt;/form&gt;    );  &#125;&#125;

React EventsReact implements the standard Event API(W3C Spec) that’s same as Dom event except React named with different styles.

React events are named using camelCase, rather than lowercase
With JSX you pass a function as the event handler, rather than a string.In html&lt;button onclick=&quot;activateLasers()&quot;&gt;  Activate Lasers&lt;/button&gt;

In React
&lt;button onClick=&#123;activateLasers&#125;&gt;  Activate Lasers&lt;/button&gt;

Note: if you refer to a method without () after it, such as onClick&#x3D;{this.handleClick}, you should bind that method otherwise you can’t use this in that handler
//*experimental*class LoggingButton extends React.Component &#123;  handleClick = () =&gt; &#123;      console.log(&#x27;this is:&#x27;, this);  &#125;  render() &#123;    return (      &lt;button onClick=&#123;this.handleClick&#125;&gt;        Click me      &lt;/button&gt;    );  &#125;&#125;// use this way.class LoggingButton extends React.Component &#123;  constructor(props) &#123;    this.handleClick = this.handleClick.bind(this);  &#125;  handleClick(e) &#123;      console.log(&#x27;this is:&#x27;, this);  &#125;  render() &#123;    return (      &lt;button onClick=&#123;this.handleClick&#125;&gt;        Click me      &lt;/button&gt;    );  &#125;&#125;class LoggingButton extends React.Component &#123;  handleClick () &#123;      console.log(&#x27;this is:&#x27;, this);  &#125;  render() &#123;    // Never use this way.    return (      &lt;button onClick=&#123; ()=&gt; this.handleClick() &#125;&gt;        Click me      &lt;/button&gt;    );  &#125;&#125;class LoggingButton extends React.Component &#123;  handleClick (id, e) &#123;      console.log(&#x27;this is:&#x27;, this);  &#125;  render() &#123;    // use this way if you want to pass another parameter to event callback    // by default, callback is called with only event parameter    return (      &lt;button onClick=&#123;this.handleClick.bind(this, id)&#125;&gt;        Click me      &lt;/button&gt;    );  &#125;&#125;
Another difference is that you cannot return false to prevent default behavior in React. You must call preventDefault explicitly.
{} treats special handling for ArrayAs you know any expression can be in {}, if an Array in the {}, it will destruct the Array!! also note when use map, each item must has a key attribute, this is required by React.
function NumberList(props) &#123;  const numbers = props.numbers;  const listItems = numbers.map((number) =&gt;    //no dom node for Fragment    &lt;React.Fragment key=&#123;number.toString()&#125;&gt;    &lt;li&gt;&#123;number&#125;&lt;/li&gt;    &lt;/React.Fragment&gt;  );  return &lt;ul&gt;&#123;listItems&#125;&lt;/ul&gt;&#125;const numbers = [1, 2, 3, 4, 5];ReactDOM.render(  &lt;NumberList numbers=&#123;numbers&#125; /&gt;,  document.getElementById(&#x27;root&#x27;));
FragmentsA common pattern in React is for a component to return multiple elements. Fragments let you group a list of children without adding extra nodes to the DOM
class Columns extends React.Component &#123;  render() &#123;    return (      &lt;&gt;        &lt;td&gt;Hello&lt;/td&gt;        &lt;td&gt;World&lt;/td&gt;      &lt;/&gt;    );  &#125;&#125;// React.Fragment supports attribute but &lt;&gt;&lt;/&gt; notfunction Glossary(props) &#123;  return (    &lt;dl&gt;      &#123;props.items.map(item =&gt; (        // Without the `key`, React will fire a key warning        &lt;React.Fragment key=&#123;item.id&#125;&gt;          &lt;dt&gt;&#123;item.term&#125;&lt;/dt&gt;          &lt;dd&gt;&#123;item.description&#125;&lt;/dd&gt;        &lt;/React.Fragment&gt;      ))&#125;    &lt;/dl&gt;  );&#125;
Use CSS moduleCSS Modules allows the scoping of CSS by automatically creating a unique classname of the format this is done Webpack, it let you use the same CSS class name in different files without worrying about naming clashes, you write CSS files like before, nothing change for you when writing CSS file.
//button.css.error &#123;  background-color: red;&#125;

//text.css.text &#123;    color: black;&#125;

//use it in react app Button.jsimport React, &#123; Component &#125; from &#x27;react&#x27;;import styles from &#x27;./button.css&#x27;; // Import css modules stylesheet as stylesimport &#x27;./text.css&#x27;  //not as a modules, use class directlyclass Button extends Component &#123;  render() &#123;    // reference as a js object, must use this way    return (&lt;div&gt;              &lt;button className=&#123;styles.error&#125;&gt;Error Button&lt;/button&gt;              &lt;button className=&quot;text&quot;&gt;Error Button&lt;/button&gt;            &lt;/div&gt;);  &#125;&#125;
The result in html is this
&lt;!-- This button has red background but not red text --&gt;&lt;div&gt;  &lt;button class=&quot;Button_error_ax7yz&quot;&gt;Error Button&lt;/button&gt;  &lt;button class=&quot;text&quot;&gt;Error Button&lt;/button&gt;&lt;/div&gt;

REF
How to Learn React
official tutorial
Video: react tutorial
react intro

]]></content>
      <categories>
        <category>react</category>
        <category>basic</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>component</tag>
        <tag>mount event</tag>
      </tags>
  </entry>
  <entry>
    <title>react_faq</title>
    <url>/2020/07/23/react-faq/</url>
    <content><![CDATA[FAQCan I use react for part of my pageThe answer is Yes, as React use render() which mounts the react element to the Dom node, as the Dom node can be any dom node of the html, so that react be part of the html or whole page if Dom node is the whole body.Here is how to add react to part of your page
ReactDOM.render(  &lt;h1&gt;Hello, World&lt;/h1&gt;,  document.getElementById(&#x27;container_id&#x27;));

What’s BabelAs any language, Javascript also has versions named ECMAScript (short for ES). Currently, most browsers support ES5. ES5 used to be good even though it was painful to code in it. Remember, this not reading from inside callback functions? The new version of Javascript, ES6, also known as ES2015 (specs of the language were finalized in June 2015) makes Javascript great again. If you want to learn about ES6, check out the links at the end of this article. All the great features of ES6 come with one big problem — majority of browsers do not fully support them. That’s when Babel comes to play. Babel is a JS transpiler that converts new JS code into old ones. It is a very flexible tool in terms of transpiling. One can easily add presets such as es2015, es2016, es2017, so that Babel compiles them to ES5.


polyfillIn order to support Promises you must include the Babel polyfill.
Learning JavaScript bundlersFor a number of good technical reasons CommonJS modules (i.e. everything in npm) cannot be used natively in the browser. You need a JavaScript “bundler” to “bundle” these modules into .js files[es6 or old version based on config] that you can include in your web page with a &lt;script&gt; tag.
WebPackwe would like to use that. We would also like to use SASS for our styles, PostCSS for autoprefixing. Plus, we would like to minify and uglify both our CSS and Javascript code. Webpack solves all of these problems using one config file (named webpack.config.js) and one CLI command webpack
Webpack is a modular build tool that has two sets of functionality — Loaders and Plugins. Loaders transform the source code of a module. For example style-loader adds CSS to DOM using style tags. sass-loader compiles SASS files to CSS. babel-loader transpiles JS code given the presets. Plugins are the core of Webpack. They can do things that loaders can’t. For example, there is a plugin called UglifyJS that minifies and uglifies the output of webpack.
Popular js bundlers

wepack
browserify
grunt or gulp

Should I use client side Router for my website“Single-page applications” are all the rage these days. These are web pages that load once, and when the user clicks on a link or a button, JavaScript running on the page updates the address bar, but the web page is not refreshed(no request send to server). Management of the address bar is done by something called a router.
Use it or not depends on your website

If you’re building a single-page application, use it unless you have a good reason not to.
Don’t use a router if you aren’t building a single-page application.

If you need a router, use react-router in your react application.
Should I use Redux?React components are arranged in a hierarchy. Most of the time, your data model also follows a hierarchy. In these situations Flux doesn’t buy you much. Sometimes, however, your data model is not hierarchical. When your React components start to receive props that feel extraneous, or you have a small number of components starting to get very complex, then you might want to look into Redux which is used for manage complex state.
You’ll know when you need Redux. If you aren’t sure if you need it, you don’t need it.
Should I use Immutable.js?Immutable.js provides a set of data structures that can help to solve certain performance issues when building React apps. It’s a great library, and you’ll probably use it a lot in your apps moving forward, but it’s completely unnecessary until you have an appreciation of the performance implications.
Use it when you find your react apps has performance issue. and there is a library for this [immutability-helper](https://github.com/kolodny/immutability-helper)
const &#123; Map &#125; = require(&#x27;immutable&#x27;);const map1 = Map(&#123; a: 1, b: 2, c: 3 &#125;);const map2 = map1.set(&#x27;b&#x27;, 50); // when you change an attr, a new object is created, but//the new object shares the unchanged part.//without immutable js (native js), map2 and map1 points to the same shared memorymap1.get(&#x27;b&#x27;) + &quot; vs. &quot; + map2.get(&#x27;b&#x27;); // 2 vs. 50// it supports List, Map, Stack, OrderedMap, Set, OrderedSet, Record

how to create React APPHere are rules and steps you should take

Step 1: Break The UI Into A Component Hierarchy

Step 2: Build A Static Version in React(no state at all use static)

Step 3: Identify The minimal (but complete) Representation Of UI State

Step 4: Identify Where Your State Should Live

Step 5: Add Inverse Data Flow


use bootstrap in ReactThere are two ways to use bootstrap in React, one is use bootstrap css the native way hence more control, the other way is use bootstrap Component which is wrapper of bootstrap native element, easy to use but less control.
bootstrap css
// $ yarn add bootstrap// add this to index.js so all js can use bootstrap css class// import &#x27;bootstrap/dist/css/bootstrap.min.css&#x27;;// App.jsimport React from &#x27;react&#x27;;import logo from &#x27;./logo.svg&#x27;;import &#x27;./App.css&#x27;;function App() &#123;  return (    // use bootstrap native component    &lt;div&gt;      &#123;&#x27;Test button &#x27;&#125;&lt;button type=&quot;button&quot; class=&quot;btn btn-success&quot;&gt;Success&lt;/button&gt;    &lt;/div&gt;  );&#125;export default App;

bootstrap component
// $ yarn add bootstrap// $ yarn add react-bootstrap// add this to index.js so all js can use bootstrap css class// import &#x27;bootstrap/dist/css/bootstrap.min.css&#x27;;//App.jsimport React from &#x27;react&#x27;;import logo from &#x27;./logo.svg&#x27;;import &#x27;./App.css&#x27;;import &#123; Button &#125; from &#x27;react-bootstrap&#x27;function App() &#123;  return (    &lt;div&gt;      &#123;&#x27; &#x27;&#125;&lt;Button variant=&quot;success&quot;&gt;Success&lt;/Button&gt;    &lt;/div&gt;  );&#125;export default App;

REF
webpack howto
react router
react router tutorial
Video: router
Video: Getting Started with Redux
Video Redux Series
React Redux Tutorial
redux official
immutable js

]]></content>
      <categories>
        <category>react</category>
        <category>tool</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>Babel</tag>
        <tag>Webpack</tag>
        <tag>gulp</tag>
      </tags>
  </entry>
  <entry>
    <title>react_setup</title>
    <url>/2020/07/23/react-setup/</url>
    <content><![CDATA[SetupYarnYarn is a superset of NPM that solves many problems that NPM has, NPM stands for Node Package Manage , It keeps track of all the packages and their versions and allows the developer to easily update or remove these dependencies. All of these external dependencies are being stored inside a file called called package.json. when you have a node project, as package.json has all dependencies and version, you can easilier to setup a same env as the development.

Yarn is a package manager that uses NPM registry as its backend, but it’s more than npm, say NPM installs packages sequentially. This slows down the performance significantly. Yarn solves this problem by installing these packages in parallel.
you can install pkg into global or particular project, for global part, there are several configs that control where to install, the global dir is used to store installed packages the global bin are symbol links to binary file installed at global dir.
$ yarn config list# show global dir and bin path$ yarn global dir$ yarn global bin# change bin path: $prefix/bin$ yarn config set prefix /root/yarn# change global dir, make sure check it again$ yarn config set global-folder /root/yarn# but this will not change global-folder in .yarnrc if has# you should change that file manually# add/del package globally$ yarn global list  --depth=0$ yarn global add package$ yarn global remove package# goto project dir show package for that project$ yarn list  --depth=0$ yarn add package$ yarn remove package# show config of yarn$ yarn config list# set repository taobao$ yarn config set registry https://registry.npm.taobao.org -g$ yarn config set disturl https://npm.taobao.org/dist --global# reset to default repo$ yarn config set registry https://registry.yarnpkg.com

More command, refer to Yarn CLI.
create react app$ yarn global add create-react-app$ yarn create react-app test_app# Starts the development server.$ cd test_app$ yarn start# Starts the test runner.$ yarn test# at last all things seems ok, passed all test# Bundles the app into static files for production, so that you can deploy this on your wbserver# this will create a build dir with static files, deploy that$ yarn build# ==========================================deploy with online static server===========================# For test only, you can deploy bundles file to surge which hosts static website for free with couples of days# so that someone else outside can see what the website should be when it&#x27;s deployed.# install surge cli to deploy weserver on surge server http://surge.sh/$ yarn global add surge# at the project dir run below commands (p means the path, -d means domain, the very first time need you configure email/password$ surge -p build -d your-domain.surge.sh# but the subsequent call, this is not required)# Later after the first call edit package.json, under “scripts”:# so that you add a new command to yar&quot;deploy&quot;: &quot;yarn run build &amp;&amp; surge -p build -d jason_lkm.surge.sh&quot;$ yarn deploy# ==========================================deploy local with static server===========================$ cd test_app$ yarn global add serve$ serve -s build

use json mock server from vs code# edit prj/package.json&quot;proxy&quot;: &quot;http://localhost:8080&quot;,...

debug react app with chrome
Install Chrome debugger for VS code
Use the following config for your launch.jsonfile to configure the VS Code debugger and put it inside .vscode in your root folder&#123;    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        &#123;            &quot;name&quot;: &quot;Chrome&quot;,            &quot;type&quot;: &quot;chrome&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;url&quot;: &quot;http://localhost:3000&quot;,            &quot;webRoot&quot;: &quot;$&#123;workspaceRoot&#125;/src&quot;        &#125;    ]&#125;
Start your React app by running yarn start in another terminal$ BROWSER=none yarn start
Start debugging in VS Code by pressing F5or by clicking the green debug iconREFBehide react-create-app

]]></content>
      <categories>
        <category>react</category>
        <category>setup</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>Yarn</tag>
        <tag>enzyme</tag>
      </tags>
  </entry>
  <entry>
    <title>storage_nbd</title>
    <url>/2021/07/06/storage-nbd/</url>
    <content><![CDATA[IntroductionNetwork block devices (NBD) are used to access remote storage device that does not physically reside in the local machine, for each network block device, it’s mapped with (&#x2F;dev&#x2F;nbdx) to client as a local block device, you can do low level operation for this block device, like partition, format with filesystem that NFS can NOT do.
NBD works according to a client&#x2F;server architecture. You use the server to make a volume available as a network block device from a host, then run the client to connect to it from another host.



All commands verified on Centos7
NBD uses TCP as its transport protocol. There is no well known port used for NBD

Client accesses /dev/nbdx after nbd mounts
Client and Server perform negotiation
Client sends a read request to the server(did by kernel) specifying the start offset and the length of the data to be read.
Server replies with a read reply, containing an error code (if any); if the error code is zero, reply header will be followed by immediate data
Client sends a write request, specifying the start offset and the length of the data to be written, immediately followed by raw data.
Server writes data out and sends a write reply, which contains an error code that may specify if an error occurred. If no error did occur, data is assumed to have been written to disk.
Client sends a disconnect request
Server disconnects.

Nbd exampleIn order to use nbd, need to install nbd server at server machine and nbd client at client side(also install nbd kernel module which exports nbd device for user, so that user sees it as a local block device).
NBD allows to export a real device or virtual disk at server, then client can mount it by NBD protocol.

Verified at Centos 7.6
# centos# install userland tools$ yum install nbd# At client side, you may need to compile nbd by yourself if not found$ modprobe nbd # this will create nbd device(unbound)$ ls /dev/nbd*/dev/nbd0     /dev/nbd1049  /dev/nbd110   /dev/nbd140  ....

export and mount a real device from remote# server side, export a local real device by: /etc/nbd-server/config, create this file if not found# section name must be unique[generic][test]    exportname = /dev/sdb$ service nbd-server restart# client side$ nbd-client $server_ip /dev/nbd0 -N test# if sdb has two partitions, you will see two devices /dev/nbd0p1 and /dev/nbd0p2 check with (fdisk /dev/nbd0 -l)# then you can create filesystem or make partition # OR if the disk already has filesystem, just mount it# mount the first partition$ fdisk /dev/nbd0 -l$ mount /dev/nbd0p1 /mnt

export and mount two disk files from remote# create a virtual disk$ dd if=/dev/zero of=vmdisk1.img bs=1G count=1# edit /etc/nbd-server/config, create if not found[generic][disk1]    exportname = /tmp/vmdisk1.img[disk2]    exportname = /tmp/vmdisk2.img$ service nbd-server restart$ nbd-client $server_ip /dev/nbd0  -N disk1$ nbd-client $server_ip /dev/nbd1  -N disk2# then you can create filesystem or make partition # OR if the disk already has filesystem, just mount it$ parted /dev/nbd0

mount a virtual disk image qcow2# edit /etc/nbd-server/config, create if not found[generic][test]    exportname = /tmp/windows.iso$ service nbd-server restart$ nbd-client $server_ip /dev/nbd0 -N test# then mount it $ fdisk /dev/nbd0 -l$ mount /dev/nbd0p1 /mnt


REF
nbd man
nbd protocol

]]></content>
      <categories>
        <category>kvm</category>
        <category>nbd</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>nbd</tag>
      </tags>
  </entry>
  <entry>
    <title>qemu-kvm</title>
    <url>/2021/11/02/qemu-kvm/</url>
    <content><![CDATA[OverviewIn this article, we only give you the knowledge of qemu-kvm without libvirt, say how to start vm by running qemu-kvm itself and others.


Simulated deviceqemu-kvm can simulate serial, block, serial, net device inside vm based on virtio driver, the simulated virtio devices is located at /sys/class/virtio-ports/, for simulated device, there are two sides need to be set from command line, backend in host, virtio in vm.
$ qemu-kvm -chardev socket,id=charch0,path=/var/run/xagent/vm-HZVsuboAJh-test/xagent.sock -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=agent.channel.0 -serial unix:/var/run/agent/vm-HZVsuboAJh-test/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2# check device name inside guest$ cat /sys/class/virtio-ports/vport0p1/nameagent.channel.0# check device id(major:minor) inside guest$ cat /sys/class/virtio-ports/vport0p1/dev252:1# echo hello&gt; /dev/vport0p1
Device Front EndA device front end is how a device is presented to the guest. The type of device presented should match the hardware that the guest operating system is expecting to see. 
# check support front end device and specific options for each type$ /usr/libexec/qemu-kvm --device virtio-serial-pci,helpvirtio-serial-pci.event_idx=bool (on/off)virtio-serial-pci.ioeventfd=bool (on/off)virtio-serial-pci.multifunction=bool (on/off)virtio-serial-pci.rombar=uint32virtio-serial-pci.x-disable-pcie=bool (on/off)virtio-serial-pci.indirect_desc=bool (on/off)virtio-serial-pci.__com.redhat_rhel6_ctrl_guest_workaround=boolvirtio-serial-pci.disable-modern=boolvirtio-serial-pci.disable-legacy=OnOffAuto (on/off/auto)virtio-serial-pci.emergency-write=bool (on/off)virtio-serial-pci.command_serr_enable=bool (on/off)virtio-serial-pci.x-pcie-lnkctl-init=bool (on/off)virtio-serial-pci.max_ports=uint32virtio-serial-pci.page-per-vq=bool (on/off)virtio-serial-pci.x-pcie-deverr-init=bool (on/off)virtio-serial-pci.x-pcie-pm-init=bool (on/off)virtio-serial-pci.x-pcie-lnksta-dllla=bool (on/off)virtio-serial-pci.any_layout=bool (on/off)virtio-serial-pci.class=uint32virtio-serial-pci.addr=int32 (Slot and optional function number, example: 06.0 or 06)virtio-serial-pci.migrate-extra=bool (on/off)virtio-serial-pci.modern-pio-notify=bool (on/off)virtio-serial-pci.vectors=uint32virtio-serial-pci.x-pcie-extcap-init=bool (on/off)virtio-serial-pci.virtio-backend=child&lt;virtio-serial-device&gt;virtio-serial-pci.x-ignore-backend-features=boolvirtio-serial-pci.notify_on_empty=bool (on/off)virtio-serial-pci.iommu_platform=bool (on/off)virtio-serial-pci.ats=bool (on/off)virtio-serial-pci.virtio-pci-bus-master-bug-migration=bool (on/off)virtio-serial-pci.romfile=str# check common options, refer to https://qemu-project.gitlab.io/qemu/system/invocation.html#hxtool-1/usr/libexec/qemu-kvm -d int -D /tmp/qemu_vm.log -trace enable=* -kernel /home/data/github/cyun/utils/tips_useful_script/qemu/kernel-5.10.7 -initrd /home/data/github/cyun/utils/tips_useful_script/qemu/initramfs.img -nographic -append console=ttyS0 -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -vnc 0.0.0.0:106 -chardev file,id=mydev0,path=/tmp/test.s -device isa-serial,chardev=mydev0

A front end is often paired with a back end, which describes how the host’s resources are used in the emulation.
Device Back EndThe back end describes how the data from the emulated device will be processed by QEMU. The configuration of the back end is usually specific to the class of device being emulated. For example serial devices will be backed by a --chardev which can redirect the data to a file or socket or some other system. Storage devices are handled by --blockdev which will specify how blocks are handled, for example being stored in a qcow2 file or accessing a raw host disk partition. Back ends can sometimes be stacked to implement features like snapshots.
While the choice of back end is generally transparent to the guest, there are cases where features will not be reported to the guest if the back end is unable to support it.
Device BusesMost devices will exist on a BUS of some sort. Depending on the machine model you choose (-M foo) a number of buses will have been automatically created. In most cases the BUS a device is attached to can be inferred, for example PCI devices are generally automatically allocated to the next free address of first PCI bus found. However in complicated configurations you can explicitly specify what bus (bus&#x3D;ID) a device is attached to along with its address (addr&#x3D;N).
Some devices, for example a PCI SCSI host controller, will add an additional buses to the system that other devices can be attached to. A hypothetical chain of devices might look like:
–device foo,bus=pci.0,addr=0,id=foo –device bar,bus=foo.0,addr=1,id=baz
which would be a bar device (with the ID of baz) which is attached to the first foo bus (foo.0) at address 1. The foo device which provides that bus is itself is attached to the first PCI bus (pci.0).
serial device backend(chardev)Qemu char device uses below format for backend, more options refer to qemu chardev options
-chardev backend,id=id[,mux=on|off][,options]
Backend is one of: null, socket, udp, file, pipe, console, serial, pty, stdio, tty, parallel and more... . The specific backend will determine the applicable options. different types have different options for that specific type.
All devices must have an id, which can be any string up to 127 characters long. It is used to uniquely identify this device in other command line directives.
A character device may be used in multiplexing mode by multiple front-ends. Specify mux&#x3D;on to enable this mode. A multiplexer is a “1:N” device, and here the “1” end is your specified chardev backend, and the “N” end is the various parts of QEMU that can talk to a chardev. by default it’s disabled
Example
-chardev file,id=mydev0,path=/tmp/test.s \-device isa-serial,chardev=mydev0 \$ /usr/libexec/qemu-kvm -d int -D /tmp/qemu_vm.log -trace enable=* -kernel /home/data/github/cyun/utils/tips_useful_script/qemu/kernel-5.10.7 -initrd /home/data/github/cyun/utils/tips_useful_script/qemu/initramfs.img -nographic -append console=ttyS0 -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -vnc 0.0.0.0:106 -chardev file,id=mydev0,path=/tmp/test.s -device isa-serial,chardev=mydev0# in guest /dev/ttyS1 is front end as the ttyS0 is used for console!!!

use virtio serial in the guest, in order to do this, you have to add a virtio pci serial device(hub) into the PCI bus, then under this virtio serial device, you can create serial and console port.
virtio-serial-pci &#x3D;&#x3D; virtio-serial
-device virtio-serial-pci,id=virtio_serial_pci0 \-chardev file,id=mydev0,path=/tmp/test.s \-device virtserialport,chardev=mydev0,name=serial0,id=vc1,bus=virtio_serial_pci0.0 \-chardev file,id=mydev1,path=/tmp/test.c \-device virtconsole,chardev=mydev1,name=serial1,id=vc2,bus=virtio_serial_pci0.0
Socket option-chardev socket,id=id[,TCP options or unix options][,server=on|off][,wait=on|off][,telnet=on|off][,websocket=on|off][,reconnect=seconds][,tls-creds=id][,tls-authz=id]

Create a two-way stream socket, which can be either a TCP or a unix socket. A unix socket will be created if path is specified. Behaviour is undefined if TCP options are specified for a unix socket.
server&#x3D;on|off specifies that the socket shall be a listening socket.
wait&#x3D;on|off specifies that QEMU should not block waiting for a client to connect to a listening socket.
telnet&#x3D;on|off specifies that traffic on the socket should interpret telnet escape sequences.
websocket&#x3D;on|off specifies that the socket uses WebSocket protocol for communication.
reconnect sets the timeout for reconnecting on non-server sockets when the remote end goes away. qemu will delay this many seconds and then attempt to reconnect. Zero disables reconnecting, and is the default.
tls-creds requests enablement of the TLS protocol for encryption, and specifies the id of the TLS credentials to use for the handshake. The credentials must be previously created with the -object tls-creds argument.
tls-auth provides the ID of the QAuthZ authorization object against which the client’s x509 distinguished name will be validated. This object is only resolved at time of use, so can be deleted and recreated on the fly while the chardev server is active. If missing, it will default to denying access.

TCP and unix socket options are given below:
TCP options: port=port[,host=host][,to=to][,ipv4=on|off][,ipv6=on|off][,nodelay=on|off]

host for a listening socket specifies the local address to be bound. For a connecting socket species the remote host to connect to. host is optional for listening sockets. If not specified it defaults to 0.0.0.0.

port for a listening socket specifies the local port to be bound. For a connecting socket specifies the port on the remote host to connect to. port can be given as either a port number or a service name. port is required.

to is only relevant to listening sockets. If it is specified, and port cannot be bound, QEMU will attempt to bind to subsequent ports up to and including to until it succeeds. to must be specified as a port number.

ipv4&#x3D;on|off and ipv6&#x3D;on|off specify that either IPv4 or IPv6 must be used. If neither is specified the socket may use either protocol.

nodelay&#x3D;on|off disables the Nagle algorithm.


unix options: path=path[,abstract=on|off][,tight=on|off]

path specifies the local path of the unix socket. path is required. abstract&#x3D;on|off specifies the use of the abstract socket namespace, rather than the filesystem. Optional, defaults to false. tight&#x3D;on|off sets the socket length of abstract sockets to their minimum, rather than the full sun_path length. Optional, defaults to true.

Pty-chardev pty,id=id  

Create a new pseudo-terminal on the host and connect to it. pty does not take any options.

Block device(backend)USBQEMU can emulate a PCI UHCI, OHCI, EHCI or XHCI USB controller. You can plug virtual USB devices, QEMU will automatically create and connect virtual USB hubs as necessary to connect multiple USB devices
XHCI controllerQEMU has XHCI host adapter support. The XHCI hardware design is much more virtualization-friendly when compared to EHCI and UHCI, thus XHCI emulation uses less resources (especially CPU). So if your guest supports XHCI (which should be the case for any operating system released around 2010 or later) we recommend using it:qemu -device qemu-xhci  or qemu -device nec-usb-xhci
XHCI supports USB 1.1, USB 2.0 and USB 3.0 devices, so this is the only controller you need. With only a single USB controller (and therefore only a single USB bus) present in the system there is no need to use the bus&#x3D; parameter when adding USB devices, as there is only one controller!!!
EHCI controllerThe QEMU EHCI Adapter supports USB 2.0 devices. It can be used either standalone or with companion controllers (UHCI, OHCI) for USB 1.1 devices. The companion controller setup is more convenient to use because it provides a single USB bus supporting both USB 2.0 and USB 1.1 devices
EHCI standaloneWhen running EHCI in standalone mode you can add UHCI or OHCI controllers for USB 1.1 devices too. Each controller creates its own bus though, so there are two completely separate USB buses: One USB 1.1 bus driven by the UHCI controller and one USB 2.0 bus driven by the EHCI controller. Devices must be attached to the correct controller manually
EHCI compansionThe UHCI and OHCI controllers can attach to a USB bus created by EHCI as companion controllers. This is done by specifying the masterbus and firstport properties. masterbus specifies the bus name the controller should attach to. firstport specifies the first port the controller should attach to, which is needed as usually one EHCI controller with six ports has three UHCI companion controllers with two ports each.
Companion controller is defined as multiple USB host controllers (EHCI/OHCI/UHCI) that are wired to the same physical connector such that a device, depending on some characteristic like speed, will be connected to a different controller even when plugged into the same connector, so that for a port on EHCI bus, it can handle usb2.0 and usb1.1 devices, but from system views, you still see several controllers, each takes one PCI addresse!!!
Bus selection for USB devices

XHCI only(support usb1.1, usb2.0, usb3.0), only one usb bus(Recommanded)
#  no need to set bus=xx as there is only one usb controller-device qemu-xhci \-device usb-tablet
EHCI bus(usb2.0) + UCHI(OHCI)(usb1.1), two separate buses, usb devices must set bus=x manually.
# USB 1.1(uhci,ohci controller) bus will carry the name usb-bus.0# usb2.0(ehci controller) will carry the name ehci.0# usb3.0(xhci controller) will carry the name usb1.0# must set bus for each usb device as there are two separate buses.# The &#x27;-usb&#x27; switch will make qemu create the UHCI controller as part of the PIIX3 chipset.  The USB 1.1 bus will carry the name &quot;usb-bus.0&quot;.-usb                                                        \-device usb-ehci,id=ehci                                    \-device usb-tablet,bus=usb-bus.0                            \-device usb-storage,bus=ehci.0,drive=usbstick
Companion controller, only one usb bus.
# usb2.0 controller ehci six ports# usb1.1 controller uchi(two ports) attached to ehci-device ich9-usb-ehci1,id=usb,bus=pci.0,addr=0x3.0x7 \-device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,multifunction=on,addr=0x3 \-device ich9-usb-uhci2,masterbus=usb.0,firstport=2,bus=pci.0,addr=0x3.0x1 \-device ich9-usb-uhci3,masterbus=usb.0,firstport=4,bus=pci.0,addr=0x3.0x2 \-device usb-tablet# inside guest$ lspci 00:03.0 USB controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #1 (rev 03)00:03.1 USB controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #2 (rev 03)00:03.2 USB controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #3 (rev 03)00:03.7 USB controller: Intel Corporation 82801I (ICH9 Family) USB2 EHCI Controller #1 (rev 03)
USB devices can be connected with the -device usb-... command line option or the device_add monitor command. Available devices are:

usb-mouse

usb-tablet

usb-kbd (Standard USB keyboard)

usb-audio

…


USB Hub
# Plugging a hub into UHCI port 2 works like this:-device usb-hub,bus=usb-bus.0,port=2# Plugging a virtual USB stick into port 4 of the hub just plugged works this way:-device usb-storage,bus=usb-bus.0,port=2.4,drive=...

network devIn order to use network device, you need to setup two sides front end and back end, but different types use different ways to setup backend, let’s say vhost-user and tap uses different way to setup back end, Here show tap interface as example, you have to do

A bridge(ovs bridge or linux bridge) created by user.
A tap interface created by user or let qemu to create it automatically.
Up tap interface and add it to the bridge, either by user manually from command line or provide /etc/qemu-ifup which is called by qemu automatically
Down tap interface and remove it from the bridge, either by user mannulay from command linie or provide /etc/qemu-ifdown whih is called by qemu automatically

There are severals way to setup network devices. In short, the -net is the legacy option, while -netdev comes in to solve issue present for -net, the newest way -nic from 2.12 is easiest way to set up an interface.

The -net option can create either a front-end or a back-end (but has disadvanges than -nic)
-netdev can only create a back-end, use -device for front end
Asingle occurrence of -nic will create both a front-end and a back-end.

NOTEif you use libvirt, all these operations will be done by libvirt itself!!
linux bridge# create a linux bridge or ovs bridge if ovs installed, or use docker bridge(docker0) which is linux bridge$ brctl showbridge name     bridge id               STP enabled     interfacesdocker0         8000.525400d85e6d       yes             docker0-nic                                                        veth6e0057e                                                        vethdb3e964                                                        vnet0# cat /etc/qemu-ifup#! /bin/shifconfig &quot;$1&quot; 0.0.0.0 upbr=&#x27;docker0&#x27;brctl addif $br &quot;$1&quot;# chmod +x /etc/qemu-ifup# cat /etc/qemu-ifdown#! /bin/shifconfig &quot;$1&quot; 0.0.0.0 downbr=&#x27;docker0&#x27;brctl delif $br &quot;$1&quot;# chmod +x /etc/qemu-ifdown# run vm with network$ /usr/libexec/qemu-kvm -kernel kernel-4.14.121 -initrd initramfs.img -nographic -append &quot;console=ttyS0&quot; -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -netdev tap,id=n1,ifname=tap0,script=/etc/qemu-ifup,downscript=/etc/qemu-ifdown -device virtio-net,netdev=n1# with vhost-net on, then check vhost worker thread by  ps -ef | grep vhost-$qemu_pid$ /usr/libexec/qemu-kvm -kernel kernel-4.14.121 -initrd initramfs.img -nographic -append &quot;console=ttyS0&quot; -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -netdev tap,id=n1,ifname=tap0,vhost=on,script=/etc/qemu-ifup,downscript=/etc/qemu-ifdown -device virtio-net,netdev=n1#======== tap0 on the host is created by qemu automatically ========$ brctl showbridge name     bridge id               STP enabled     interfacesdocker0         8000.525400d85e6d       yes             docker0-nic                                                        tap0                                                        veth6e0057e                                                        vethdb3e964                                                        vnet0# then inside vm, if you have dhcp client, it will try get ip or dns from docker0 whih must have dnsmsq runs on it, or other dhcp server# if no dhcp client or server, you need to config an ip and setup dns server, route, then VM can access outside network!!!# =====================================Expose vm port for outside like:ssh ==============================================# As the it uses docker0 network like 172.17.0.x which is private address# Inside VM it can access external network as docker already setup iptables for us automatically.# but outside can NOT access VM as it&#x27;s private network, in order to let outisde acesss VM like port 22 or 80# you have to expose VM port to Host Port by iptables like this.# expose VM port 22 to host port 6622!!! VM IP: 172.17.0.2$ cat exponse.shiptables -t nat -C DOCKER ! -i docker0 -p tcp -m tcp --dport 6622 -j DNAT --to-destination 172.17.0.2:22 &gt;/dev/null 2&gt;&amp;1if [ $? -ne 0 ]; then  iptables -t nat -A DOCKER ! -i docker0 -p tcp -m tcp --dport 6622 -j DNAT --to-destination 172.17.0.2:22fiiptables -t filter -C DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 6622 -j ACCEPT &gt;/dev/null 2&gt;&amp;1if [ $? -ne 0 ]; then  iptables -A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 6622 -j ACCEPTfi

Tap port with linux bridge example
ovs bridge# show ovs bridge$ ovs-vsctl show23b480b4-4a65-4163-8840-439ef102449e    Bridge ovs-br0        Port enp0s3            Interface enp0s3        Port ovs-br0            Interface ovs-br0                type: internal    ovs_version: &quot;2.16.0&quot;# touch /etc/ovs-ifup#! /bin/shifconfig &quot;$1&quot; 0.0.0.0 upbr=&#x27;ovs-br0&#x27;ovs-vsctl add-port $&#123;br&#125; $1# chmod +x /etc/ovs-ifup# touch /etc/ovs-ifdown#! /bin/shifconfig &quot;$1&quot; 0.0.0.0 downbr=&#x27;ovs-br0&#x27;ovs-vsctl del-port $&#123;br&#125; $1# chmod +x /etc/ovs-ifdown# run vm with network$ /usr/libexec/qemu-kvm -kernel kernel-4.14.121 -initrd initramfs.img -nographic -append &quot;console=ttyS0&quot; -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -netdev tap,id=n1,ifname=vnet0,script=/etc/ovs-ifup,downscript=/etc/ovs-ifdown -device virtio-net,netdev=n1# Or with vhost-net enabled, then check vhost worker thread by ps -ef | grep vhost-$qemu_pid$ /usr/libexec/qemu-kvm -kernel kernel-4.14.121 -initrd initramfs.img -nographic -append &quot;console=ttyS0&quot; -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -netdev tap,id=n1,ifname=vnet0,vhost=on,script=/etc/ovs-ifup,downscript=/etc/ovs-ifdown -device virtio-net,netdev=n1$ ovs-vsctl show23b480b4-4a65-4163-8840-439ef102449e    Bridge ovs-br0        Port vnet0            Interface vnet0        Port enp0s3            Interface enp0s3        Port ovs-br0            Interface ovs-br0                type: internal    ovs_version: &quot;2.16.0&quot;# vnet0 is created by qemu automatically$ ethtool  -i vnet0driver: tunversion: 1.6firmware-version: expansion-rom-version: bus-info: tap         -----------------&gt;tap devicesupports-statistics: nosupports-test: nosupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: no# then inside vm, if you have dhcp client, it will try get ip or dns from docker0 which has dnsmasq runs on it# if no dhcp client or server, you need to config an ip and setup dns server, route, then VM can access outside network!!!

Tap device with OVS
Kernel and rootfsChange vm setting runtimeThe QEMU Monitor Protocol (QMP) is a JSON-based protocol which allows applications to communicate with a QEMU instance. there are several ways to talk with QEMU instance.

virsh&#x2F;libvirt way using ‘qemu-monitor-command’ which provided by virsh command
if qemu-instance is not started by libvirt or no virsh installed, talk with qmp socket directly.
use telnet, nc over tcp qmp socket with json format
use nc, socat over unix qmp socket with json format, while qemu-shell with human way as it’s python tool that does json encode for you.



Same thing happens for HMP(human machine protocol) as well who creates monitor socket for talking.
Qemu parameters  

-monitor dev    redirect the monitor to char device ‘dev’, will create monitor socket.
-qmp dev        like -monitor but opens in ‘control’ mode, will create qmp socket. same with &#x3D;&#x3D; -monitor chardev=mon0,mode=control

# ==================================QMP socket===================================================================================# -qmp tcp:localhost:1234,server,nowaitSame as below (qmp uses tcp socket)# -chardev socket,id=mon0,host=localhost,port=1234,server,nowait -mon chardev=mon0,mode=control# qemu instance sends output with pretty json# -chardev socket,id=mon0,host=localhost,port=1234,server,nowait -mon chardev=mon0,mode=control,pretty=on# qmp uses unix socket# -qmp unix:/var/run/qmp.sock,server,nowait$ nc -U /var/run/qmp.sock &#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;nano&quot;: 1, &quot;micro&quot;: 0, &quot;minor&quot;: 9, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot; (-dirty)&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;# -chardev socket,id=mon0,path=/var/run/qmp.sock,server,nowait -mon chardev=mon0,id=monitor,mode=control$ nc -U /var/run/qmp.sock &#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;nano&quot;: 1, &quot;micro&quot;: 0, &quot;minor&quot;: 9, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot; (-dirty)&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;$# ==================================Monitor socket===================================================================================# -chardev socket,id=mon0,path=/var/run/monitor.sock,server,nowait -mon chardev=mon0,id=monitor# if monitor with readline mode(default), only support human like command and output is formated$ nc -U /var/run/monitor.sock QEMU 2.9.0.1 monitor - type &#x27;help&#x27; for more information(qemu) (qemu) info blockinfo blockide1-cd0: [not inserted]    Removable device: not locked, tray closedfloppy0: [not inserted]    Removable device: not locked, tray closedsd0: [not inserted]    Removable device: not locked, tray closed

NOTE

both sockets(if monitor in control mode) support QMP(json based) and HMP(human machine protocol), they proivde different commands for similiar purpose, 
The outputs are different, QMP has more details and with json output, HMP is different.
if monitor socket with readline mode, only support human commands and output is formated.

Suggestion

With virsh&#x2F;libvirt

Use virsh qemu-monitor-command


Without virsh&#x2F;libvirt

For CLI troubleshooting only, run monitor socket with readline mode, provides man subcommands
For programable, run monitor socket with control mode, or qmp socket, but you still has tool like qemu-shell which proivdes CLI for user to use with man subcommands. qemu-shell similiar to qemu-monitor-command


  # without qmp-shell, when -nographic option is in use, you can switch between the monitor console # by pressing Ctrl–a, then press c, back to console ctrl-a, then press c again.$ ./qmp-shell  /var/run/qmp.sockWelcome to the QMP low-level shell!Connected to QEMU 2.12.0(QEMU) query-block&#123;&quot;return&quot;: [&#123;&quot;locked&quot;: false, &quot;tray_open&quot;: false, &quot;io-status&quot;: &quot;ok&quot;, &quot;qdev&quot;: &quot;/machine/unattached/device[22]&quot;, &quot;removable&quot;: true, &quot;device&quot;: &quot;ide1-cd0&quot;, &quot;type&quot;: &quot;unknown&quot;&#125;, &#123;&quot;device&quot;: &quot;floppy0&quot;, &quot;type&quot;: &quot;unknown&quot;, &quot;qdev&quot;: &quot;/machine/unattached/device[15]&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true&#125;, &#123;&quot;device&quot;: &quot;sd0&quot;, &quot;type&quot;: &quot;unknown&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true&#125;]&#125;# HMP format$ ./qmp-shell -H /var/run/qmp.sockWelcome to the HMP shell!Connected to QEMU 2.12.0(QEMU) info blockide1-cd0: [not inserted]    Attached to:      /machine/unattached/device[22]    Removable device: not locked, tray closedfloppy0: [not inserted]    Attached to:      /machine/unattached/device[15]    Removable device: not locked, tray closedsd0: [not inserted]    Removable device: not locked, tray closed(QEMU) $ ./qmp-shell -p /var/run/qmp.sockWelcome to the QMP low-level shell!Connected to QEMU 2.12.0(QEMU) query-block&#123;    &quot;return&quot;: [        &#123;            &quot;locked&quot;: false,             &quot;tray_open&quot;: false,             &quot;io-status&quot;: &quot;ok&quot;,             &quot;qdev&quot;: &quot;/machine/unattached/device[22]&quot;,             &quot;removable&quot;: true,             &quot;device&quot;: &quot;ide1-cd0&quot;,             &quot;type&quot;: &quot;unknown&quot;        &#125;,         &#123;            &quot;device&quot;: &quot;floppy0&quot;,             &quot;type&quot;: &quot;unknown&quot;,             &quot;qdev&quot;: &quot;/machine/unattached/device[15]&quot;,             &quot;locked&quot;: false,             &quot;removable&quot;: true        &#125;,         &#123;            &quot;device&quot;: &quot;sd0&quot;,             &quot;type&quot;: &quot;unknown&quot;,             &quot;locked&quot;: false,             &quot;removable&quot;: true        &#125;    ]&#125;(QEMU) 
command outputMost QMP commands
&quot;blockdev-add&quot;&quot;chardev-remove&quot;&quot;chardev-add&quot;&quot;query-cpu-definitions&quot;&quot;query-machines&quot;&quot;device-list-properties&quot;&quot;change-vnc-password&quot;&quot;nbd-server-stop&quot;&quot;nbd-server-add&quot;&quot;nbd-server-start&quot;&quot;query-block-jobs&quot;&quot;query-balloon&quot;&quot;query-migrate-capabilities&quot;&quot;migrate-set-capabilities&quot;&quot;query-migrate&quot;&quot;query-command-line-options&quot;&quot;query-uuid&quot;&quot;query-name&quot;&quot;query-spice&quot;&quot;query-vnc&quot;&quot;query-mice&quot;&quot;query-status&quot;&quot;query-kvm&quot;&quot;query-pci&quot;&quot;query-cpus&quot;&quot;query-blockstats&quot;&quot;query-block&quot;&quot;query-chardev&quot;&quot;query-events&quot;&quot;query-commands&quot;&quot;query-version&quot;&quot;human-monitor-command&quot;&quot;qmp_capabilities&quot;&quot;expire_password&quot;&quot;set_password&quot;&quot;block_set_io_throttle&quot;&quot;block_passwd&quot;&quot;query-fdsets&quot;&quot;remove-fd&quot;&quot;add-fd&quot;&quot;closefd&quot;&quot;getfd&quot;&quot;set_link&quot;&quot;balloon&quot;&quot;block_resize&quot;&quot;netdev_del&quot;&quot;netdev_add&quot;&quot;client_migrate_info&quot;&quot;migrate_set_downtime&quot;&quot;migrate_set_speed&quot;&quot;query-migrate-cache-size&quot;&quot;migrate-set-cache-size&quot;&quot;migrate_cancel&quot;&quot;migrate&quot;&quot;cpu-add&quot;&quot;cpu&quot;&quot;device_del&quot;&quot;device_add&quot;&quot;system_powerdown&quot;&quot;system_reset&quot;&quot;system_wakeup&quot;

QMP by virsh qemu-monitor-command
# 6095 is domain id# HMP protocol$ virsh qemu-monitor-command –hmp 6095 info blockdrive-virtio-disk0: removable=0 file=/export/jvirt/jcs-agent/instances/i-sm6pxr4068/vda backing_file=/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq backing_file_depth=1 ro=0 drv=qcow2 encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0# QMP protocol --pretty means format json output$ virsh qemu-monitor-command  6095 --pretty &#x27;&#123; &quot;execute&quot;: &quot;query-block&quot;&#125;&#x27;&#123;  &quot;return&quot;: [    &#123;      &quot;device&quot;: &quot;drive-virtio-disk0&quot;,      &quot;locked&quot;: false,      &quot;removable&quot;: false,      &quot;inserted&quot;: &#123;        &quot;iops_rd&quot;: 0,        &quot;image&quot;: &#123;          &quot;backing-image&quot;: &#123;            &quot;virtual-size&quot;: 42949672960,            &quot;filename&quot;: &quot;/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq&quot;,            &quot;cluster-size&quot;: 65536,            &quot;format&quot;: &quot;qcow2&quot;,            &quot;actual-size&quot;: 24866193408,            &quot;format-specific&quot;: &#123;              &quot;type&quot;: &quot;qcow2&quot;,              &quot;data&quot;: &#123;                &quot;compat&quot;: &quot;1.1&quot;,                &quot;lazy-refcounts&quot;: false              &#125;            &#125;,            &quot;dirty-flag&quot;: false          &#125;,          &quot;virtual-size&quot;: 42949672960,          &quot;filename&quot;: &quot;/export/jvirt/jcs-agent/instances/i-sm6pxr4068/vda&quot;,          &quot;cluster-size&quot;: 65536,          &quot;format&quot;: &quot;qcow2&quot;,          &quot;actual-size&quot;: 21068431360,          &quot;format-specific&quot;: &#123;            &quot;type&quot;: &quot;qcow2&quot;,            &quot;data&quot;: &#123;              &quot;compat&quot;: &quot;1.1&quot;,              &quot;lazy-refcounts&quot;: false            &#125;          &#125;,          &quot;backing-filename&quot;: &quot;/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq&quot;,          &quot;dirty-flag&quot;: false        &#125;,        &quot;iops_wr&quot;: 0,        &quot;ro&quot;: false,        &quot;backing_file_depth&quot;: 1,        &quot;drv&quot;: &quot;qcow2&quot;,        &quot;iops&quot;: 0,        &quot;bps_wr&quot;: 0,        &quot;backing_file&quot;: &quot;/export/jvirt/jcs-agent/instances/_base/img-8sdjnj4qbq&quot;,        &quot;encrypted&quot;: false,        &quot;bps&quot;: 0,        &quot;bps_rd&quot;: 0,        &quot;file&quot;: &quot;/export/jvirt/jcs-agent/instances/i-sm6pxr4068/vda&quot;,        &quot;encryption_key_missing&quot;: false      &#125;,      &quot;type&quot;: &quot;unknown&quot;    &#125;  ],  &quot;id&quot;: &quot;libvirt-8302918&quot;&#125;

QMP over tcp socket
# run your qemu instance:   -qmp tcp:127.0.0.1:12345,server,nowait$ /usr/libexec/qemu-kvm  -kernel ./kernel -initrd ./initramfs.img -nographic -append &quot;console=ttyS0&quot; -qmp tcp:127.0.0.1:12345,server,nowait -serial mon:stdio[root@dev jason]# nc localhost 12345&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;nano&quot;: 1, &quot;micro&quot;: 0, &quot;minor&quot;: 9, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot; (-dirty)&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125; # must run this firstly&#123;&quot;return&quot;: &#123;&#125;&#125;# HMP&#123;&quot;execute&quot;: &quot;human-monitor-command&quot;, &quot;arguments&quot;: &#123;&quot;command-line&quot;: &quot;info block&quot;&#125;&#125;&#123;&quot;return&quot;: &quot;ide1-cd0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n\r\nfloppy0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n\r\nsd0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n&quot;&#125;# QMP&#123;&quot;execute&quot;: &quot;query-block&quot;&#125;&#123;&quot;return&quot;: [&#123;&quot;io-status&quot;: &quot;ok&quot;, &quot;device&quot;: &quot;ide1-cd0&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true, &quot;tray_open&quot;: false, &quot;type&quot;: &quot;unknown&quot;&#125;, &#123;&quot;device&quot;: &quot;floppy0&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true, &quot;type&quot;: &quot;unknown&quot;&#125;, &#123;&quot;device&quot;: &quot;sd0&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true, &quot;type&quot;: &quot;unknown&quot;&#125;]&#125;ctrl +C[root@dev jason]# telnet 127.0.0.1 12345Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is &#x27;^]&#x27;.&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;nano&quot;: 1, &quot;micro&quot;: 0, &quot;minor&quot;: 9, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot; (-dirty)&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125; # must run this firstly&#123;&quot;return&quot;: &#123;&#125;&#125;&#123; &quot;execute&quot;: &quot;query-commands&quot; &#125; # check all QMP commands support# HMP&#123;&quot;execute&quot;: &quot;human-monitor-command&quot;, &quot;arguments&quot;: &#123;&quot;command-line&quot;: &quot;info block&quot;&#125;&#125;&#123;&quot;return&quot;: &quot;ide1-cd0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n\r\nfloppy0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n\r\nsd0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n&quot;&#125;# QMP&#123;&quot;execute&quot;: &quot;query-block&quot;&#125;&#123;&quot;return&quot;: [&#123;&quot;io-status&quot;: &quot;ok&quot;, &quot;device&quot;: &quot;ide1-cd0&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true, &quot;tray_open&quot;: false, &quot;type&quot;: &quot;unknown&quot;&#125;, &#123;&quot;device&quot;: &quot;floppy0&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true, &quot;type&quot;: &quot;unknown&quot;&#125;, &#123;&quot;device&quot;: &quot;sd0&quot;, &quot;locked&quot;: false, &quot;removable&quot;: true, &quot;type&quot;: &quot;unknown&quot;&#125;]&#125;ctrl + &#125;telnet&gt;quit

QMP over unix socket
$ /usr/libexec/qemu-kvm  -kernel ./kernel -initrd ./initramfs.img -nographic -append &quot;console=ttyS0&quot;  -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio$ nc -U /var/run/qmp.sock&#123;&quot;QMP&quot;: &#123;&quot;version&quot;: &#123;&quot;qemu&quot;: &#123;&quot;nano&quot;: 1, &quot;micro&quot;: 0, &quot;minor&quot;: 9, &quot;major&quot;: 2&#125;, &quot;package&quot;: &quot; (-dirty)&quot;&#125;, &quot;capabilities&quot;: []&#125;&#125;&#123; &quot;execute&quot;: &quot;qmp_capabilities&quot; &#125; # must run this firstly&#123;&quot;return&quot;: &#123;&#125;&#125;# HMP&#123;&quot;execute&quot;: &quot;human-monitor-command&quot;, &quot;arguments&quot;: &#123;&quot;command-line&quot;: &quot;info block&quot;&#125;&#125;&#123;&quot;return&quot;: &quot;ide1-cd0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n\r\nfloppy0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n\r\nsd0: [not inserted]\r\n    Removable device: not locked, tray closed\r\n&quot;&#125;

disk imageDifferent hypervisor softwares uses different image format, here is a list of them.

VDI is the native format of VirtualBox. Other virtualization software generally don’t support VDI

VMDK is developed by and for VMWare, but VirtualBox also support it. This format might be the the best choice for you because you want wide compatibility with other virtualization software. it has smaller disk size than qcow2

VHD is the native format of Microsoft Virtual PC. Windows Server 2012 introduced VHDX as the successor to VHD, but VirtualBox does not support VHDX.

QCOW is the old original version of the qcow format. It has been superseded by qcow2, which VirtualBox does not support.

QED was an abandoned enhancement of qcow2. QEMU advises against using QED(not use it).


qemu-img is a tool which supports converting from one image format to another. if you want to run VM between hypervisors
Operationqemu-img allows you to create, convert and modify images offline. It can handle all image formats supported by QEMU
$ qemu-img create -f qcow2 /var/lib/libvirt/images/disk1.img 10G# Raw type: Raw disk image format is default. This format has the advantage of being simple and easily exportable to all other emulators$ dd if=/dev/zero of=/var/lib/libvirt/images/disk1.img bs=2M count=5120 status=progress$ qemu-img create -f raw /var/lib/libvirt/images/disk1.img 10G$ qemu-img info /var/lib/libvirt/images/disk1.img# vhd to qcow2$ qemu-img convert -p -f vpc -O qcow2 centos6.9.vhd centos6.9.qcow2# vmdk to qcow$ qemu-img convert -p -f vmdk -O qcow2 centos6.9.vmdk centos6.9.qcow2# resize a disk, add 3G to guest disk# Can&#x27;t resize an image which has snapshots# Note the new 3G is not seen by guest right now you have to create partition on it using parted command$ qemu-img resize data.qcow2 +3G# shrink the free space(not used, unallocated in guest)$ qemu-img resize --shrink data.qcow2 -3G# to 100G$ qemu-img resize data.qcow2 100G$ qemu-img info data.qcow2# create snapshots of image(snapshot stores in image itself)$ qemu-img snapshot -c org CentOS-7-x86_64-GenericCloud.qcow2# show snapshots of image$ qemu-img snapshot -l CentOS-7-x86_64-GenericCloud.qcow2 Snapshot list:ID        TAG                 VM SIZE                DATE       VM CLOCK1         1646107061             268M 2022-03-01 11:57:41   00:15:03.8572         1646107194             268M 2022-03-01 11:59:54   00:17:14.997# delete snapshots with given ID$ qemu-img snapshot -d  1  CentOS-7-x86_64-GenericCloud.qcow2

NOTE: make sure make a copy of disk before operation
backing file(qcow2)In essence, QCOW2(Qemu Copy-On-Write) gives you an ability to create a base-image, and create several ‘disposable’ copy-on-write overlay disk images on top of the base image(also called backing file). Backing files and overlays are extremely useful to rapidly instantiate thin-privisoned virtual machines(more on it below). Especially quite useful in development &amp; test environments, so that one could quickly revert to a known state &amp; discard the overlay. It can also be used to start 100 virtual machines from a common backing image, thus saving space.
use case
.--------------.    .-------------.    .-------------.    .-------------.|              |    |             |    |             |    |             || RootBase     |&lt;---| Overlay-1   |&lt;---| Overlay-1A  &lt;--- | Overlay-1B  || (raw/qcow2)  |    | (qcow2)     |    | (qcow2)     |    | (qcow2)     |&#x27;--------------&#x27;    &#x27;-------------&#x27;    &#x27;-------------&#x27;    &#x27;-------------&#x27;The above figure illustrates - RootBase is the backing file for Overlay-1, which in turn is backing file for Overlay-2, which in turn is backing file for Overlay-3..-----------.   .-----------.   .------------.  .------------.  .------------.|           |   |           |   |            |  |            |  |            || RootBase  |&lt;--- Overlay-1 |&lt;--- Overlay-1A &lt;--- Overlay-1B &lt;--- Overlay-1C ||           |   |           |   |            |  |            |  | (Active)   |&#x27;-----------&#x27;   &#x27;-----------&#x27;   &#x27;------------&#x27;  &#x27;------------&#x27;  &#x27;------------&#x27;   ^    ^   |    |   |    |       .-----------.    .------------.   |    |       |           |    |            |   |    &#x27;-------| Overlay-2 |&lt;---| Overlay-2A |   |            |           |    | (Active)   |   |            &#x27;-----------&#x27;    &#x27;------------&#x27;   |   |   |            .-----------.    .------------.   |            |           |    |            |   &#x27;------------| Overlay-3 |&lt;---| Overlay-3A |                |           |    | (Active)   |                &#x27;-----------&#x27;    &#x27;------------&#x27;The above figure is just another representation which indicates, we can use a &#x27;single&#x27; backing file, and create several overlays -- which can be used further, to create overlays on top of them.NOTE: Backing files are always opened read-only. In other words, once an overlay is created, its backing file should not be modified(as the overlay depends on a particular state of the backing file)

# base &lt;- sn1 &lt;- sn2 &lt;- sn3# create sn1$ qemu-img create -b /home/data/tmp/base.img -f qcow2 /home/data/tmp/sn1.qcow2$ qemu-img info /home/data/tmp/sn1.qcow2qemu-img info /home/data/tmp/sn1.qcow2image: /home/data/tmp/sn1.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/base.imgFormat specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false# create sn2$ qemu-img create -b /home/data/tmp/sn1.qcow2 -f qcow2 /home/data/tmp/sn2.qcow2$ qemu-img info /home/data/tmp/sn2.qcow2image: /home/data/tmp/sn2.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/sn1.qcow2Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false# create sn3$ qemu-img create -b /home/data/tmp/sn2.qcow2 -f qcow2 /home/data/tmp/sn3.qcow2$ qemu-img info /home/data/tmp/sn3.qcow2image: /home/data/tmp/sn3.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/sn2.qcow2Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false# an entire backing chain can be recursively $ qemu-img info --backing-chain /home/data/tmp/sn3.qcow2image: /home/data/tmp/sn3.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/sn2.qcow2Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: falseimage: /home/data/tmp/sn2.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/sn1.qcow2Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: falseimage: /home/data/tmp/sn1.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/base.imgFormat specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: falseimage: /home/data/tmp/base.imgfile format: rawvirtual size: 25G (26843545600 bytes)disk size: 16M# size of overlays is smaller!!!$ ls  -alh sn*.qcow2-rw-r--r-- 1 root root 193K Aug  2 16:38 sn1.qcow2-rw-r--r-- 1 root root 193K Aug  2 16:39 sn2.qcow2-rw-r--r-- 1 root root 193K Aug  2 16:39 sn3.qcow2# merge sn2 with sn1(commit sn2 changes to sn1)$ qemu-img commit /home/data/tmp/sn2.qcow2$ qemu-img rebase -u -b /home/data/tmp/sn1.qcow2 /home/data/tmp/sn3.qcow2# we need to rebase sn3 as before it points to sn2 as backing file!!!$ qemu-img info --backing-chain /home/data/tmp/sn3.qcow2image: /home/data/tmp/sn3.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/sn1.qcow2Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: falseimage: /home/data/tmp/sn1.qcow2file format: qcow2virtual size: 25G (26843545600 bytes)disk size: 196Kcluster_size: 65536backing file: /home/data/tmp/base.imgFormat specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: falseimage: /home/data/tmp/base.imgfile format: rawvirtual size: 25G (26843545600 bytes)disk size: 16M

FAQBoot with disk&#x2F;diskless# root filesystem is in an ext2 &quot;hard disk&quot;$ /usr/libexec/qemu-kvm -kernel normal/bzImage -drive file=rootfs.ext2# root filesystem is in initramfs$ /usr/libexec/qemu-kvm -kernel normal/bzImage -initrd initramfs.img# full command to run$ /usr/libexec/qemu-kvm  -kernel normal/bzImage -initrd initramfs.img -nographic -append &quot;console=ttyS0&quot;# root filesystem is built in kernel$ /usr/libexec/qemu-kvm -kernel with_initramfs/bzImage# Neither -drive nor -initrd are given.# with_initramfs/bzImage is a kernel compiled with options identical to normal/bzImage, except for one: CONFIG_INITRAMFS_SOURCE=initramfs.img pointing to the exact same CPIO as from the -initrd example.

how to terminate qemu process from console# must have  -serial mon:stdio, otherwise Ctrl+A, then `X` not working$ /usr/libexec/qemu-kvm  -kernel ./kernel -initrd ./initramfs.img -nographic -append &quot;console=ttyS0&quot;  -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio

Ctrl + A, then press X

$ virt-install --boot kernel=./kernel,initrd=./initramfs.img,kernel_args=&quot;console=ttyS0,115200n8&quot; --name $VM_NAME --memory 500 --vcpus 1 --disk none --os-type linux --graphics none --network default

Ctrl + ] to quit virt-install

how to start qemu with vncAdd vnc parameter with specified ID, qemu-process will listen on particular port as for vnc id 0==5900, so id 88 means qemu-process will listen on 5988.
$ id=88$ /usr/libexec/qemu-kvm  -kernel ./kernel -initrd ./initramfs.img -nographic -append &quot;console=ttyS0&quot;  -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -vnc 0.0.0.0:$id# OR let qemu select the vnc port automatically$ /usr/libexec/qemu-kvm  -kernel ./kernel -initrd ./initramfs.img -nographic -append &quot;console=ttyS0&quot;  -qmp unix:/var/run/qmp.sock,server,nowait -serial mon:stdio -vnc 0.0.0.0$ netstat -nltp | grep kvm5:tcp        0      0 0.0.0.0:5988            0.0.0.0:*               LISTEN      8566/qemu-kvm  

how to mount raw disk# create raw disk by dd or qemu-img create$ dd if=/dev/zero of=disk.raw bs=1024k seek=25600 count=0# check disk info (type) etc$ qemu-img info disk.rawimage: disk.rawfile format: rawvirtual size: 25G (26843545600 bytes)disk size: 13M# format with xfs$ mkfs.xfs disk.raw$ blkid disk.rawdisk.raw: UUID=&quot;aa26d14d-267f-477b-b1a1-3848c448a0b3&quot; TYPE=&quot;xfs&quot; ########################################### mount raw disk ####################### make disk image as a block device!!!# The -f option will search for the next free loop device to attach the image to. # The -P option will trigger a scan for partitions on the attached image and create devices for each partition detected# losetup only supports raw disk, not qcow2#================ one way ====================================$ losetup -f -P disk.raw$ losetup -lNAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE/dev/loop0         0      0         0  0 /root/jason/disk.raw# mount block device to a dir, then we can access its files$ mount /dev/loop0  /tmp/raw#================ another way ====================================$ mount -t xfs disk.raw /mnt/disk

after enter qmp command, no outputThis is probably, qmp.sock is connected with another client, as one qmp.sock can only talk with only on client!!!, if multiple clients want to connect with qemu instance, create multiple qmp socks when start qemu instance like this
$ /usr/libexec/qemu-kvm  -kernel ./kernel -initrd ./initramfs.img -nographic -append &quot;console=ttyS0&quot;  -qmp unix:/var/run/qmp1.sock,server,nowait  -qmp unix:/var/run/qmp2.sock,server,nowait -serial mon:stdio

difference between -net, -netdev and -nicIn short, the -net is the legacy option, while -netdev comes in to solve issue present for -net, the newest way -nic from 2.12 is easiest way to set up an interface. 

The -net option can create either a front-end or a back-end (but has disadvanges than -nic)
-netdev can only create a back-end, use -device for front end
Asingle occurrence of -nic will create both a front-end and a back-end.

Suggestion

The new -nic option gives you an easy and quick way to configure the networking of your guest.
For more detailed configuration, e.g. when you need to tweak the details(like queue size, buffer etc) of the emulated NIC hardware, you can use -device together with -netdev.
The -net option should be avoided these days unless you really want to configure a set-up with a hub between the front-ends and back-ends.

-nic like a wrapper of -netdev and -deivce pair, easy to use but less control.
-net(legacy option)QEMU’s initial way of configuring the network for the guest was the -net option, the emulated NIC and the host back-end are not directly connected. They are rather both connected to an emulated hub by default vlan0 (called “vlan” in older versions of QEMU). Therefore, if you start QEMU with -net nic,model=e1000 -net user -net nic,model=virtio -net tap for example, you get a setup where all the front-ends and back-ends are connected together via a hub(vlan)
front end -net nic,model=xyz or -net nic,model=virtio and  backend -net user or -net tap (e.g. -net user for the SLIRP back-end)

That means the e1000 NIC also gets the network traffic from the virtio-net NIC and both host back-ends, this can be solved by giving one hub for each nic, for example -net nic,model&#x3D;e1000,vlan&#x3D;0 -net user,vlan&#x3D;0 -net nic,model&#x3D;virtio,vlan&#x3D;1 -net tap,vlan&#x3D;1 moves the virtio-net NIC and the “tap” back-end to a second hub (with ID #1), Please note that the **“vlan” parameter will be dropped in QEMU v3.0 since the term was rather confusing (it’s not related to IEEE 802.1Q for example) *** and caused a lot of misconfigurations in the past.
-netdevTo configure a network connection where the emulated NIC is directly connected to a host network back-end, without a hub in between, the well-established solution is to use the -netdev option for the back-end, together with -device for the front-end. 
-netdev user,id=n1 -device e1000,netdev=n1 -netdev tap,id=n2 -device virtio-net,netdev=n2

Now while -netdev together with -device provide a very flexible and extensive way to configure a network connection, there are still two drawbacks with this option pair which prevented us from deprecating the legacy -net option completely:

The -device option can only be used for pluggable NICs. Boards (e.g. embedded boards) which feature an on-board NIC cannot be configured with -device yet, so -net nic,netdev&#x3D; must be used here instead.
In some cases, the -net option is easier to use (less to type). For example, assuming you want to set up a “tap” network connection and your default scripts &#x2F;etc&#x2F;qemu-ifup and -down are already in place, it’s enough to type -net nic -net tap to start your guest. To do the same with -netdev, you always have to specify an ID here, too, for example like this: -netdev tap,id&#x3D;n1 -device e1000,netdev&#x3D;n1.

-nicLooking at the disadvantages listed above, users could benefit from a convenience option that:

is easier to use (and shorter to type) than -netdev ,id&#x3D; -device ,netdev&#x3D;
can be used to configure on-board &#x2F; non-pluggable NICs, too
does not place a hub between the NIC and the host back-end.

This is where the new -nic option kicks in!! this option can be used to configure both the guest’s NIC hardware and the host back-end in one go, instead of -netdev tap,id=n1 -device e1000,netdev=n1 you can simply type -nic tap,model=e1000,  you can simply run QEMU with -nic model&#x3D;help. Beside being easier to use, the -nic option can be used to configure on-board NICs, For machines that have on-board NICs, the first -nic option configures the first on-board NIC, the second -nic option configures the second on-board NIC, and so forth.
qemu-kvm and qemu-kvm-evqemu-kvm and qemu-kvm-ev are usually built from the same src.rpm. Some newer or advanced virtualization features have been implemented in qemu-kvm-ev which are not able to be backported to qemu-kvm for compatibility reasons. Also, recently qemu-kvm-ev has a newer qemu-kvm version than the one provided by qemu-kvm being qemu-kvm-ev rebuilt from Red Hat Enterprise Virtualization.
why qemu needs nvdimm??Really fast writes particularly interesting for:

In-memory databases – get persistence for free*!
Databases – transaction logs
File &amp; storage systems – frequently updated metadata

why PCI address(in guest) is not the same as we set?Most of that each PCI device would show up in the guest OS with a PCI address that matches the one present in command line set by user, but that’s not guaranteed to happen and will in fact not be the case in all but the simplest scenarios. refer to Qemu pci address
reduce qcow file size on hostThe virt-sparsify utility, as we just saw, is what we want to use if we are dealing with a qcow2 image, which by default makes use of thin-provisioning, and we want to make the space previously allocated on the disk image and now not used anymore, available again on the host.
$virt-sparsify --in-place disk.qcow2

how to enable IOMMU for VMEnable IOMMU for VM, so that we can start embeded VM with device passthrough.
$ qemu-kvm -machine q35,accel=kvm -cpu host -device intel-iommu ... this like enable iommu from hardware and enable it in bios, later we need add parameter to boot command line with intel_iommu=on iommu=pt.
Inside this vm, we can start another vm with passthrough device from host(parent vm).
REF
Install qemu-shell python tool
qmp commands to change qemu instance runtime
hmp commands of qemu in code
qemu parameter
libvirt with vhost on&#x2F;off

]]></content>
      <categories>
        <category>kvm</category>
        <category>qemu</category>
      </categories>
      <tags>
        <tag>qemu</tag>
      </tags>
  </entry>
  <entry>
    <title>Single_Packet_Authorization</title>
    <url>/2020/11/12/single-packet-authorization/</url>
    <content><![CDATA[OverviewSingle Packet Authorization (SPA) is defined as the communication of authentication information over spa server port, together with the dynamic reconfiguration of a default-drop firewall policy to allow access to services that would otherwise be blocked, SPA communicates authentication information within the payload portion of a single packet. Because packet payloads are used, SPA offers many enhancements over PK such as stronger usage of cryptography, protection from replay attacks, minimal network footprint (in terms of what IDS’s may alert on - PK sequences look like port scans after all), the ability to transmit full commands and complex access requests, and better performance.
For simple, access service is blocked by firewall, SPA client requests to open that service for itself, SPA server authenticates the request, adds proper firewall rules to that client, after that client can access server as normal.


WorkflowA basic outline for using fwknop to conceal an SSH daemon(or any service) with Single Packet Authorization (SPA) involves the following steps. This assumes an SPA client system (hostname: spaclient), and an SPA server system spaserver.domain.com where fwknopd is installed and the SSH daemon listens:

Generate encryption and HMAC keys with fwknop –key-gen.
Transfer the keys you just generated fwknopd to the server (this is where SSHD is listening too).
Start fwknopd and deploy a default-drop firewall policy against all inbound SSH connections.
From anywhere on the Internet, use the fwknop client to send an SPA packets and have fwknopd open the firewall.
Use your SSH client as usual now that you have access. No one else can even see that SSHD is listening.


NOTE

SPA sends request in payload to grant access(add firewall rule at spa server) for IP in the payload.
Firewall(iptables) checks protocol headers for access.

InstallHere are the steps to install fwknop from source code, first make sure you have dev machine and env to compile from source, install from package is available for ubuntu as well, but here I want to share how to install from source.
install from package
# Ubuntu18$ apt-get install -y fwknop-server$ apt-get install -y fwknop-client# Centos7yum install -y fwknop

install from source
# install dependencies$ apt-get update$ apt-get install -y build-essential autoconf autotools-dev libtool textinfo git
install server
$ yes yes | git clone https://github.com/mrash/fwknop.git$ cd fwknop/$ ./autogen.sh$ ./configure --disable-client --prefix=/usr$ make$ make install# make sure install correctly$ fwknopd -V# enable systemd service, then you can check fwknopd by `service fwknopd status`$ cp extras/systemd/fwknopd.service  /lib/systemd/system/# config location# /usr/sbin/fwknopd# /usr/etc/fwknop/fwknopd.conf# /usr/etc/fwknop/access.conf

install client
$ yes yes | git clone https://github.com/mrash/fwknop.git$ cd fwknop/$ ./autogen.sh$ ./configure --disable-server --prefix=/usr$ make$ make install# make sure install correctly$ fwknop -V# /usr/bin/fwknop

ExamplesAfter installation, you need to config both client and server together to make them work, but the config depends on network topology you’re using, here are some typical use cases.
Network topology no NAT(same network or public ip), SPA server and service on same machineserver side
# create key for hmac and aes$ fwknopd --key-genKEY_BASE64: +RCO0Gi2oTWbmBaylY4mVsDXzggQKDKwYU2FaLBL0Jo=HMAC_KEY_BASE64: 729XvADuHzSs6I9vz9tVIOFITF/ckAAIZGWVU4Hy4c82kbAbsFzkZRfddNZPr10AJlUCVdtx9D6vNN2XQ6ribQ==# edit /usr/etc/fwknop/access.confSOURCE                      ANYKEY_BASE64                  WO3yTj1oMCoFr3l8WlCLSJAGikvej2BqYgwEcFPwiHg=HMAC_KEY_BASE64             06EL9sbYsYUm396U/H44BnEm/qIOi1iiWVPILv8jWJg3NMm52I4Whu/AyL0v4CnR1IcHV27QUjFJ8NFNllTatA==# edit /usr/etc/fwknopd/fwknopd.confFWKNOP_RUN_DIR              /var/run/fwknop;FWKNOP_PID_FILE             $FWKNOP_RUN_DIR/fwknopd.pid;PCAP_INTF ens160# start fwknopd service$ service fwknopd start$ service fwknopd status# insert two rules at header, the second one before the first one in iptables# hence for established tcp connection, keep it, others default to drop$ iptables -I INPUT 1 -i ens160  -p tcp --dport 22 -j DROP$ iptables -I INPUT 1  -i ens160  -p tcp --dport 22 -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

client side
# we use named config, the config file is at /user/.fwknoprc, create it if no# edit /user/.fwknoprc[default]USE_HMAC                    Y[spaserver-ssl]ALLOW_IP                    10.10.10.10ACCESS                      tcp/22SPA_SERVER                  10.10.10.1KEY_BASE64                  WO3yTj1oMCoFr3l8WlCLSJAGikvej2BqYgwEcFPwiHg=HMAC_KEY_BASE64             06EL9sbYsYUm396U/H44BnEm/qIOi1iiWVPILv8jWJg3NMm52I4Whu/AyL0v4CnR1IcHV27QUjFJ8NFNllTatA==# default section is global setting# each named section can has it own setting# ALLOW_IP, ACCESS, request access for this IP and port# request access for info in spaserver-ssl$ fwknop -n spaserver-ssl# then normal sshssh root@10.10.10.1
Network topology with NAT, service(private address) behind SPA server on different machines

client ip: 192.168.1.2
SPA server ip: 10.10.10.3(public)
SSH server ip: 192.168.100.10

server side
# edit /usr/etc/fwknopd/fwknopd.confENABLE_IPT_FORWARDING      Y;# after client sent SPA request, check iptables rules$ fwknopd --fw-listListing rules in fwknopd iptables chains...Chain FWKNOP_INPUT (1 references)num  target     prot opt source               destinationChain FWKNOP_FORWARD (1 references)num  target     prot opt source               destination1    ACCEPT     tcp  --  2.2.2.2              0.0.0.0/0            tcp dpt:2 /* _exp_1605161576 */Chain FWKNOP_PREROUTING (1 references)num  target     prot opt source               destination1    DNAT       tcp  --  2.2.2.2              0.0.0.0/0            tcp dpt:22 /* _exp_1605161576 */ to:192.168.100.10:2

client side
$ cat /user/.fwknoprc[default]USE_HMAC                    Y[spaserver-ssl]ACCESS                      tcp/22SPA_SERVER                  10.10.10.3KEY_BASE64                  WO3yTj1oMCoFr3l8WlCLSJAGikvej2BqYgwEcFPwiHg=HMAC_KEY_BASE64             06EL9sbYsYUm396U/H44BnEm/qIOi1iiWVPILv8jWJg3NMm52I4Whu/AyL0v4CnR1IcHV27QUjFJ8NFNllTatA==RESOLVE_IP_HTTPS            YNAT_ACCESS                  192.168.100.10:22# with RESOLVE_IP_HTTPS, the real ip in SPA packet, is the external ip, get by fwknop atomically.# but if you know the external ip. use &#x27;ALLOW_IP&#x27; remove &#x27;RESOLVE_IP_HTTPS&#x27;# the external ip for client is 2.2.2.2$ fwknop -n spaserver-ssl$ ssh root@10.10.10.3

Network topology with SNAT(at spa server), service behind SPA server on different machinesAs you know SNAT is after POSTROUTING, that means when SPA server forwards packet, the SRC must be replaced, otherwise packet can not be routed to service server, it’s total SPA server config, client even not care!
# edit /usr/etc/fwknopd/fwknopd.confENABLE_IPT_FORWARDING Y;ENABLE_IPT_SNAT Y;$ service fwknopd restart

Network topology with DNAT(port only)DNAT is needed when spa server does NOT want to grant access to well-known port like ssh(22), but instead grants access to a temporary port which is only known to SPA client who triggers the request for safe.
server
# edit /usr/etc/fwknopd/fwknopd.confENABLE_IPT_LOCAL_NAT        Y;$ service fwknopd restart# after client sent SPA packet$ fwknopd --fw-listListing rules in fwknopd iptables chains...Chain FWKNOP_INPUT (1 references)num  target     prot opt source               destination1    ACCEPT     tcp  --  10.10.10.1           0.0.0.0/0            tcp dpt:22 /* _exp_1605156202 */Chain FWKNOP_FORWARD (1 references)num  target     prot opt source               destination# this the new added rule for DNATChain FWKNOP_PREROUTING (1 references)num  target     prot opt source               destination1    DNAT       tcp  --  10.10.10.1           0.0.0.0/0            tcp dpt:29935 /* _exp_1605156202 */ to:10.10.10.3:22

client
$ cat /user/.fwknoprc[default]USE_HMAC                    Y[spaserver-ssl]ALLOW_IP                    10.10.10.1ACCESS                      tcp/22SPA_SERVER                  10.10.10,3KEY_BASE64                  WO3yTj1oMCoFr3l8WlCLSJAGikvej2BqYgwEcFPwiHg=HMAC_KEY_BASE64             06EL9sbYsYUm396U/H44BnEm/qIOi1iiWVPILv8jWJg3NMm52I4Whu/AyL0v4CnR1IcHV27QUjFJ8NFNllTatA==NAT_LOCAL                   YNAT_RAND_PORT               Y$ fwknop -n spaserver-ssl[+] Randomly assigned port &#x27;29935&#x27; on: &#x27;10.10.10.1,tcp/29935&#x27; will grant access to: &#x27;10.10.10.3,22&#x27;# access by 29935, not port 22$ ssh root@10.10.10.3 -p 29935

Grant access with limit scope(source ip, user, port)By default, spa server allows ANY address with ANY user to request ANY port(on server) access, but we can limit the scope by several variablesserver
# edit /usr/etc/fwknop/access.confSOURCE                      1.1.1.0/24, 2.2.2.2OPEN_PORTS                  tcp/22, tcp/993REQUIRE_USERNAME            bobREQUIRE_SOURCE_ADDRESS      YKEY_BASE64                  WO3yTj1oMCoFr3l8WlCLSJAGikvej2BqYgwEcFPwiHg=HMAC_KEY_BASE64             06EL9sbYsYUm396U/H44BnEm/qIOi1iiWVPILv8jWJg3NMm52I4Whu/AyL0v4CnR1IcHV27QUjFJ8NFNllTatA==

client with user name
$ cat /user/.fwknoprc[default]USE_HMAC                    Y[spaserver-ssl]ALLOW_IP                    10.10.10.1ACCESS                      tcp/22SPA_SERVER                  10.10.10.3KEY_BASE64                  WO3yTj1oMCoFr3l8WlCLSJAGikvej2BqYgwEcFPwiHg=HMAC_KEY_BASE64             06EL9sbYsYUm396U/H44BnEm/qIOi1iiWVPILv8jWJg3NMm52I4Whu/AyL0v4CnR1IcHV27QUjFJ8NFNllTatA==SPOOF_USER                  bob

troubleshooting# check service status$ service fwknopd status# check log if not working$ grep fwknopd /var/log/syslog# dump the config$ fwknopd -D# Listing rules in fwknopd iptables chains...$ fwknopd --fw-list# Flush all firewall rules created by fwknop.$ fwknopd --fw-flush# show more info for SPA packet when sending$ fwknop -n spaserver-ssl -v
FAQif client behinds NAT what special setting neededAs SPA server uses iptables which only see external IP used by client after SNAT, hence we should sent this ip in SPA packet.
# edit /user/.fwknoprc at spa client, but never set ALLOW_IP which is the real ip in SPA messageRESOLVE_IP_HTTPS   Y

REF
SPA tutorial
SPA client man
SPA server man
SPA founder page

]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>SPA</tag>
        <tag>Authentication</tag>
      </tags>
  </entry>
  <entry>
    <title>react-testing</title>
    <url>/2020/07/24/react-testing/</url>
    <content><![CDATA[FrameworksThere are two popular test frameworks for React application, one is Enzyme developed by Airebnb, Enzyme is used by React early days, later on React developed its own framework React Testing Library(RTT), Both two frameworks are specific to React Application, lack of assert, mock which is provided by other framework like chai, sinon and Jest developed by FackBook.

Note: chai, sinon and Jest are independent, they can be used to test pure javascript code, they are test framework for javascript, not specific to React.
The popular way is Enzyme + Jest which is used early by Facebook, now Facebook suggests RTT + Jest both are developed by itself.
Enzyme vs RTTEnzyme allows you to access the internal workings of your components. You can read and set the state, and you can mock children to make tests run faster, the render output is Component, you deal with that.
react-testing-library doesn’t give you any access to the implementation details. It renders the components and provides utility methods to interact with them(Dom output). The idea is that you should communicate with your application in the same way a user would. you act on Dom output not Component.
RTT usage]]></content>
      <categories>
        <category>react</category>
        <category>test</category>
      </categories>
      <tags>
        <tag>react</tag>
        <tag>test framework</tag>
      </tags>
  </entry>
  <entry>
    <title>stress_workload_generator</title>
    <url>/2021/04/15/stress-workload-generator/</url>
    <content><![CDATA[OverviewIt is a simple workload generator for POSIX systems. It imposes a configurable amount of CPU, memory, I&#x2F;O, and disk stress on the system. It will stress test a server for the following features:

CPU compute
Cache thrashing
VM stress
Drive stress
I&#x2F;O syncs
Socket stressing
Context switching
Process creation and termination
It includes over 60 different stress tests, over 50 CPU specific stress tests that exercise floating point, integer, bit manipulation and control flow, over 20 virtual memory stress tests.
Lots of stressors

It is not a benchmark, but is rather a tool designed
StressorThe stress-ng stressors are grouped together in different classes, each class may have many stressors or one stressor, when you start stress, you can run a class(all stressor in that class) or just a specific stressor or several stressors at a time.
Classes:  

cpu - CPU intensive
cpu-cache - stress CPU instruction and&#x2F;or data caches
device - raw device driver stressors
io - generic input&#x2F;output
interrupt - high interrupt load generators
filesystem - file system activity
memory - stack, heap, memory mapping, shared memory stressors
network - TCP&#x2F;IP, UDP and UNIX domain socket stressors
os - core kernel stressors
pipe - pipe and UNIX socket stressors
scheduler - force high levels of context switching
security - AppArmor stressor
vm - Virtual Memory stressor (paging and memory)

Stressors
$ yum install -y stress-ng$ stress-ng --stressoraccess af-alg affinity aio aiol apparmor atomic bad-altstack bad-ioctl bigheap bind-mount binderfs branch brk bsearch cache cap chattr chdir chmod chown chroot clock clone close context copy-file cpu cpu-online crypt cyclic daemon dccp dentry dev dev-shm dir dirdeep dnotify dup dynlib efivar enosys env epoll eventfd exec fallocate fanotify fault fcntl fiemap fifo file-ioctl filename flock fork fp-error fstat full funccall funcret futex get getdent getrandom handle hdd heapsort hrtimers hsearch icache icmp-flood idle-page inode-flags inotify io iomix ioport ioprio io-uring ipsec-mb itimer judy kcmp key kill klog l1cache lease link locka lockbus lockf lockofd longjmp loop lsearch madvise malloc matrix matrix-3d mcontend membarrier memcpy memfd memhotplug memrate memthrash mergesort mincore mknod mlock mlockmany mmap mmapaddr mmapfixed mmapfork mmapmany mq mremap msg msync nanosleep netdev netlink-proc netlink-task nice nop null numa oom-pipe opcode open personality physpage pidfd ping-sock pipe pipeherd pkey poll prctl procfs pthread ptrace pty qsort quota radixsort ramfs rawdev rawpkt rawsock rawudp rdrand readahead reboot remap rename resources revio rlimit rmap rseq rtc schedpolicy sctp seal seccomp secretmem seek sem sem-sysv sendfile session set shellsort shm shm-sysv sigabrt sigchld sigfd sigfpe sigio signal sigpending sigpipe sigq sigrt sigsegv sigsuspend sigtrap skiplist sleep sock sockabuse sockdiag sockfd sockpair sockmany softlockup spawn splice stack stackmmap str stream swap switch symlink sync-file sysbadaddr sysinfo sysinval sysfs tee timer timerfd tlb-shootdown tmpfs tree tsc tsearch tun udp udp-flood unshare uprobe urandom userfaultfd utime vdso vecmath verity vfork vforkmany vm vm-addr vm-rw vm-segv vm-splice wait watchdog wcs x86syscall xattr yield zero zlib zombie

Options  
-a N, --all N, --parallel N (mostly used combined with  --class)    start N instances of all stressors in parallel. If N is less than  zero,  then  the    number  of CPUs online is used for the number of instances.  If N is zero, then the    number of CPUs in the system is used.--sequential N    sequentially  run  all  the  stressors  one by one for a default of 60 seconds. The    number of instances of each of the individual stressors to be started is N.   If  N    is  less  than  zero,  then  the  number  of  CPUs online is used for the number of    instances.  If N is zero, then the number of CPUs in the system is used.   Use  the    --timeout option to specify the duration to run each stressor.--taskset list    set CPU affinity based on the list of CPUs provided; stress-ng is bound to just use    these CPUs (Linux only). The CPUs to be used are specified  by  a  comma  separated    list  of  CPU  (0  to N-1). One can specify a range of CPUs using &#x27;-&#x27;, for example:    --taskset 0,2-3,6,7-11-t N, --timeout T    stop  stress  test  after  T  seconds.  One  can  also specify the units of time in    seconds, minutes, hours, days or years with the suffix s, m, h, d or  y.   Note:  A    timeout of 0 will run stress-ng without any timeouts (run forever).--vm-keep    don not continually unmap and map memory, just keep on re-writing to    it.--vm-populate    populate (prefault) page tables for the memory mappings; this can    stress swapping. Only available on systems that support    MAP_POPULATE (since Linux 2.5.46).

ExamplesCPU# fork two processes for 10 seconds$ stress-ng -c 2 -t 10s --metrics-brief$ stress-ng -c 2 -t 1m --metrics-brief# fork process of inline CPU$ stress-ng -c 0 -t 1m --metrics-brief# run 3 instances of the CPU stressor and pin them to CPUs 0, 2 and 3.$ stress-ng --taskset 0,2-3 --cpu 3
Memory# fork 1 process malloc/free 128M$ stress-ng -m 1 --vm-bytes 128M -t 10s --metrics-brief# fork 1 process malloc/free 1G$ stress-ng -m 1 --vm-bytes 1G -t 10s --metrics-brief$ stress-ng -m 1 --vm-bytes 1G -t 10s --vm-keep --vm-populate --metrics-brief# fork 2 processes malloc/free 50% available memory, one 25 %$ stress-ng -m 2 --vm-bytes 50% -t 10s --metrics-brief
interrupt load# 32 instances at 1MHz:$ stress-ng --timer 32 --timer-freq 1000000
page faults# You can generate major page faults (by accessing a page is not loaded in memory at the time of the fault) and see the page fault rate using:$ stress-ng --fault 0 --perf -t 1m# or with newer kernels use the userfaultfd stressor to force even more major faults:$ stress-ng --userfaultfd 0 --perf -t 1m]]></content>
      <categories>
        <category>stress</category>
        <category>generator</category>
      </categories>
      <tags>
        <tag>stress</tag>
        <tag>generator</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-IO</title>
    <url>/2021/08/18/virtualization-IO/</url>
    <content><![CDATA[IntroductionVirtual machine uses host device by three different types.

Full Emulated devices
Paravirtualized devices
Physically shared devices

As the first way has low performance for IO intensive VM, hence it should not be used in cloud provider, so let’s focus on the later two.Paravirtualization provides a fast and efficient means of communication for guests to use devices on the host machine, The popular one is virtio.
Physically shared devices(Direct&#x2F;IO, Passthrough) is more efficient than Paravirtualization for IO intensive VM, but it needs hardware IOMMU support.


Virtual DevicesParavirtulizationSome paravirtualized devices decrease I&#x2F;O latency and increase I&#x2F;O throughput to near bare-metal levels, while other paravirtualized devices add functionality to virtual machines that is not otherwise available.  
All virtio devices have two parts: the host device(backend) and the guest driver(frontend).
Front end driver for different type of virtual device

The paravirtualized network device (virtio-net)  The paravirtualized network device is a virtual network device that provides network access to virtual machines with increased I&#x2F;O performance and lower latency. 

The paravirtualized block device (virtio-blk)  The paravirtualized block device is a high-performance virtual storage device that provides storage to virtual machines with increased I&#x2F;O performance and lower latency. The paravirtualized block device is supported by the hypervisor and is attached to the virtual machine (except for floppy disk drives, which must be emulated). 

The paravirtualized controller device (virtio-scsi)  The paravirtualized SCSI controller device provides a more flexible and scalable alternative to virtio-blk. A virtio-scsi guest is capable of inheriting the feature set of the target device, and can handle hundreds of devices compared to virtio-blk, which can only handle 28 devices. 

The paravirtualized serial device (virtio-serial)  The paravirtualized serial device is a bytestream-oriented, character stream device, and provides a simple communication interface between the host’s user space and the guest’s user space.


Requires:

Guest: Must install virtio-xx front end driver(most linux already install it)
Host: Must install vhost-xxx backend driver in host(Should install for KVM enabled machine), Host also needs to install driver for physical device, but Guest never accesses it directly. but backend driver in KVM accesses it.

PassthroughVFIO attaches PCI devices on the host system directly to virtual machines, providing guests with exclusive access to PCI devices for a range of tasks. This enables PCI devices to appear and behave as if they were physically attached to the guest virtual machine.  
VFIO improves on previous PCI device assignment architecture by moving device assignment out of the KVM hypervisor, and enforcing device isolation at the kernel level.
With VFIO and IOMMU from hardware, Hypervisor can assign physical device to VM directly, like GPU to a VM, but in cloud environment, we usually we do not assign the whole physical device(GPU, network card, block device) to one VM directly, as we want to share physical device by different VMs, but still need isolation, this what SR-IOV does, SR-IOR is a feature provided by PIC-E device, it’s virtualization from hardware.

SR-IOV

SR-IOV (Single Root I&#x2F;O Virtualization) is a PCI Express (PCI-e) standard that extends a single physical PCI function to share its PCI resources as separate virtual functions (VFs). Each function can be used by a different virtual machine via PCI device assignment. An SR-IOV-capable PCI-e device provides a Single Root function (for example, a single Ethernet port) and presents multiple, separate virtual devices as unique PCI device functions. Each virtual device may have its own unique PCI configuration space, memory-mapped registers, and individual MSI-based interrupts. 

SR-IOV has two main functions:

Physical functions (PFs) which are a full PCI device including discovery, managing and configuring as normal PCI devices. There is a single PF pair per NIC and it provides the actual configuration for the full NIC device

Virtual functions (VFs) are simple PCI functions that only control part of the device and are derived from physical functions. Multiple VFs can reside on the same NIC.


There are two solutions for device pass through, the solution two wins if two solutions are available for the device, as for second solution, it needs virtio data plane supported in hardware.
Solution one(vendoer specific driver in guest)There are two cases for solution one: vendor specific driver in Guest os

Using the guest kernel driver: In this approach we use the NIC (vendor specific) driver in the kernel of the guest, while directly mapping the IO memory, so that the HW device can directly access the memory on the guest kernel.

Using the DPDK pmd driver in the guest: In this approach we use the NIC (vendor specific) DPDK pmd driver in the guest userspace, while directly mapping the IO memory, so that the HW device can directly access the memory on the specific userspace process in the guest.


Vendor-VF-PMD in userspace or Vendor-VF driver in guest kernel


Note

The data plane is vendor specific and goes directly to the VF.

For SRIOV, Vendor NIC specific drivers are required both in the host kernel (PF driver) and the guest userspace (VF driver) to enable this solution.

The host kernel driver(for PF) and the guest userspace VF driver don’t communicate directly. The PF&#x2F;VF drivers are configured through other interfaces (e.g. the host PF driver can be configured by libvirt).

The vendor-VF in the guest is responsible for configuring the NICs VF while the vendor-PF-driver in the host kernel space is managing the full NIC.

Host: IOMMU support from hardware and must install VFIO driver for VF and vendor driver for PF.


Installed vendor driver in guest has many drawbacks

It requires a match between the drivers running in the guest and the actual physical NIC. 
If the NIC firmware is upgraded, the guest application driver may need to be upgraded as well.
If the NIC is replaced with a NIC from another vendor, the guest must use another driver the NIC.
Migration of a VM can only be done to a host with the exact same configuration.

solution twoUse generic driver(virtio) in Guest OS, but this required virtio data plane supported from hardware(VF)
Can we still use virtio driver in guest os to operate VF directly?Without hardware support, NO, as VF is vendor specific, different vendors use different formats for data transferring, it does not know data sent by virtio driver, so VF vendor in hardware must implement virtio spec(ring layout), so that for datapplane, it can know data sent with virtio format and send it on wire!!!, this is only for dataplane, in order to send, we need control plan for VF to prepare sending setting like set register etc, if we use virtio, there must be a guy in middle to convert virtio control to vendor specific, that’s virtio data path acceleration (vDPA) framework does, it’s only for control plane, In summary, offload dataplane to hardware, use vDPA framework for control plane.
Why not offload control plan to HW as well?Control plan supports the virtio control spec including discovery, feature negotiation, establishing&#x2F;terminating the data plane, and so on,it is more complicated and requires interactions with memory management units, hence it’s not offload to HW.
with vDPA, Data plane goes directly from the NIC to the guest using the virtio ring layout. a generic vDPA driver is added to the kernel which calls vendor specific driver(vendor vDPA driver) for virtio control plane.

virtio-net-pmd can be replaced with virtio-net in guest kernel and vDPA can be in DPDK as well(actually vDPA framework in kernel merged March 2020, while in DPDK, it's April 2018! and deployed in production environment)

Solution two requires

Guest: virtio driver
Host: IOMMU support from CPU, VFIO driver, virtio support in PCI-E hardware(smartNIC), vDPA generic framework and vDPA for vendor specific plugin, vendor driver(vDPA generic-&gt;vDPA vendor plugin-&gt;vendor driver)

Cons

Live migration—Providing live migration between different vendor NICs and versions given the ring layout is now standard in the guest.
The bare-metal vision—a single generic NIC driver—Forward looking, the virtio-net driver can be enabled as a bare-metal driver, while using the vDPA SW infrastructure in the kernel to enable a single generic NIC driver to drive different HW NICs (similar, e.g. to the NVMe driver for storage devices).

Ref
virtual device
vDPA introduction

]]></content>
      <categories>
        <category>virtualization</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
        <tag>passthrough</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-cpu</title>
    <url>/2021/08/18/virtualization-cpu/</url>
    <content><![CDATA[FAQIntel vt-x vs vt-dVT-x is CPU virtualization technology from intel, while VT-d is IO virtualization technology from intel as well.

VT-x which supports root mode and vmx mode for cpu and series of instructions for use to support cpu virtualization.
VT-d(Intel® Virtualization Technology for Directed I&#x2F;O) is technology which supports IOMMU, hence can provide Direct&#x2F;IO, IO passthrough to guest VM.

How to enable vt-x and vt-dIf you CPU supports these, there is a switch in BIOS to enable&#x2F;disable them, the switch is mostly located at Processor tab.
check VT-x enabled or not
# vmx Virtual Machine Extensions$ lscpu | grep vmxFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts
check VT-d enabled or notWhen vt-d is enable and IOMMU kernel is loaded, at system boot time, IOMMU is initialized
# centos7$ dmesg | grep &quot;DMAR:&quot;[    0.000000] DMAR: IOMMU enabled...

]]></content>
  </entry>
  <entry>
    <title>systemd_resource_control</title>
    <url>/2022/09/21/systemd-resource-control/</url>
    <content><![CDATA[Overviewsystemd is a Linux initialization system and service manager that includes features like on-demand starting of daemons, mount and automount point maintenance, snapshot support, and processes tracking using Linux control groups. systemd provides a logging daemon and other tools and utilities to help with common system administration tasks.


resource controlAll processes running on the system are child processes of the systemd init process. Systemd provides three unit types that are used for the purpose of resource control

Service  A process or a group of processes, which systemd started based on a unit configuration file. Services encapsulate the specified processes so that they can be started and stopped as one set. Services are named in the following way: name.service

Scope  A group of externally created processes. Scopes encapsulate processes that are started and stopped by arbitrary processes through the fork() function and then registered by systemd at runtime. For instance, user sessions, containers, and virtual machines are treated as scopes. Scopes are named as follows: name.scope

Slice  A group of hierarchically organized units. Slices do not contain processes, they organize a hierarchy in which scopes and services are placed. The actual processes are contained in scopes or in services. In this hierarchical tree, every name of a slice unit corresponds to the path to a location in the hierarchy. The dash (&quot;-&quot;) character acts as a separator of the path components. For example, if the name of a slice looks as follows: parent-name.slice.


NOTE  

Service, scope, and slice units directly map to objects in the cgroup tree. When these units are activated, they map directly to cgroup paths built from the unit names. For example, the ex.service residing in the test-waldo.sliceis mapped to the cgroup test.slice/test-waldo.slice/ex.service/

cgroup dir for slice is created when

A service starts when it uses that slice
systemctl start test.slice even no one uses it



troubleshootingsystemd cgroups# get cgroup tree $systemd-cglssystemd-cgls├─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 22├─docker│ └─febf819341b7bee63374f6f666077b04fbc53bce4cb091db0ecd2327db6d8546│   └─13204 /bin/bash├─machine.slice│ └─machine-qemu\x2d1\x2dvm100.scope│   └─12841 /usr/libexec/qemu-kvm -name guest=vm100,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-1-vm100/master-key.aes -machine pc-i440fx-├─user.slice│ └─user-0.slice│   ├─session-121.scope│   │ ├─12400 sshd: root@pts/2    │   │ └─12403 -bash│   ├─session-80.scope$systemd-cgls cpu$systemd-cgls memory...# get cpu/memory/io usage of each cgroup$systemd-cgtopPath                                                                                                                                                                                                                    Tasks   %CPU   Memory  Input/s Output/s/                                                                                                                                                                                                                         210   16.7    12.1G        -        -/system.slice                                                                                                                                                                                                               1   11.1     2.2G        -        -/system.slice/systemd-journald.service                                                                                                                                                                                      1    8.3   112.1M        -        -/system.slice/rsyslog.service                                                                                                                                                                                               1    2.3     9.8M        -        -/machine.slice                                                                                                                                                                                                              -    1.1     2.0G        -        -/machine.slice/machine-qemu\x2d1\x2dvm100.scope                                                                                                                                                                             3    1.1     2.0G        -        -/machine.slice/machine-qemu\x2d1\x2dvm100.scope/vcpu2                                                                                                                                                                       1    0.6        -        -        -/machine.slice/machine-qemu\x2d1\x2dvm100.scope/vcpu3                                                                                                                                                                       1    0.5        -        -        -/system.slice/mysqld.service                                                                                                                                                                                                1    0.2        -        -        -/system.slice/ovs-vswitchd.service                                                                                                                                                                                          1    0.2        -        -        -/system.slice/ovsdb-server.service                                                                                                                                                                                          1    0.1        -        -        -...############# Creating transient service using systemd-run command########################## this will create a service unit at /run with command under slice$systemd-run --unit=&lt;name&gt; --slice=&lt;name&gt;.slice &lt;command&gt;# these files are removed when os reboot or it exits correctly or systemctl stop sleep$systemd-run --unit=sleep --slice=system.slice sleep 10000Running as unit sleep.service.# these files are created$ls /run/systemd/system/sleep.service$ls /run/systemd/system/sleep.service.d/50-Description.conf  50-ExecStart.conf  50-Slice.conf# this will create a service scope unit at /run with command under slice$systemd-run --unit=&lt;name&gt; --scope --slice=&lt;name&gt;.slice &lt;command&gt;

service and unit management# list all units by type$systemctl -t service$systemctl -t slice# show runtime info of given service$systemctl show libvirtd.service# show runtime info of slice$systemctl show test.sliceSlice=-.sliceControlGroup=/test.sliceMemoryCurrent=56729600TasksCurrent=23Delegate=noCPUAccounting=noCPUShares=18446744073709551615StartupCPUShares=18446744073709551615Slice=-.sliceControlGroup=/test.sliceMemoryCurrent=56729600TasksCurrent=23Delegate=noCPUAccounting=noCPUShares=18446744073709551615...############# set property of unix(slice, service, scop etc) from command line ######################## set does not change /usr/lib/systemd/system/test.slice # but create a file(overide the above) at /etc/systemd/system/test.slice.d/50-MemoryAccounting.conf$systemctl set-property test.slice MemoryAccounting=no$systemctl set-property &lt;service name&gt; &lt;unit file option&gt;=&lt;value&gt;# check the new setting$systemctl show --property &lt;unit file option&gt; &lt;service name&gt;########### set property of unix(slice, service, scop etc) by editing file############################$vi /usr/lib/systemd/system/test.slice [Slice]                                                                         MemoryAccounting=no$vi xx.service[Service]MemoryLimit=16G

FAQwithout systemd, run a program in a pairs of cgroups$cgexec -g controllers:path_to_cgroup command arguments # example$mkdir /sys/fs/cgroup/memory/test$mkdir /sys/fs/cgroup/cpu/test$cgexec -g memory:test -g cpu:test sleep 10
without systemd, move a process in a pairs of cgroups# check the cgroups of the given process$cat /proc/12546/cgroup11:blkio:/10:cpuset:/9:devices:/8:cpuacct,cpu:/test27:net_prio,net_cls:/6:memory:/test5:pids:/4:freezer:/3:hugetlb:/2:perf_event:/1:name=systemd:/user.slice/user-0.slice/session-80.scope# move the process and its child to the given cgroups$cgclassify -g cpu:test2 12546# to all subsystem of cgroups(must create it each before)$cgclassify -g *:test2 12546

how to set limit for a given service by systemdThese can be done from command line with set-property command or edit service file directly.
# vi xx.service[Service]MemoryLimit=16G# if command line with &#x27;set-property`, a override file is created at# /etc/systemd/system/xx.service.d/50-MemoryLimit.conf [Service]MemoryLimit=16G

how to set limit for a group of servicesYou need to create a slice which defines the limitation for the services.
$touch /usr/lib/systemd/system/test.slice[Unit]Description=Test SliceBefore=slices.target[Slice]MemoryAccounting=trueMemoryLimit=2048MCPUAccounting=trueCPUQuota=25%TasksMax=4096...# edit each service unit file$vi /etc/systemd/system/libvirtd.service...[Service]Slice=docker.slice...$systemctl daemon-reload$systemctl stop libvirtd$systemctl start libvirtd$systemctl status libvirtd● libvirtd.service - Virtualization daemon   Loaded: loaded (/etc/systemd/system/libvirtd.service; disabled; vendor preset: enabled)   Active: active (running) since Wed 2022-09-21 14:38:46 CST; 1h 0min ago     Docs: man:libvirtd(8)           https://libvirt.org Main PID: 13573 (libvirtd)    Tasks: 19 (limit: 32768)   CGroup: /test.slice/libvirtd.service           ├─12657 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper           ├─12659 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper           └─13573 /usr/sbin/libvirtd --listen
Ref
systemd directive
understand systemd
resource control with systemd
systemctl man
systemctl guide

]]></content>
      <categories>
        <category>linux</category>
        <category>systemd</category>
      </categories>
      <tags>
        <tag>systemd</tag>
        <tag>cgroup</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-mem-balloon</title>
    <url>/2022/04/07/virtualization-mem-balloon/</url>
    <content><![CDATA[OverviewMemory ballooning is a memory management feature used in most virtualization platforms which allows a host system to artificially enlarge its pool of memory by taking advantage or reclaiming unused memory previously allocated to various virtual machines.This is achieved through a balloon driver which is installed on the guest operating system which the hypervisor communicates with when it needs to reclaim memory through ballooning.
Through memory ballooning, a host server can reclaim unused memory from other less busy virtual machines and reassign it to ones that require it more. Theoretically, a server with 32GB of memory might be able to support a combined virtual machine memory capacity allocation of 64GB simply because all of those virtual machines will not be using the maximum amount of memory they have been assigned at the same time.

InsideHow balloon driver worksVirtIO provides Memory Ballooning: the host system can reclaim memory from virtual machines (VM) by telling them to give back part of their memory to the host system. This is achieved by inflating the memory balloon(balloon driver) inside the VM, which reduced the memory available to other tasks inside the VM. Which memory pages are given back is the decision of the guest operating system (OS): It just tells the host OS which pages it does no longer need and will no longer access. The host OS then un-maps(kvm in kernel) those pages from the guests and marks them as unavailable for the guest VM. The host system can then use them for other tasks like starting even more VMs or other processes.
If later on the VM need more free memory itself, the host can later on return pages to the guest and shrink the holes. This allows to dynamically adjust the memory available to each VM even while the VMs keep running.
The balloon driver(virtio_balloon) in each guest operating system keeps track of the excess memory of each VM and when the hypervisor calls for a memory reclamation through ballooning, the balloon driver in the VM pins down a specific amount of memory so that the VM cannot consume it, and then the hypervisor reclaims that pinned memory for reallocation. If there is a scarcity of unused memory then a memory swap might be initiated in order to fulfill the balloon quota. If this happens too much, there would be a lot of I&#x2F;O overhead between the various VMs that are doing memory swapping with the disk and might adversely affect overall performance of the virtual system
The ballooning driver inflation is a smart way to claim memory from the virtual machine to the host. It’s beneficial because it will only generally claim back memory which is free inside the virtual machine, thus is usually a non-disruptive memory reclaim technique.
Cost of memory balloonMemory ballooning while an impressive solution can cause issues to virtual machines that have reoccurring spikes in memory demand. Also, ballooning will not work if your virtual machines are using all of their memory by applications within the virtual machine, this is commonly noticed in applications such as databases.
Ballooning can also become a problem if it’s relied upon too much by the hypervisor. In an ideal situation, there would be no ballooning taking place. This would indicate a healthy environment. Ballooning would only commence if there is too much demand for memory on the host, in other words when the ESXi host does not have enough free physical memory to allocate to virtual machines. If ballooning is happening all the time then a performance issue can start on the host.
Many ballooning operations could cause additional CPU cycles to be used to perform the ballooning operations. This, in turn, could reduce the amount of physical CPU available to virtual machines on the host.
Suggestion
Set proper balloon size of each VM
Disable memory balloon for vm used for database

libvirt
# disable memballoon&lt;devices&gt;  &lt;memballoon model=&#x27;none&#x27;/&gt;&lt;/devices&gt;# enable it &lt;devices&gt;  &lt;memballoon model=&#x27;virtio&#x27;/&gt;&lt;/devices&gt;

Ref
memory balloon
memory balloon pro and cons

]]></content>
      <categories>
        <category>virtualization</category>
        <category>memory</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>memory balloon</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-memory</title>
    <url>/2021/08/18/virtualization-memory/</url>
    <content><![CDATA[FAQHow to enable EPTEPT is feature of CPU, but there is no independent switch for EPT feature, to turn it on or off, that means if you CPU supports it, it&#39;s turned on or off along with VT-X from BIOScheck EPT supported or not
 lscpu | grep eptFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts]]></content>
  </entry>
  <entry>
    <title>virtualization-virtio-vdpa-dpdk</title>
    <url>/2023/10/19/virtualization-virtio-vdpa-dpdk/</url>
    <content><![CDATA[Overview# 06:00.3 is intel vDPA device or simulated vDPA devices# qemu as vhost-user server, dpdk as client$ ./dpdk-vdpa -c 0x2 -n 4 --socket-mem 1024,1024 \        -a 0000:06:00.4,vdpa=1 \        -- --client --iface=/tmp/qemu-vhost-user-net.sock$ modprobe vfio-pci$ ./usertools/dpdk-devbind.py -b vfio-pci 06:00.4# start vm TODO$ qemu-system-x86_64 -cpu host -enable-kvm \-mem-prealloc \-chardev socket,id=char0,path=/tmp/qemu-vhost-user-net.sock \-netdev type=vhost-user,id=vdpa,chardev=char0 \-device virtio-net-pci,netdev=vdpa,mac=00:aa:bb:cc:dd:ee,page-per-vq=on \# ==================== more options==========================================================# 0000:ca:0f.5 is VF NET PCI device created by ConnectX-6, bind with vfio-pci kernel driver# vhost-user-net-server.sock is created by qemu$/usr/local/bin/vdpa-dpdk --lcore 0@(0-127) -n 4 --huge-dir=/mnt/huge_2MB --log-level=9 -w 0000:ca:0f.5,class=vdpa,event_mode=2 --log-level=pmd.vdpa.mlx5:7 --log-level=pmd:8 --file-prefix=vfnet1 -- --client --iface=vhost-user-net-server.sock

Ref
mlx vf driver for virtio control
intel fpga vf driver for virtio control
vdpa sample application from dpdk

]]></content>
      <categories>
        <category>virtualization</category>
        <category>vdpa</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
        <tag>passthrough</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-virtio-vdpa-hands-on</title>
    <url>/2023/10/20/virtualization-virtio-vdpa-hands-on/</url>
    <content><![CDATA[hands on with vDPA device simulatorThe vdpa management tool(from iproute2) can be used to communicate with the vDPA framework in the kernel using netlink vDPA API. It allows to create and destroy new devices, and control their parameters.An example using the in-kernel simulators:# Load vDPA net and block simulators kernel modules$ modprobe vdpa-sim-net$ modprobe vdpa-sim-blk# List vdpa management device attributes$ vdpa mgmtdev showvdpasim_blk:   supported_classes blockvdpasim_net:   supported_classes net# Add `vdpa-net1` device through `vdpasim_net` management device$ vdpa dev add name vdpa-net1 mgmtdev vdpasim_net# Add `vdpa-blk1` device through `vdpasim_blk` management device$ vdpa dev add name vdpa-blk1 mgmtdev vdpasim_blk# List all vdpa devices on the system$ vdpa dev showvdpa-net1: type network mgmtdev vdpasim_net vendor_id 0 max_vqs 2 max_vq_size 256vdpa-blk1: type block mgmtdev vdpasim_blk vendor_id 0 max_vqs 1 max_vq_size 256# As above, but using pretty[-p] JSON[-j] output$ vdpa dev show -jp&#123;    &quot;dev&quot;: &#123;        &quot;vdpa-net1&quot;: &#123;            &quot;type&quot;: &quot;network&quot;,            &quot;mgmtdev&quot;: &quot;vdpasim_net&quot;,            &quot;vendor_id&quot;: 0,            &quot;max_vqs&quot;: 2,            &quot;max_vq_size&quot;: 256        &#125;,        &quot;vdpa-blk1&quot;: &#123;            &quot;type&quot;: &quot;block&quot;,            &quot;mgmtdev&quot;: &quot;vdpasim_blk&quot;,            &quot;vendor_id&quot;: 0,            &quot;max_vqs&quot;: 1,            &quot;max_vq_size&quot;: 256        &#125;    &#125;&#125;# check all vdpa device(each VF is an vdpa device)$ ls /sys/bus/vdpa/devices# check bus driver of a vdpa device$  ls -l /sys/bus/vdpa/devices/vdpa0/driver# switch bus driver$ echo vdpa0 &gt; /sys/bus/vdpa/drivers/vhost_vdpa/unbind$ echo vdpa0 &gt; /sys/bus/vdpa/drivers/virtio_vdpa/bind# check all bus drivers(virtio, vhost)$ ls  /sys/bus/vdpa/drivers]]></content>
      <categories>
        <category>virtualization</category>
        <category>vdpa</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
        <tag>passthrough</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-vfio-sriov</title>
    <url>/2023/10/18/virtualization-vfio-sriov/</url>
    <content><![CDATA[VFIOVirtual Function I&#x2F;O is a framework for userspace I&#x2F;O, it’s not limited to SRIOV, but SRIOV VF is the common use case.
The VFIO driver is an IOMMU&#x2F;device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment. In other words, this allows safe non-privileged, userspace drivers.
Why do we want that? Virtual machines often make use of direct device access (“device assignment”) when configured for the highest possible I&#x2F;O performance. From a device and host perspective, this simply turns the VM into a userspace driver, with the benefits of significantly reduced latency, higher bandwidth, and direct use of bare-metal device drivers.



Summary  
Userspace driver interface(VFIO kernel module[vfio_pci] exposes PCI device resource to user by ioctl)
Hardware IOMMU based DMA mapping and isolation
IOMMU group based


Modular IOMMU and bus driver support
PCI and platform devices supported
IOMMU API(type1) and ppc64 models


Full device access, DMA and interrupt support
read&#x2F;write &amp; mmap support of device resources
Mapping of user memory to I&#x2F;O virtual address
eventfd and irqfd based signaling mechanisms



SRIOVWhat is SR-IOV?Single-root I&#x2F;O virtualization (SR-IOV) is a specification that enables a single PCI Express (PCIe) device to present multiple separate PCI devices, called virtual functions (VFs), to the host system. Each of these devices:

Is able to provide the same or similar service as the original PCIe device.
Appears at a different address on the host PCI bus.
Can be assigned to a different VM using VFIO assignment.For example, a single SR-IOV capable network device can present VFs to multiple VMs. While all of the VFs use the same physical card, the same network connection, and the same network cable, each of the VMs directly controls its own hardware network device, and uses no extra resources from the host.


How SR-IOV works
The SR-IOV functionality is possible thanks to the introduction of the following PCIe functions:

Physical functions (PFs) - A PCIe function that provides the functionality of its device (for example networking) to the host, but can also create and manage a set of VFs. Each SR-IOV capable device has one or more PFs.
Virtual functions (VFs) - Lightweight PCIe functions that behave as independent devices. Each VF is derived from a PF. The maximum number of VFs a device can have depends on the device hardware. Each VF can be assigned only to a single VM at a time, but a VM can have multiple VFs assigned to it.VMs recognize VFs as virtual devices. For example, a VF created by an SR-IOV network device appears as a network card to a VM to which it is assigned, in the same way as a physical network card appears to the host system.



Advantages
The primary advantages of using SR-IOV VFs rather than emulated devices are:

Improved performance
Reduced use of host CPU and memory resourcesFor example, a VF attached to a VM as a vNIC performs at almost the same level as a physical NIC, and much better than paravirtualized or emulated NICs. In particular, when multiple VFs are used simultaneously on a single host, the performance benefits can be significant.


Inconvenient

To modify the configuration of a PF, you must first change the number of VFs exposed by the PF to zero. Therefore, you also need to remove the devices provided by these VFs from the VM to which they are assigned.
A VM with an VFIO-assigned devices attached, including SR-IOV VFs, cannot be migrated to another host. In some cases, you can work around this limitation by pairing the assigned device with an emulated device. For example, you can bond an assigned networking VF to an emulated vNIC, and remove the VF before the migration.
In addition, VFIO-assigned devices require pinning of VM memory, which increases the memory consumption of the VM and prevents the use of memory ballooning on the VM.


examples
# if a device has SRIOV and we already set max vf number$ lspci -s 0000:4b:00.1 -v | grep IOV	Capabilities: [180] Single Root I/O Virtualization (SR-IOV)# if 0000:4b:00.1 is ethernet device and attached with PF driver# you will see ethernet device on system# ens9f1 is the device name as it&#x27;s pci device is 0000:4b:00.1$ ls -al  /sys/class/net/ens9f1lrwxrwxrwx 1 root root 0 Oct 16 17:40 /sys/class/net/ens9f1 -&gt; ../../devices/pci0000:4a/0000:4a:02.0/0000:4b:00.1/net/ens9f1$ethtool -i ens9f1driver: mlx5_coreversion: 5.5-1.0.3firmware-version: 24.98.1401 (MT_0000000539)expansion-rom-version: bus-info: 0000:4b:00.1   ---&gt; bus info...# check support max vf number and current set number$ cat  /sys/class/net/ens9f1/device/sriov_totalvfs 127$ cat  /sys/class/net/ens9f1/device/sriov_numvfs 127$ echo 127 &gt; /sys/class/net/ens9f1/device/sriov_numvfs # create udev rule /etc/udev/rules.d/ens9f1.rules to auto configure vfACTION==&quot;add&quot;, SUBSYSTEM==&quot;net&quot;, ENV&#123;ID_NET_DRIVER&#125;==&quot;mlx5_core&quot;, ATTR&#123;device/sriov_numvfs&#125;=&quot;127&quot;# check VF number and it&#x27;s pci bus$ ls -al  /sys/class/net/ens9f1/device/virtfn*lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn0 -&gt; ../0000:4b:10.3lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn1 -&gt; ../0000:4b:10.4lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn10 -&gt; ../0000:4b:11.5lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn100 -&gt; ../0000:4b:1c.7lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn101 -&gt; ../0000:4b:1d.0lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn102 -&gt; ../0000:4b:1d.1lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn103 -&gt; ../0000:4b:1d.2lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn104 -&gt; ../0000:4b:1d.3lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn105 -&gt; ../0000:4b:1d.4lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn106 -&gt; ../0000:4b:1d.5lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn107 -&gt; ../0000:4b:1d.6lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn108 -&gt; ../0000:4b:1d.7lrwxrwxrwx 1 root root 0 Oct 16 18:50 /sys/class/net/ens9f1/device/virtfn109 -&gt; ../0000:4b:1e.0...

SRIOV driverThe SR-IOV drivers are implemented in the kernel. The core implementation is contained in the PCI subsystem, but there must also be driver support for both the Physical Function (PF) and Virtual Function (VF) devices.

Intel 82599ES 10 Gigabit Ethernet Controller - uses the ixgbe driver
Mellanox ConnectX-5 Ethernet Adapter Cards - use the mlx5_core driver
Broadcom NetXtreme II BCM57810 - uses the bnx2x driver

# Intel 82599ES 10 Gigabit Ethernet Controller - uses the ixgbe driver# em2 is PF$ethtool -i em2driver: ixgbebus-info: 0000:01:00.1....# 01:00.1 is PF PCI device, as you can see driver is ixgbe$lspci -s 0000:01:00.1 -v01:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)	Subsystem: Dell Ethernet 10G 4P X520/I350 rNDC  ...	Capabilities: [160] Single Root I/O Virtualization (SR-IOV)	Kernel driver in use: ixgbe# enable VF on this PF$cat /sys/class/net/em2/device/sriov_totalvfs 63$echo 1 &gt; /sys/class/net/em2/device/sriov_numvfs # 01:10.1 is VF PCI device, as you can see driver is ixgbevf$lspci -s 01:10.1 -v01:10.1 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)	Subsystem: Dell Device 1f72	Flags: bus master, fast devsel, latency 0  ...	[virtual] Memory at 30000200000 (64-bit, prefetchable) [size=16K]	[virtual] Memory at 30000300000 (64-bit, prefetchable) [size=16K]	Kernel driver in use: ixgbevf# VF network device with ixgbevf$ethtool -i em2_0driver: ixgbevfbus-info: 0000:01:10.1...# MAC is auto generate, if you disable and enable it again, the MAC can change$ifconfig em2_0em2_0: flags=4098&lt;BROADCAST,MULTICAST&gt;  mtu 1500        ether ba:5b:24:6e:27:39  txqueuelen 1000  (Ethernet)# VF with no MAC set$ip link show em25: em2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 2000 qdisc mq state UP mode DEFAULT qlen 1000    link/ether ec:f4:bb:e9:06:62 brd ff:ff:ff:ff:ff:ff    vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto

NOTE

SR-IOV VF network devices do not have permanent unique MAC addresses, then host reboot, mac is gone
each vendor has it own SR-IOV VF driver
SR-IOV not tight with vDPA(virtio Datapath Acceleration), vDPA device is a device that has virtio datapath support, SR-IOV can be enabled or disable(mostly enabled)
vDPA device usually has SR-IOV capability, but device that has SR-IOV cap may not be a vDPA device.

IOMMURoles:

Translation: I&#x2F;O Virtual Address(IOVA) space
Isolation: per device translation and invalid accesses blocked

DMA remappingIn a direct assignment model(Direct&#x2F;IO), the guest OS device driver is in control of the device and is providing GPA instead of HPA required by the DMA capable device. DMA remapping hardware can be used to do the appropriate conversion. Since the GPA is provided by the VMM it knows the conversion from the GPA to the HPA. The VMM programs the DMA remapping hardware with the GPA to HPA conversion information so the DMA remapping hardware can perform the necessary translation. Using the remapping, the data can now be transferred directly to the appropriate buffer of the guests rather than going through an intermediate software emulation layer.

The basic idea of IOMMU DMA remapping is the same as the MMU for address translation. When the physical IO device do DMA, the address for DMA is called IOVA, IOMMU first using the device’s address(PCI BDF address) provided by PCI-E when raising interrupt(PCI device does not include this!!!) to find a page table(page table of domain) then using the the IOVA to walk this page table and finally get the host physical address. This is very like that how the MMU work to translate a virtual address to a physical address. Following figure show the basic idea of DMA remapping, this is the legacy mode, there is also a scalable mode, though the detail differs, the idea is the same.

The device’s bus is useds to index in Root Table, the root table is 4-KByte in size and contains 256 root-entries. The root-table-entry contains the context-table pointer which references the context-table for all the devices on the bus identified by the root-entry.
A context-entry maps a specific I&#x2F;O device on a bus to the domain to which it is assigned, and, in turn, to the address translation structures for the domain. Each context-table contains 256 entries, with each entry corresponding to a PCI device function on the bus. For a PCI device, the device and function numbers (lower 8-bits) are used to index into the context-table.
The root-table and context table is setup by the IOMMU driver, the page table is usually setup by the VMM. Of course, any process can do setup this page table. The IOVA is used as the input for the IOMMU translation, this address is device’s view address. The IOVA can be any address that is meaning for for the guest or process. For example, the qemu&#x2F;kvm uses the GPA as the IOVA and also you can uses another address as the IOVA. The VFIO uses IOMMU to do the translation from GPA to HPA.
Interrupt remappingIOMMU groups
VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine, which would impact host and guest functionality.
An IOMMU group is defined as the smallest set of devices that can be considered isolated from the IOMMU’s perspective, devices in the same group can be only assigned to same VM, devices in the same group use the same BDF as request ID when interrupted happens, VMM uses this request ID to identify VM and set proper IO page table of that VM, if devices in the same group are assigned to different machine, VMM can NOT know which IO page table to use to translated GPA to HPA.
Each IOMMU group may contain one or more devices. When multiple devices are present, all endpoints within the IOMMU group must be claimed for any device within the group to be assigned to a guest. This can be accomplished either by also assigning the extra endpoints to the guest or by detaching them from the host driver. Devices contained within a single group may not be split between multiple guests or split between host and guest. Non-endpoint devices such as PCIe root ports, switch ports, and bridges should not be detached from the host drivers and will not interfere with assignment of endpoints.

For endpoint devices within on IOMMU group, they must be in three cases

all sits in host
all assigned to only one vm
part assigned to vm, the left detached from host driver


enable IOMMU steps for intel cpu
First make sure, intel cpu has vt-d support by looking lookup user guide.

turn it on from BIOS as there is on vt-d switch there
Then add ‘iommu&#x3D;pt intel_iommu&#x3D;on’ to boot parameter

$ dmesg | grep IOMMU[    0.000000] Intel-IOMMU: enabled...# list all iommu groups and devices of one group$ls /sys/kernel/iommu_groups$ls /sys/kernel/iommu_groups/10/devices

NOTE

Devices assigned(PCI passthrough) need IOMMU supported, but no SRIOV required by that device
In guest kernel, we should install proper driver for any passthrough device to make it work
for normal device, we should install vendor specific driver, for VF device, we should install VF driver from that vendor, but if VF device(vdpa device) has virtio support from hardware, we can use virtio-net for it with vDPA framework to make it work.
before we pass through a pci device, we should detach it from host(unbind it from original driver and bind it with vfio-pci driver)

Ref
VFIO kernel-api
VFIO vs UIO
VFIO driver for e1000
SRIOV Overview
libvirt uses SRIOV
IOMMU Introduction

]]></content>
      <categories>
        <category>virtualization</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-virtio-vdpa</title>
    <url>/2023/10/19/virtualization-virtio-vdpa-kernel/</url>
    <content><![CDATA[IntroductionFor IO virtualization, A MM supports two well known models: Emulation of devices or Paravirtualization., like we see Qemu for emulation and virtio for paravirtualization, but both have performance issue, as guest os(guest driver) can’t access physical device directly, it must send IO to intermediate layer(VMM) firstly which reduces performance. there is another voice comes up:Can we assign HW resources directly to the VM? if we do, what extra work should support by CPU?


When a VM or a Guest is launched over the VMM, the address space that the Guest OS is provided as its physical address range, known as Guest Physical Address (GPA), may not be the same as the real Host Physical Address (HPA). DMA capable devices need HPA to transfer the data to and from physical memory locations, we need CPU can translate GPA to HPA when DMA happens, that’s what IOMMU does, DMA remapping(similar thing for interrupt as well, Interrupt remapping)
The VT-d DMA-remapping hardware logic in the chipset sits between the DMA capable peripheral I&#x2F;O devices and the computer’s physical memory.
IOMMU allows virtual machines to have direct access to hardware I&#x2F;O devices, such as network cards, storage controllers (HBAs), and GPUs.


vDPAVirtual data path acceleration (vDPA) in essence is an approach to standardize the NIC SRIOV data plane using the virtio ring layout and placing a single standard virtio driver in the guest decoupled from any vendor implementation, while adding a generic control plane and SW infrastructure to support it.
with vDPA, Data plane goes directly from the NIC to the guest using the virtio ring layout(need IOMMU support from CPU). However each NIC vendor can now continue using its own driver (with a small vDPA add-on) and a generic vDPA driver is added to the kernel to translate the vendor NIC driver&#x2F;control-plane to the virtio control plane.
A “vDPA device” means a type of device whose datapath complies with the virtio specification, but whose control path is vendor specific. like smartNIC(Mellox Bluefield).
vDPA devices can be both physically located on the hardware or emulated by software


Passthrough with vDPA framework in kernel(can work also with vDPA in DPDK) and virtio-net-pmd can moved to guest kernel as well, it’s virtio-net


In the VM use case since the QEMU exposes a virtio PCI device to the guest, the virtio-net-pmd will use PCI commands on the control plane and the QEMU will translate them to the vDPA kernel APIs (system calls).

Passthrough with vDPA framework for container!!!

In the container use case the virtio-net-pmd will invoke the vDPA kernel APIs (system calls) directly.

vDPA kernel frameworkHere are some more details about vDPA kernel framework(actually vDPA firstly introduced in DPDK), Basic introduction about vDPA, please refer to vDPA basics.
Why vDPA kernel approach is required even DPDK already implement it?Because there are a number of limitations for DPDK approach:

DPDK Library dependency on the host side is required for supporting this framework which is another dependency to take into account.

Since the vhost-user only provides userspace APIs, it can’t be connected to kernel subsystems. This means that the consumer of the vDPA interface will lose kernel functionality such as eBPF support.

Since DPDK focuses on the datapath then it doesn’t provide tooling for provisioning and controlling the HW. This also applies to the vDPA DPDK framework.




The vhost subsystem is the historical data path implementation of virtio inside the kernel. It was used for emulating the datapath of virtio devices on the HOST side. It exposes mature userspace APIs for setting up the kernel data path through vhost device (which are char devices). Various backends have been developed for using different types of vhost device (e.g. networking or SCSI). The vDPA kernel framework leverages these APIs through this subsystem for control plane 

The virtio subsystem is the historical virtio kernel driver framework used for connecting guests&#x2F;processes to a virtio device. It was used for controlling emulated virtio devices on the guest side. Usually we have the virtio device on the host and virtio driver on the guest which combine to create the virtio interface. This basically enables running applications that leverage vDPA (via the vhost subsystem) and those who do not (via the virtio subsystem) on the same physical vDPA NIC.


As you can see vDPA provides two ways to communicate with it

One is from vhost(host kernel exposes a char device), user space can operate vDPA by this char device, like Qemu can uses vhost driver to operate vDPA device(control it).
The other is from virtio driver, like Guest OS kernel uses virtio-net as front end, virtio backend call vDPA to operate vDPA device.

By combining the vDPA framework and the vhost&#x2F;virtio subsystems, kernel virtio drivers or userspace vhost drivers think they are controlling a vhost or virtio device while in practice it’s a vDPA device
vDPA framework is a generic layer, it calls vendor driver for controlling device, so for each device, there must be a vDPA driver(driver plugin which calls vendor driver) inside vDPA framework, a typical vDPA driver is required to implement the following capabilities for control plane.

Device probing&#x2F;removing: Vendor specific device probing and removing procedure.

Interrupt processing: Vendor or platform specific allocation and processing of the interrupt.

vDPA device abstraction: Implement the functions that are required by the vDPA framework most of which are the translation between virtio control command, vendor specific device interface and registering the vDPA device to the framework

DMA operation: For the device that has its own DMA translation logic, it can require the framework to pass DMA mapping to the driver for implementing vendor specific DMA translation


Since the datapath is offloaded to the vDPA hardware, the hardware vDPA driver becomes thin and simple to implement. The userspace vhost drivers or kernel virtio drivers control and setup the hardware datapath via vhost ioctls or virtio bus commands (depending on the subsystem you chose). The vDPA framework will then forward the virtio&#x2F;vhost commands to the hardware vDPA drivers which will implement those commands in a vendor specific way. 
vDPA framework will also relay the interrupts from vDPA hardware to the userspace vhost drivers and kernel virtio drivers. Doorbell and interrupt passthrough will be supported by the framework as well to achieve the device native performance.
Inside vDPA kernel framework

vhost-vDPA bus driver - This driver connects the vDPA bus to the vhost subsystem and presents a vhost char device to the userspace. This is useful for cases when the datapath is expected to bypass the kernel completely. Userspace drivers can control the vDPA device via vhost ioctls as if a vhost device. A typical use case is for performing direct I&#x2F;O to userspace (or VM). 

virtio-vDPA bus driver - This driver bridges the vDPA bus to a virtio bus and from there to a virtio interface. With the help of a virtio-vDPA bus driver, the vDPA device behaves as a virtio device so it can be used by various kernel subsystems such as networking, block, crypto etc. Applications that do not use vhost userspace APIs can keep using userspace APIs that are provided by kernel networking, block and other subsystems.


NOTE: a vdpa device can be bound to only one bus driver
FAQWhich hardware vDPA devices are supported in Linux?Currently upstream Linux contains drivers for the following vDPA devices:(these two drivers are also exported to dpdk(user driver) as well)

virtio-net
Intel IFC VF vDPA driver (CONFIG_IFCVF)
Mellanox ConnectX vDPA driver (CONFIG_MLX5_VDPA_NET)



What are in-kernel vDPA device simulators useful for?The vDPA device simulators are useful for testing, prototyping, and development of the vDPA software stack. Starting with layers in the kernel (e.g. vhost-vdpa), up to the VMMs.
The following kernel modules are currently available:

vdpa-sim-net (CONFIG_VDPA_SIM_NET)

vDPA networking device simulator which loops TX traffic back to RX


vdpa-sim-blk (CONFIG_VDPA_SIM_BLK)

vDPA block device simulator which terminates I&#x2F;O requests in a memory buffer (i.e. ramdisk)Ref


vdpa dev

VT-d from intel

vDPA kernel framework

vDPA bus driver


]]></content>
      <categories>
        <category>virtualization</category>
        <category>vdpa</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
        <tag>passthrough</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-virtio-vhost-net</title>
    <url>/2023/10/20/virtualization-virtio-vhost-net/</url>
    <content><![CDATA[vhost-netVirtio Backend(dataplane) in host kernel leverage vhost protocol.
The vhost-net is a kernel driver that implements the handler side of the vhost protocol to implement an efficient data plane, i.e., packet forwarding. In this implementation, qemu and the vhost-net kernel driver (handler) use ioctls to exchange vhost messages and a couple of eventfd-like file descriptors called irqfd and ioeventfd(created by Qemu process with ioctl with kvm, then pass it with ioctl with vhost-net) are used to exchange notifications with the guest.


When vhost-net kernel driver is loaded, it exposes a character device on &#x2F;dev&#x2F;vhost-net. When qemu is launched with vhost-net support it opens it and initializes the vhost-net instance with several ioctl(2) calls. These are necessary to associate the hypervisor(qemu) process with the vhost-net instance, prepare for virtio feature negotiation and pass the guest physical memory mapping to the vhost-net driver by this fd with ioctl.
During the initialization the vhost-net kernel driver creates a kernel thread called vhost-$pid, where $pid is the hypervisor(qemu) process pid. This thread is called the “vhost worker thread”.
A tap device is still used to communicate the VM with the host but now the worker thread handles the I&#x2F;O events i.e. it polls for driver notifications or tap events, and forwards data.
Qemu allocates one eventfd and registers it to both vhost and KVM in order to achieve the notification bypass. The vhost-$pid kernel thread polls it, and KVM writes to it when the guest writes in a specific address. This mechanism is named ioeventfd. This way, a simple read&#x2F;write operation to a specific guest memory address does not need to go through the expensive QEMU process wakeup and can be routed to the vhost worker thread directly. This also has the advantage of being asynchronous, no need for the vCPU to stop (so no need to do an immediate context switch).
On the other hand, qemu allocates another eventfd and registers it to both KVM and vhost again for direct vCPU interruption injection. This mechanism is called irqfd, and it allows any process in the host to inject vCPU interrupts to the guest by writing to it, with the same advantages (asynchronous, no need for immediate context switching, etc).
Sending pkt diagram

More detail with OVS(add tap interface to ovs or create ovs port by ovs-tool)

]]></content>
      <categories>
        <category>virtualization</category>
        <category>virtio</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-virtio-vhost-user</title>
    <url>/2023/10/20/virtualization-virtio-vhost-user/</url>
    <content><![CDATA[vhost-userIn the DPDK architecture the devices are accessed by constant polling. This avoids the context switching and interrupt processing overhead at the cost of dedicating 100% of part of the CPU cores to handle packet processing.
In practice DPDK offers a series of poll mode drivers (PMDs) that enable direct transfer of packets between user space and the  physical interfaces which bypass the kernel network stack all together. This approach provides a significant performance boost over  the kernel forwarding by eliminating interrupt handling and bypassing the kernel stack.
So we want to implement Virtio Backend(dataplane) in dpdk leverage vhost protocol, that’s vhost-user

vhost-user library. This library, built in DPDK, is a userspace implementation of the vhost protocol that allows qemu to offload the virtio device packet processing to any DPDK application (such as Open vSwitch). The main difference between the vhost-user library and the vhost-net kernel driver is the communication channel. While the vhost-net kernel driver implements this channel using ioctls, the vhost-user library defines the structure of messages that are sent over a unix socket.
The vhost-user library allows the primary to configure the dataplane offloading by performing the following important actions:

Feature negotiation: Both the virtio features and the vhost-user-specific features are negotiated in a similar way: First the primary “gets” the handler’s supported features bitmask and then it “sets” the subset of them it also supports.
Memory region configuration: The master sets the memory map regions so the handler can mmap() them.
Vring configuration: The primary sets the number of virtqueues to use and their addresses within the memory regions. Note that vhost-user supports multiqueue so more than one virtqueue can be configured to improve performance.
Kick and Call file descriptors sending: Usually, the irqfd and the ioeventfd mechanisms are used.

ovs-vswitchd and vhost-user

vhost-user (backend) - Running on the host userspace as part of the OVSDPDK userspace application. As mentioned DPDK is a library and the vhostuser module are additional APIs inside that library. The OVS-DPDK is the actual application linked with this library and invoking the APIs. For each guest VM created on the host, another vhost-user backend will beinstantiated to communicate with the guest’s virtio frontend.

virtio-pmd (frontend) - Running on the guest userspace and is a poll mode driver consuming dedicated cores and performing polling with no interrupts. For an application running on the user space to consume the virtio-pmd it needs to be linked with the DPDK library as well


We can replace virtio-pmd with virtio-net in guest kernel for another solution
Ref
why vhost-user
vhost-user realm

]]></content>
      <categories>
        <category>virtualization</category>
        <category>virtio</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
      </tags>
  </entry>
  <entry>
    <title>virtualization-virtio</title>
    <url>/2021/08/18/virtualization-virtio/</url>
    <content><![CDATA[OverviewWhen talking about virtio-networking we can separate the discussion into two layers:

Control plane - Used for capability exchange negotiation between the host and guest both for establishing and terminating the data plane.
Data plane - Used for transferring the actual data (packets) between host and guest.

It’s important to distinguish between these layers since they have different requirements (such as performance) and different implementations
Fundamentally the data plane is required to be as efficient as possible for moving the packets fast while the control plane is required to be as flexible as possible for supporting different devices and vendors in future architectures.


virtio specificationvirtio spec defines virtual device(like a device from vendor)Virtio is an open specification for virtual machines’ data I&#x2F;O communication, offering a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms. It uses the fact that the guest can share memory with the host for I/O to implement that.
The virtio specification is based on two elements: devices and drivers. In a typical implementation, the hypervisor(qemu) exposes the virtio devices to the guest through a number of transport methods. By design they look like physical devices to the guest within the virtual machine.
The most common transport method is PCI or PCIe bus. However, the device can be available at some predefined guest’s memory address (MMIO transport). These devices can be completely virtual with no physical counterpart or physical ones exposing a compatible interface.
The typical (and easiest) way to expose a virtio device is through a PCI port since we can leverage the fact that PCI is a mature and well supported protocol in QEMU and Linux drivers. Real PCI hardware exposes its configuration space using a specific physical memory address range (i.e., the driver can read or write the device’s registers by accessing that memory range) and&#x2F;or special processor instructions. In the VM world, the hypervisor captures accesses to that memory range and performs device emulation, exposing the same memory layout that a real machine would have and offering the same responses. The virtio specification also defines the layout of its PCI Configuration space, so implementing it is straightforward.
When the guest boots and uses the PCI&#x2F;PCIe auto discovering mechanism, the virtio devices identify themselves with with the PCI vendor ID and their PCI Device ID. The guest’s kernel uses these identifiers to know which driver must handle the device. In particular, the linux kernel already includes virtio drivers.
The virtio drivers must be able to allocate memory regions that both the hypervisor and the devices can access for reading and writing, i.e., via memory sharing. We call data plane the part of the data communication that uses these memory regions, and control plane the process of setting them up.
The virtio kernel drivers share a generic transport-specific interface (e.g: virtio-pci), used by the actual transport and device implementation (such as virtio-net, or virtio-scsi).
The virtio network device is a virtual ethernet card, and it supports multiqueue for TX&#x2F;RX. Empty buffers are placed in N virtqueues for receiving packets, and outgoing packets are enqueued into another N virtqueues for transmission. Another virtqueue is used for driver-device communication outside of the data plane, like to control advanced filtering features, settings like the mac address, or the number of active queues. As a physical NIC, the virtio device supports features such as many offloadings, and can let the real host’s device do them.
Virtio specification: virtqueues
Virtqueues are the mechanism for bulk data transport on virtio devices. Each device can have zero or more virtqueues (link). It consists of a queue of guest-allocated buffers that the host interacts with either by reading them or by writing to them. In addition, the virtio specification also defines bi-directional notifications:

Available Buffer Notification: Used by the driver to signal there are buffers that are ready to be processed by the device
Used Buffer Notification: Used by the device to signal that it has finished processing some buffers.

In the PCI case, the guest sends the available buffer notification by writing to a specific memory address, and the device (in this case, QEMU) uses a vCPU interrupt to send the used buffer notification.
The virtio specification also allows the notifications to be enabled or disabled dynamically. That way, devices and drivers can batch buffer notifications or even actively poll for new buffers in virtqueues (busy polling). This approach is better suited for high traffic rates.
In summary, the virtio driver interface exposes:  

Device’s feature bits (which device and guest have to negotiate)
Status bits
Configuration space (that contains device specific information, like MAC address)
Notification system (configuration changed, buffer available, buffer used)
Zero or more virtqueues
Transport specific interface to the device

virtio in Qemu(virtio backend(dataplane) implement in Qemu)Implement data plan in Qemu is the first(early) way, easy to understand, but low performance, not used in production env at all.As backend in qemu, so no vhost protocol is used.

Sending data diagram  

vhostvhost protocol enables us to implement a data plane going directly from the kernel (host) to the guest bypassing the qemu process. 
The vhost protocol itself only describes how to establish the data plane, however. Whoever implements it is also expected to implement the ring layout for describing the data buffers (both host and guest) and the actual send&#x2F;receive packets.  
The vhost API is a message based protocol that allows the hypervisor(it says qemu) to offload the data plane to another component (handler) that performs data forwarding more efficiently. Using this protocol, the master sends the following configuration information to the handler:

The hypervisor’s memory layout. This way, the handler can locate the virtqueues and buffer within the hypervisor’s memory space.
A pair of file descriptors that are used for the handler to send and receive the notifications defined in the virtio spec. These file descriptors are shared between the handler and KVM so they can communicate directly without requiring the hypervisor’s intervention. Note that this notifications can still be dynamically disabled per virtqueue.

After this process, the hypervisor(here it says qemu) will no longer process packets (read or write to&#x2F;from the virtqueues). Instead, the dataplane will be completely offloaded to the handler, which can now access the virtqueues’ memory region directly as well as send and receive notifications directly to and from the guest.
The vhost messages can be exchanged in any host-local transport protocol, such as Unix sockets or character devices and the hypervisor can act as a server or as a client (in the context of the communication channel). The hypervisor is the leader of the protocol, the offloading device is a handler and any of them can send messages.
The handler can be in kernel (vhost-net) or in user application(vhost-user)
vDPAVirtio Backend(data plane) in hardware
Virtio data path acceleration (vDPA) in essence is an approach to standardize the NIC SRIOV data plane using the virtio ring layout and placing a single standard virtio driver in the guest decoupled from any vendor implementation, while adding a generic control plane and SW infrastructure to support it.
with vDPA, Data plane goes directly from the NIC to the guest using the virtio ring layout(need IOMMU support from CPU). However each NIC vendor can now continue using its own driver (with a small vDPA add-on) and a generic vDPA driver is added to the kernel to translate the vendor NIC driver&#x2F;control-plane to the virtio control plane.
A “vDPA device” means a type of device whose datapath complies with the virtio specification, but whose control path is vendor specific. like smartNIC(Mellox Bluefield). virtio dataplane implemented in hardware.
vDPA devices can be both physically located on the hardware or emulated by software

REF
vhost-net from redhat
deep dive vhost-net 
how vhost-net works
vhost-user details
vhost-user introduction
vhost-net inside

]]></content>
      <categories>
        <category>virtualization</category>
        <category>virtio</category>
      </categories>
      <tags>
        <tag>virtualization</tag>
        <tag>virtio</tag>
      </tags>
  </entry>
  <entry>
    <title>yum-rpm-pkg</title>
    <url>/2021/07/14/yum-rpm-pkg/</url>
    <content><![CDATA[IntroductionYUM is the primary package management tool for installing, updating, removing, and managing software packages in Red Hat Enterprise Linux. YUM performs dependency resolution when installing, updating, and removing software packages.
RPM is a popular package management tool in Red Hat Enterprise Linux-based distros. Using RPM, you can install, uninstall, and query individual software packages. Still, it cannot manage dependency resolution like YUM. RPM does provide you useful output, including a list of required packages. An RPM package consists of an archive of files and metadata. Metadata includes helper scripts, file attributes, and information about packages.
YumYum is much more useful than rpm as it resolves dependencies of the package and installs them all!, First yum download meta data of packages from enalbed repo, rpm is downloaded only when it’s installing. any another command like search, list only check the meta data.
Useful commandconf

&#x2F;etc&#x2F;yum.conf: global conf
&#x2F;etc&#x2F;yum.repos.d&#x2F;: each repos

# show all repos, disabled and enabled$ yum repolist all # yum cache$ yum clean all$ yum makecache# show packages from local cache, no auto cache update$ yum list                   # installed and not$ yum list libvirt           # installed and not of given package$ yum list installed         # installed only$ yum list installed libvirt # installed only of given package# shell-style glob$ yum list mysql*    # installed and not pkg with prefix mysql$ yum install yum-utils # can trigger cache update# show latest version of ipvsadm(will trigger cache update) of repo$ repoquery ipvsadm# show files of given pkg from local cache and will trigger cache update# bash can be installed or not(list files of given packages)$ repoquery --list bash/etc/skel/.bash_logout/etc/skel/.bash_profile/etc/skel/.bashrc/usr/bin/alias/usr/bin/bash/usr/bin/bashbug...mysql++.x86_64  3.1.0-12.el7    epel  # not install available from epel repoipvsadm.x86_64  1.27-8.el7      @base # installed from base repo: @ indicate installed# check all packages contains ipvs in its name(package name)$ yum search ipvs # wildcard searching$ yum info ipvsadm        # show basic info of a package installed or not, no auto cache update$ yum provides /etc/fstab # show which package(installed or not) has such file from local cache, no auto cache update$ yum provides rm         # show which package(installed or not) has rm binary from local cache, no auto cache update# Install, will trigger cache update$ yum install -y ipvsadm$ yum remove ipvsadm$ yum grouplist$ yum groupinfo &quot;Development tools&quot; # show what packages in this group$ yum groupinstall -y &quot;Development Tools&quot; # install group of packages$ yum groupremove######################## Download file of given rpm #########################$ yumdownloader ipvsadm                                 # save at current dir(no matter it&#x27;s installed or not)$ yum install --downloadonly ipvsadm                    # saved at /var/cache/yum/x86_64/$ yum install --downloadonly --downloaddir=/tmp ipvsadm# if packet is already installed$ yum reinstall --downloadonly --downloaddir=/tmp ipvsadm$ yumdownloader --source ipvsadm                        # download ipvsadm-xxx-src.rpm######################## Download file of given rpm #########################$ yum history # show what operations happened$ yum history undo &lt;id&gt; # undo an operation#####################upgrade and downgrade##################################### list all possible version of given packet from local cache$ yum provides ipvsadm# check current installed version$ rpm -qa | grep ipvsadm# upgrade to latest or specific version# NOTE for upgrade, package must be installed before!!!$ yum upgrade libvirtPackage(s) libvirt available, but not installed.# upgrade does not run daemon-reload and service restart!!!$ yum upgrade -y ipvsadm # can trigger local cache update$ yum upgrade -y ipvsadm-1.27-7.el7.x86_64 # can trigger local cache update# yum upgrade return 0 (OK) !!!# if packet does not exist # or upgrade to specific version or latest$ yum upgrade -y no-exist-packageLoaded plugins: fastestmirror, langpacks, prioritiesLoading mirror speeds from cached hostfile47 packages excluded due to repository priority protectionsNo Match for argument: no-exist-packageNo package no-exist-package available.No packages marked for update$ echo $?# downgrade to previous version or specific version# NOTE for upgrade, package must be installed before!!!$ yum downgrade ipvsadm$ yum downgrade ipvsadm-1.27-7.el7.x86_64#####################upgrade and downgrade####################################
RPMName Format of RPM name-version-release.os.arch., example: bash-4.3.2-5.el6.x86_64.rpm

name: package name
version: package version
release: identifier of RPM itself, not related to package
os: operation system
arch：i686、x86_64、amd64、ppc（power-pc）、noarch

RPM repo

http://mirrors.sohu.com
http://rpmfind.net
http://rpm.pbone.net/

epel: extended packages provided by RedHat, Maintained by Fedora.
Useful command for rpm# Install$ rpm -ivh xx.rpm$ rpm -ivh https://xxx.rpm# rebuild xx.rpm from source, then install it$ rpm -rebuild *.src.rpm$ rpm -ivh /usr/src/dist/RPMS/xx.rpm# Query$ rpm -qa                   # check all installed packages$ rpm -q ipvsadm            # check if ipvsadm installed or not$ rpm -qi ipvsadm           # check basic info of this installed rpm# show files of given pkg, NOTE this requires ipvsadm must be installed!!!$ rpm -qlv ipvsadm          # check all installed files of given installed rpm# show files of given local rpm$ rpm -qlp ./ipvsadm.x86_64 0:1.27-8.el7# preinstall scriptlet – this will run before a package is installed# postinstall scriptlet – this will run after a package is installed# preuninstall scriptlet – this will run before a package is uninstalled# postuninstall scriptlet – this will run after a package is uninstalled# check scripts of the installed rpm, post/pre etc$ rpm -q --scripts libvirtpreinstall scriptlet (using /bin/sh):# &#x27;libvirt&#x27; group is just to allow password-less polkit access to# libvirtd. The uid number is irrelevant, so we use dynamic allocation# described at the above link.getent group libvirt &gt;/dev/null || groupadd -r libvirtexit 0postinstall scriptlet (using /bin/sh):        if [ $1 -eq 1 ] ; then         # Initial installation         systemctl preset virtlockd.socket virtlogd.socket libvirtd.service &gt;/dev/null 2&gt;&amp;1 || : fipreuninstall scriptlet (using /bin/sh):        if [ $1 -eq 0 ] ; then         # Package removal, not upgrade         systemctl --no-reload disable libvirtd.service virtlogd.socket virtlogd.service virtlockd.socket virtlockd.service &gt; /dev/null 2&gt;&amp;1 || :         systemctl stop libvirtd.service virtlogd.socket virtlogd.service virtlockd.socket virtlockd.service &gt; /dev/null 2&gt;&amp;1 || : fipostuninstall scriptlet (using /bin/sh):/bin/systemctl daemon-reload &gt;/dev/null 2&gt;&amp;1 || :if [ $1 -ge 1 ] ; then    /bin/systemctl reload-or-try-restart virtlockd.service &gt;/dev/null 2&gt;&amp;1 || :    /bin/systemctl reload-or-try-restart virtlogd.service &gt;/dev/null 2&gt;&amp;1 || :    /bin/systemctl try-restart libvirtd.service &gt;/dev/null 2&gt;&amp;1 || :fi# In upgrade scenario we must explicitly enable virtlockd/virtlogd# sockets, if libvirtd is already enabled and start them if# libvirtd is running, otherwise you&#x27;ll get failures to start# guests$ rpm -qf /usr/sbin/ipvsadm # check which rpm this file belongs to# Uninstall$ rpm -e ipvsadm# Extract rpm files$ rpm2cpio ./packagecloud-test-1.1-1.x86_64.rpm | cpio -idmv############################ For uninstalled package ############################################################ -p, --package PACKAGE_FILE#    Query  an (uninstalled) package PACKAGE_FILE.  The PACKAGE_FILE may be specified as an ftp or http style URL############################ For uninstalled package ############################################################ show files of given rpm$ rpm -qlp ./ipvsadm.x86_64 0:1.27-8.el7# Rebuild rpm from installed files then deploy it with rebuild rpm# edit some conf files then rebuild it$ rpmrebuild ipvsadm# with editing ipvsadm.spec prompt, then rebuild$ rpmrebuild -e ipvsadm

rpm scriptlets

When scriptlets are called, they will be supplied with an argument. This argument, accessed via $1 (for shell scripts) is the number of packages of this name which will be left on the system when the action completes. So for the common case of install, upgrade, and uninstall we have:












install
upgrade
uninstall


%pretrans
$1 &#x3D;&#x3D; 1
$1 &#x3D;&#x3D; 2
(N&#x2F;A)


%pre
$1 &#x3D;&#x3D; 1
$1 &#x3D;&#x3D; 2
(N&#x2F;A)


%post
$1 &#x3D;&#x3D; 1
$1 &#x3D;&#x3D; 2
(N&#x2F;A)


%preun
(N&#x2F;A)
$1 &#x3D;&#x3D; 1
$1 &#x3D;&#x3D; 0


%postun
(N&#x2F;A)
$1 &#x3D;&#x3D; 1
$1 &#x3D;&#x3D; 0


%posttrans
$1 &#x3D;&#x3D; 1
$1 &#x3D;&#x3D; 1
(N&#x2F;A)


Note: even some scripts in rpm(built-in, no such files after installation) come from .spec, there is NO spec file in xxx.rpm but xxx.src.rpm.
Build rpmRefer to RPM guideline
FAQ?does service auto start after rpm -i xx.rpm?It depends on the xx.rpm, as RPM allows to add scripts, hence you can do this there, but most of time, service is not started automatically after installation.
show all services with systemctl# show all active systemd service$ systemctl --type=service# show all active/inactive systemd service$ systemctl --type=service --all# enable means start at boot time$ systemctl enable htg

yum upgrade vs yum install
yum upgrade needs the package installed before
yum upgrade does NOT touch conf file, it remains what changed, actually, this depends on rpm.spec file
with noreplace, if it exists, no replace



$ cat libvirtd.spec...%config(noreplace) %&#123;_sysconfdir&#125;/sysconfig/libvirtd                            %config(noreplace) %&#123;_sysconfdir&#125;/sysconfig/virtlogd                            %config(noreplace) %&#123;_sysconfdir&#125;/sysconfig/virtlockd                           %config(noreplace) %&#123;_sysconfdir&#125;/libvirt/libvirtd.conf                         %config(noreplace) %&#123;_sysconfdir&#125;/libvirt/virtlogd.conf                         %config(noreplace) %&#123;_sysconfdir&#125;/libvirt/virtlockd.conf                        %config(noreplace) %&#123;_prefix&#125;/lib/sysctl.d/60-libvirtd.conf                                                                                                     %config(noreplace) %&#123;_sysconfdir&#125;/logrotate.d/libvirtd                          


yum install copy conf rom rpm and backup existing conf suffixed with xx.rpmsave(like libivrtd.conf.rpmsave)

list rpm installation date$rpm -qa --lastlibreport-filesystem-2.1.11-38.el7.centos.x86_64 Wed 11 Sep 2019 05:22:55 PM CSTgrub2-common-2.02-0.64.el7.centos.noarch      Wed 11 Sep 2019 05:22:55 PM CSTfilesystem-3.2-21.el7.x86_64                  Wed 11 Sep 2019 05:22:55 PM CSTcentos-release-7-4.1708.el7.centos.x86_64     Wed 11 Sep 2019 05:22:55 PM CSTlibgcc-4.8.5-16.el7.x86_64                    Wed 11 Sep 2019 05:22:54 PM CSTmicrocode_ctl-2.1-22.2.el7.centos.x86_64      Wed 11 Sep 2019 09:33:20 AM CST

prevent rpmbuild to strip symbolYou can add %global __os_install_post %&#123;nil&#125; at the top line of xx.spec file.
remove multiple package with wildcard# remove dependency as well$ yum remove &#x27;libvirt*&#x27;# only remove package, their dependency is kept$rpm -e $(rpm -qa &#x27;libvirt*&#x27;)

rpm command hangsrpm may be waiting for a lock to be freed.
$ ps -ef | grep rpm $ kill -9 $output_above$/bin/rm -rf /var/lib/rpm/__db.0*# Then try your rpm command again. If it still doesn&#x27;t work, repeat as above# but then also run rpm --rebuilddb before trying your rpm command again.
]]></content>
      <categories>
        <category>pkg</category>
        <category>rpm</category>
      </categories>
      <tags>
        <tag>yum</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
</search>
